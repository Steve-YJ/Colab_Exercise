{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01.Tutorial-ResNet-VAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOCn1JEt5ivQtgwwdeW/Thj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Steve-YJ/Colab_Exercise/blob/master/01_Tutorial_ResNet_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF2mQr8fdBJE",
        "colab_type": "text"
      },
      "source": [
        "# README.MD\n",
        "\n",
        "* Post-InfoSec-Exp\n",
        "* 01. Tutorial-ResNet-VAE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcIG0ad7a63U",
        "colab_type": "text"
      },
      "source": [
        "* mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vbYVvoNa35z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "29b622c3-9601-4cd5-a6c5-6deafab69610"
      },
      "source": [
        "# drive mount\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2t5puLibOGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lGGE4xtb59M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "2f4a3cc6-df64-4b45-ff28-5f811f711efc"
      },
      "source": [
        "%cd drive/My\\ Drive/Post_InfoSec_Exps/ResNet-VAE/ResNetVAE-master.zip (Unzipped Files)/ResNetVAE-master\n",
        "! ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Post_InfoSec_Exps/ResNet-VAE/ResNetVAE-master.zip (Unzipped Files)/ResNetVAE-master\n",
            "01.Tutorial-ResNet-VAE.ipynb  ResNetVAE_cifar10.py\n",
            "fig\t\t\t      ResNetVAE_FACE.py\n",
            "modules.py\t\t      ResNetVAE_MNIST.py\n",
            "plot_latent.ipynb\t      ResNetVAE_reconstruction.ipynb\n",
            "__pycache__\t\t      results_Malimg\n",
            "README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4MQCs0Vdspm",
        "colab_type": "text"
      },
      "source": [
        "## 01. Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxJh0s6bbhTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "680c210a-2286-4bf8-f0fb-65b08e3a66a4"
      },
      "source": [
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        " \n",
        "# https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n",
        "import seaborn as sns\n",
        "\n",
        "# save single numpy array\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.save.html#numpy.save\n",
        "from tempfile import TemporaryFile\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# load modules\n",
        "from torchvision import models\n",
        "from modules import *\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG79j8Wjhjd4",
        "colab_type": "text"
      },
      "source": [
        "## 02. Data Preparation\n",
        "* Make Custom Dataset\n",
        "* Make Custom DataLoader\n",
        "* Train_Test Split\n",
        "\n",
        "### Work Flow\n",
        "* transforms module 사용, Image data compose(전처리, transform)\n",
        "* re-sizing, normalizing, tensor\n",
        "* ImageFolder사용 dataloader\n",
        "* dataset split: train dataset, test dataset\n",
        "* DataLoader: batch단위 dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40wjsfCSgYIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transforms = transforms.Compose([\n",
        "                                transforms.Resize((224, 224)),                # Change size of Image to (224, 224)\n",
        "                                transforms.Grayscale(num_output_channels=3),  # Makes it 1-dimension channel\n",
        "                                # transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # gray -> GRB 3 channel (lambda function)\n",
        "                                                                                 # Reference: https://github.com/hsinyilin19/ResNetVAE/blob/master/ResNetVAE_MNIST.py\n",
        "\n",
        "\n",
        "                                transforms.ToTensor(),                        # Convert a PIL Image or numpy.ndarray to tensor.\n",
        "                                                                              # Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8\n",
        "                                                                              # In the other cases, tensors are returned without scaling.\n",
        "                                # transforms.Normalize(mean=[0.5], std=[0.5]), \n",
        "                                ])\n",
        "\n",
        "# make custom dataset\n",
        "trainset = torchvision.datasets.ImageFolder(root='../../../../InformationSecurity_Summer/malimg',\n",
        "                                            transform=transforms)  # make custom dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYwMgJ5UdvgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "09fee647-7c41-4762-fe22-650afa829357"
      },
      "source": [
        "# classes = trainset.classes\n",
        "classes = trainset.classes\n",
        "classes"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Adialer.C',\n",
              " 'Agent.FYI',\n",
              " 'Allaple.A',\n",
              " 'Allaple.L',\n",
              " 'Alueron.gen!J',\n",
              " 'Autorun.K',\n",
              " 'C2LOP.P',\n",
              " 'C2LOP.gen!g',\n",
              " 'Dialplatform.B',\n",
              " 'Dontovo.A',\n",
              " 'Fakerean',\n",
              " 'Instantaccess',\n",
              " 'Lolyda.AA1',\n",
              " 'Lolyda.AA2',\n",
              " 'Lolyda.AA3',\n",
              " 'Lolyda.AT',\n",
              " 'Malex.gen!J',\n",
              " 'Obfuscator.AD',\n",
              " 'Rbot!gen',\n",
              " 'Skintrim.N',\n",
              " 'Swizzor.gen!E',\n",
              " 'Swizzor.gen!I',\n",
              " 'VB.AT',\n",
              " 'Wintrim.BX',\n",
              " 'Yuner.A']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRNtxJgpdy6x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e1e0640-7674-4074-e768-3c8d58d34461"
      },
      "source": [
        "full_dataset = trainset\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "print(train_size, test_size)\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7471 1868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laqQVjbpkVsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_dataset,\n",
        "                         batch_size=16,\n",
        "                         shuffle=True,\n",
        "                         pin_memory=True) \n",
        "valid_loader = DataLoader(test_dataset,\n",
        "                        batch_size=16,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True)  # Instead, we recommend using automatic memory pinning (i.e., setting pin_memory=True)\n",
        "                                          #  which enables fast data transfer to CUDA-enabled GPUs\n",
        "\n",
        "# First, insert all test dataset\n",
        "# test_loader_10: testloader for latent vector visualization\n",
        "test_loader_10 = DataLoader(test_dataset,\n",
        "                        batch_size=1868,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2tvYihLhDd0",
        "colab_type": "text"
      },
      "source": [
        "## 03. Modeling\n",
        "* save model: <code>save_model_path</code>\n",
        "* save_loss_list\n",
        "* etc...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyiJB8iSiqTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EncoderCNN architecture\n",
        "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 1024\n",
        "CNN_embed_dim = 256     # latent dim extracted by 2D CNN\n",
        "res_size = 224        # ResNet image size\n",
        "dropout_p = 0.2       # dropout probability\n",
        "\n",
        "# training parameters\n",
        "epochs = 20        # training epochs\n",
        "batch_size = 50\n",
        "learning_rate = 1e-3\n",
        "log_interval = 10   # interval for displaying training info"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjFMhGMEiRG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save model\n",
        "save_model_path = './results_Malimg'  # save_model parameter"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK2IkNBgifV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_mkdir(dir_name):\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.mkdir(dir_name)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLrFg4euigpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_mkdir(dir_name):\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.mkdir(dir_name)\n",
        "\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    # MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    MSE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return MSE + KLD"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHryk-eBitM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
        "    # set model as training mode\n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "    all_y, all_z, all_mu, all_logvar = [], [], [], []\n",
        "    N_count = 0   # counting total trained sample in one epoch\n",
        "    for batch_idx, (X, y) in enumerate(train_loader):\n",
        "        # distribute data to device\n",
        "        X, y = X.to(device), y.to(device).view(-1, )\n",
        "        N_count += X.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        X_reconst, z, mu, logvar = model(X)  # VAE\n",
        "        loss = loss_function(X_reconst, X, mu, logvar)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        all_y.extend(y.data.cpu().numpy())\n",
        "        all_z.extend(z.data.cpu().numpy())\n",
        "        all_mu.extend(mu.data.cpu().numpy())\n",
        "        all_logvar.extend(logvar.data.cpu().numpy())\n",
        "\n",
        "        # show information\n",
        "        if (batch_idx + 1) % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "    all_y = np.stack(all_y, axis=0)\n",
        "    all_z = np.stack(all_z, axis=0)\n",
        "    all_mu = np.stack(all_mu, axis=0)\n",
        "    all_logvar = np.stack(all_logvar, axis=0)\n",
        "\n",
        "    # save Pytorch models of best record\n",
        "    torch.save(model.state_dict(), os.path.join(save_model_path, 'model_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
        "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
        "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
        "\n",
        "    return X.data.cpu().numpy(), all_y, all_z, all_mu, all_logvar, losses\n",
        "\n",
        "\n",
        "def validation(model, device, optimizer, test_loader):\n",
        "    # set model as testing mode\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    all_y, all_z, all_mu, all_logvar = [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            # distribute data to device\n",
        "            X, y = X.to(device), y.to(device).view(-1, )\n",
        "            X_reconst, z, mu, logvar = model(X)\n",
        "\n",
        "            loss = loss_function(X_reconst, X, mu, logvar)\n",
        "            test_loss += loss.item()  # sum up batch loss\n",
        "\n",
        "            all_y.extend(y.data.cpu().numpy())\n",
        "            all_z.extend(z.data.cpu().numpy())\n",
        "            all_mu.extend(mu.data.cpu().numpy())\n",
        "            all_logvar.extend(logvar.data.cpu().numpy())\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    all_y = np.stack(all_y, axis=0)\n",
        "    all_z = np.stack(all_z, axis=0)\n",
        "    all_mu = np.stack(all_mu, axis=0)\n",
        "    all_logvar = np.stack(all_logvar, axis=0)\n",
        "\n",
        "    # show information\n",
        "    print('\\nTest set ({:d} samples): Average loss: {:.4f}\\n'.format(len(test_loader.dataset), test_loss))\n",
        "    return X.data.cpu().numpy(), all_y, all_z, all_mu, all_logvar, test_loss\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0uohqjsixS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Detect devices\n",
        "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a9FFdiljGPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8397dfac-5877-4aa5-bb27-4cb321d7d368"
      },
      "source": [
        "# Create model\n",
        "resnet_vae = ResNet_VAE(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
        "\n",
        "print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n",
        "model_params = list(resnet_vae.parameters())\n",
        "optimizer = torch.optim.Adam(model_params, lr=learning_rate)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 1 GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mDezxJUjKSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# record training process\n",
        "'''\n",
        "list_epoch = []\n",
        "list_train_loss = []\n",
        "list_val_loss = []\n",
        "list_acc = []\n",
        "list_acc_epoch = []\n",
        "'''\n",
        "list_epoch=[]\n",
        "epoch_train_losses = []\n",
        "epoch_test_losses = []\n",
        "list_acc = []\n",
        "list_acc_epoch =[]\n",
        "check_mkdir(save_model_path)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcJRmBUEhT5W",
        "colab_type": "text"
      },
      "source": [
        "## 04. Train-it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLFMtJaUhMCN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "241e59cd-f577-4380-f414-531bd683b109"
      },
      "source": [
        "# start training\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # train, test model\n",
        "    X_train, y_train, z_train, mu_train, logvar_train, train_losses = train(log_interval, resnet_vae, device, train_loader, optimizer, epoch)\n",
        "    X_test, y_test, z_test, mu_test, logvar_test, epoch_test_loss = validation(resnet_vae, device, optimizer, valid_loader)\n",
        "\n",
        "    # save results\n",
        "    list_epoch.append(epoch)\n",
        "    epoch_train_losses.append(train_losses)\n",
        "    epoch_test_losses.append(epoch_test_loss)\n",
        "\n",
        "    \n",
        "    # save all train test results\n",
        "    A = np.array(epoch_train_losses)\n",
        "    C = np.array(epoch_test_losses)\n",
        "    \n",
        "    np.save(os.path.join(save_model_path, 'ResNet_VAE_training_loss.npy'), A)\n",
        "    np.save(os.path.join(save_model_path, 'X_Malimg_train_epoch{}.npy'.format(epoch + 1)), X_train) #save last batch\n",
        "    np.save(os.path.join(save_model_path, 'y_Malimg_train_epoch{}.npy'.format(epoch + 1)), y_train)\n",
        "    np.save(os.path.join(save_model_path, 'z_Malimg_train_epoch{}.npy'.format(epoch + 1)), z_train)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [160/7471 (2%)]\tLoss: 1596429.500000\n",
            "Train Epoch: 1 [320/7471 (4%)]\tLoss: 1649055.125000\n",
            "Train Epoch: 1 [480/7471 (6%)]\tLoss: 1649406.125000\n",
            "Train Epoch: 1 [640/7471 (9%)]\tLoss: 1602083.875000\n",
            "Train Epoch: 1 [800/7471 (11%)]\tLoss: 1623616.750000\n",
            "Train Epoch: 1 [960/7471 (13%)]\tLoss: 1607044.875000\n",
            "Train Epoch: 1 [1120/7471 (15%)]\tLoss: 1621048.000000\n",
            "Train Epoch: 1 [1280/7471 (17%)]\tLoss: 1665955.000000\n",
            "Train Epoch: 1 [1440/7471 (19%)]\tLoss: 1628549.000000\n",
            "Train Epoch: 1 [1600/7471 (21%)]\tLoss: 1642092.125000\n",
            "Train Epoch: 1 [1760/7471 (24%)]\tLoss: 1612175.000000\n",
            "Train Epoch: 1 [1920/7471 (26%)]\tLoss: 1595330.500000\n",
            "Train Epoch: 1 [2080/7471 (28%)]\tLoss: 1618138.125000\n",
            "Train Epoch: 1 [2240/7471 (30%)]\tLoss: 1620372.500000\n",
            "Train Epoch: 1 [2400/7471 (32%)]\tLoss: 1648094.125000\n",
            "Train Epoch: 1 [2560/7471 (34%)]\tLoss: 1602843.250000\n",
            "Train Epoch: 1 [2720/7471 (36%)]\tLoss: 1594325.750000\n",
            "Train Epoch: 1 [2880/7471 (39%)]\tLoss: 1614280.125000\n",
            "Train Epoch: 1 [3040/7471 (41%)]\tLoss: 1629147.500000\n",
            "Train Epoch: 1 [3200/7471 (43%)]\tLoss: 1635868.875000\n",
            "Train Epoch: 1 [3360/7471 (45%)]\tLoss: 1623020.625000\n",
            "Train Epoch: 1 [3520/7471 (47%)]\tLoss: 1613633.625000\n",
            "Train Epoch: 1 [3680/7471 (49%)]\tLoss: 1624938.375000\n",
            "Train Epoch: 1 [3840/7471 (51%)]\tLoss: 1642697.375000\n",
            "Train Epoch: 1 [4000/7471 (54%)]\tLoss: 1639189.250000\n",
            "Train Epoch: 1 [4160/7471 (56%)]\tLoss: 1648208.625000\n",
            "Train Epoch: 1 [4320/7471 (58%)]\tLoss: 1605684.500000\n",
            "Train Epoch: 1 [4480/7471 (60%)]\tLoss: 1630828.750000\n",
            "Train Epoch: 1 [4640/7471 (62%)]\tLoss: 1608537.000000\n",
            "Train Epoch: 1 [4800/7471 (64%)]\tLoss: 1596866.750000\n",
            "Train Epoch: 1 [4960/7471 (66%)]\tLoss: 1617981.125000\n",
            "Train Epoch: 1 [5120/7471 (69%)]\tLoss: 1628498.375000\n",
            "Train Epoch: 1 [5280/7471 (71%)]\tLoss: 1615914.125000\n",
            "Train Epoch: 1 [5440/7471 (73%)]\tLoss: 1631085.375000\n",
            "Train Epoch: 1 [5600/7471 (75%)]\tLoss: 1606950.750000\n",
            "Train Epoch: 1 [5760/7471 (77%)]\tLoss: 1614157.375000\n",
            "Train Epoch: 1 [5920/7471 (79%)]\tLoss: 1664421.750000\n",
            "Train Epoch: 1 [6080/7471 (81%)]\tLoss: 1649915.750000\n",
            "Train Epoch: 1 [6240/7471 (84%)]\tLoss: 1599575.375000\n",
            "Train Epoch: 1 [6400/7471 (86%)]\tLoss: 1650585.750000\n",
            "Train Epoch: 1 [6560/7471 (88%)]\tLoss: 1632331.875000\n",
            "Train Epoch: 1 [6720/7471 (90%)]\tLoss: 1625672.000000\n",
            "Train Epoch: 1 [6880/7471 (92%)]\tLoss: 1651350.125000\n",
            "Train Epoch: 1 [7040/7471 (94%)]\tLoss: 1635847.875000\n",
            "Train Epoch: 1 [7200/7471 (96%)]\tLoss: 1578888.625000\n",
            "Train Epoch: 1 [7360/7471 (99%)]\tLoss: 1627274.375000\n",
            "Epoch 1 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 2 [160/7471 (2%)]\tLoss: 1648632.250000\n",
            "Train Epoch: 2 [320/7471 (4%)]\tLoss: 1586888.375000\n",
            "Train Epoch: 2 [480/7471 (6%)]\tLoss: 1622510.375000\n",
            "Train Epoch: 2 [640/7471 (9%)]\tLoss: 1618148.625000\n",
            "Train Epoch: 2 [800/7471 (11%)]\tLoss: 1638383.000000\n",
            "Train Epoch: 2 [960/7471 (13%)]\tLoss: 1620866.750000\n",
            "Train Epoch: 2 [1120/7471 (15%)]\tLoss: 1613314.875000\n",
            "Train Epoch: 2 [1280/7471 (17%)]\tLoss: 1615497.000000\n",
            "Train Epoch: 2 [1440/7471 (19%)]\tLoss: 1634759.000000\n",
            "Train Epoch: 2 [1600/7471 (21%)]\tLoss: 1642692.375000\n",
            "Train Epoch: 2 [1760/7471 (24%)]\tLoss: 1617013.000000\n",
            "Train Epoch: 2 [1920/7471 (26%)]\tLoss: 1622665.875000\n",
            "Train Epoch: 2 [2080/7471 (28%)]\tLoss: 1606940.250000\n",
            "Train Epoch: 2 [2240/7471 (30%)]\tLoss: 1608081.750000\n",
            "Train Epoch: 2 [2400/7471 (32%)]\tLoss: 1631437.000000\n",
            "Train Epoch: 2 [2560/7471 (34%)]\tLoss: 1611300.000000\n",
            "Train Epoch: 2 [2720/7471 (36%)]\tLoss: 1572931.250000\n",
            "Train Epoch: 2 [2880/7471 (39%)]\tLoss: 1625547.125000\n",
            "Train Epoch: 2 [3040/7471 (41%)]\tLoss: 1619731.000000\n",
            "Train Epoch: 2 [3200/7471 (43%)]\tLoss: 1629722.250000\n",
            "Train Epoch: 2 [3360/7471 (45%)]\tLoss: 1609231.500000\n",
            "Train Epoch: 2 [3520/7471 (47%)]\tLoss: 1598368.250000\n",
            "Train Epoch: 2 [3680/7471 (49%)]\tLoss: 1620404.000000\n",
            "Train Epoch: 2 [3840/7471 (51%)]\tLoss: 1624677.125000\n",
            "Train Epoch: 2 [4000/7471 (54%)]\tLoss: 1596048.250000\n",
            "Train Epoch: 2 [4160/7471 (56%)]\tLoss: 1645666.375000\n",
            "Train Epoch: 2 [4320/7471 (58%)]\tLoss: 1584298.250000\n",
            "Train Epoch: 2 [4480/7471 (60%)]\tLoss: 1651086.875000\n",
            "Train Epoch: 2 [4640/7471 (62%)]\tLoss: 1626088.375000\n",
            "Train Epoch: 2 [4800/7471 (64%)]\tLoss: 1627447.375000\n",
            "Train Epoch: 2 [4960/7471 (66%)]\tLoss: 1594611.750000\n",
            "Train Epoch: 2 [5120/7471 (69%)]\tLoss: 1602740.250000\n",
            "Train Epoch: 2 [5280/7471 (71%)]\tLoss: 1610570.500000\n",
            "Train Epoch: 2 [5440/7471 (73%)]\tLoss: 1625571.875000\n",
            "Train Epoch: 2 [5600/7471 (75%)]\tLoss: 1582001.125000\n",
            "Train Epoch: 2 [5760/7471 (77%)]\tLoss: 1556140.500000\n",
            "Train Epoch: 2 [5920/7471 (79%)]\tLoss: 1620187.250000\n",
            "Train Epoch: 2 [6080/7471 (81%)]\tLoss: 1568685.375000\n",
            "Train Epoch: 2 [6240/7471 (84%)]\tLoss: 1622283.500000\n",
            "Train Epoch: 2 [6400/7471 (86%)]\tLoss: 1642225.625000\n",
            "Train Epoch: 2 [6560/7471 (88%)]\tLoss: 1610197.375000\n",
            "Train Epoch: 2 [6720/7471 (90%)]\tLoss: 1650049.625000\n",
            "Train Epoch: 2 [6880/7471 (92%)]\tLoss: 1630473.875000\n",
            "Train Epoch: 2 [7040/7471 (94%)]\tLoss: 1655268.500000\n",
            "Train Epoch: 2 [7200/7471 (96%)]\tLoss: 1634641.375000\n",
            "Train Epoch: 2 [7360/7471 (99%)]\tLoss: 1651799.375000\n",
            "Epoch 2 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 101095.9594\n",
            "\n",
            "Train Epoch: 3 [160/7471 (2%)]\tLoss: 1599272.000000\n",
            "Train Epoch: 3 [320/7471 (4%)]\tLoss: 1650365.625000\n",
            "Train Epoch: 3 [480/7471 (6%)]\tLoss: 1659876.500000\n",
            "Train Epoch: 3 [640/7471 (9%)]\tLoss: 1614875.125000\n",
            "Train Epoch: 3 [800/7471 (11%)]\tLoss: 1658782.500000\n",
            "Train Epoch: 3 [960/7471 (13%)]\tLoss: 1594374.000000\n",
            "Train Epoch: 3 [1120/7471 (15%)]\tLoss: 1635820.125000\n",
            "Train Epoch: 3 [1280/7471 (17%)]\tLoss: 1582481.125000\n",
            "Train Epoch: 3 [1440/7471 (19%)]\tLoss: 1609384.250000\n",
            "Train Epoch: 3 [1600/7471 (21%)]\tLoss: 1576220.000000\n",
            "Train Epoch: 3 [1760/7471 (24%)]\tLoss: 1587509.625000\n",
            "Train Epoch: 3 [1920/7471 (26%)]\tLoss: 1650984.250000\n",
            "Train Epoch: 3 [2080/7471 (28%)]\tLoss: 1632409.375000\n",
            "Train Epoch: 3 [2240/7471 (30%)]\tLoss: 1656535.000000\n",
            "Train Epoch: 3 [2400/7471 (32%)]\tLoss: 1633562.500000\n",
            "Train Epoch: 3 [2560/7471 (34%)]\tLoss: 1597606.125000\n",
            "Train Epoch: 3 [2720/7471 (36%)]\tLoss: 1618616.875000\n",
            "Train Epoch: 3 [2880/7471 (39%)]\tLoss: 1646762.750000\n",
            "Train Epoch: 3 [3040/7471 (41%)]\tLoss: 1629976.875000\n",
            "Train Epoch: 3 [3200/7471 (43%)]\tLoss: 1657548.125000\n",
            "Train Epoch: 3 [3360/7471 (45%)]\tLoss: 1617879.750000\n",
            "Train Epoch: 3 [3520/7471 (47%)]\tLoss: 1622798.000000\n",
            "Train Epoch: 3 [3680/7471 (49%)]\tLoss: 1606344.125000\n",
            "Train Epoch: 3 [3840/7471 (51%)]\tLoss: 1606315.500000\n",
            "Train Epoch: 3 [4000/7471 (54%)]\tLoss: 1624085.000000\n",
            "Train Epoch: 3 [4160/7471 (56%)]\tLoss: 1612946.000000\n",
            "Train Epoch: 3 [4320/7471 (58%)]\tLoss: 1643393.750000\n",
            "Train Epoch: 3 [4480/7471 (60%)]\tLoss: 1617495.375000\n",
            "Train Epoch: 3 [4640/7471 (62%)]\tLoss: 1587324.375000\n",
            "Train Epoch: 3 [4800/7471 (64%)]\tLoss: 1625343.250000\n",
            "Train Epoch: 3 [4960/7471 (66%)]\tLoss: 1589618.750000\n",
            "Train Epoch: 3 [5120/7471 (69%)]\tLoss: 1608438.500000\n",
            "Train Epoch: 3 [5280/7471 (71%)]\tLoss: 1587626.750000\n",
            "Train Epoch: 3 [5440/7471 (73%)]\tLoss: 1604226.500000\n",
            "Train Epoch: 3 [5600/7471 (75%)]\tLoss: 1566398.375000\n",
            "Train Epoch: 3 [5760/7471 (77%)]\tLoss: 1600434.875000\n",
            "Train Epoch: 3 [5920/7471 (79%)]\tLoss: 1585899.750000\n",
            "Train Epoch: 3 [6080/7471 (81%)]\tLoss: 1613009.875000\n",
            "Train Epoch: 3 [6240/7471 (84%)]\tLoss: 1595132.875000\n",
            "Train Epoch: 3 [6400/7471 (86%)]\tLoss: 1592422.875000\n",
            "Train Epoch: 3 [6560/7471 (88%)]\tLoss: 1578202.875000\n",
            "Train Epoch: 3 [6720/7471 (90%)]\tLoss: 1631909.000000\n",
            "Train Epoch: 3 [6880/7471 (92%)]\tLoss: 1563858.875000\n",
            "Train Epoch: 3 [7040/7471 (94%)]\tLoss: 1583984.125000\n",
            "Train Epoch: 3 [7200/7471 (96%)]\tLoss: 1615643.875000\n",
            "Train Epoch: 3 [7360/7471 (99%)]\tLoss: 1617683.125000\n",
            "Epoch 3 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 100494.9122\n",
            "\n",
            "Train Epoch: 4 [160/7471 (2%)]\tLoss: 1590436.000000\n",
            "Train Epoch: 4 [320/7471 (4%)]\tLoss: 1640258.500000\n",
            "Train Epoch: 4 [480/7471 (6%)]\tLoss: 1580536.500000\n",
            "Train Epoch: 4 [640/7471 (9%)]\tLoss: 1590270.500000\n",
            "Train Epoch: 4 [800/7471 (11%)]\tLoss: 1638960.000000\n",
            "Train Epoch: 4 [960/7471 (13%)]\tLoss: 1631351.375000\n",
            "Train Epoch: 4 [1120/7471 (15%)]\tLoss: 1656095.625000\n",
            "Train Epoch: 4 [1280/7471 (17%)]\tLoss: 1628335.500000\n",
            "Train Epoch: 4 [1440/7471 (19%)]\tLoss: 1626964.625000\n",
            "Train Epoch: 4 [1600/7471 (21%)]\tLoss: 1587913.625000\n",
            "Train Epoch: 4 [1760/7471 (24%)]\tLoss: 1610723.000000\n",
            "Train Epoch: 4 [1920/7471 (26%)]\tLoss: 1620390.625000\n",
            "Train Epoch: 4 [2080/7471 (28%)]\tLoss: 1626063.000000\n",
            "Train Epoch: 4 [2240/7471 (30%)]\tLoss: 1623232.750000\n",
            "Train Epoch: 4 [2400/7471 (32%)]\tLoss: 1606267.875000\n",
            "Train Epoch: 4 [2560/7471 (34%)]\tLoss: 1604715.375000\n",
            "Train Epoch: 4 [2720/7471 (36%)]\tLoss: 1562257.000000\n",
            "Train Epoch: 4 [2880/7471 (39%)]\tLoss: 1604512.000000\n",
            "Train Epoch: 4 [3040/7471 (41%)]\tLoss: 1627935.000000\n",
            "Train Epoch: 4 [3200/7471 (43%)]\tLoss: 1649041.625000\n",
            "Train Epoch: 4 [3360/7471 (45%)]\tLoss: 1574358.750000\n",
            "Train Epoch: 4 [3520/7471 (47%)]\tLoss: 1609982.750000\n",
            "Train Epoch: 4 [3680/7471 (49%)]\tLoss: 1626731.625000\n",
            "Train Epoch: 4 [3840/7471 (51%)]\tLoss: 1570930.750000\n",
            "Train Epoch: 4 [4000/7471 (54%)]\tLoss: 1609849.500000\n",
            "Train Epoch: 4 [4160/7471 (56%)]\tLoss: 1567175.125000\n",
            "Train Epoch: 4 [4320/7471 (58%)]\tLoss: 1620257.500000\n",
            "Train Epoch: 4 [4480/7471 (60%)]\tLoss: 1601743.125000\n",
            "Train Epoch: 4 [4640/7471 (62%)]\tLoss: 1556282.625000\n",
            "Train Epoch: 4 [4800/7471 (64%)]\tLoss: 1590958.000000\n",
            "Train Epoch: 4 [4960/7471 (66%)]\tLoss: 1589895.000000\n",
            "Train Epoch: 4 [5120/7471 (69%)]\tLoss: 1623498.375000\n",
            "Train Epoch: 4 [5280/7471 (71%)]\tLoss: 1615268.000000\n",
            "Train Epoch: 4 [5440/7471 (73%)]\tLoss: 1567787.750000\n",
            "Train Epoch: 4 [5600/7471 (75%)]\tLoss: 1650805.000000\n",
            "Train Epoch: 4 [5760/7471 (77%)]\tLoss: 1643758.500000\n",
            "Train Epoch: 4 [5920/7471 (79%)]\tLoss: 1617334.375000\n",
            "Train Epoch: 4 [6080/7471 (81%)]\tLoss: 1596026.375000\n",
            "Train Epoch: 4 [6240/7471 (84%)]\tLoss: 1608756.500000\n",
            "Train Epoch: 4 [6400/7471 (86%)]\tLoss: 1552530.750000\n",
            "Train Epoch: 4 [6560/7471 (88%)]\tLoss: 1610846.875000\n",
            "Train Epoch: 4 [6720/7471 (90%)]\tLoss: 1572631.250000\n",
            "Train Epoch: 4 [6880/7471 (92%)]\tLoss: 1588986.750000\n",
            "Train Epoch: 4 [7040/7471 (94%)]\tLoss: 1572313.375000\n",
            "Train Epoch: 4 [7200/7471 (96%)]\tLoss: 1601580.250000\n",
            "Train Epoch: 4 [7360/7471 (99%)]\tLoss: 1609219.500000\n",
            "Epoch 4 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 101017.2056\n",
            "\n",
            "Train Epoch: 5 [160/7471 (2%)]\tLoss: 1633656.000000\n",
            "Train Epoch: 5 [320/7471 (4%)]\tLoss: 1582655.625000\n",
            "Train Epoch: 5 [480/7471 (6%)]\tLoss: 1604070.375000\n",
            "Train Epoch: 5 [640/7471 (9%)]\tLoss: 1624715.250000\n",
            "Train Epoch: 5 [800/7471 (11%)]\tLoss: 1619520.750000\n",
            "Train Epoch: 5 [960/7471 (13%)]\tLoss: 1635216.500000\n",
            "Train Epoch: 5 [1120/7471 (15%)]\tLoss: 1628705.000000\n",
            "Train Epoch: 5 [1280/7471 (17%)]\tLoss: 1638415.250000\n",
            "Train Epoch: 5 [1440/7471 (19%)]\tLoss: 1636706.750000\n",
            "Train Epoch: 5 [1600/7471 (21%)]\tLoss: 1609349.625000\n",
            "Train Epoch: 5 [1760/7471 (24%)]\tLoss: 1594501.250000\n",
            "Train Epoch: 5 [1920/7471 (26%)]\tLoss: 1607388.000000\n",
            "Train Epoch: 5 [2080/7471 (28%)]\tLoss: 1599631.125000\n",
            "Train Epoch: 5 [2240/7471 (30%)]\tLoss: 1607295.750000\n",
            "Train Epoch: 5 [2400/7471 (32%)]\tLoss: 1551347.000000\n",
            "Train Epoch: 5 [2560/7471 (34%)]\tLoss: 1607210.375000\n",
            "Train Epoch: 5 [2720/7471 (36%)]\tLoss: 1554657.875000\n",
            "Train Epoch: 5 [2880/7471 (39%)]\tLoss: 1612800.125000\n",
            "Train Epoch: 5 [3040/7471 (41%)]\tLoss: 1579537.750000\n",
            "Train Epoch: 5 [3200/7471 (43%)]\tLoss: 1577530.375000\n",
            "Train Epoch: 5 [3360/7471 (45%)]\tLoss: 1599446.625000\n",
            "Train Epoch: 5 [3520/7471 (47%)]\tLoss: 1603493.250000\n",
            "Train Epoch: 5 [3680/7471 (49%)]\tLoss: 1606532.375000\n",
            "Train Epoch: 5 [3840/7471 (51%)]\tLoss: 1629225.125000\n",
            "Train Epoch: 5 [4000/7471 (54%)]\tLoss: 1559393.250000\n",
            "Train Epoch: 5 [4160/7471 (56%)]\tLoss: 1608122.625000\n",
            "Train Epoch: 5 [4320/7471 (58%)]\tLoss: 1589783.000000\n",
            "Train Epoch: 5 [4480/7471 (60%)]\tLoss: 1616556.250000\n",
            "Train Epoch: 5 [4640/7471 (62%)]\tLoss: 1627164.375000\n",
            "Train Epoch: 5 [4800/7471 (64%)]\tLoss: 1646613.500000\n",
            "Train Epoch: 5 [4960/7471 (66%)]\tLoss: 1604176.625000\n",
            "Train Epoch: 5 [5120/7471 (69%)]\tLoss: 1579190.500000\n",
            "Train Epoch: 5 [5280/7471 (71%)]\tLoss: 1596752.625000\n",
            "Train Epoch: 5 [5440/7471 (73%)]\tLoss: 1600210.000000\n",
            "Train Epoch: 5 [5600/7471 (75%)]\tLoss: 1560554.250000\n",
            "Train Epoch: 5 [5760/7471 (77%)]\tLoss: 1600488.000000\n",
            "Train Epoch: 5 [5920/7471 (79%)]\tLoss: 1641764.125000\n",
            "Train Epoch: 5 [6080/7471 (81%)]\tLoss: 1605000.250000\n",
            "Train Epoch: 5 [6240/7471 (84%)]\tLoss: 1617440.500000\n",
            "Train Epoch: 5 [6400/7471 (86%)]\tLoss: 1535952.125000\n",
            "Train Epoch: 5 [6560/7471 (88%)]\tLoss: 1578907.250000\n",
            "Train Epoch: 5 [6720/7471 (90%)]\tLoss: 1632174.000000\n",
            "Train Epoch: 5 [6880/7471 (92%)]\tLoss: 1591244.000000\n",
            "Train Epoch: 5 [7040/7471 (94%)]\tLoss: 1624165.250000\n",
            "Train Epoch: 5 [7200/7471 (96%)]\tLoss: 1596705.750000\n",
            "Train Epoch: 5 [7360/7471 (99%)]\tLoss: 1575138.375000\n",
            "Epoch 5 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 99733.8165\n",
            "\n",
            "Train Epoch: 6 [160/7471 (2%)]\tLoss: 1567218.625000\n",
            "Train Epoch: 6 [320/7471 (4%)]\tLoss: 1606270.125000\n",
            "Train Epoch: 6 [480/7471 (6%)]\tLoss: 1560074.375000\n",
            "Train Epoch: 6 [640/7471 (9%)]\tLoss: 1574427.000000\n",
            "Train Epoch: 6 [800/7471 (11%)]\tLoss: 1542621.250000\n",
            "Train Epoch: 6 [960/7471 (13%)]\tLoss: 1540158.375000\n",
            "Train Epoch: 6 [1120/7471 (15%)]\tLoss: 1627157.000000\n",
            "Train Epoch: 6 [1280/7471 (17%)]\tLoss: 1581728.625000\n",
            "Train Epoch: 6 [1440/7471 (19%)]\tLoss: 1645727.625000\n",
            "Train Epoch: 6 [1600/7471 (21%)]\tLoss: 1584228.750000\n",
            "Train Epoch: 6 [1760/7471 (24%)]\tLoss: 1599763.750000\n",
            "Train Epoch: 6 [1920/7471 (26%)]\tLoss: 1605601.000000\n",
            "Train Epoch: 6 [2080/7471 (28%)]\tLoss: 1572969.875000\n",
            "Train Epoch: 6 [2240/7471 (30%)]\tLoss: 1620635.500000\n",
            "Train Epoch: 6 [2400/7471 (32%)]\tLoss: 1645751.750000\n",
            "Train Epoch: 6 [2560/7471 (34%)]\tLoss: 1648334.750000\n",
            "Train Epoch: 6 [2720/7471 (36%)]\tLoss: 1617278.750000\n",
            "Train Epoch: 6 [2880/7471 (39%)]\tLoss: 1632115.125000\n",
            "Train Epoch: 6 [3040/7471 (41%)]\tLoss: 1608432.125000\n",
            "Train Epoch: 6 [3200/7471 (43%)]\tLoss: 1644550.625000\n",
            "Train Epoch: 6 [3360/7471 (45%)]\tLoss: 1640723.750000\n",
            "Train Epoch: 6 [3520/7471 (47%)]\tLoss: 1644136.375000\n",
            "Train Epoch: 6 [3680/7471 (49%)]\tLoss: 1592851.500000\n",
            "Train Epoch: 6 [3840/7471 (51%)]\tLoss: 1546181.000000\n",
            "Train Epoch: 6 [4000/7471 (54%)]\tLoss: 1574763.750000\n",
            "Train Epoch: 6 [4160/7471 (56%)]\tLoss: 1601383.625000\n",
            "Train Epoch: 6 [4320/7471 (58%)]\tLoss: 1628363.125000\n",
            "Train Epoch: 6 [4480/7471 (60%)]\tLoss: 1551944.750000\n",
            "Train Epoch: 6 [4640/7471 (62%)]\tLoss: 1623350.875000\n",
            "Train Epoch: 6 [4800/7471 (64%)]\tLoss: 1638214.250000\n",
            "Train Epoch: 6 [4960/7471 (66%)]\tLoss: 1654713.500000\n",
            "Train Epoch: 6 [5120/7471 (69%)]\tLoss: 1523228.125000\n",
            "Train Epoch: 6 [5280/7471 (71%)]\tLoss: 1636374.500000\n",
            "Train Epoch: 6 [5440/7471 (73%)]\tLoss: 1626008.000000\n",
            "Train Epoch: 6 [5600/7471 (75%)]\tLoss: 1628605.500000\n",
            "Train Epoch: 6 [5760/7471 (77%)]\tLoss: 1651764.000000\n",
            "Train Epoch: 6 [5920/7471 (79%)]\tLoss: 1646455.000000\n",
            "Train Epoch: 6 [6080/7471 (81%)]\tLoss: 1560781.250000\n",
            "Train Epoch: 6 [6240/7471 (84%)]\tLoss: 1615196.250000\n",
            "Train Epoch: 6 [6400/7471 (86%)]\tLoss: 1638630.875000\n",
            "Train Epoch: 6 [6560/7471 (88%)]\tLoss: 1606004.000000\n",
            "Train Epoch: 6 [6720/7471 (90%)]\tLoss: 1581732.375000\n",
            "Train Epoch: 6 [6880/7471 (92%)]\tLoss: 1632764.875000\n",
            "Train Epoch: 6 [7040/7471 (94%)]\tLoss: 1664104.875000\n",
            "Train Epoch: 6 [7200/7471 (96%)]\tLoss: 1599790.875000\n",
            "Train Epoch: 6 [7360/7471 (99%)]\tLoss: 1642853.625000\n",
            "Epoch 6 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 7 [160/7471 (2%)]\tLoss: 1576239.500000\n",
            "Train Epoch: 7 [320/7471 (4%)]\tLoss: 1616653.375000\n",
            "Train Epoch: 7 [480/7471 (6%)]\tLoss: 1614666.500000\n",
            "Train Epoch: 7 [640/7471 (9%)]\tLoss: 1641202.000000\n",
            "Train Epoch: 7 [800/7471 (11%)]\tLoss: 1641808.500000\n",
            "Train Epoch: 7 [960/7471 (13%)]\tLoss: 1605363.500000\n",
            "Train Epoch: 7 [1120/7471 (15%)]\tLoss: 1623921.250000\n",
            "Train Epoch: 7 [1280/7471 (17%)]\tLoss: 1645804.250000\n",
            "Train Epoch: 7 [1440/7471 (19%)]\tLoss: 1606317.750000\n",
            "Train Epoch: 7 [1600/7471 (21%)]\tLoss: 1631076.375000\n",
            "Train Epoch: 7 [1760/7471 (24%)]\tLoss: 1595348.375000\n",
            "Train Epoch: 7 [1920/7471 (26%)]\tLoss: 1661583.625000\n",
            "Train Epoch: 7 [2080/7471 (28%)]\tLoss: 1648551.250000\n",
            "Train Epoch: 7 [2240/7471 (30%)]\tLoss: 1569377.000000\n",
            "Train Epoch: 7 [2400/7471 (32%)]\tLoss: 1638627.750000\n",
            "Train Epoch: 7 [2560/7471 (34%)]\tLoss: 1648213.500000\n",
            "Train Epoch: 7 [2720/7471 (36%)]\tLoss: 1625047.000000\n",
            "Train Epoch: 7 [2880/7471 (39%)]\tLoss: 1538366.750000\n",
            "Train Epoch: 7 [3040/7471 (41%)]\tLoss: 1624938.125000\n",
            "Train Epoch: 7 [3200/7471 (43%)]\tLoss: 1600988.750000\n",
            "Train Epoch: 7 [3360/7471 (45%)]\tLoss: 1602814.250000\n",
            "Train Epoch: 7 [3520/7471 (47%)]\tLoss: 1603133.500000\n",
            "Train Epoch: 7 [3680/7471 (49%)]\tLoss: 1605073.875000\n",
            "Train Epoch: 7 [3840/7471 (51%)]\tLoss: 1615975.375000\n",
            "Train Epoch: 7 [4000/7471 (54%)]\tLoss: 1610439.375000\n",
            "Train Epoch: 7 [4160/7471 (56%)]\tLoss: 1604625.750000\n",
            "Train Epoch: 7 [4320/7471 (58%)]\tLoss: 1608590.375000\n",
            "Train Epoch: 7 [4480/7471 (60%)]\tLoss: 1632484.750000\n",
            "Train Epoch: 7 [4640/7471 (62%)]\tLoss: 1564126.750000\n",
            "Train Epoch: 7 [4800/7471 (64%)]\tLoss: 1611304.875000\n",
            "Train Epoch: 7 [4960/7471 (66%)]\tLoss: 1654547.250000\n",
            "Train Epoch: 7 [5120/7471 (69%)]\tLoss: 1578760.500000\n",
            "Train Epoch: 7 [5280/7471 (71%)]\tLoss: 1629973.500000\n",
            "Train Epoch: 7 [5440/7471 (73%)]\tLoss: 1617004.125000\n",
            "Train Epoch: 7 [5600/7471 (75%)]\tLoss: 1591526.500000\n",
            "Train Epoch: 7 [5760/7471 (77%)]\tLoss: 1588071.125000\n",
            "Train Epoch: 7 [5920/7471 (79%)]\tLoss: 1584444.000000\n",
            "Train Epoch: 7 [6080/7471 (81%)]\tLoss: 1597293.750000\n",
            "Train Epoch: 7 [6240/7471 (84%)]\tLoss: 1639234.000000\n",
            "Train Epoch: 7 [6400/7471 (86%)]\tLoss: 1614019.625000\n",
            "Train Epoch: 7 [6560/7471 (88%)]\tLoss: 1621239.375000\n",
            "Train Epoch: 7 [6720/7471 (90%)]\tLoss: 1593058.375000\n",
            "Train Epoch: 7 [6880/7471 (92%)]\tLoss: 1661414.750000\n",
            "Train Epoch: 7 [7040/7471 (94%)]\tLoss: 1626796.875000\n",
            "Train Epoch: 7 [7200/7471 (96%)]\tLoss: 1617037.125000\n",
            "Train Epoch: 7 [7360/7471 (99%)]\tLoss: 1555431.625000\n",
            "Epoch 7 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 99986.5400\n",
            "\n",
            "Train Epoch: 8 [160/7471 (2%)]\tLoss: 1581371.375000\n",
            "Train Epoch: 8 [320/7471 (4%)]\tLoss: 1633212.875000\n",
            "Train Epoch: 8 [480/7471 (6%)]\tLoss: 1647639.500000\n",
            "Train Epoch: 8 [640/7471 (9%)]\tLoss: 1628789.375000\n",
            "Train Epoch: 8 [800/7471 (11%)]\tLoss: 1623240.625000\n",
            "Train Epoch: 8 [960/7471 (13%)]\tLoss: 1591445.875000\n",
            "Train Epoch: 8 [1120/7471 (15%)]\tLoss: 1639221.250000\n",
            "Train Epoch: 8 [1280/7471 (17%)]\tLoss: 1624823.125000\n",
            "Train Epoch: 8 [1440/7471 (19%)]\tLoss: 1595797.750000\n",
            "Train Epoch: 8 [1600/7471 (21%)]\tLoss: 1591373.875000\n",
            "Train Epoch: 8 [1760/7471 (24%)]\tLoss: 1623718.750000\n",
            "Train Epoch: 8 [1920/7471 (26%)]\tLoss: 1554810.000000\n",
            "Train Epoch: 8 [2080/7471 (28%)]\tLoss: 1579060.375000\n",
            "Train Epoch: 8 [2240/7471 (30%)]\tLoss: 1544320.875000\n",
            "Train Epoch: 8 [2400/7471 (32%)]\tLoss: 1585969.875000\n",
            "Train Epoch: 8 [2560/7471 (34%)]\tLoss: 1530153.625000\n",
            "Train Epoch: 8 [2720/7471 (36%)]\tLoss: 1613327.750000\n",
            "Train Epoch: 8 [2880/7471 (39%)]\tLoss: 1575860.250000\n",
            "Train Epoch: 8 [3040/7471 (41%)]\tLoss: 1581012.000000\n",
            "Train Epoch: 8 [3200/7471 (43%)]\tLoss: 1644336.500000\n",
            "Train Epoch: 8 [3360/7471 (45%)]\tLoss: 1586556.375000\n",
            "Train Epoch: 8 [3520/7471 (47%)]\tLoss: 1607591.000000\n",
            "Train Epoch: 8 [3680/7471 (49%)]\tLoss: 1603560.500000\n",
            "Train Epoch: 8 [3840/7471 (51%)]\tLoss: 1595081.000000\n",
            "Train Epoch: 8 [4000/7471 (54%)]\tLoss: 1619857.625000\n",
            "Train Epoch: 8 [4160/7471 (56%)]\tLoss: 1586027.500000\n",
            "Train Epoch: 8 [4320/7471 (58%)]\tLoss: 1584418.375000\n",
            "Train Epoch: 8 [4480/7471 (60%)]\tLoss: 1580587.000000\n",
            "Train Epoch: 8 [4640/7471 (62%)]\tLoss: 1559013.375000\n",
            "Train Epoch: 8 [4800/7471 (64%)]\tLoss: 1613407.000000\n",
            "Train Epoch: 8 [4960/7471 (66%)]\tLoss: 1549976.250000\n",
            "Train Epoch: 8 [5120/7471 (69%)]\tLoss: 1577137.625000\n",
            "Train Epoch: 8 [5280/7471 (71%)]\tLoss: 1666799.250000\n",
            "Train Epoch: 8 [5440/7471 (73%)]\tLoss: 1614040.500000\n",
            "Train Epoch: 8 [5600/7471 (75%)]\tLoss: 1660690.875000\n",
            "Train Epoch: 8 [5760/7471 (77%)]\tLoss: 1619118.500000\n",
            "Train Epoch: 8 [5920/7471 (79%)]\tLoss: 1560304.125000\n",
            "Train Epoch: 8 [6080/7471 (81%)]\tLoss: 1616035.000000\n",
            "Train Epoch: 8 [6240/7471 (84%)]\tLoss: 1615721.250000\n",
            "Train Epoch: 8 [6400/7471 (86%)]\tLoss: 1576316.500000\n",
            "Train Epoch: 8 [6560/7471 (88%)]\tLoss: 1637360.250000\n",
            "Train Epoch: 8 [6720/7471 (90%)]\tLoss: 1616636.875000\n",
            "Train Epoch: 8 [6880/7471 (92%)]\tLoss: 1629918.625000\n",
            "Train Epoch: 8 [7040/7471 (94%)]\tLoss: 1604697.375000\n",
            "Train Epoch: 8 [7200/7471 (96%)]\tLoss: 1566456.000000\n",
            "Train Epoch: 8 [7360/7471 (99%)]\tLoss: 1620114.500000\n",
            "Epoch 8 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 101585.2882\n",
            "\n",
            "Train Epoch: 9 [160/7471 (2%)]\tLoss: 1650862.125000\n",
            "Train Epoch: 9 [320/7471 (4%)]\tLoss: 1516593.750000\n",
            "Train Epoch: 9 [480/7471 (6%)]\tLoss: 1633871.375000\n",
            "Train Epoch: 9 [640/7471 (9%)]\tLoss: 1601456.375000\n",
            "Train Epoch: 9 [800/7471 (11%)]\tLoss: 1589987.125000\n",
            "Train Epoch: 9 [960/7471 (13%)]\tLoss: 1594104.500000\n",
            "Train Epoch: 9 [1120/7471 (15%)]\tLoss: 1594487.500000\n",
            "Train Epoch: 9 [1280/7471 (17%)]\tLoss: 1620938.875000\n",
            "Train Epoch: 9 [1440/7471 (19%)]\tLoss: 1637459.250000\n",
            "Train Epoch: 9 [1600/7471 (21%)]\tLoss: 1566485.000000\n",
            "Train Epoch: 9 [1760/7471 (24%)]\tLoss: 1606345.375000\n",
            "Train Epoch: 9 [1920/7471 (26%)]\tLoss: 1573575.250000\n",
            "Train Epoch: 9 [2080/7471 (28%)]\tLoss: 1574430.500000\n",
            "Train Epoch: 9 [2240/7471 (30%)]\tLoss: 1615813.375000\n",
            "Train Epoch: 9 [2400/7471 (32%)]\tLoss: 1569203.875000\n",
            "Train Epoch: 9 [2560/7471 (34%)]\tLoss: 1614355.750000\n",
            "Train Epoch: 9 [2720/7471 (36%)]\tLoss: 1562401.875000\n",
            "Train Epoch: 9 [2880/7471 (39%)]\tLoss: 1585535.375000\n",
            "Train Epoch: 9 [3040/7471 (41%)]\tLoss: 1607686.875000\n",
            "Train Epoch: 9 [3200/7471 (43%)]\tLoss: 1590386.125000\n",
            "Train Epoch: 9 [3360/7471 (45%)]\tLoss: 1613890.500000\n",
            "Train Epoch: 9 [3520/7471 (47%)]\tLoss: 1541759.500000\n",
            "Train Epoch: 9 [3680/7471 (49%)]\tLoss: 1590480.000000\n",
            "Train Epoch: 9 [3840/7471 (51%)]\tLoss: 1601454.625000\n",
            "Train Epoch: 9 [4000/7471 (54%)]\tLoss: 1631037.250000\n",
            "Train Epoch: 9 [4160/7471 (56%)]\tLoss: 1568032.875000\n",
            "Train Epoch: 9 [4320/7471 (58%)]\tLoss: 1643500.375000\n",
            "Train Epoch: 9 [4480/7471 (60%)]\tLoss: 1574999.125000\n",
            "Train Epoch: 9 [4640/7471 (62%)]\tLoss: 1622346.875000\n",
            "Train Epoch: 9 [4800/7471 (64%)]\tLoss: 1557130.750000\n",
            "Train Epoch: 9 [4960/7471 (66%)]\tLoss: 1560977.625000\n",
            "Train Epoch: 9 [5120/7471 (69%)]\tLoss: 1520902.875000\n",
            "Train Epoch: 9 [5280/7471 (71%)]\tLoss: 1653593.750000\n",
            "Train Epoch: 9 [5440/7471 (73%)]\tLoss: 1590242.125000\n",
            "Train Epoch: 9 [5600/7471 (75%)]\tLoss: 1661997.125000\n",
            "Train Epoch: 9 [5760/7471 (77%)]\tLoss: 1623756.500000\n",
            "Train Epoch: 9 [5920/7471 (79%)]\tLoss: 1617044.750000\n",
            "Train Epoch: 9 [6080/7471 (81%)]\tLoss: 1612223.250000\n",
            "Train Epoch: 9 [6240/7471 (84%)]\tLoss: 1602870.125000\n",
            "Train Epoch: 9 [6400/7471 (86%)]\tLoss: 1500421.750000\n",
            "Train Epoch: 9 [6560/7471 (88%)]\tLoss: 1631254.625000\n",
            "Train Epoch: 9 [6720/7471 (90%)]\tLoss: 1493564.625000\n",
            "Train Epoch: 9 [6880/7471 (92%)]\tLoss: 1573422.250000\n",
            "Train Epoch: 9 [7040/7471 (94%)]\tLoss: 1586957.875000\n",
            "Train Epoch: 9 [7200/7471 (96%)]\tLoss: 1601350.625000\n",
            "Train Epoch: 9 [7360/7471 (99%)]\tLoss: 1643790.250000\n",
            "Epoch 9 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 10 [160/7471 (2%)]\tLoss: 1585310.500000\n",
            "Train Epoch: 10 [320/7471 (4%)]\tLoss: 1613467.000000\n",
            "Train Epoch: 10 [480/7471 (6%)]\tLoss: 1626844.500000\n",
            "Train Epoch: 10 [640/7471 (9%)]\tLoss: 1596984.750000\n",
            "Train Epoch: 10 [800/7471 (11%)]\tLoss: 1574285.000000\n",
            "Train Epoch: 10 [960/7471 (13%)]\tLoss: 1591818.250000\n",
            "Train Epoch: 10 [1120/7471 (15%)]\tLoss: 1587685.250000\n",
            "Train Epoch: 10 [1280/7471 (17%)]\tLoss: 1621053.625000\n",
            "Train Epoch: 10 [1440/7471 (19%)]\tLoss: 1612694.500000\n",
            "Train Epoch: 10 [1600/7471 (21%)]\tLoss: 1603796.250000\n",
            "Train Epoch: 10 [1760/7471 (24%)]\tLoss: 1643348.875000\n",
            "Train Epoch: 10 [1920/7471 (26%)]\tLoss: 1567309.000000\n",
            "Train Epoch: 10 [2080/7471 (28%)]\tLoss: 1621130.250000\n",
            "Train Epoch: 10 [2240/7471 (30%)]\tLoss: 1621370.750000\n",
            "Train Epoch: 10 [2400/7471 (32%)]\tLoss: 1604450.500000\n",
            "Train Epoch: 10 [2560/7471 (34%)]\tLoss: 1540666.875000\n",
            "Train Epoch: 10 [2720/7471 (36%)]\tLoss: 1555171.625000\n",
            "Train Epoch: 10 [2880/7471 (39%)]\tLoss: 1625414.750000\n",
            "Train Epoch: 10 [3040/7471 (41%)]\tLoss: 1642024.250000\n",
            "Train Epoch: 10 [3200/7471 (43%)]\tLoss: 1586357.500000\n",
            "Train Epoch: 10 [3360/7471 (45%)]\tLoss: 1599441.750000\n",
            "Train Epoch: 10 [3520/7471 (47%)]\tLoss: 1612838.875000\n",
            "Train Epoch: 10 [3680/7471 (49%)]\tLoss: 1540800.250000\n",
            "Train Epoch: 10 [3840/7471 (51%)]\tLoss: 1614234.500000\n",
            "Train Epoch: 10 [4000/7471 (54%)]\tLoss: 1586223.375000\n",
            "Train Epoch: 10 [4160/7471 (56%)]\tLoss: 1569601.000000\n",
            "Train Epoch: 10 [4320/7471 (58%)]\tLoss: 1626455.250000\n",
            "Train Epoch: 10 [4480/7471 (60%)]\tLoss: 1550854.375000\n",
            "Train Epoch: 10 [4640/7471 (62%)]\tLoss: 1586618.875000\n",
            "Train Epoch: 10 [4800/7471 (64%)]\tLoss: 1518826.000000\n",
            "Train Epoch: 10 [4960/7471 (66%)]\tLoss: 1557796.375000\n",
            "Train Epoch: 10 [5120/7471 (69%)]\tLoss: 1629042.000000\n",
            "Train Epoch: 10 [5280/7471 (71%)]\tLoss: 1612469.000000\n",
            "Train Epoch: 10 [5440/7471 (73%)]\tLoss: 1552311.875000\n",
            "Train Epoch: 10 [5600/7471 (75%)]\tLoss: 1640833.875000\n",
            "Train Epoch: 10 [5760/7471 (77%)]\tLoss: 1581470.625000\n",
            "Train Epoch: 10 [5920/7471 (79%)]\tLoss: 1604547.250000\n",
            "Train Epoch: 10 [6080/7471 (81%)]\tLoss: 1577307.750000\n",
            "Train Epoch: 10 [6240/7471 (84%)]\tLoss: 1577212.250000\n",
            "Train Epoch: 10 [6400/7471 (86%)]\tLoss: 1632664.375000\n",
            "Train Epoch: 10 [6560/7471 (88%)]\tLoss: 1540928.625000\n",
            "Train Epoch: 10 [6720/7471 (90%)]\tLoss: 1632921.000000\n",
            "Train Epoch: 10 [6880/7471 (92%)]\tLoss: 1626437.125000\n",
            "Train Epoch: 10 [7040/7471 (94%)]\tLoss: 1617705.125000\n",
            "Train Epoch: 10 [7200/7471 (96%)]\tLoss: 1560210.250000\n",
            "Train Epoch: 10 [7360/7471 (99%)]\tLoss: 1549749.500000\n",
            "Epoch 10 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 11 [160/7471 (2%)]\tLoss: 1554449.625000\n",
            "Train Epoch: 11 [320/7471 (4%)]\tLoss: 1555163.750000\n",
            "Train Epoch: 11 [480/7471 (6%)]\tLoss: 1605878.625000\n",
            "Train Epoch: 11 [640/7471 (9%)]\tLoss: 1525744.125000\n",
            "Train Epoch: 11 [800/7471 (11%)]\tLoss: 1619160.000000\n",
            "Train Epoch: 11 [960/7471 (13%)]\tLoss: 1612581.125000\n",
            "Train Epoch: 11 [1120/7471 (15%)]\tLoss: 1633805.625000\n",
            "Train Epoch: 11 [1280/7471 (17%)]\tLoss: 1597804.375000\n",
            "Train Epoch: 11 [1440/7471 (19%)]\tLoss: 1598505.250000\n",
            "Train Epoch: 11 [1600/7471 (21%)]\tLoss: 1639300.625000\n",
            "Train Epoch: 11 [1760/7471 (24%)]\tLoss: 1568908.125000\n",
            "Train Epoch: 11 [1920/7471 (26%)]\tLoss: 1564782.125000\n",
            "Train Epoch: 11 [2080/7471 (28%)]\tLoss: 1580930.125000\n",
            "Train Epoch: 11 [2240/7471 (30%)]\tLoss: 1575817.375000\n",
            "Train Epoch: 11 [2400/7471 (32%)]\tLoss: 1581957.750000\n",
            "Train Epoch: 11 [2560/7471 (34%)]\tLoss: 1633372.000000\n",
            "Train Epoch: 11 [2720/7471 (36%)]\tLoss: 1590461.250000\n",
            "Train Epoch: 11 [2880/7471 (39%)]\tLoss: 1576754.250000\n",
            "Train Epoch: 11 [3040/7471 (41%)]\tLoss: 1579820.750000\n",
            "Train Epoch: 11 [3200/7471 (43%)]\tLoss: 1605594.125000\n",
            "Train Epoch: 11 [3360/7471 (45%)]\tLoss: 1602029.125000\n",
            "Train Epoch: 11 [3520/7471 (47%)]\tLoss: 1577628.000000\n",
            "Train Epoch: 11 [3680/7471 (49%)]\tLoss: 1653837.750000\n",
            "Train Epoch: 11 [3840/7471 (51%)]\tLoss: 1611525.750000\n",
            "Train Epoch: 11 [4000/7471 (54%)]\tLoss: 1565339.625000\n",
            "Train Epoch: 11 [4160/7471 (56%)]\tLoss: 1598357.125000\n",
            "Train Epoch: 11 [4320/7471 (58%)]\tLoss: 1594280.750000\n",
            "Train Epoch: 11 [4480/7471 (60%)]\tLoss: 1600961.875000\n",
            "Train Epoch: 11 [4640/7471 (62%)]\tLoss: 1531443.750000\n",
            "Train Epoch: 11 [4800/7471 (64%)]\tLoss: 1635521.125000\n",
            "Train Epoch: 11 [4960/7471 (66%)]\tLoss: 1628022.750000\n",
            "Train Epoch: 11 [5120/7471 (69%)]\tLoss: 1613990.000000\n",
            "Train Epoch: 11 [5280/7471 (71%)]\tLoss: 1603824.125000\n",
            "Train Epoch: 11 [5440/7471 (73%)]\tLoss: 1562942.375000\n",
            "Train Epoch: 11 [5600/7471 (75%)]\tLoss: 1617678.625000\n",
            "Train Epoch: 11 [5760/7471 (77%)]\tLoss: 1572519.125000\n",
            "Train Epoch: 11 [5920/7471 (79%)]\tLoss: 1607602.125000\n",
            "Train Epoch: 11 [6080/7471 (81%)]\tLoss: 1627462.250000\n",
            "Train Epoch: 11 [6240/7471 (84%)]\tLoss: 1597630.250000\n",
            "Train Epoch: 11 [6400/7471 (86%)]\tLoss: 1570348.625000\n",
            "Train Epoch: 11 [6560/7471 (88%)]\tLoss: 1562274.000000\n",
            "Train Epoch: 11 [6720/7471 (90%)]\tLoss: 1553922.750000\n",
            "Train Epoch: 11 [6880/7471 (92%)]\tLoss: 1564341.000000\n",
            "Train Epoch: 11 [7040/7471 (94%)]\tLoss: 1623490.500000\n",
            "Train Epoch: 11 [7200/7471 (96%)]\tLoss: 1575410.750000\n",
            "Train Epoch: 11 [7360/7471 (99%)]\tLoss: 1588390.375000\n",
            "Epoch 11 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 12 [160/7471 (2%)]\tLoss: 1642910.875000\n",
            "Train Epoch: 12 [320/7471 (4%)]\tLoss: 1563857.875000\n",
            "Train Epoch: 12 [480/7471 (6%)]\tLoss: 1519667.750000\n",
            "Train Epoch: 12 [640/7471 (9%)]\tLoss: 1583319.875000\n",
            "Train Epoch: 12 [800/7471 (11%)]\tLoss: 1545009.875000\n",
            "Train Epoch: 12 [960/7471 (13%)]\tLoss: 1595194.000000\n",
            "Train Epoch: 12 [1120/7471 (15%)]\tLoss: 1590167.125000\n",
            "Train Epoch: 12 [1280/7471 (17%)]\tLoss: 1616872.625000\n",
            "Train Epoch: 12 [1440/7471 (19%)]\tLoss: 1609242.375000\n",
            "Train Epoch: 12 [1600/7471 (21%)]\tLoss: 1608400.500000\n",
            "Train Epoch: 12 [1760/7471 (24%)]\tLoss: 1576072.375000\n",
            "Train Epoch: 12 [1920/7471 (26%)]\tLoss: 1622336.750000\n",
            "Train Epoch: 12 [2080/7471 (28%)]\tLoss: 1606671.875000\n",
            "Train Epoch: 12 [2240/7471 (30%)]\tLoss: 1613724.500000\n",
            "Train Epoch: 12 [2400/7471 (32%)]\tLoss: 1633445.250000\n",
            "Train Epoch: 12 [2560/7471 (34%)]\tLoss: 1581444.000000\n",
            "Train Epoch: 12 [2720/7471 (36%)]\tLoss: 1625547.375000\n",
            "Train Epoch: 12 [2880/7471 (39%)]\tLoss: 1584616.750000\n",
            "Train Epoch: 12 [3040/7471 (41%)]\tLoss: 1598965.750000\n",
            "Train Epoch: 12 [3200/7471 (43%)]\tLoss: 1540955.875000\n",
            "Train Epoch: 12 [3360/7471 (45%)]\tLoss: 1590782.750000\n",
            "Train Epoch: 12 [3520/7471 (47%)]\tLoss: 1588838.250000\n",
            "Train Epoch: 12 [3680/7471 (49%)]\tLoss: 1577532.625000\n",
            "Train Epoch: 12 [3840/7471 (51%)]\tLoss: 1609082.500000\n",
            "Train Epoch: 12 [4000/7471 (54%)]\tLoss: 1603265.375000\n",
            "Train Epoch: 12 [4160/7471 (56%)]\tLoss: 1562772.875000\n",
            "Train Epoch: 12 [4320/7471 (58%)]\tLoss: 1593599.500000\n",
            "Train Epoch: 12 [4480/7471 (60%)]\tLoss: 1609674.875000\n",
            "Train Epoch: 12 [4640/7471 (62%)]\tLoss: 1643924.750000\n",
            "Train Epoch: 12 [4800/7471 (64%)]\tLoss: 1636735.000000\n",
            "Train Epoch: 12 [4960/7471 (66%)]\tLoss: 1626011.875000\n",
            "Train Epoch: 12 [5120/7471 (69%)]\tLoss: 1601966.625000\n",
            "Train Epoch: 12 [5280/7471 (71%)]\tLoss: 1594032.875000\n",
            "Train Epoch: 12 [5440/7471 (73%)]\tLoss: 1498080.500000\n",
            "Train Epoch: 12 [5600/7471 (75%)]\tLoss: 1567434.375000\n",
            "Train Epoch: 12 [5760/7471 (77%)]\tLoss: 1629378.750000\n",
            "Train Epoch: 12 [5920/7471 (79%)]\tLoss: 1571962.750000\n",
            "Train Epoch: 12 [6080/7471 (81%)]\tLoss: 1578259.125000\n",
            "Train Epoch: 12 [6240/7471 (84%)]\tLoss: 1601968.375000\n",
            "Train Epoch: 12 [6400/7471 (86%)]\tLoss: 1648675.500000\n",
            "Train Epoch: 12 [6560/7471 (88%)]\tLoss: 1611764.375000\n",
            "Train Epoch: 12 [6720/7471 (90%)]\tLoss: 1630226.875000\n",
            "Train Epoch: 12 [6880/7471 (92%)]\tLoss: 1607565.500000\n",
            "Train Epoch: 12 [7040/7471 (94%)]\tLoss: 1635822.500000\n",
            "Train Epoch: 12 [7200/7471 (96%)]\tLoss: 1603975.875000\n",
            "Train Epoch: 12 [7360/7471 (99%)]\tLoss: 1608078.750000\n",
            "Epoch 12 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 13 [160/7471 (2%)]\tLoss: 1577312.375000\n",
            "Train Epoch: 13 [320/7471 (4%)]\tLoss: 1618014.375000\n",
            "Train Epoch: 13 [480/7471 (6%)]\tLoss: 1636746.125000\n",
            "Train Epoch: 13 [640/7471 (9%)]\tLoss: 1597879.500000\n",
            "Train Epoch: 13 [800/7471 (11%)]\tLoss: 1632251.375000\n",
            "Train Epoch: 13 [960/7471 (13%)]\tLoss: 1585565.500000\n",
            "Train Epoch: 13 [1120/7471 (15%)]\tLoss: 1576399.875000\n",
            "Train Epoch: 13 [1280/7471 (17%)]\tLoss: 1621018.375000\n",
            "Train Epoch: 13 [1440/7471 (19%)]\tLoss: 1647576.125000\n",
            "Train Epoch: 13 [1600/7471 (21%)]\tLoss: 1655039.875000\n",
            "Train Epoch: 13 [1760/7471 (24%)]\tLoss: 1583883.375000\n",
            "Train Epoch: 13 [1920/7471 (26%)]\tLoss: 1609658.375000\n",
            "Train Epoch: 13 [2080/7471 (28%)]\tLoss: 1552247.500000\n",
            "Train Epoch: 13 [2240/7471 (30%)]\tLoss: 1608644.125000\n",
            "Train Epoch: 13 [2400/7471 (32%)]\tLoss: 1615569.875000\n",
            "Train Epoch: 13 [2560/7471 (34%)]\tLoss: 1598753.625000\n",
            "Train Epoch: 13 [2720/7471 (36%)]\tLoss: 1633814.375000\n",
            "Train Epoch: 13 [2880/7471 (39%)]\tLoss: 1613896.750000\n",
            "Train Epoch: 13 [3040/7471 (41%)]\tLoss: 1570376.125000\n",
            "Train Epoch: 13 [3200/7471 (43%)]\tLoss: 1579520.250000\n",
            "Train Epoch: 13 [3360/7471 (45%)]\tLoss: 1633298.000000\n",
            "Train Epoch: 13 [3520/7471 (47%)]\tLoss: 1531091.000000\n",
            "Train Epoch: 13 [3680/7471 (49%)]\tLoss: 1601408.000000\n",
            "Train Epoch: 13 [3840/7471 (51%)]\tLoss: 1633417.000000\n",
            "Train Epoch: 13 [4000/7471 (54%)]\tLoss: 1631389.000000\n",
            "Train Epoch: 13 [4160/7471 (56%)]\tLoss: 1615019.500000\n",
            "Train Epoch: 13 [4320/7471 (58%)]\tLoss: 1553794.250000\n",
            "Train Epoch: 13 [4480/7471 (60%)]\tLoss: 1627168.250000\n",
            "Train Epoch: 13 [4640/7471 (62%)]\tLoss: 1616485.125000\n",
            "Train Epoch: 13 [4800/7471 (64%)]\tLoss: 1651561.875000\n",
            "Train Epoch: 13 [4960/7471 (66%)]\tLoss: 1613980.500000\n",
            "Train Epoch: 13 [5120/7471 (69%)]\tLoss: 1640919.375000\n",
            "Train Epoch: 13 [5280/7471 (71%)]\tLoss: 1589665.625000\n",
            "Train Epoch: 13 [5440/7471 (73%)]\tLoss: 1545134.125000\n",
            "Train Epoch: 13 [5600/7471 (75%)]\tLoss: 1599283.375000\n",
            "Train Epoch: 13 [5760/7471 (77%)]\tLoss: 1596569.000000\n",
            "Train Epoch: 13 [5920/7471 (79%)]\tLoss: 1556292.625000\n",
            "Train Epoch: 13 [6080/7471 (81%)]\tLoss: 1558219.875000\n",
            "Train Epoch: 13 [6240/7471 (84%)]\tLoss: 1594336.625000\n",
            "Train Epoch: 13 [6400/7471 (86%)]\tLoss: 1628160.625000\n",
            "Train Epoch: 13 [6560/7471 (88%)]\tLoss: 1625843.125000\n",
            "Train Epoch: 13 [6720/7471 (90%)]\tLoss: 1610214.125000\n",
            "Train Epoch: 13 [6880/7471 (92%)]\tLoss: 1552037.125000\n",
            "Train Epoch: 13 [7040/7471 (94%)]\tLoss: 1571893.750000\n",
            "Train Epoch: 13 [7200/7471 (96%)]\tLoss: 1565770.250000\n",
            "Train Epoch: 13 [7360/7471 (99%)]\tLoss: 1514067.125000\n",
            "Epoch 13 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 14 [160/7471 (2%)]\tLoss: 1615918.125000\n",
            "Train Epoch: 14 [320/7471 (4%)]\tLoss: 1639711.750000\n",
            "Train Epoch: 14 [480/7471 (6%)]\tLoss: 1560946.875000\n",
            "Train Epoch: 14 [640/7471 (9%)]\tLoss: 1581998.125000\n",
            "Train Epoch: 14 [800/7471 (11%)]\tLoss: 1560733.750000\n",
            "Train Epoch: 14 [960/7471 (13%)]\tLoss: 1547212.375000\n",
            "Train Epoch: 14 [1120/7471 (15%)]\tLoss: 1607984.250000\n",
            "Train Epoch: 14 [1280/7471 (17%)]\tLoss: 1645673.125000\n",
            "Train Epoch: 14 [1440/7471 (19%)]\tLoss: 1625694.250000\n",
            "Train Epoch: 14 [1600/7471 (21%)]\tLoss: 1642085.875000\n",
            "Train Epoch: 14 [1760/7471 (24%)]\tLoss: 1638089.750000\n",
            "Train Epoch: 14 [1920/7471 (26%)]\tLoss: 1545891.875000\n",
            "Train Epoch: 14 [2080/7471 (28%)]\tLoss: 1564571.500000\n",
            "Train Epoch: 14 [2240/7471 (30%)]\tLoss: 1495658.375000\n",
            "Train Epoch: 14 [2400/7471 (32%)]\tLoss: 1603206.750000\n",
            "Train Epoch: 14 [2560/7471 (34%)]\tLoss: 1548002.750000\n",
            "Train Epoch: 14 [2720/7471 (36%)]\tLoss: 1578436.250000\n",
            "Train Epoch: 14 [2880/7471 (39%)]\tLoss: 1690550.125000\n",
            "Train Epoch: 14 [3040/7471 (41%)]\tLoss: 1625878.500000\n",
            "Train Epoch: 14 [3200/7471 (43%)]\tLoss: 1614252.500000\n",
            "Train Epoch: 14 [3360/7471 (45%)]\tLoss: 1597799.250000\n",
            "Train Epoch: 14 [3520/7471 (47%)]\tLoss: 1506932.375000\n",
            "Train Epoch: 14 [3680/7471 (49%)]\tLoss: 1573129.750000\n",
            "Train Epoch: 14 [3840/7471 (51%)]\tLoss: 1594384.750000\n",
            "Train Epoch: 14 [4000/7471 (54%)]\tLoss: 1537294.125000\n",
            "Train Epoch: 14 [4160/7471 (56%)]\tLoss: 1599378.125000\n",
            "Train Epoch: 14 [4320/7471 (58%)]\tLoss: 1633016.875000\n",
            "Train Epoch: 14 [4480/7471 (60%)]\tLoss: 1598949.250000\n",
            "Train Epoch: 14 [4640/7471 (62%)]\tLoss: 1627534.500000\n",
            "Train Epoch: 14 [4800/7471 (64%)]\tLoss: 1581577.000000\n",
            "Train Epoch: 14 [4960/7471 (66%)]\tLoss: 1553617.625000\n",
            "Train Epoch: 14 [5120/7471 (69%)]\tLoss: 1583352.125000\n",
            "Train Epoch: 14 [5280/7471 (71%)]\tLoss: 1581906.625000\n",
            "Train Epoch: 14 [5440/7471 (73%)]\tLoss: 1542619.375000\n",
            "Train Epoch: 14 [5600/7471 (75%)]\tLoss: 1609997.875000\n",
            "Train Epoch: 14 [5760/7471 (77%)]\tLoss: 1615351.000000\n",
            "Train Epoch: 14 [5920/7471 (79%)]\tLoss: 1581728.625000\n",
            "Train Epoch: 14 [6080/7471 (81%)]\tLoss: 1565073.750000\n",
            "Train Epoch: 14 [6240/7471 (84%)]\tLoss: 1597746.125000\n",
            "Train Epoch: 14 [6400/7471 (86%)]\tLoss: 1609993.750000\n",
            "Train Epoch: 14 [6560/7471 (88%)]\tLoss: 1610966.000000\n",
            "Train Epoch: 14 [6720/7471 (90%)]\tLoss: 1589842.375000\n",
            "Train Epoch: 14 [6880/7471 (92%)]\tLoss: 1616300.250000\n",
            "Train Epoch: 14 [7040/7471 (94%)]\tLoss: 1567381.250000\n",
            "Train Epoch: 14 [7200/7471 (96%)]\tLoss: 1598942.875000\n",
            "Train Epoch: 14 [7360/7471 (99%)]\tLoss: 1602403.500000\n",
            "Epoch 14 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 139507.2508\n",
            "\n",
            "Train Epoch: 15 [160/7471 (2%)]\tLoss: 1555180.250000\n",
            "Train Epoch: 15 [320/7471 (4%)]\tLoss: 1560203.750000\n",
            "Train Epoch: 15 [480/7471 (6%)]\tLoss: 1570195.500000\n",
            "Train Epoch: 15 [640/7471 (9%)]\tLoss: 1594842.000000\n",
            "Train Epoch: 15 [800/7471 (11%)]\tLoss: 1628342.875000\n",
            "Train Epoch: 15 [960/7471 (13%)]\tLoss: 1589959.625000\n",
            "Train Epoch: 15 [1120/7471 (15%)]\tLoss: 1561833.375000\n",
            "Train Epoch: 15 [1280/7471 (17%)]\tLoss: 1581385.125000\n",
            "Train Epoch: 15 [1440/7471 (19%)]\tLoss: 1580232.375000\n",
            "Train Epoch: 15 [1600/7471 (21%)]\tLoss: 1562738.625000\n",
            "Train Epoch: 15 [1760/7471 (24%)]\tLoss: 1570844.500000\n",
            "Train Epoch: 15 [1920/7471 (26%)]\tLoss: 1554285.000000\n",
            "Train Epoch: 15 [2080/7471 (28%)]\tLoss: 1585777.750000\n",
            "Train Epoch: 15 [2240/7471 (30%)]\tLoss: 1549350.625000\n",
            "Train Epoch: 15 [2400/7471 (32%)]\tLoss: 1571518.875000\n",
            "Train Epoch: 15 [2560/7471 (34%)]\tLoss: 1569842.125000\n",
            "Train Epoch: 15 [2720/7471 (36%)]\tLoss: 1570267.250000\n",
            "Train Epoch: 15 [2880/7471 (39%)]\tLoss: 1611740.875000\n",
            "Train Epoch: 15 [3040/7471 (41%)]\tLoss: 1583024.250000\n",
            "Train Epoch: 15 [3200/7471 (43%)]\tLoss: 1576863.125000\n",
            "Train Epoch: 15 [3360/7471 (45%)]\tLoss: 1615116.375000\n",
            "Train Epoch: 15 [3520/7471 (47%)]\tLoss: 1568301.750000\n",
            "Train Epoch: 15 [3680/7471 (49%)]\tLoss: 1583071.500000\n",
            "Train Epoch: 15 [3840/7471 (51%)]\tLoss: 1566930.500000\n",
            "Train Epoch: 15 [4000/7471 (54%)]\tLoss: 1603851.000000\n",
            "Train Epoch: 15 [4160/7471 (56%)]\tLoss: 1601992.750000\n",
            "Train Epoch: 15 [4320/7471 (58%)]\tLoss: 1596689.125000\n",
            "Train Epoch: 15 [4480/7471 (60%)]\tLoss: 1517750.375000\n",
            "Train Epoch: 15 [4640/7471 (62%)]\tLoss: 1588573.125000\n",
            "Train Epoch: 15 [4800/7471 (64%)]\tLoss: 1606026.625000\n",
            "Train Epoch: 15 [4960/7471 (66%)]\tLoss: 1557789.625000\n",
            "Train Epoch: 15 [5120/7471 (69%)]\tLoss: 1572946.750000\n",
            "Train Epoch: 15 [5280/7471 (71%)]\tLoss: 1591867.000000\n",
            "Train Epoch: 15 [5440/7471 (73%)]\tLoss: 1624900.625000\n",
            "Train Epoch: 15 [5600/7471 (75%)]\tLoss: 1587634.250000\n",
            "Train Epoch: 15 [5760/7471 (77%)]\tLoss: 1615973.250000\n",
            "Train Epoch: 15 [5920/7471 (79%)]\tLoss: 1575021.125000\n",
            "Train Epoch: 15 [6080/7471 (81%)]\tLoss: 1518871.125000\n",
            "Train Epoch: 15 [6240/7471 (84%)]\tLoss: 1575648.625000\n",
            "Train Epoch: 15 [6400/7471 (86%)]\tLoss: 1558714.500000\n",
            "Train Epoch: 15 [6560/7471 (88%)]\tLoss: 1577239.000000\n",
            "Train Epoch: 15 [6720/7471 (90%)]\tLoss: 1547030.250000\n",
            "Train Epoch: 15 [6880/7471 (92%)]\tLoss: 1624496.500000\n",
            "Train Epoch: 15 [7040/7471 (94%)]\tLoss: 1627311.125000\n",
            "Train Epoch: 15 [7200/7471 (96%)]\tLoss: 1650821.125000\n",
            "Train Epoch: 15 [7360/7471 (99%)]\tLoss: 1634268.750000\n",
            "Epoch 15 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 8734700913790733516800.0000\n",
            "\n",
            "Train Epoch: 16 [160/7471 (2%)]\tLoss: 1578475.375000\n",
            "Train Epoch: 16 [320/7471 (4%)]\tLoss: 1552788.125000\n",
            "Train Epoch: 16 [480/7471 (6%)]\tLoss: 1621420.625000\n",
            "Train Epoch: 16 [640/7471 (9%)]\tLoss: 1595399.500000\n",
            "Train Epoch: 16 [800/7471 (11%)]\tLoss: 1561630.875000\n",
            "Train Epoch: 16 [960/7471 (13%)]\tLoss: 1558939.375000\n",
            "Train Epoch: 16 [1120/7471 (15%)]\tLoss: 1551489.000000\n",
            "Train Epoch: 16 [1280/7471 (17%)]\tLoss: 1570491.625000\n",
            "Train Epoch: 16 [1440/7471 (19%)]\tLoss: 1611820.625000\n",
            "Train Epoch: 16 [1600/7471 (21%)]\tLoss: 1601572.125000\n",
            "Train Epoch: 16 [1760/7471 (24%)]\tLoss: 1613691.000000\n",
            "Train Epoch: 16 [1920/7471 (26%)]\tLoss: 1550920.500000\n",
            "Train Epoch: 16 [2080/7471 (28%)]\tLoss: 1583222.625000\n",
            "Train Epoch: 16 [2240/7471 (30%)]\tLoss: 1591977.500000\n",
            "Train Epoch: 16 [2400/7471 (32%)]\tLoss: 1599005.875000\n",
            "Train Epoch: 16 [2560/7471 (34%)]\tLoss: 1591760.500000\n",
            "Train Epoch: 16 [2720/7471 (36%)]\tLoss: 1529836.250000\n",
            "Train Epoch: 16 [2880/7471 (39%)]\tLoss: 1592620.625000\n",
            "Train Epoch: 16 [3040/7471 (41%)]\tLoss: 1610872.125000\n",
            "Train Epoch: 16 [3200/7471 (43%)]\tLoss: 1558871.750000\n",
            "Train Epoch: 16 [3360/7471 (45%)]\tLoss: 1627826.375000\n",
            "Train Epoch: 16 [3520/7471 (47%)]\tLoss: 1615565.250000\n",
            "Train Epoch: 16 [3680/7471 (49%)]\tLoss: 1567798.375000\n",
            "Train Epoch: 16 [3840/7471 (51%)]\tLoss: 1620403.750000\n",
            "Train Epoch: 16 [4000/7471 (54%)]\tLoss: 1626526.500000\n",
            "Train Epoch: 16 [4160/7471 (56%)]\tLoss: 1600942.750000\n",
            "Train Epoch: 16 [4320/7471 (58%)]\tLoss: 1609115.625000\n",
            "Train Epoch: 16 [4480/7471 (60%)]\tLoss: 1546124.875000\n",
            "Train Epoch: 16 [4640/7471 (62%)]\tLoss: 1568535.625000\n",
            "Train Epoch: 16 [4800/7471 (64%)]\tLoss: 1606251.125000\n",
            "Train Epoch: 16 [4960/7471 (66%)]\tLoss: 1603146.000000\n",
            "Train Epoch: 16 [5120/7471 (69%)]\tLoss: 1615964.125000\n",
            "Train Epoch: 16 [5280/7471 (71%)]\tLoss: 1532468.625000\n",
            "Train Epoch: 16 [5440/7471 (73%)]\tLoss: 1616234.000000\n",
            "Train Epoch: 16 [5600/7471 (75%)]\tLoss: 1593025.750000\n",
            "Train Epoch: 16 [5760/7471 (77%)]\tLoss: 1587704.000000\n",
            "Train Epoch: 16 [5920/7471 (79%)]\tLoss: 1628513.250000\n",
            "Train Epoch: 16 [6080/7471 (81%)]\tLoss: 1619666.625000\n",
            "Train Epoch: 16 [6240/7471 (84%)]\tLoss: 1621539.000000\n",
            "Train Epoch: 16 [6400/7471 (86%)]\tLoss: 1604331.500000\n",
            "Train Epoch: 16 [6560/7471 (88%)]\tLoss: 1575451.000000\n",
            "Train Epoch: 16 [6720/7471 (90%)]\tLoss: 1592652.625000\n",
            "Train Epoch: 16 [6880/7471 (92%)]\tLoss: 1613753.125000\n",
            "Train Epoch: 16 [7040/7471 (94%)]\tLoss: 1602194.750000\n",
            "Train Epoch: 16 [7200/7471 (96%)]\tLoss: 1538152.625000\n",
            "Train Epoch: 16 [7360/7471 (99%)]\tLoss: 1571837.000000\n",
            "Epoch 16 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 17 [160/7471 (2%)]\tLoss: 1610796.125000\n",
            "Train Epoch: 17 [320/7471 (4%)]\tLoss: 1565934.125000\n",
            "Train Epoch: 17 [480/7471 (6%)]\tLoss: 1612238.500000\n",
            "Train Epoch: 17 [640/7471 (9%)]\tLoss: 1560950.375000\n",
            "Train Epoch: 17 [800/7471 (11%)]\tLoss: 1576009.000000\n",
            "Train Epoch: 17 [960/7471 (13%)]\tLoss: 1565963.750000\n",
            "Train Epoch: 17 [1120/7471 (15%)]\tLoss: 1634357.000000\n",
            "Train Epoch: 17 [1280/7471 (17%)]\tLoss: 1625832.000000\n",
            "Train Epoch: 17 [1440/7471 (19%)]\tLoss: 1605660.000000\n",
            "Train Epoch: 17 [1600/7471 (21%)]\tLoss: 1571450.250000\n",
            "Train Epoch: 17 [1760/7471 (24%)]\tLoss: 1635473.250000\n",
            "Train Epoch: 17 [1920/7471 (26%)]\tLoss: 1622751.875000\n",
            "Train Epoch: 17 [2080/7471 (28%)]\tLoss: 1607692.875000\n",
            "Train Epoch: 17 [2240/7471 (30%)]\tLoss: 1604686.500000\n",
            "Train Epoch: 17 [2400/7471 (32%)]\tLoss: 1553894.500000\n",
            "Train Epoch: 17 [2560/7471 (34%)]\tLoss: 1616990.500000\n",
            "Train Epoch: 17 [2720/7471 (36%)]\tLoss: 1527342.625000\n",
            "Train Epoch: 17 [2880/7471 (39%)]\tLoss: 1630763.750000\n",
            "Train Epoch: 17 [3040/7471 (41%)]\tLoss: 1611466.625000\n",
            "Train Epoch: 17 [3200/7471 (43%)]\tLoss: 1598464.375000\n",
            "Train Epoch: 17 [3360/7471 (45%)]\tLoss: 1574730.375000\n",
            "Train Epoch: 17 [3520/7471 (47%)]\tLoss: 1549309.500000\n",
            "Train Epoch: 17 [3680/7471 (49%)]\tLoss: 1590435.625000\n",
            "Train Epoch: 17 [3840/7471 (51%)]\tLoss: 1632766.000000\n",
            "Train Epoch: 17 [4000/7471 (54%)]\tLoss: 1605277.875000\n",
            "Train Epoch: 17 [4160/7471 (56%)]\tLoss: 1614394.000000\n",
            "Train Epoch: 17 [4320/7471 (58%)]\tLoss: 1640106.375000\n",
            "Train Epoch: 17 [4480/7471 (60%)]\tLoss: 1628062.875000\n",
            "Train Epoch: 17 [4640/7471 (62%)]\tLoss: 1569742.625000\n",
            "Train Epoch: 17 [4800/7471 (64%)]\tLoss: 1604642.625000\n",
            "Train Epoch: 17 [4960/7471 (66%)]\tLoss: 1633442.625000\n",
            "Train Epoch: 17 [5120/7471 (69%)]\tLoss: 1553804.000000\n",
            "Train Epoch: 17 [5280/7471 (71%)]\tLoss: 1586105.250000\n",
            "Train Epoch: 17 [5440/7471 (73%)]\tLoss: 1574200.000000\n",
            "Train Epoch: 17 [5600/7471 (75%)]\tLoss: 1610652.250000\n",
            "Train Epoch: 17 [5760/7471 (77%)]\tLoss: 1574137.750000\n",
            "Train Epoch: 17 [5920/7471 (79%)]\tLoss: 1633013.250000\n",
            "Train Epoch: 17 [6080/7471 (81%)]\tLoss: 1594507.500000\n",
            "Train Epoch: 17 [6240/7471 (84%)]\tLoss: 1579387.625000\n",
            "Train Epoch: 17 [6400/7471 (86%)]\tLoss: 1618973.750000\n",
            "Train Epoch: 17 [6560/7471 (88%)]\tLoss: 1551345.875000\n",
            "Train Epoch: 17 [6720/7471 (90%)]\tLoss: 1574192.750000\n",
            "Train Epoch: 17 [6880/7471 (92%)]\tLoss: 1643774.250000\n",
            "Train Epoch: 17 [7040/7471 (94%)]\tLoss: 1624034.750000\n",
            "Train Epoch: 17 [7200/7471 (96%)]\tLoss: 1623926.000000\n",
            "Train Epoch: 17 [7360/7471 (99%)]\tLoss: 1526145.625000\n",
            "Epoch 17 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 887177821.6206\n",
            "\n",
            "Train Epoch: 18 [160/7471 (2%)]\tLoss: 1539631.250000\n",
            "Train Epoch: 18 [320/7471 (4%)]\tLoss: 1613055.000000\n",
            "Train Epoch: 18 [480/7471 (6%)]\tLoss: 1586613.500000\n",
            "Train Epoch: 18 [640/7471 (9%)]\tLoss: 1580827.625000\n",
            "Train Epoch: 18 [800/7471 (11%)]\tLoss: 1623435.750000\n",
            "Train Epoch: 18 [960/7471 (13%)]\tLoss: 1599471.500000\n",
            "Train Epoch: 18 [1120/7471 (15%)]\tLoss: 1634222.375000\n",
            "Train Epoch: 18 [1280/7471 (17%)]\tLoss: 1600300.500000\n",
            "Train Epoch: 18 [1440/7471 (19%)]\tLoss: 1634506.375000\n",
            "Train Epoch: 18 [1600/7471 (21%)]\tLoss: 1574362.250000\n",
            "Train Epoch: 18 [1760/7471 (24%)]\tLoss: 1586152.500000\n",
            "Train Epoch: 18 [1920/7471 (26%)]\tLoss: 1628942.375000\n",
            "Train Epoch: 18 [2080/7471 (28%)]\tLoss: 1637708.250000\n",
            "Train Epoch: 18 [2240/7471 (30%)]\tLoss: 1601555.125000\n",
            "Train Epoch: 18 [2400/7471 (32%)]\tLoss: 1621317.750000\n",
            "Train Epoch: 18 [2560/7471 (34%)]\tLoss: 1622395.000000\n",
            "Train Epoch: 18 [2720/7471 (36%)]\tLoss: 1648456.500000\n",
            "Train Epoch: 18 [2880/7471 (39%)]\tLoss: 1563434.250000\n",
            "Train Epoch: 18 [3040/7471 (41%)]\tLoss: 1594250.750000\n",
            "Train Epoch: 18 [3200/7471 (43%)]\tLoss: 1602957.500000\n",
            "Train Epoch: 18 [3360/7471 (45%)]\tLoss: 1588854.750000\n",
            "Train Epoch: 18 [3520/7471 (47%)]\tLoss: 1553008.250000\n",
            "Train Epoch: 18 [3680/7471 (49%)]\tLoss: 1561944.875000\n",
            "Train Epoch: 18 [3840/7471 (51%)]\tLoss: 1601892.625000\n",
            "Train Epoch: 18 [4000/7471 (54%)]\tLoss: 1570701.125000\n",
            "Train Epoch: 18 [4160/7471 (56%)]\tLoss: 1578997.000000\n",
            "Train Epoch: 18 [4320/7471 (58%)]\tLoss: 1584730.625000\n",
            "Train Epoch: 18 [4480/7471 (60%)]\tLoss: 1610474.625000\n",
            "Train Epoch: 18 [4640/7471 (62%)]\tLoss: 1546546.000000\n",
            "Train Epoch: 18 [4800/7471 (64%)]\tLoss: 1583995.625000\n",
            "Train Epoch: 18 [4960/7471 (66%)]\tLoss: 1642224.500000\n",
            "Train Epoch: 18 [5120/7471 (69%)]\tLoss: 1540613.250000\n",
            "Train Epoch: 18 [5280/7471 (71%)]\tLoss: 1636067.000000\n",
            "Train Epoch: 18 [5440/7471 (73%)]\tLoss: 1551258.625000\n",
            "Train Epoch: 18 [5600/7471 (75%)]\tLoss: 1604381.000000\n",
            "Train Epoch: 18 [5760/7471 (77%)]\tLoss: 1611958.625000\n",
            "Train Epoch: 18 [5920/7471 (79%)]\tLoss: 1521711.250000\n",
            "Train Epoch: 18 [6080/7471 (81%)]\tLoss: 1562317.625000\n",
            "Train Epoch: 18 [6240/7471 (84%)]\tLoss: 1643885.375000\n",
            "Train Epoch: 18 [6400/7471 (86%)]\tLoss: 1602220.250000\n",
            "Train Epoch: 18 [6560/7471 (88%)]\tLoss: 1608473.750000\n",
            "Train Epoch: 18 [6720/7471 (90%)]\tLoss: 1585565.375000\n",
            "Train Epoch: 18 [6880/7471 (92%)]\tLoss: 1537963.750000\n",
            "Train Epoch: 18 [7040/7471 (94%)]\tLoss: 1620842.875000\n",
            "Train Epoch: 18 [7200/7471 (96%)]\tLoss: 1646967.875000\n",
            "Train Epoch: 18 [7360/7471 (99%)]\tLoss: 1581432.750000\n",
            "Epoch 18 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 19 [160/7471 (2%)]\tLoss: 1628224.625000\n",
            "Train Epoch: 19 [320/7471 (4%)]\tLoss: 1645523.750000\n",
            "Train Epoch: 19 [480/7471 (6%)]\tLoss: 1566593.250000\n",
            "Train Epoch: 19 [640/7471 (9%)]\tLoss: 1596934.750000\n",
            "Train Epoch: 19 [800/7471 (11%)]\tLoss: 1584490.625000\n",
            "Train Epoch: 19 [960/7471 (13%)]\tLoss: 1583898.125000\n",
            "Train Epoch: 19 [1120/7471 (15%)]\tLoss: 1518395.125000\n",
            "Train Epoch: 19 [1280/7471 (17%)]\tLoss: 1558473.500000\n",
            "Train Epoch: 19 [1440/7471 (19%)]\tLoss: 1638445.625000\n",
            "Train Epoch: 19 [1600/7471 (21%)]\tLoss: 1623271.750000\n",
            "Train Epoch: 19 [1760/7471 (24%)]\tLoss: 1552169.125000\n",
            "Train Epoch: 19 [1920/7471 (26%)]\tLoss: 1578934.000000\n",
            "Train Epoch: 19 [2080/7471 (28%)]\tLoss: 1619052.750000\n",
            "Train Epoch: 19 [2240/7471 (30%)]\tLoss: 1603074.750000\n",
            "Train Epoch: 19 [2400/7471 (32%)]\tLoss: 1532747.875000\n",
            "Train Epoch: 19 [2560/7471 (34%)]\tLoss: 1621164.125000\n",
            "Train Epoch: 19 [2720/7471 (36%)]\tLoss: 1631450.500000\n",
            "Train Epoch: 19 [2880/7471 (39%)]\tLoss: 1653047.875000\n",
            "Train Epoch: 19 [3040/7471 (41%)]\tLoss: 1585596.500000\n",
            "Train Epoch: 19 [3200/7471 (43%)]\tLoss: 1625587.125000\n",
            "Train Epoch: 19 [3360/7471 (45%)]\tLoss: 1512309.000000\n",
            "Train Epoch: 19 [3520/7471 (47%)]\tLoss: 1573247.125000\n",
            "Train Epoch: 19 [3680/7471 (49%)]\tLoss: 1540478.750000\n",
            "Train Epoch: 19 [3840/7471 (51%)]\tLoss: 1536513.500000\n",
            "Train Epoch: 19 [4000/7471 (54%)]\tLoss: 1602910.375000\n",
            "Train Epoch: 19 [4160/7471 (56%)]\tLoss: 1628293.750000\n",
            "Train Epoch: 19 [4320/7471 (58%)]\tLoss: 1613807.875000\n",
            "Train Epoch: 19 [4480/7471 (60%)]\tLoss: 1600935.125000\n",
            "Train Epoch: 19 [4640/7471 (62%)]\tLoss: 1588481.500000\n",
            "Train Epoch: 19 [4800/7471 (64%)]\tLoss: 1517018.500000\n",
            "Train Epoch: 19 [4960/7471 (66%)]\tLoss: 1575993.250000\n",
            "Train Epoch: 19 [5120/7471 (69%)]\tLoss: 1552702.500000\n",
            "Train Epoch: 19 [5280/7471 (71%)]\tLoss: 1590499.125000\n",
            "Train Epoch: 19 [5440/7471 (73%)]\tLoss: 1587610.000000\n",
            "Train Epoch: 19 [5600/7471 (75%)]\tLoss: 1588459.875000\n",
            "Train Epoch: 19 [5760/7471 (77%)]\tLoss: 1637163.625000\n",
            "Train Epoch: 19 [5920/7471 (79%)]\tLoss: 1631400.625000\n",
            "Train Epoch: 19 [6080/7471 (81%)]\tLoss: 1512339.625000\n",
            "Train Epoch: 19 [6240/7471 (84%)]\tLoss: 1616111.000000\n",
            "Train Epoch: 19 [6400/7471 (86%)]\tLoss: 1537701.500000\n",
            "Train Epoch: 19 [6560/7471 (88%)]\tLoss: 1656371.500000\n",
            "Train Epoch: 19 [6720/7471 (90%)]\tLoss: 1643817.250000\n",
            "Train Epoch: 19 [6880/7471 (92%)]\tLoss: 1605694.375000\n",
            "Train Epoch: 19 [7040/7471 (94%)]\tLoss: 1528540.125000\n",
            "Train Epoch: 19 [7200/7471 (96%)]\tLoss: 1603793.000000\n",
            "Train Epoch: 19 [7360/7471 (99%)]\tLoss: 1577584.625000\n",
            "Epoch 19 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 20 [160/7471 (2%)]\tLoss: 1552893.250000\n",
            "Train Epoch: 20 [320/7471 (4%)]\tLoss: 1567290.500000\n",
            "Train Epoch: 20 [480/7471 (6%)]\tLoss: 1578326.250000\n",
            "Train Epoch: 20 [640/7471 (9%)]\tLoss: 1493928.375000\n",
            "Train Epoch: 20 [800/7471 (11%)]\tLoss: 1505568.375000\n",
            "Train Epoch: 20 [960/7471 (13%)]\tLoss: 1589665.500000\n",
            "Train Epoch: 20 [1120/7471 (15%)]\tLoss: 1617411.750000\n",
            "Train Epoch: 20 [1280/7471 (17%)]\tLoss: 1600314.750000\n",
            "Train Epoch: 20 [1440/7471 (19%)]\tLoss: 1587745.750000\n",
            "Train Epoch: 20 [1600/7471 (21%)]\tLoss: 1501949.375000\n",
            "Train Epoch: 20 [1760/7471 (24%)]\tLoss: 1563841.625000\n",
            "Train Epoch: 20 [1920/7471 (26%)]\tLoss: 1603832.750000\n",
            "Train Epoch: 20 [2080/7471 (28%)]\tLoss: 1575353.375000\n",
            "Train Epoch: 20 [2240/7471 (30%)]\tLoss: 1621118.375000\n",
            "Train Epoch: 20 [2400/7471 (32%)]\tLoss: 1584392.125000\n",
            "Train Epoch: 20 [2560/7471 (34%)]\tLoss: 1582677.500000\n",
            "Train Epoch: 20 [2720/7471 (36%)]\tLoss: 1561243.125000\n",
            "Train Epoch: 20 [2880/7471 (39%)]\tLoss: 1644242.625000\n",
            "Train Epoch: 20 [3040/7471 (41%)]\tLoss: 1603049.000000\n",
            "Train Epoch: 20 [3200/7471 (43%)]\tLoss: 1602871.250000\n",
            "Train Epoch: 20 [3360/7471 (45%)]\tLoss: 1535378.875000\n",
            "Train Epoch: 20 [3520/7471 (47%)]\tLoss: 1592240.750000\n",
            "Train Epoch: 20 [3680/7471 (49%)]\tLoss: 1592192.875000\n",
            "Train Epoch: 20 [3840/7471 (51%)]\tLoss: 1560225.250000\n",
            "Train Epoch: 20 [4000/7471 (54%)]\tLoss: 1587504.250000\n",
            "Train Epoch: 20 [4160/7471 (56%)]\tLoss: 1606767.875000\n",
            "Train Epoch: 20 [4320/7471 (58%)]\tLoss: 1586053.375000\n",
            "Train Epoch: 20 [4480/7471 (60%)]\tLoss: 1594025.125000\n",
            "Train Epoch: 20 [4640/7471 (62%)]\tLoss: 1596116.250000\n",
            "Train Epoch: 20 [4800/7471 (64%)]\tLoss: 1599094.000000\n",
            "Train Epoch: 20 [4960/7471 (66%)]\tLoss: 1588759.250000\n",
            "Train Epoch: 20 [5120/7471 (69%)]\tLoss: 1543024.125000\n",
            "Train Epoch: 20 [5280/7471 (71%)]\tLoss: 1638541.500000\n",
            "Train Epoch: 20 [5440/7471 (73%)]\tLoss: 1606221.875000\n",
            "Train Epoch: 20 [5600/7471 (75%)]\tLoss: 1604590.625000\n",
            "Train Epoch: 20 [5760/7471 (77%)]\tLoss: 1568559.250000\n",
            "Train Epoch: 20 [5920/7471 (79%)]\tLoss: 1585831.625000\n",
            "Train Epoch: 20 [6080/7471 (81%)]\tLoss: 1593804.875000\n",
            "Train Epoch: 20 [6240/7471 (84%)]\tLoss: 1638786.125000\n",
            "Train Epoch: 20 [6400/7471 (86%)]\tLoss: 1535964.875000\n",
            "Train Epoch: 20 [6560/7471 (88%)]\tLoss: 1588285.250000\n",
            "Train Epoch: 20 [6720/7471 (90%)]\tLoss: 1607909.750000\n",
            "Train Epoch: 20 [6880/7471 (92%)]\tLoss: 1610391.000000\n",
            "Train Epoch: 20 [7040/7471 (94%)]\tLoss: 1593081.125000\n",
            "Train Epoch: 20 [7200/7471 (96%)]\tLoss: 1581783.125000\n",
            "Train Epoch: 20 [7360/7471 (99%)]\tLoss: 1521135.500000\n",
            "Epoch 20 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCneJB6XkQ5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}