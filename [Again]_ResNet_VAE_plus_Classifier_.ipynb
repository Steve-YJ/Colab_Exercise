{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Again] ResNet-VAE_plus_Classifier .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPvdofY2a1MsDOm19vvCFcj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "16e74f9162ad44369010bcf3a114355f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_efad3e1d5aad43639afdb258068236fb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_828a0366f4a34fd7ae4be732e8ff35d3",
              "IPY_MODEL_4119c7505bd94c558be6659448b1a57e"
            ]
          }
        },
        "efad3e1d5aad43639afdb258068236fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "828a0366f4a34fd7ae4be732e8ff35d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_41e426129d004a18ac030e470b1ee46f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_de2be11e7d70455b93c26efab6ef933d"
          }
        },
        "4119c7505bd94c558be6659448b1a57e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3a443c54a8b7456889f7e81bb8c7db7b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [00:02&lt;00:00, 95.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd16ea05f12443e5b050b946340147a7"
          }
        },
        "41e426129d004a18ac030e470b1ee46f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "de2be11e7d70455b93c26efab6ef933d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a443c54a8b7456889f7e81bb8c7db7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd16ea05f12443e5b050b946340147a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Steve-YJ/Colab_Exercise/blob/master/%5BAgain%5D_ResNet_VAE_plus_Classifier_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyu1S97RFUTe",
        "colab_type": "text"
      },
      "source": [
        "# README.ME\n",
        "* From <code>[Again] Re-Start_Training_ResNet-VAE .ipynb</code>\n",
        "* Add CNN-based Clasifier...!!\n",
        "* LEGO~\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHP0jjvZFhfQ",
        "colab_type": "text"
      },
      "source": [
        "✅ Check Point<br>\n",
        "> 1. Add CNN Classifier\n",
        "> 2. Classify Model\n",
        "\n",
        "<br>\n",
        "<code>::start:: 20.09.15.Tue pm4:10 ~ </code><br>\n",
        "<code>::Continue:: ~</code><br>\n",
        "<code>::Add:</code><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC9TFykmnfUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1b203aa6-f92e-4f3b-b79a-7b0fcff70da1"
      },
      "source": [
        "\"\"\"\n",
        "Can Do so many things... like,\n",
        "1) Just Train CNN to our 3-channel Data\n",
        "2) User pre-trained ResNet-VAE Encoder to Classifiy Malware Families\n",
        "3) Extend Our Research to more...!\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nCan Do so many things... like,\\n1) Just Train CNN to our 3-channel Data\\n2) User pre-trained ResNet-VAE Encoder to Classifiy Malware Families\\n3) Extend Our Research to more...!\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7_BuKPEYXfH",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "< Questions ><br>\n",
        "Q1.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vigexrljzsdN",
        "colab_type": "text"
      },
      "source": [
        "*  Reference: https://www.programcreek.com/python/example/108010/torchvision.models.resnet152"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5L94y80VeP1",
        "colab_type": "text"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGODey7EFOAI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ed95a125-690d-4ca2-cec1-e9ffe82e2a15"
      },
      "source": [
        "# Mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZttVYB7WlCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuAGvGTZWwIK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "outputId": "44e95e54-89a0-4b86-8c5b-4a40a09ba798"
      },
      "source": [
        "%cd drive/My\\ Drive/Post_InfoSec_Exps/ResNet-VAE/ResNetVAE-master.zip (Unzipped Files)/ResNetVAE-master\n",
        "! ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Post_InfoSec_Exps/ResNet-VAE/ResNetVAE-master.zip (Unzipped Files)/ResNetVAE-master\n",
            "'01.Tutorial-ResNet-VAE.ipynb의 사본'\n",
            " 01.Tutorial-ResNet-VAE-Recon.ipynb\n",
            " 02.Tutorial-ResNet-VAE-Tunning.ipynb\n",
            " 03.Tutorial-ResNet-VAE-Tunning.ipynb\n",
            "'03.Tutorial-ResNet-VAE-Tunning.ipynb의 사본'\n",
            "'04.Post-01.Tutorial-ResNet-VAE.ipynb사본의 사본'\n",
            "'05.Post-01.Tutorial-ResNet-VAE_Train_Again.ipynb의 사본'\n",
            " 19train_val_plot.png\n",
            " 39train_val_plot.png\n",
            " 59train_val_plot.png\n",
            " Again_ResNet-VAE_Exp01\n",
            "'[Again] ResNet-VAE_plus_Classifier .ipynb'\n",
            "'[Again] Re-Start_Training_ResNet-VAE .ipynb'\n",
            " fig\n",
            " generated_Malimg.png\n",
            " modules.py\n",
            " plot_latent.ipynb\n",
            " plot_latent_vector\n",
            " plot_train_test_loss\n",
            "'[Post_Exp]04-2.ResNet-VAE_Train_Again.ipynb'\n",
            "'[Post_Exp]04-2.ResNet-VAE_Train_Again.ipynb의 사본'\n",
            "'[Post_Exp]04-3.ResNet-VAE_Train_Again.ipynb'\n",
            "'[Post_Exp]04-3.ResNet-VAE_Train_Again.ipynb의 사본'\n",
            "'[Post_Exp]05-2.ResNet-VAE_Reduce_lr.ipynb'\n",
            " __pycache__\n",
            " README.md\n",
            " recon_sampling\n",
            " reconstruction_Malimg.png\n",
            " ResNetVAE_cifar10.py\n",
            " ResNetVAE_FACE.py\n",
            " ResNetVAE_MNIST.py\n",
            " ResNetVAE_reconstruction.ipynb\n",
            " results_Malimg\n",
            " results_Malimg_Exp4\n",
            " results_Malimg_Exp4_3\n",
            " results_ResNet-VAE_Exp01\n",
            " train_test_loss_plot.png\n",
            " train_val_plot.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YmzTtwbW7nl",
        "colab_type": "text"
      },
      "source": [
        " our working directory results should be saved in 'Again_ResNet-VAE_Exp01'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObxrqpqMVg-x",
        "colab_type": "text"
      },
      "source": [
        "## 01. Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LO1BCfgVh9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "304b72ba-5334-4247-e343-ca6ce3a9fe6c"
      },
      "source": [
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# save single numpy array\n",
        "from tempfile import TemporaryFile\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import torch\n",
        "import torch.utils.data  # torch.utils.data\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torchvision \n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# load modules\n",
        "from torchvision import models\n",
        "from modules import *"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG2NjnRqVi7s",
        "colab_type": "text"
      },
      "source": [
        "## 02. Data Preparation\n",
        "\n",
        "* Make Custom Dataset\n",
        "* Make Custom DataLoader\n",
        "* 📌 edit Train_Val_Test Split\n",
        "    * From <code>Train_Test Split</code> => <code>Train_Val_Test Split</code> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtOGbYZ9ZcaG",
        "colab_type": "text"
      },
      "source": [
        "### 📍 < Notice ><br>\n",
        "* ResNet-VAE 모델을 훈련시킬 때는 데이터셋을 train, test만 준비한다.(Validation set 없이!)<br>\n",
        "이는 일반적인 VAE Model Tutorial의 학습방식을 따른다. <code>-20.09.08.Tue. pm3:00-</code>\n",
        "---\n",
        "* ResNet-VAE Model 이후 Classifier 학습\n",
        "    * Malware Family Claassification을 위해 Dataset을 다시 구성한다.\n",
        "    * Train, Test Set이외에도 Validation Set을 구성할 수 있도록 하자...!\n",
        "    * Reference: Stand-DL  <code>-20.09.15.Tue-</code>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAIvvzgqVlnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor()])  # Composes several transforms together.\n",
        "\n",
        "# make custom dataset\n",
        "trainset = torchvision.datasets.ImageFolder(root='../../../../InformationSecurity_Summer/malimg',\n",
        "                                            transform=transforms)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O6Dfsmfbeoa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "07fac289-ed26-4d59-c06d-bc769c4ee7df"
      },
      "source": [
        "classes = trainset.classes\n",
        "classes"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Adialer.C',\n",
              " 'Agent.FYI',\n",
              " 'Allaple.A',\n",
              " 'Allaple.L',\n",
              " 'Alueron.gen!J',\n",
              " 'Autorun.K',\n",
              " 'C2LOP.P',\n",
              " 'C2LOP.gen!g',\n",
              " 'Dialplatform.B',\n",
              " 'Dontovo.A',\n",
              " 'Fakerean',\n",
              " 'Instantaccess',\n",
              " 'Lolyda.AA1',\n",
              " 'Lolyda.AA2',\n",
              " 'Lolyda.AA3',\n",
              " 'Lolyda.AT',\n",
              " 'Malex.gen!J',\n",
              " 'Obfuscator.AD',\n",
              " 'Rbot!gen',\n",
              " 'Skintrim.N',\n",
              " 'Swizzor.gen!E',\n",
              " 'Swizzor.gen!I',\n",
              " 'VB.AT',\n",
              " 'Wintrim.BX',\n",
              " 'Yuner.A']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPMemVTjbkPX",
        "colab_type": "text"
      },
      "source": [
        "Split Train Data to Train, Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z84JNpgWboc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "79be5270-8d84-4045-fb2c-006a1e17e4b1"
      },
      "source": [
        "full_dataset = trainset\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = int(0.1 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "print('print train_size, val_size, test_size: ', train_size, val_size, test_size)\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])  # Randomly split a dataset into non-overlapping new datasets of given lengths. Optionally fix the generator for reproducible results, e.g.:\n",
        "print('print train_dataset, val_dataset, test_dataset: ', len(train_dataset), len(val_dataset), len(test_dataset))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "print train_size, val_size, test_size:  7471 933 935\n",
            "print train_dataset, val_dataset, test_dataset:  7471 933 935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bmFNAWlbjpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=16,  # 16 to args.batch_size\n",
        "                                           shuffle=True,\n",
        "                                           pin_memory=True)\n",
        "valid_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                           batch_size=16,\n",
        "                                           shuffle=True,\n",
        "                                           pin_memory=True)  # (i.e., setting pin_memory=True)\n",
        "                                                             #  which enables fast data transfer to CUDA-enabled GPUs\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                           batch_size=16,\n",
        "                                           shuffle=True,\n",
        "                                           pin_memory=True) "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9eC3rSSdeu_",
        "colab_type": "text"
      },
      "source": [
        "3-channel Image 출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjS2jwTLdgds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    np_img = img.numpy()\n",
        "\n",
        "    plt.imshow(np.transpose(np_img, (1, 2, 0)))  # Convert (C, W, H) to (W, H, C)\n",
        "\n",
        "    print(np_img.shape)  # np_img shape\n",
        "    print((np.transpose(np_img, (1, 2, 0))).shape)  # transposed shape "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh144ur6dx9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a161ac90-ea62-41a4-d660-9b1ffc193b7f"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "print(labels)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10,  2,  8,  5,  2,  3,  4,  4,  2,  3, 17,  2, 18,  2, 24, 24])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-McJWBIRdyO8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "873c3abe-99ec-4e0b-c9f2-91c55609584b"
      },
      "source": [
        "print(images.shape)\n",
        "imshow(torchvision.utils.make_grid(images, nrow=4))\n",
        "print(images.shape)\n",
        "print((torchvision.utils.make_grid(images)).shape)\n",
        "print(\"\".join(\"%5s \"%classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 3, 224, 224])\n",
            "(3, 906, 906)\n",
            "(906, 906, 3)\n",
            "torch.Size([16, 3, 224, 224])\n",
            "torch.Size([3, 454, 1810])\n",
            "Fakerean Allaple.A Dialplatform.B Autorun.K \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGrfMu6cx0Hw",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVH33sgwxyxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test ResNet Classifier to Malimg 3-Channel Data"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pOQCqNosbon",
        "colab_type": "text"
      },
      "source": [
        "## 03. Model Architecture\n",
        "* <code>pre-trained ResNet-152</code>\n",
        "* <code>Transfer Learning</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6IF7Bdyx7HU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reference: https://www.programcreek.com/python/example/108010/torchvision.models.resnet152\n",
        "\n",
        "def get_pretrained_resnet(new_fc_dim=None):\n",
        "    \"\"\"\n",
        "    Fetches a pretrained resnet model (downloading if necessary) and chops off the top linear\n",
        "    layer. If new_fc_dim isn't None, then a new linear layer is added.\n",
        "    :param new_fc_dim: \n",
        "    :return: \n",
        "    \"\"\"\n",
        "\n",
        "    # resnet152 = models.resnet152(pretrained=True, progress=True)\n",
        "    resnet152 = models.resnet152(pretrained=True)\n",
        "    # del resnet152.fc\n",
        "\n",
        "    # num_ftrs\n",
        "    # Reference: https://tutorials.pytorch.kr/beginner/transfer_learning_tutorial.html\n",
        "    num_ftrs = resnet152.fc.in_features\n",
        "\n",
        "    if new_fc_dim is not None:\n",
        "        # resnet152.fc = nn.Linear(ENCODING_SIZE, new_fc_dim)\n",
        "        \n",
        "        resnet152.fc = nn.Linear(num_ftrs, new_fc_dim)\n",
        "\n",
        "        # _init_fc(resnet152.fc)\n",
        "    else:\n",
        "        \n",
        "        resnet152.fc = lambda x:x\n",
        "\n",
        "    return resnet152"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id_BKtZuyj3W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "16e74f9162ad44369010bcf3a114355f",
            "efad3e1d5aad43639afdb258068236fb",
            "828a0366f4a34fd7ae4be732e8ff35d3",
            "4119c7505bd94c558be6659448b1a57e",
            "41e426129d004a18ac030e470b1ee46f",
            "de2be11e7d70455b93c26efab6ef933d",
            "3a443c54a8b7456889f7e81bb8c7db7b",
            "dd16ea05f12443e5b050b946340147a7"
          ]
        },
        "outputId": "b710305f-7e89-48f4-9e5c-bc1e19005296"
      },
      "source": [
        "resnet152 = get_pretrained_resnet(25)  # load pre-trained ResNet152\n",
        "resnet152"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16e74f9162ad44369010bcf3a114355f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (6): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (7): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (6): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (7): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (8): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (9): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (10): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (11): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (12): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (13): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (14): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (15): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (16): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (17): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (18): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (19): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (20): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (21): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (22): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (23): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (24): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (25): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (26): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (27): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (28): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (29): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (30): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (31): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (32): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (33): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (34): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (35): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=25, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh0dA2eMBT-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet152 = resnet152.to(device = ('cuda:0' if torch.cuda.is_available() else 'cpu'))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(resnet152.parameters(), lr=0.001)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81_9gs2rs28K",
        "colab_type": "text"
      },
      "source": [
        "* Change\n",
        "    * 기존 함수 코드에서 실행 코드로 변경해보자...!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVy4vK1HtSAW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a964eaa-ed45-4be0-96e8-6c857aba7a79"
      },
      "source": [
        "import argparse\n",
        "\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "args.net = resnet152\n",
        "args.criterion = criterion\n",
        "args.optim = optimizer\n",
        "\n",
        "args.train_loader = train_loader\n",
        "args.val_loader = valid_loader\n",
        "args.test_loader = test_loader\n",
        "\n",
        "# args.n_layer = 5\n",
        "# args.in_dim = 3072\n",
        "# args.out_dim = 10\n",
        "# args.hid_dim = 100\n",
        "# args.act = 'relu'\n",
        "\n",
        "args.lr = 0.001\n",
        "args.mm = 0.9\n",
        "args.epoch = 100\n",
        "\n",
        "\n",
        "print(args)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(criterion=CrossEntropyLoss(), epoch=100, lr=0.001, mm=0.9, net=ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (7): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (7): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (12): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (13): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (14): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (15): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (16): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (17): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (18): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (19): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (20): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (21): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (22): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (23): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (24): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (25): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (26): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (27): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (28): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (29): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (30): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (31): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (32): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (33): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (34): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (35): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=25, bias=True)\n",
            "), optim=Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            "), test_loader=<torch.utils.data.dataloader.DataLoader object at 0x7f4022ef6748>, train_loader=<torch.utils.data.dataloader.DataLoader object at 0x7f4022ef66a0>, val_loader=<torch.utils.data.dataloader.DataLoader object at 0x7f4022ef6668>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNKlBGN6_jhY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "797891c8-e376-4d77-ad28-b63f488f7d94"
      },
      "source": [
        "# Reference: https://github.com/Steve-YJ/Exp-Standalone-DeepLearning/blob/master/%5BPractice%5D_Cifar10.ipynb\n",
        "\n",
        "net = args.net\n",
        "# criterion = criterion\n",
        "# optimizer = optimizer\n",
        "\n",
        "list_epoch = []\n",
        "list_train_loss = []\n",
        "list_val_loss = []\n",
        "list_test_acc = []\n",
        "list_acc_epoch = []\n",
        "\n",
        "for epoch in range(args.epoch):  # loop over the dataset multiple itmes\n",
        "\n",
        "    # ===== Train ===== #\n",
        "    net.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(args.train_loader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)  # input 값의 shape이 맞는지 확인을 했는가? -20.09.16.Wed- am11:10...\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        train_loss += loss.item()\n",
        "        if i % 2 == 0:  # print every 2000 mini-batches => print every 2 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch+1, i+1, running_loss / 2))\n",
        "            running_loss = 0.0\n",
        "    # append list_values\n",
        "    list_epoch.append(epoch)\n",
        "    list_train_loss.append(train_loss)\n",
        "    \n",
        "    # save Pytorch models of best record\n",
        "    torch.save(net.state_dict(), os.path.join(save_model_path, 'model_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
        "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
        "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
        "    # ===== Validation ===== #\n",
        "    net.eval()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in args.val_loader:\n",
        "            images, labels = data\n",
        "            # images = images.view(-1, 3072)\n",
        "\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            '''\n",
        "            What this code mean?\n",
        "            '''\n",
        "            _, predicted = torch.max(outputs.data, 1)  # return: values, indices\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(args.val_loader)\n",
        "        val_acc = 100 * correct / total\n",
        "    list_val_loss.append(val_loss)\n",
        "    print('Epoch {}, Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(epoch, train_loss, val_loss, val_acc))\n",
        "    \n",
        "    # ===== Evaluation ===== #\n",
        "    net.eval()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in args.test_loader:\n",
        "            images, labels = data\n",
        "            # images = images.view(-1, 3072)\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "        print('Epoch {}, Test Acc: {}'.format(epoch, test_acc))\n",
        "    list_test_acc.append(test_acc)\n",
        "    list_acc_epoch.append(epoch)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,     1] loss: 1.846\n",
            "[1,     3] loss: 5.138\n",
            "[1,     5] loss: 5.744\n",
            "[1,     7] loss: 3.502\n",
            "[1,     9] loss: 6.176\n",
            "[1,    11] loss: 6.250\n",
            "[1,    13] loss: 4.252\n",
            "[1,    15] loss: 4.221\n",
            "[1,    17] loss: 3.454\n",
            "[1,    19] loss: 4.350\n",
            "[1,    21] loss: 4.265\n",
            "[1,    23] loss: 3.951\n",
            "[1,    25] loss: 4.156\n",
            "[1,    27] loss: 3.064\n",
            "[1,    29] loss: 5.467\n",
            "[1,    31] loss: 3.321\n",
            "[1,    33] loss: 3.684\n",
            "[1,    35] loss: 4.343\n",
            "[1,    37] loss: 4.761\n",
            "[1,    39] loss: 4.017\n",
            "[1,    41] loss: 3.861\n",
            "[1,    43] loss: 4.865\n",
            "[1,    45] loss: 2.955\n",
            "[1,    47] loss: 4.961\n",
            "[1,    49] loss: 2.848\n",
            "[1,    51] loss: 3.282\n",
            "[1,    53] loss: 2.444\n",
            "[1,    55] loss: 3.791\n",
            "[1,    57] loss: 3.919\n",
            "[1,    59] loss: 2.720\n",
            "[1,    61] loss: 3.606\n",
            "[1,    63] loss: 3.067\n",
            "[1,    65] loss: 3.173\n",
            "[1,    67] loss: 2.712\n",
            "[1,    69] loss: 3.609\n",
            "[1,    71] loss: 3.005\n",
            "[1,    73] loss: 3.107\n",
            "[1,    75] loss: 3.267\n",
            "[1,    77] loss: 2.286\n",
            "[1,    79] loss: 2.959\n",
            "[1,    81] loss: 2.647\n",
            "[1,    83] loss: 2.278\n",
            "[1,    85] loss: 2.118\n",
            "[1,    87] loss: 2.397\n",
            "[1,    89] loss: 2.478\n",
            "[1,    91] loss: 3.311\n",
            "[1,    93] loss: 2.695\n",
            "[1,    95] loss: 2.896\n",
            "[1,    97] loss: 3.310\n",
            "[1,    99] loss: 3.087\n",
            "[1,   101] loss: 2.852\n",
            "[1,   103] loss: 3.398\n",
            "[1,   105] loss: 3.833\n",
            "[1,   107] loss: 3.918\n",
            "[1,   109] loss: 3.848\n",
            "[1,   111] loss: 3.644\n",
            "[1,   113] loss: 2.646\n",
            "[1,   115] loss: 3.095\n",
            "[1,   117] loss: 2.640\n",
            "[1,   119] loss: 3.160\n",
            "[1,   121] loss: 3.434\n",
            "[1,   123] loss: 2.831\n",
            "[1,   125] loss: 3.372\n",
            "[1,   127] loss: 2.628\n",
            "[1,   129] loss: 2.307\n",
            "[1,   131] loss: 3.051\n",
            "[1,   133] loss: 2.824\n",
            "[1,   135] loss: 3.301\n",
            "[1,   137] loss: 2.547\n",
            "[1,   139] loss: 2.578\n",
            "[1,   141] loss: 3.037\n",
            "[1,   143] loss: 2.398\n",
            "[1,   145] loss: 2.499\n",
            "[1,   147] loss: 2.549\n",
            "[1,   149] loss: 2.704\n",
            "[1,   151] loss: 3.528\n",
            "[1,   153] loss: 2.435\n",
            "[1,   155] loss: 2.785\n",
            "[1,   157] loss: 2.836\n",
            "[1,   159] loss: 2.606\n",
            "[1,   161] loss: 2.718\n",
            "[1,   163] loss: 3.227\n",
            "[1,   165] loss: 2.712\n",
            "[1,   167] loss: 2.713\n",
            "[1,   169] loss: 2.695\n",
            "[1,   171] loss: 2.941\n",
            "[1,   173] loss: 2.514\n",
            "[1,   175] loss: 2.728\n",
            "[1,   177] loss: 2.736\n",
            "[1,   179] loss: 2.684\n",
            "[1,   181] loss: 2.844\n",
            "[1,   183] loss: 2.661\n",
            "[1,   185] loss: 3.206\n",
            "[1,   187] loss: 2.675\n",
            "[1,   189] loss: 2.484\n",
            "[1,   191] loss: 3.125\n",
            "[1,   193] loss: 2.726\n",
            "[1,   195] loss: 2.680\n",
            "[1,   197] loss: 2.890\n",
            "[1,   199] loss: 2.520\n",
            "[1,   201] loss: 2.450\n",
            "[1,   203] loss: 2.278\n",
            "[1,   205] loss: 2.576\n",
            "[1,   207] loss: 2.822\n",
            "[1,   209] loss: 2.647\n",
            "[1,   211] loss: 2.879\n",
            "[1,   213] loss: 2.533\n",
            "[1,   215] loss: 2.506\n",
            "[1,   217] loss: 2.839\n",
            "[1,   219] loss: 3.064\n",
            "[1,   221] loss: 2.463\n",
            "[1,   223] loss: 3.227\n",
            "[1,   225] loss: 2.693\n",
            "[1,   227] loss: 2.867\n",
            "[1,   229] loss: 2.552\n",
            "[1,   231] loss: 3.222\n",
            "[1,   233] loss: 3.062\n",
            "[1,   235] loss: 2.338\n",
            "[1,   237] loss: 2.902\n",
            "[1,   239] loss: 2.498\n",
            "[1,   241] loss: 2.522\n",
            "[1,   243] loss: 2.658\n",
            "[1,   245] loss: 2.542\n",
            "[1,   247] loss: 3.148\n",
            "[1,   249] loss: 2.342\n",
            "[1,   251] loss: 2.240\n",
            "[1,   253] loss: 2.501\n",
            "[1,   255] loss: 2.652\n",
            "[1,   257] loss: 2.686\n",
            "[1,   259] loss: 2.245\n",
            "[1,   261] loss: 2.361\n",
            "[1,   263] loss: 2.465\n",
            "[1,   265] loss: 2.370\n",
            "[1,   267] loss: 2.325\n",
            "[1,   269] loss: 2.006\n",
            "[1,   271] loss: 2.159\n",
            "[1,   273] loss: 2.785\n",
            "[1,   275] loss: 2.788\n",
            "[1,   277] loss: 1.469\n",
            "[1,   279] loss: 2.330\n",
            "[1,   281] loss: 2.429\n",
            "[1,   283] loss: 2.589\n",
            "[1,   285] loss: 1.737\n",
            "[1,   287] loss: 2.498\n",
            "[1,   289] loss: 2.220\n",
            "[1,   291] loss: 2.495\n",
            "[1,   293] loss: 2.433\n",
            "[1,   295] loss: 1.925\n",
            "[1,   297] loss: 2.224\n",
            "[1,   299] loss: 1.948\n",
            "[1,   301] loss: 2.625\n",
            "[1,   303] loss: 2.294\n",
            "[1,   305] loss: 2.525\n",
            "[1,   307] loss: 2.847\n",
            "[1,   309] loss: 2.195\n",
            "[1,   311] loss: 2.664\n",
            "[1,   313] loss: 2.468\n",
            "[1,   315] loss: 2.959\n",
            "[1,   317] loss: 2.444\n",
            "[1,   319] loss: 2.077\n",
            "[1,   321] loss: 2.454\n",
            "[1,   323] loss: 2.917\n",
            "[1,   325] loss: 2.855\n",
            "[1,   327] loss: 2.057\n",
            "[1,   329] loss: 2.296\n",
            "[1,   331] loss: 2.403\n",
            "[1,   333] loss: 2.549\n",
            "[1,   335] loss: 2.465\n",
            "[1,   337] loss: 2.179\n",
            "[1,   339] loss: 2.046\n",
            "[1,   341] loss: 2.505\n",
            "[1,   343] loss: 2.461\n",
            "[1,   345] loss: 2.826\n",
            "[1,   347] loss: 2.286\n",
            "[1,   349] loss: 2.417\n",
            "[1,   351] loss: 2.355\n",
            "[1,   353] loss: 1.838\n",
            "[1,   355] loss: 2.044\n",
            "[1,   357] loss: 1.963\n",
            "[1,   359] loss: 2.432\n",
            "[1,   361] loss: 2.345\n",
            "[1,   363] loss: 3.167\n",
            "[1,   365] loss: 2.742\n",
            "[1,   367] loss: 2.728\n",
            "[1,   369] loss: 2.402\n",
            "[1,   371] loss: 2.019\n",
            "[1,   373] loss: 2.533\n",
            "[1,   375] loss: 2.684\n",
            "[1,   377] loss: 2.853\n",
            "[1,   379] loss: 3.566\n",
            "[1,   381] loss: 2.394\n",
            "[1,   383] loss: 2.368\n",
            "[1,   385] loss: 2.384\n",
            "[1,   387] loss: 2.131\n",
            "[1,   389] loss: 2.154\n",
            "[1,   391] loss: 2.109\n",
            "[1,   393] loss: 2.297\n",
            "[1,   395] loss: 2.604\n",
            "[1,   397] loss: 2.046\n",
            "[1,   399] loss: 2.344\n",
            "[1,   401] loss: 2.192\n",
            "[1,   403] loss: 2.567\n",
            "[1,   405] loss: 1.890\n",
            "[1,   407] loss: 2.685\n",
            "[1,   409] loss: 1.922\n",
            "[1,   411] loss: 2.519\n",
            "[1,   413] loss: 2.492\n",
            "[1,   415] loss: 2.704\n",
            "[1,   417] loss: 2.537\n",
            "[1,   419] loss: 2.099\n",
            "[1,   421] loss: 2.476\n",
            "[1,   423] loss: 3.006\n",
            "[1,   425] loss: 2.662\n",
            "[1,   427] loss: 2.493\n",
            "[1,   429] loss: 2.256\n",
            "[1,   431] loss: 2.006\n",
            "[1,   433] loss: 2.394\n",
            "[1,   435] loss: 2.787\n",
            "[1,   437] loss: 3.053\n",
            "[1,   439] loss: 2.436\n",
            "[1,   441] loss: 1.767\n",
            "[1,   443] loss: 2.538\n",
            "[1,   445] loss: 2.423\n",
            "[1,   447] loss: 2.514\n",
            "[1,   449] loss: 1.633\n",
            "[1,   451] loss: 2.175\n",
            "[1,   453] loss: 2.016\n",
            "[1,   455] loss: 2.838\n",
            "[1,   457] loss: 2.301\n",
            "[1,   459] loss: 1.675\n",
            "[1,   461] loss: 2.109\n",
            "[1,   463] loss: 2.166\n",
            "[1,   465] loss: 2.493\n",
            "[1,   467] loss: 2.610\n",
            "Epoch 0, Train Loss: 1305.2712079286575, Val Loss: 11.63752614239515, Val Acc: 27.224008574490888\n",
            "Epoch 0, Test Acc: 25.668449197860962\n",
            "[2,     1] loss: 1.534\n",
            "[2,     3] loss: 2.587\n",
            "[2,     5] loss: 2.849\n",
            "[2,     7] loss: 2.227\n",
            "[2,     9] loss: 2.220\n",
            "[2,    11] loss: 3.075\n",
            "[2,    13] loss: 1.935\n",
            "[2,    15] loss: 2.148\n",
            "[2,    17] loss: 2.033\n",
            "[2,    19] loss: 2.560\n",
            "[2,    21] loss: 2.789\n",
            "[2,    23] loss: 1.771\n",
            "[2,    25] loss: 1.970\n",
            "[2,    27] loss: 2.849\n",
            "[2,    29] loss: 2.966\n",
            "[2,    31] loss: 2.379\n",
            "[2,    33] loss: 1.996\n",
            "[2,    35] loss: 2.436\n",
            "[2,    37] loss: 2.175\n",
            "[2,    39] loss: 2.456\n",
            "[2,    41] loss: 1.960\n",
            "[2,    43] loss: 2.456\n",
            "[2,    45] loss: 1.638\n",
            "[2,    47] loss: 2.143\n",
            "[2,    49] loss: 2.630\n",
            "[2,    51] loss: 1.854\n",
            "[2,    53] loss: 1.723\n",
            "[2,    55] loss: 1.704\n",
            "[2,    57] loss: 1.746\n",
            "[2,    59] loss: 2.008\n",
            "[2,    61] loss: 2.787\n",
            "[2,    63] loss: 2.296\n",
            "[2,    65] loss: 2.349\n",
            "[2,    67] loss: 2.236\n",
            "[2,    69] loss: 2.824\n",
            "[2,    71] loss: 2.215\n",
            "[2,    73] loss: 2.172\n",
            "[2,    75] loss: 2.403\n",
            "[2,    77] loss: 2.495\n",
            "[2,    79] loss: 2.969\n",
            "[2,    81] loss: 2.335\n",
            "[2,    83] loss: 2.680\n",
            "[2,    85] loss: 2.288\n",
            "[2,    87] loss: 2.257\n",
            "[2,    89] loss: 2.580\n",
            "[2,    91] loss: 2.137\n",
            "[2,    93] loss: 2.860\n",
            "[2,    95] loss: 2.409\n",
            "[2,    97] loss: 1.837\n",
            "[2,    99] loss: 2.198\n",
            "[2,   101] loss: 2.194\n",
            "[2,   103] loss: 2.367\n",
            "[2,   105] loss: 2.135\n",
            "[2,   107] loss: 2.140\n",
            "[2,   109] loss: 2.269\n",
            "[2,   111] loss: 2.552\n",
            "[2,   113] loss: 1.855\n",
            "[2,   115] loss: 2.350\n",
            "[2,   117] loss: 2.186\n",
            "[2,   119] loss: 2.048\n",
            "[2,   121] loss: 2.155\n",
            "[2,   123] loss: 2.129\n",
            "[2,   125] loss: 2.030\n",
            "[2,   127] loss: 1.647\n",
            "[2,   129] loss: 2.300\n",
            "[2,   131] loss: 2.098\n",
            "[2,   133] loss: 2.284\n",
            "[2,   135] loss: 1.391\n",
            "[2,   137] loss: 2.186\n",
            "[2,   139] loss: 2.162\n",
            "[2,   141] loss: 2.314\n",
            "[2,   143] loss: 1.642\n",
            "[2,   145] loss: 2.302\n",
            "[2,   147] loss: 2.651\n",
            "[2,   149] loss: 3.031\n",
            "[2,   151] loss: 2.198\n",
            "[2,   153] loss: 1.811\n",
            "[2,   155] loss: 2.435\n",
            "[2,   157] loss: 1.941\n",
            "[2,   159] loss: 2.855\n",
            "[2,   161] loss: 1.884\n",
            "[2,   163] loss: 3.151\n",
            "[2,   165] loss: 2.366\n",
            "[2,   167] loss: 2.238\n",
            "[2,   169] loss: 3.004\n",
            "[2,   171] loss: 2.613\n",
            "[2,   173] loss: 1.821\n",
            "[2,   175] loss: 2.275\n",
            "[2,   177] loss: 1.784\n",
            "[2,   179] loss: 1.707\n",
            "[2,   181] loss: 3.428\n",
            "[2,   183] loss: 2.939\n",
            "[2,   185] loss: 1.799\n",
            "[2,   187] loss: 2.055\n",
            "[2,   189] loss: 2.157\n",
            "[2,   191] loss: 3.494\n",
            "[2,   193] loss: 2.767\n",
            "[2,   195] loss: 2.201\n",
            "[2,   197] loss: 1.472\n",
            "[2,   199] loss: 1.590\n",
            "[2,   201] loss: 1.806\n",
            "[2,   203] loss: 1.732\n",
            "[2,   205] loss: 1.928\n",
            "[2,   207] loss: 2.471\n",
            "[2,   209] loss: 2.846\n",
            "[2,   211] loss: 2.438\n",
            "[2,   213] loss: 2.843\n",
            "[2,   215] loss: 1.908\n",
            "[2,   217] loss: 1.578\n",
            "[2,   219] loss: 1.840\n",
            "[2,   221] loss: 2.821\n",
            "[2,   223] loss: 2.795\n",
            "[2,   225] loss: 1.628\n",
            "[2,   227] loss: 1.430\n",
            "[2,   229] loss: 1.447\n",
            "[2,   231] loss: 1.667\n",
            "[2,   233] loss: 1.750\n",
            "[2,   235] loss: 1.597\n",
            "[2,   237] loss: 2.040\n",
            "[2,   239] loss: 2.343\n",
            "[2,   241] loss: 1.615\n",
            "[2,   243] loss: 1.875\n",
            "[2,   245] loss: 1.657\n",
            "[2,   247] loss: 2.017\n",
            "[2,   249] loss: 1.605\n",
            "[2,   251] loss: 2.067\n",
            "[2,   253] loss: 1.607\n",
            "[2,   255] loss: 1.567\n",
            "[2,   257] loss: 1.715\n",
            "[2,   259] loss: 1.798\n",
            "[2,   261] loss: 1.672\n",
            "[2,   263] loss: 2.222\n",
            "[2,   265] loss: 2.558\n",
            "[2,   267] loss: 2.098\n",
            "[2,   269] loss: 1.626\n",
            "[2,   271] loss: 1.906\n",
            "[2,   273] loss: 1.801\n",
            "[2,   275] loss: 1.777\n",
            "[2,   277] loss: 1.753\n",
            "[2,   279] loss: 1.707\n",
            "[2,   281] loss: 1.545\n",
            "[2,   283] loss: 2.062\n",
            "[2,   285] loss: 1.748\n",
            "[2,   287] loss: 1.903\n",
            "[2,   289] loss: 1.583\n",
            "[2,   291] loss: 2.086\n",
            "[2,   293] loss: 2.626\n",
            "[2,   295] loss: 1.798\n",
            "[2,   297] loss: 2.195\n",
            "[2,   299] loss: 1.790\n",
            "[2,   301] loss: 1.799\n",
            "[2,   303] loss: 2.369\n",
            "[2,   305] loss: 1.983\n",
            "[2,   307] loss: 2.231\n",
            "[2,   309] loss: 2.154\n",
            "[2,   311] loss: 1.857\n",
            "[2,   313] loss: 1.936\n",
            "[2,   315] loss: 2.064\n",
            "[2,   317] loss: 1.983\n",
            "[2,   319] loss: 2.102\n",
            "[2,   321] loss: 2.411\n",
            "[2,   323] loss: 2.296\n",
            "[2,   325] loss: 1.948\n",
            "[2,   327] loss: 2.395\n",
            "[2,   329] loss: 2.453\n",
            "[2,   331] loss: 2.453\n",
            "[2,   333] loss: 1.796\n",
            "[2,   335] loss: 1.764\n",
            "[2,   337] loss: 2.265\n",
            "[2,   339] loss: 1.751\n",
            "[2,   341] loss: 1.699\n",
            "[2,   343] loss: 1.666\n",
            "[2,   345] loss: 1.957\n",
            "[2,   347] loss: 1.895\n",
            "[2,   349] loss: 1.959\n",
            "[2,   351] loss: 2.185\n",
            "[2,   353] loss: 1.969\n",
            "[2,   355] loss: 1.662\n",
            "[2,   357] loss: 1.532\n",
            "[2,   359] loss: 1.843\n",
            "[2,   361] loss: 1.994\n",
            "[2,   363] loss: 1.806\n",
            "[2,   365] loss: 1.622\n",
            "[2,   367] loss: 1.979\n",
            "[2,   369] loss: 1.830\n",
            "[2,   371] loss: 1.977\n",
            "[2,   373] loss: 2.109\n",
            "[2,   375] loss: 1.957\n",
            "[2,   377] loss: 2.076\n",
            "[2,   379] loss: 1.894\n",
            "[2,   381] loss: 2.143\n",
            "[2,   383] loss: 1.595\n",
            "[2,   385] loss: 1.885\n",
            "[2,   387] loss: 1.997\n",
            "[2,   389] loss: 1.858\n",
            "[2,   391] loss: 1.848\n",
            "[2,   393] loss: 2.380\n",
            "[2,   395] loss: 1.801\n",
            "[2,   397] loss: 2.013\n",
            "[2,   399] loss: 1.946\n",
            "[2,   401] loss: 1.914\n",
            "[2,   403] loss: 1.867\n",
            "[2,   405] loss: 2.547\n",
            "[2,   407] loss: 1.973\n",
            "[2,   409] loss: 1.974\n",
            "[2,   411] loss: 2.036\n",
            "[2,   413] loss: 1.993\n",
            "[2,   415] loss: 1.603\n",
            "[2,   417] loss: 2.045\n",
            "[2,   419] loss: 1.985\n",
            "[2,   421] loss: 1.530\n",
            "[2,   423] loss: 1.994\n",
            "[2,   425] loss: 1.486\n",
            "[2,   427] loss: 1.632\n",
            "[2,   429] loss: 1.699\n",
            "[2,   431] loss: 1.649\n",
            "[2,   433] loss: 1.683\n",
            "[2,   435] loss: 1.843\n",
            "[2,   437] loss: 2.147\n",
            "[2,   439] loss: 1.543\n",
            "[2,   441] loss: 1.413\n",
            "[2,   443] loss: 1.356\n",
            "[2,   445] loss: 1.942\n",
            "[2,   447] loss: 1.618\n",
            "[2,   449] loss: 1.470\n",
            "[2,   451] loss: 1.243\n",
            "[2,   453] loss: 1.798\n",
            "[2,   455] loss: 1.944\n",
            "[2,   457] loss: 2.057\n",
            "[2,   459] loss: 2.009\n",
            "[2,   461] loss: 1.861\n",
            "[2,   463] loss: 1.563\n",
            "[2,   465] loss: 2.809\n",
            "[2,   467] loss: 2.397\n",
            "Epoch 1, Train Loss: 972.2918526530266, Val Loss: 18.560448094949884, Val Acc: 44.69453376205788\n",
            "Epoch 1, Test Acc: 43.20855614973262\n",
            "[3,     1] loss: 1.297\n",
            "[3,     3] loss: 1.838\n",
            "[3,     5] loss: 1.632\n",
            "[3,     7] loss: 1.716\n",
            "[3,     9] loss: 1.619\n",
            "[3,    11] loss: 1.601\n",
            "[3,    13] loss: 2.185\n",
            "[3,    15] loss: 1.748\n",
            "[3,    17] loss: 1.571\n",
            "[3,    19] loss: 1.559\n",
            "[3,    21] loss: 2.167\n",
            "[3,    23] loss: 1.665\n",
            "[3,    25] loss: 1.896\n",
            "[3,    27] loss: 1.404\n",
            "[3,    29] loss: 1.389\n",
            "[3,    31] loss: 1.648\n",
            "[3,    33] loss: 1.712\n",
            "[3,    35] loss: 1.735\n",
            "[3,    37] loss: 1.730\n",
            "[3,    39] loss: 1.890\n",
            "[3,    41] loss: 1.988\n",
            "[3,    43] loss: 1.308\n",
            "[3,    45] loss: 1.441\n",
            "[3,    47] loss: 1.506\n",
            "[3,    49] loss: 1.699\n",
            "[3,    51] loss: 2.173\n",
            "[3,    53] loss: 1.809\n",
            "[3,    55] loss: 1.583\n",
            "[3,    57] loss: 1.399\n",
            "[3,    59] loss: 2.014\n",
            "[3,    61] loss: 1.795\n",
            "[3,    63] loss: 1.818\n",
            "[3,    65] loss: 1.628\n",
            "[3,    67] loss: 1.734\n",
            "[3,    69] loss: 1.886\n",
            "[3,    71] loss: 1.879\n",
            "[3,    73] loss: 2.294\n",
            "[3,    75] loss: 1.957\n",
            "[3,    77] loss: 1.838\n",
            "[3,    79] loss: 1.989\n",
            "[3,    81] loss: 2.019\n",
            "[3,    83] loss: 2.333\n",
            "[3,    85] loss: 1.408\n",
            "[3,    87] loss: 1.724\n",
            "[3,    89] loss: 1.888\n",
            "[3,    91] loss: 1.872\n",
            "[3,    93] loss: 1.901\n",
            "[3,    95] loss: 1.705\n",
            "[3,    97] loss: 1.732\n",
            "[3,    99] loss: 1.497\n",
            "[3,   101] loss: 1.589\n",
            "[3,   103] loss: 1.632\n",
            "[3,   105] loss: 1.527\n",
            "[3,   107] loss: 1.548\n",
            "[3,   109] loss: 2.202\n",
            "[3,   111] loss: 1.694\n",
            "[3,   113] loss: 1.441\n",
            "[3,   115] loss: 1.569\n",
            "[3,   117] loss: 1.866\n",
            "[3,   119] loss: 1.605\n",
            "[3,   121] loss: 1.672\n",
            "[3,   123] loss: 1.739\n",
            "[3,   125] loss: 1.513\n",
            "[3,   127] loss: 1.292\n",
            "[3,   129] loss: 1.394\n",
            "[3,   131] loss: 2.044\n",
            "[3,   133] loss: 1.518\n",
            "[3,   135] loss: 1.382\n",
            "[3,   137] loss: 1.620\n",
            "[3,   139] loss: 1.475\n",
            "[3,   141] loss: 2.011\n",
            "[3,   143] loss: 1.693\n",
            "[3,   145] loss: 1.671\n",
            "[3,   147] loss: 1.928\n",
            "[3,   149] loss: 1.534\n",
            "[3,   151] loss: 1.609\n",
            "[3,   153] loss: 1.641\n",
            "[3,   155] loss: 1.652\n",
            "[3,   157] loss: 1.887\n",
            "[3,   159] loss: 1.667\n",
            "[3,   161] loss: 1.952\n",
            "[3,   163] loss: 2.131\n",
            "[3,   165] loss: 1.754\n",
            "[3,   167] loss: 1.940\n",
            "[3,   169] loss: 1.963\n",
            "[3,   171] loss: 1.716\n",
            "[3,   173] loss: 2.143\n",
            "[3,   175] loss: 1.896\n",
            "[3,   177] loss: 1.457\n",
            "[3,   179] loss: 1.670\n",
            "[3,   181] loss: 1.877\n",
            "[3,   183] loss: 1.601\n",
            "[3,   185] loss: 1.614\n",
            "[3,   187] loss: 1.766\n",
            "[3,   189] loss: 1.919\n",
            "[3,   191] loss: 1.602\n",
            "[3,   193] loss: 1.589\n",
            "[3,   195] loss: 1.436\n",
            "[3,   197] loss: 1.197\n",
            "[3,   199] loss: 1.158\n",
            "[3,   201] loss: 1.682\n",
            "[3,   203] loss: 1.727\n",
            "[3,   205] loss: 1.607\n",
            "[3,   207] loss: 1.609\n",
            "[3,   209] loss: 1.871\n",
            "[3,   211] loss: 1.337\n",
            "[3,   213] loss: 1.598\n",
            "[3,   215] loss: 1.215\n",
            "[3,   217] loss: 1.441\n",
            "[3,   219] loss: 1.590\n",
            "[3,   221] loss: 1.715\n",
            "[3,   223] loss: 1.838\n",
            "[3,   225] loss: 1.698\n",
            "[3,   227] loss: 1.526\n",
            "[3,   229] loss: 1.783\n",
            "[3,   231] loss: 1.337\n",
            "[3,   233] loss: 1.675\n",
            "[3,   235] loss: 1.367\n",
            "[3,   237] loss: 2.078\n",
            "[3,   239] loss: 1.543\n",
            "[3,   241] loss: 1.592\n",
            "[3,   243] loss: 1.711\n",
            "[3,   245] loss: 1.568\n",
            "[3,   247] loss: 1.280\n",
            "[3,   249] loss: 1.614\n",
            "[3,   251] loss: 1.622\n",
            "[3,   253] loss: 1.512\n",
            "[3,   255] loss: 1.601\n",
            "[3,   257] loss: 1.631\n",
            "[3,   259] loss: 1.434\n",
            "[3,   261] loss: 1.583\n",
            "[3,   263] loss: 1.472\n",
            "[3,   265] loss: 1.750\n",
            "[3,   267] loss: 1.755\n",
            "[3,   269] loss: 1.847\n",
            "[3,   271] loss: 1.942\n",
            "[3,   273] loss: 1.803\n",
            "[3,   275] loss: 1.307\n",
            "[3,   277] loss: 1.819\n",
            "[3,   279] loss: 1.692\n",
            "[3,   281] loss: 1.546\n",
            "[3,   283] loss: 1.404\n",
            "[3,   285] loss: 1.833\n",
            "[3,   287] loss: 1.869\n",
            "[3,   289] loss: 1.437\n",
            "[3,   291] loss: 1.537\n",
            "[3,   293] loss: 1.497\n",
            "[3,   295] loss: 1.213\n",
            "[3,   297] loss: 1.651\n",
            "[3,   299] loss: 1.614\n",
            "[3,   301] loss: 1.632\n",
            "[3,   303] loss: 1.181\n",
            "[3,   305] loss: 1.544\n",
            "[3,   307] loss: 1.607\n",
            "[3,   309] loss: 1.723\n",
            "[3,   311] loss: 1.576\n",
            "[3,   313] loss: 1.441\n",
            "[3,   315] loss: 1.410\n",
            "[3,   317] loss: 1.651\n",
            "[3,   319] loss: 1.343\n",
            "[3,   321] loss: 1.879\n",
            "[3,   323] loss: 1.285\n",
            "[3,   325] loss: 1.673\n",
            "[3,   327] loss: 1.180\n",
            "[3,   329] loss: 1.281\n",
            "[3,   331] loss: 1.264\n",
            "[3,   333] loss: 1.330\n",
            "[3,   335] loss: 1.221\n",
            "[3,   337] loss: 1.813\n",
            "[3,   339] loss: 1.150\n",
            "[3,   341] loss: 1.606\n",
            "[3,   343] loss: 1.189\n",
            "[3,   345] loss: 1.285\n",
            "[3,   347] loss: 1.454\n",
            "[3,   349] loss: 1.108\n",
            "[3,   351] loss: 1.537\n",
            "[3,   353] loss: 1.649\n",
            "[3,   355] loss: 1.069\n",
            "[3,   357] loss: 1.345\n",
            "[3,   359] loss: 2.281\n",
            "[3,   361] loss: 1.150\n",
            "[3,   363] loss: 1.060\n",
            "[3,   365] loss: 1.424\n",
            "[3,   367] loss: 1.922\n",
            "[3,   369] loss: 1.191\n",
            "[3,   371] loss: 1.462\n",
            "[3,   373] loss: 1.950\n",
            "[3,   375] loss: 1.429\n",
            "[3,   377] loss: 1.337\n",
            "[3,   379] loss: 1.067\n",
            "[3,   381] loss: 1.427\n",
            "[3,   383] loss: 1.303\n",
            "[3,   385] loss: 1.689\n",
            "[3,   387] loss: 1.663\n",
            "[3,   389] loss: 1.507\n",
            "[3,   391] loss: 1.338\n",
            "[3,   393] loss: 1.276\n",
            "[3,   395] loss: 1.816\n",
            "[3,   397] loss: 1.204\n",
            "[3,   399] loss: 1.783\n",
            "[3,   401] loss: 1.653\n",
            "[3,   403] loss: 1.539\n",
            "[3,   405] loss: 1.482\n",
            "[3,   407] loss: 1.383\n",
            "[3,   409] loss: 1.016\n",
            "[3,   411] loss: 1.066\n",
            "[3,   413] loss: 0.754\n",
            "[3,   415] loss: 1.307\n",
            "[3,   417] loss: 1.346\n",
            "[3,   419] loss: 1.741\n",
            "[3,   421] loss: 1.606\n",
            "[3,   423] loss: 1.387\n",
            "[3,   425] loss: 1.277\n",
            "[3,   427] loss: 1.297\n",
            "[3,   429] loss: 1.588\n",
            "[3,   431] loss: 1.323\n",
            "[3,   433] loss: 1.170\n",
            "[3,   435] loss: 1.541\n",
            "[3,   437] loss: 1.199\n",
            "[3,   439] loss: 1.645\n",
            "[3,   441] loss: 1.538\n",
            "[3,   443] loss: 1.442\n",
            "[3,   445] loss: 1.202\n",
            "[3,   447] loss: 0.917\n",
            "[3,   449] loss: 1.976\n",
            "[3,   451] loss: 0.912\n",
            "[3,   453] loss: 1.170\n",
            "[3,   455] loss: 1.497\n",
            "[3,   457] loss: 1.659\n",
            "[3,   459] loss: 1.387\n",
            "[3,   461] loss: 1.403\n",
            "[3,   463] loss: 0.987\n",
            "[3,   465] loss: 1.218\n",
            "[3,   467] loss: 1.182\n",
            "Epoch 2, Train Loss: 741.3869382739067, Val Loss: 2.8712701726767977, Val Acc: 57.234726688102896\n",
            "Epoch 2, Test Acc: 56.57754010695187\n",
            "[4,     1] loss: 0.608\n",
            "[4,     3] loss: 0.990\n",
            "[4,     5] loss: 1.274\n",
            "[4,     7] loss: 1.296\n",
            "[4,     9] loss: 1.367\n",
            "[4,    11] loss: 1.250\n",
            "[4,    13] loss: 1.151\n",
            "[4,    15] loss: 0.968\n",
            "[4,    17] loss: 1.673\n",
            "[4,    19] loss: 1.104\n",
            "[4,    21] loss: 0.912\n",
            "[4,    23] loss: 1.406\n",
            "[4,    25] loss: 0.981\n",
            "[4,    27] loss: 1.478\n",
            "[4,    29] loss: 0.964\n",
            "[4,    31] loss: 0.954\n",
            "[4,    33] loss: 0.922\n",
            "[4,    35] loss: 0.699\n",
            "[4,    37] loss: 1.127\n",
            "[4,    39] loss: 1.072\n",
            "[4,    41] loss: 1.004\n",
            "[4,    43] loss: 1.407\n",
            "[4,    45] loss: 0.946\n",
            "[4,    47] loss: 1.174\n",
            "[4,    49] loss: 1.157\n",
            "[4,    51] loss: 1.105\n",
            "[4,    53] loss: 1.119\n",
            "[4,    55] loss: 0.787\n",
            "[4,    57] loss: 0.959\n",
            "[4,    59] loss: 0.759\n",
            "[4,    61] loss: 1.071\n",
            "[4,    63] loss: 2.027\n",
            "[4,    65] loss: 1.327\n",
            "[4,    67] loss: 1.209\n",
            "[4,    69] loss: 1.271\n",
            "[4,    71] loss: 1.039\n",
            "[4,    73] loss: 1.479\n",
            "[4,    75] loss: 1.045\n",
            "[4,    77] loss: 1.328\n",
            "[4,    79] loss: 1.031\n",
            "[4,    81] loss: 1.117\n",
            "[4,    83] loss: 1.905\n",
            "[4,    85] loss: 1.241\n",
            "[4,    87] loss: 1.135\n",
            "[4,    89] loss: 1.448\n",
            "[4,    91] loss: 1.268\n",
            "[4,    93] loss: 2.451\n",
            "[4,    95] loss: 0.963\n",
            "[4,    97] loss: 0.946\n",
            "[4,    99] loss: 0.861\n",
            "[4,   101] loss: 1.191\n",
            "[4,   103] loss: 1.126\n",
            "[4,   105] loss: 1.448\n",
            "[4,   107] loss: 1.081\n",
            "[4,   109] loss: 1.160\n",
            "[4,   111] loss: 0.948\n",
            "[4,   113] loss: 0.820\n",
            "[4,   115] loss: 1.164\n",
            "[4,   117] loss: 1.248\n",
            "[4,   119] loss: 1.036\n",
            "[4,   121] loss: 1.192\n",
            "[4,   123] loss: 0.882\n",
            "[4,   125] loss: 1.251\n",
            "[4,   127] loss: 1.279\n",
            "[4,   129] loss: 1.104\n",
            "[4,   131] loss: 1.171\n",
            "[4,   133] loss: 1.084\n",
            "[4,   135] loss: 1.184\n",
            "[4,   137] loss: 0.985\n",
            "[4,   139] loss: 1.100\n",
            "[4,   141] loss: 1.138\n",
            "[4,   143] loss: 1.366\n",
            "[4,   145] loss: 0.990\n",
            "[4,   147] loss: 0.669\n",
            "[4,   149] loss: 1.245\n",
            "[4,   151] loss: 0.892\n",
            "[4,   153] loss: 1.026\n",
            "[4,   155] loss: 0.743\n",
            "[4,   157] loss: 0.991\n",
            "[4,   159] loss: 1.130\n",
            "[4,   161] loss: 0.960\n",
            "[4,   163] loss: 0.846\n",
            "[4,   165] loss: 0.909\n",
            "[4,   167] loss: 1.017\n",
            "[4,   169] loss: 1.621\n",
            "[4,   171] loss: 1.073\n",
            "[4,   173] loss: 0.828\n",
            "[4,   175] loss: 0.824\n",
            "[4,   177] loss: 1.014\n",
            "[4,   179] loss: 2.051\n",
            "[4,   181] loss: 0.821\n",
            "[4,   183] loss: 2.217\n",
            "[4,   185] loss: 1.139\n",
            "[4,   187] loss: 1.592\n",
            "[4,   189] loss: 1.159\n",
            "[4,   191] loss: 0.671\n",
            "[4,   193] loss: 0.965\n",
            "[4,   195] loss: 1.691\n",
            "[4,   197] loss: 1.219\n",
            "[4,   199] loss: 0.888\n",
            "[4,   201] loss: 0.533\n",
            "[4,   203] loss: 1.184\n",
            "[4,   205] loss: 1.249\n",
            "[4,   207] loss: 0.818\n",
            "[4,   209] loss: 0.836\n",
            "[4,   211] loss: 0.923\n",
            "[4,   213] loss: 0.942\n",
            "[4,   215] loss: 1.058\n",
            "[4,   217] loss: 1.992\n",
            "[4,   219] loss: 1.055\n",
            "[4,   221] loss: 1.052\n",
            "[4,   223] loss: 1.536\n",
            "[4,   225] loss: 1.807\n",
            "[4,   227] loss: 1.561\n",
            "[4,   229] loss: 1.540\n",
            "[4,   231] loss: 1.849\n",
            "[4,   233] loss: 1.123\n",
            "[4,   235] loss: 0.950\n",
            "[4,   237] loss: 1.182\n",
            "[4,   239] loss: 1.433\n",
            "[4,   241] loss: 1.486\n",
            "[4,   243] loss: 1.825\n",
            "[4,   245] loss: 1.418\n",
            "[4,   247] loss: 1.777\n",
            "[4,   249] loss: 1.556\n",
            "[4,   251] loss: 1.221\n",
            "[4,   253] loss: 1.156\n",
            "[4,   255] loss: 1.483\n",
            "[4,   257] loss: 0.991\n",
            "[4,   259] loss: 1.211\n",
            "[4,   261] loss: 1.096\n",
            "[4,   263] loss: 1.311\n",
            "[4,   265] loss: 1.222\n",
            "[4,   267] loss: 1.190\n",
            "[4,   269] loss: 1.396\n",
            "[4,   271] loss: 1.237\n",
            "[4,   273] loss: 0.679\n",
            "[4,   275] loss: 0.877\n",
            "[4,   277] loss: 1.505\n",
            "[4,   279] loss: 1.128\n",
            "[4,   281] loss: 0.921\n",
            "[4,   283] loss: 0.547\n",
            "[4,   285] loss: 1.094\n",
            "[4,   287] loss: 0.649\n",
            "[4,   289] loss: 1.084\n",
            "[4,   291] loss: 0.986\n",
            "[4,   293] loss: 0.666\n",
            "[4,   295] loss: 1.421\n",
            "[4,   297] loss: 1.156\n",
            "[4,   299] loss: 1.036\n",
            "[4,   301] loss: 1.926\n",
            "[4,   303] loss: 0.927\n",
            "[4,   305] loss: 1.703\n",
            "[4,   307] loss: 0.885\n",
            "[4,   309] loss: 1.675\n",
            "[4,   311] loss: 1.298\n",
            "[4,   313] loss: 1.118\n",
            "[4,   315] loss: 1.885\n",
            "[4,   317] loss: 1.048\n",
            "[4,   319] loss: 1.067\n",
            "[4,   321] loss: 2.137\n",
            "[4,   323] loss: 0.786\n",
            "[4,   325] loss: 1.630\n",
            "[4,   327] loss: 1.263\n",
            "[4,   329] loss: 1.035\n",
            "[4,   331] loss: 1.388\n",
            "[4,   333] loss: 1.531\n",
            "[4,   335] loss: 1.474\n",
            "[4,   337] loss: 1.293\n",
            "[4,   339] loss: 0.900\n",
            "[4,   341] loss: 1.101\n",
            "[4,   343] loss: 1.742\n",
            "[4,   345] loss: 0.628\n",
            "[4,   347] loss: 1.021\n",
            "[4,   349] loss: 1.655\n",
            "[4,   351] loss: 1.068\n",
            "[4,   353] loss: 1.090\n",
            "[4,   355] loss: 0.903\n",
            "[4,   357] loss: 1.049\n",
            "[4,   359] loss: 0.785\n",
            "[4,   361] loss: 1.153\n",
            "[4,   363] loss: 0.922\n",
            "[4,   365] loss: 1.731\n",
            "[4,   367] loss: 0.860\n",
            "[4,   369] loss: 1.290\n",
            "[4,   371] loss: 1.091\n",
            "[4,   373] loss: 1.008\n",
            "[4,   375] loss: 1.508\n",
            "[4,   377] loss: 1.554\n",
            "[4,   379] loss: 1.083\n",
            "[4,   381] loss: 1.448\n",
            "[4,   383] loss: 2.212\n",
            "[4,   385] loss: 1.091\n",
            "[4,   387] loss: 0.761\n",
            "[4,   389] loss: 0.651\n",
            "[4,   391] loss: 0.825\n",
            "[4,   393] loss: 2.452\n",
            "[4,   395] loss: 0.897\n",
            "[4,   397] loss: 1.726\n",
            "[4,   399] loss: 0.800\n",
            "[4,   401] loss: 1.624\n",
            "[4,   403] loss: 0.685\n",
            "[4,   405] loss: 0.904\n",
            "[4,   407] loss: 0.779\n",
            "[4,   409] loss: 0.585\n",
            "[4,   411] loss: 0.962\n",
            "[4,   413] loss: 1.477\n",
            "[4,   415] loss: 1.343\n",
            "[4,   417] loss: 1.571\n",
            "[4,   419] loss: 1.409\n",
            "[4,   421] loss: 1.144\n",
            "[4,   423] loss: 1.082\n",
            "[4,   425] loss: 0.865\n",
            "[4,   427] loss: 1.116\n",
            "[4,   429] loss: 0.816\n",
            "[4,   431] loss: 0.604\n",
            "[4,   433] loss: 1.007\n",
            "[4,   435] loss: 1.009\n",
            "[4,   437] loss: 1.247\n",
            "[4,   439] loss: 1.726\n",
            "[4,   441] loss: 1.210\n",
            "[4,   443] loss: 1.670\n",
            "[4,   445] loss: 0.759\n",
            "[4,   447] loss: 1.078\n",
            "[4,   449] loss: 1.987\n",
            "[4,   451] loss: 1.460\n",
            "[4,   453] loss: 1.010\n",
            "[4,   455] loss: 1.193\n",
            "[4,   457] loss: 0.923\n",
            "[4,   459] loss: 1.795\n",
            "[4,   461] loss: 1.113\n",
            "[4,   463] loss: 1.305\n",
            "[4,   465] loss: 0.873\n",
            "[4,   467] loss: 1.274\n",
            "Epoch 3, Train Loss: 555.0944165289402, Val Loss: 2.6796041509862674, Val Acc: 68.16720257234726\n",
            "Epoch 3, Test Acc: 64.06417112299465\n",
            "[5,     1] loss: 0.675\n",
            "[5,     3] loss: 0.659\n",
            "[5,     5] loss: 1.297\n",
            "[5,     7] loss: 0.884\n",
            "[5,     9] loss: 1.199\n",
            "[5,    11] loss: 1.117\n",
            "[5,    13] loss: 0.912\n",
            "[5,    15] loss: 0.735\n",
            "[5,    17] loss: 1.103\n",
            "[5,    19] loss: 1.074\n",
            "[5,    21] loss: 0.964\n",
            "[5,    23] loss: 1.202\n",
            "[5,    25] loss: 1.363\n",
            "[5,    27] loss: 1.751\n",
            "[5,    29] loss: 0.776\n",
            "[5,    31] loss: 0.821\n",
            "[5,    33] loss: 0.866\n",
            "[5,    35] loss: 0.461\n",
            "[5,    37] loss: 1.177\n",
            "[5,    39] loss: 0.576\n",
            "[5,    41] loss: 1.049\n",
            "[5,    43] loss: 0.598\n",
            "[5,    45] loss: 0.634\n",
            "[5,    47] loss: 1.021\n",
            "[5,    49] loss: 0.783\n",
            "[5,    51] loss: 0.709\n",
            "[5,    53] loss: 0.775\n",
            "[5,    55] loss: 0.803\n",
            "[5,    57] loss: 1.561\n",
            "[5,    59] loss: 1.089\n",
            "[5,    61] loss: 0.758\n",
            "[5,    63] loss: 0.639\n",
            "[5,    65] loss: 0.488\n",
            "[5,    67] loss: 0.847\n",
            "[5,    69] loss: 0.526\n",
            "[5,    71] loss: 0.786\n",
            "[5,    73] loss: 0.522\n",
            "[5,    75] loss: 1.292\n",
            "[5,    77] loss: 0.837\n",
            "[5,    79] loss: 0.908\n",
            "[5,    81] loss: 1.157\n",
            "[5,    83] loss: 1.239\n",
            "[5,    85] loss: 1.155\n",
            "[5,    87] loss: 1.286\n",
            "[5,    89] loss: 0.771\n",
            "[5,    91] loss: 1.251\n",
            "[5,    93] loss: 2.452\n",
            "[5,    95] loss: 0.658\n",
            "[5,    97] loss: 1.034\n",
            "[5,    99] loss: 0.638\n",
            "[5,   101] loss: 0.819\n",
            "[5,   103] loss: 0.919\n",
            "[5,   105] loss: 1.021\n",
            "[5,   107] loss: 0.667\n",
            "[5,   109] loss: 0.855\n",
            "[5,   111] loss: 0.548\n",
            "[5,   113] loss: 0.618\n",
            "[5,   115] loss: 1.325\n",
            "[5,   117] loss: 0.538\n",
            "[5,   119] loss: 0.569\n",
            "[5,   121] loss: 0.538\n",
            "[5,   123] loss: 1.449\n",
            "[5,   125] loss: 0.885\n",
            "[5,   127] loss: 0.665\n",
            "[5,   129] loss: 0.760\n",
            "[5,   131] loss: 0.865\n",
            "[5,   133] loss: 0.928\n",
            "[5,   135] loss: 0.910\n",
            "[5,   137] loss: 0.820\n",
            "[5,   139] loss: 0.722\n",
            "[5,   141] loss: 0.794\n",
            "[5,   143] loss: 1.646\n",
            "[5,   145] loss: 1.328\n",
            "[5,   147] loss: 1.102\n",
            "[5,   149] loss: 1.296\n",
            "[5,   151] loss: 1.277\n",
            "[5,   153] loss: 0.436\n",
            "[5,   155] loss: 0.472\n",
            "[5,   157] loss: 1.699\n",
            "[5,   159] loss: 1.845\n",
            "[5,   161] loss: 0.842\n",
            "[5,   163] loss: 0.737\n",
            "[5,   165] loss: 2.146\n",
            "[5,   167] loss: 1.026\n",
            "[5,   169] loss: 0.726\n",
            "[5,   171] loss: 1.098\n",
            "[5,   173] loss: 1.388\n",
            "[5,   175] loss: 0.606\n",
            "[5,   177] loss: 1.236\n",
            "[5,   179] loss: 0.499\n",
            "[5,   181] loss: 0.601\n",
            "[5,   183] loss: 0.741\n",
            "[5,   185] loss: 0.553\n",
            "[5,   187] loss: 0.727\n",
            "[5,   189] loss: 1.370\n",
            "[5,   191] loss: 0.825\n",
            "[5,   193] loss: 0.652\n",
            "[5,   195] loss: 1.042\n",
            "[5,   197] loss: 0.766\n",
            "[5,   199] loss: 1.239\n",
            "[5,   201] loss: 1.045\n",
            "[5,   203] loss: 1.392\n",
            "[5,   205] loss: 0.855\n",
            "[5,   207] loss: 1.385\n",
            "[5,   209] loss: 1.468\n",
            "[5,   211] loss: 0.682\n",
            "[5,   213] loss: 2.898\n",
            "[5,   215] loss: 1.349\n",
            "[5,   217] loss: 1.132\n",
            "[5,   219] loss: 1.083\n",
            "[5,   221] loss: 1.024\n",
            "[5,   223] loss: 0.797\n",
            "[5,   225] loss: 0.758\n",
            "[5,   227] loss: 0.864\n",
            "[5,   229] loss: 1.497\n",
            "[5,   231] loss: 1.573\n",
            "[5,   233] loss: 1.359\n",
            "[5,   235] loss: 0.616\n",
            "[5,   237] loss: 0.931\n",
            "[5,   239] loss: 2.132\n",
            "[5,   241] loss: 1.024\n",
            "[5,   243] loss: 0.861\n",
            "[5,   245] loss: 0.813\n",
            "[5,   247] loss: 0.901\n",
            "[5,   249] loss: 1.140\n",
            "[5,   251] loss: 0.967\n",
            "[5,   253] loss: 1.014\n",
            "[5,   255] loss: 1.058\n",
            "[5,   257] loss: 0.597\n",
            "[5,   259] loss: 1.494\n",
            "[5,   261] loss: 1.044\n",
            "[5,   263] loss: 1.017\n",
            "[5,   265] loss: 0.839\n",
            "[5,   267] loss: 0.649\n",
            "[5,   269] loss: 0.766\n",
            "[5,   271] loss: 0.693\n",
            "[5,   273] loss: 1.740\n",
            "[5,   275] loss: 0.878\n",
            "[5,   277] loss: 0.820\n",
            "[5,   279] loss: 0.726\n",
            "[5,   281] loss: 0.941\n",
            "[5,   283] loss: 0.838\n",
            "[5,   285] loss: 1.243\n",
            "[5,   287] loss: 0.442\n",
            "[5,   289] loss: 0.912\n",
            "[5,   291] loss: 0.784\n",
            "[5,   293] loss: 0.632\n",
            "[5,   295] loss: 1.314\n",
            "[5,   297] loss: 0.589\n",
            "[5,   299] loss: 1.100\n",
            "[5,   301] loss: 1.853\n",
            "[5,   303] loss: 0.543\n",
            "[5,   305] loss: 1.285\n",
            "[5,   307] loss: 1.190\n",
            "[5,   309] loss: 1.007\n",
            "[5,   311] loss: 1.388\n",
            "[5,   313] loss: 1.099\n",
            "[5,   315] loss: 1.308\n",
            "[5,   317] loss: 1.242\n",
            "[5,   319] loss: 0.599\n",
            "[5,   321] loss: 0.600\n",
            "[5,   323] loss: 0.878\n",
            "[5,   325] loss: 0.710\n",
            "[5,   327] loss: 0.662\n",
            "[5,   329] loss: 1.213\n",
            "[5,   331] loss: 1.821\n",
            "[5,   333] loss: 1.439\n",
            "[5,   335] loss: 0.927\n",
            "[5,   337] loss: 0.871\n",
            "[5,   339] loss: 0.686\n",
            "[5,   341] loss: 1.417\n",
            "[5,   343] loss: 2.767\n",
            "[5,   345] loss: 1.312\n",
            "[5,   347] loss: 0.777\n",
            "[5,   349] loss: 1.039\n",
            "[5,   351] loss: 0.529\n",
            "[5,   353] loss: 0.670\n",
            "[5,   355] loss: 0.964\n",
            "[5,   357] loss: 0.942\n",
            "[5,   359] loss: 0.738\n",
            "[5,   361] loss: 0.379\n",
            "[5,   363] loss: 0.663\n",
            "[5,   365] loss: 0.967\n",
            "[5,   367] loss: 0.843\n",
            "[5,   369] loss: 1.106\n",
            "[5,   371] loss: 1.145\n",
            "[5,   373] loss: 1.215\n",
            "[5,   375] loss: 0.993\n",
            "[5,   377] loss: 1.104\n",
            "[5,   379] loss: 0.642\n",
            "[5,   381] loss: 1.193\n",
            "[5,   383] loss: 1.109\n",
            "[5,   385] loss: 1.056\n",
            "[5,   387] loss: 1.222\n",
            "[5,   389] loss: 1.305\n",
            "[5,   391] loss: 0.943\n",
            "[5,   393] loss: 0.954\n",
            "[5,   395] loss: 1.129\n",
            "[5,   397] loss: 0.662\n",
            "[5,   399] loss: 0.767\n",
            "[5,   401] loss: 0.711\n",
            "[5,   403] loss: 0.891\n",
            "[5,   405] loss: 1.032\n",
            "[5,   407] loss: 1.157\n",
            "[5,   409] loss: 1.872\n",
            "[5,   411] loss: 1.378\n",
            "[5,   413] loss: 0.961\n",
            "[5,   415] loss: 1.053\n",
            "[5,   417] loss: 0.895\n",
            "[5,   419] loss: 1.325\n",
            "[5,   421] loss: 1.366\n",
            "[5,   423] loss: 1.387\n",
            "[5,   425] loss: 0.647\n",
            "[5,   427] loss: 1.177\n",
            "[5,   429] loss: 0.883\n",
            "[5,   431] loss: 1.088\n",
            "[5,   433] loss: 1.260\n",
            "[5,   435] loss: 1.092\n",
            "[5,   437] loss: 0.477\n",
            "[5,   439] loss: 0.608\n",
            "[5,   441] loss: 1.408\n",
            "[5,   443] loss: 1.532\n",
            "[5,   445] loss: 0.550\n",
            "[5,   447] loss: 0.954\n",
            "[5,   449] loss: 0.543\n",
            "[5,   451] loss: 0.622\n",
            "[5,   453] loss: 0.528\n",
            "[5,   455] loss: 0.631\n",
            "[5,   457] loss: 1.011\n",
            "[5,   459] loss: 1.045\n",
            "[5,   461] loss: 0.450\n",
            "[5,   463] loss: 0.782\n",
            "[5,   465] loss: 0.569\n",
            "[5,   467] loss: 0.735\n",
            "Epoch 4, Train Loss: 464.38445107638836, Val Loss: 2.145618970111265, Val Acc: 76.31296891747053\n",
            "Epoch 4, Test Acc: 74.8663101604278\n",
            "[6,     1] loss: 0.261\n",
            "[6,     3] loss: 0.574\n",
            "[6,     5] loss: 0.514\n",
            "[6,     7] loss: 0.862\n",
            "[6,     9] loss: 0.799\n",
            "[6,    11] loss: 0.506\n",
            "[6,    13] loss: 0.859\n",
            "[6,    15] loss: 0.407\n",
            "[6,    17] loss: 0.714\n",
            "[6,    19] loss: 0.557\n",
            "[6,    21] loss: 0.792\n",
            "[6,    23] loss: 0.260\n",
            "[6,    25] loss: 0.900\n",
            "[6,    27] loss: 0.674\n",
            "[6,    29] loss: 0.881\n",
            "[6,    31] loss: 0.904\n",
            "[6,    33] loss: 0.362\n",
            "[6,    35] loss: 0.930\n",
            "[6,    37] loss: 0.471\n",
            "[6,    39] loss: 0.971\n",
            "[6,    41] loss: 0.710\n",
            "[6,    43] loss: 1.063\n",
            "[6,    45] loss: 0.927\n",
            "[6,    47] loss: 0.687\n",
            "[6,    49] loss: 0.943\n",
            "[6,    51] loss: 0.567\n",
            "[6,    53] loss: 0.349\n",
            "[6,    55] loss: 0.505\n",
            "[6,    57] loss: 0.627\n",
            "[6,    59] loss: 0.366\n",
            "[6,    61] loss: 0.791\n",
            "[6,    63] loss: 0.861\n",
            "[6,    65] loss: 0.820\n",
            "[6,    67] loss: 0.506\n",
            "[6,    69] loss: 0.936\n",
            "[6,    71] loss: 0.833\n",
            "[6,    73] loss: 0.545\n",
            "[6,    75] loss: 1.128\n",
            "[6,    77] loss: 0.496\n",
            "[6,    79] loss: 0.694\n",
            "[6,    81] loss: 0.454\n",
            "[6,    83] loss: 0.359\n",
            "[6,    85] loss: 0.375\n",
            "[6,    87] loss: 0.739\n",
            "[6,    89] loss: 0.850\n",
            "[6,    91] loss: 0.624\n",
            "[6,    93] loss: 0.344\n",
            "[6,    95] loss: 0.977\n",
            "[6,    97] loss: 0.684\n",
            "[6,    99] loss: 0.475\n",
            "[6,   101] loss: 0.686\n",
            "[6,   103] loss: 1.106\n",
            "[6,   105] loss: 0.637\n",
            "[6,   107] loss: 0.945\n",
            "[6,   109] loss: 0.492\n",
            "[6,   111] loss: 0.692\n",
            "[6,   113] loss: 0.584\n",
            "[6,   115] loss: 0.803\n",
            "[6,   117] loss: 1.164\n",
            "[6,   119] loss: 1.024\n",
            "[6,   121] loss: 0.999\n",
            "[6,   123] loss: 0.501\n",
            "[6,   125] loss: 0.504\n",
            "[6,   127] loss: 0.894\n",
            "[6,   129] loss: 0.406\n",
            "[6,   131] loss: 0.583\n",
            "[6,   133] loss: 0.595\n",
            "[6,   135] loss: 0.773\n",
            "[6,   137] loss: 1.004\n",
            "[6,   139] loss: 1.166\n",
            "[6,   141] loss: 0.695\n",
            "[6,   143] loss: 0.575\n",
            "[6,   145] loss: 0.934\n",
            "[6,   147] loss: 0.736\n",
            "[6,   149] loss: 0.720\n",
            "[6,   151] loss: 0.498\n",
            "[6,   153] loss: 0.527\n",
            "[6,   155] loss: 0.392\n",
            "[6,   157] loss: 0.564\n",
            "[6,   159] loss: 0.487\n",
            "[6,   161] loss: 0.931\n",
            "[6,   163] loss: 0.656\n",
            "[6,   165] loss: 0.860\n",
            "[6,   167] loss: 0.650\n",
            "[6,   169] loss: 0.328\n",
            "[6,   171] loss: 0.541\n",
            "[6,   173] loss: 0.576\n",
            "[6,   175] loss: 1.137\n",
            "[6,   177] loss: 0.867\n",
            "[6,   179] loss: 0.328\n",
            "[6,   181] loss: 0.515\n",
            "[6,   183] loss: 0.284\n",
            "[6,   185] loss: 0.134\n",
            "[6,   187] loss: 0.532\n",
            "[6,   189] loss: 0.515\n",
            "[6,   191] loss: 0.623\n",
            "[6,   193] loss: 0.699\n",
            "[6,   195] loss: 0.519\n",
            "[6,   197] loss: 0.372\n",
            "[6,   199] loss: 0.641\n",
            "[6,   201] loss: 0.559\n",
            "[6,   203] loss: 0.532\n",
            "[6,   205] loss: 0.569\n",
            "[6,   207] loss: 0.661\n",
            "[6,   209] loss: 0.621\n",
            "[6,   211] loss: 0.857\n",
            "[6,   213] loss: 0.537\n",
            "[6,   215] loss: 0.833\n",
            "[6,   217] loss: 0.420\n",
            "[6,   219] loss: 0.999\n",
            "[6,   221] loss: 0.481\n",
            "[6,   223] loss: 0.765\n",
            "[6,   225] loss: 0.593\n",
            "[6,   227] loss: 0.610\n",
            "[6,   229] loss: 1.003\n",
            "[6,   231] loss: 0.963\n",
            "[6,   233] loss: 0.757\n",
            "[6,   235] loss: 0.567\n",
            "[6,   237] loss: 0.347\n",
            "[6,   239] loss: 0.635\n",
            "[6,   241] loss: 0.468\n",
            "[6,   243] loss: 0.583\n",
            "[6,   245] loss: 0.798\n",
            "[6,   247] loss: 0.722\n",
            "[6,   249] loss: 0.728\n",
            "[6,   251] loss: 0.092\n",
            "[6,   253] loss: 0.374\n",
            "[6,   255] loss: 0.497\n",
            "[6,   257] loss: 0.840\n",
            "[6,   259] loss: 0.443\n",
            "[6,   261] loss: 0.306\n",
            "[6,   263] loss: 0.529\n",
            "[6,   265] loss: 0.585\n",
            "[6,   267] loss: 0.985\n",
            "[6,   269] loss: 0.660\n",
            "[6,   271] loss: 1.154\n",
            "[6,   273] loss: 0.794\n",
            "[6,   275] loss: 0.639\n",
            "[6,   277] loss: 0.564\n",
            "[6,   279] loss: 0.773\n",
            "[6,   281] loss: 1.160\n",
            "[6,   283] loss: 0.475\n",
            "[6,   285] loss: 0.663\n",
            "[6,   287] loss: 0.676\n",
            "[6,   289] loss: 0.443\n",
            "[6,   291] loss: 0.312\n",
            "[6,   293] loss: 0.877\n",
            "[6,   295] loss: 1.232\n",
            "[6,   297] loss: 0.750\n",
            "[6,   299] loss: 0.568\n",
            "[6,   301] loss: 0.563\n",
            "[6,   303] loss: 0.980\n",
            "[6,   305] loss: 0.445\n",
            "[6,   307] loss: 0.864\n",
            "[6,   309] loss: 0.699\n",
            "[6,   311] loss: 1.067\n",
            "[6,   313] loss: 1.132\n",
            "[6,   315] loss: 0.738\n",
            "[6,   317] loss: 0.444\n",
            "[6,   319] loss: 0.967\n",
            "[6,   321] loss: 0.642\n",
            "[6,   323] loss: 0.283\n",
            "[6,   325] loss: 0.609\n",
            "[6,   327] loss: 0.250\n",
            "[6,   329] loss: 0.815\n",
            "[6,   331] loss: 0.621\n",
            "[6,   333] loss: 0.456\n",
            "[6,   335] loss: 0.530\n",
            "[6,   337] loss: 0.564\n",
            "[6,   339] loss: 0.503\n",
            "[6,   341] loss: 0.524\n",
            "[6,   343] loss: 0.589\n",
            "[6,   345] loss: 0.669\n",
            "[6,   347] loss: 0.737\n",
            "[6,   349] loss: 0.503\n",
            "[6,   351] loss: 0.588\n",
            "[6,   353] loss: 0.563\n",
            "[6,   355] loss: 1.185\n",
            "[6,   357] loss: 0.509\n",
            "[6,   359] loss: 1.181\n",
            "[6,   361] loss: 0.517\n",
            "[6,   363] loss: 0.772\n",
            "[6,   365] loss: 1.007\n",
            "[6,   367] loss: 1.209\n",
            "[6,   369] loss: 0.235\n",
            "[6,   371] loss: 0.528\n",
            "[6,   373] loss: 1.454\n",
            "[6,   375] loss: 2.162\n",
            "[6,   377] loss: 0.424\n",
            "[6,   379] loss: 0.212\n",
            "[6,   381] loss: 0.655\n",
            "[6,   383] loss: 0.855\n",
            "[6,   385] loss: 0.813\n",
            "[6,   387] loss: 0.790\n",
            "[6,   389] loss: 0.332\n",
            "[6,   391] loss: 0.892\n",
            "[6,   393] loss: 0.343\n",
            "[6,   395] loss: 0.540\n",
            "[6,   397] loss: 0.339\n",
            "[6,   399] loss: 1.150\n",
            "[6,   401] loss: 0.215\n",
            "[6,   403] loss: 0.342\n",
            "[6,   405] loss: 0.783\n",
            "[6,   407] loss: 0.735\n",
            "[6,   409] loss: 0.795\n",
            "[6,   411] loss: 0.588\n",
            "[6,   413] loss: 0.420\n",
            "[6,   415] loss: 0.620\n",
            "[6,   417] loss: 0.355\n",
            "[6,   419] loss: 0.557\n",
            "[6,   421] loss: 0.562\n",
            "[6,   423] loss: 0.712\n",
            "[6,   425] loss: 0.530\n",
            "[6,   427] loss: 0.739\n",
            "[6,   429] loss: 0.362\n",
            "[6,   431] loss: 0.600\n",
            "[6,   433] loss: 0.667\n",
            "[6,   435] loss: 0.685\n",
            "[6,   437] loss: 0.296\n",
            "[6,   439] loss: 0.943\n",
            "[6,   441] loss: 1.415\n",
            "[6,   443] loss: 0.645\n",
            "[6,   445] loss: 0.584\n",
            "[6,   447] loss: 0.679\n",
            "[6,   449] loss: 0.856\n",
            "[6,   451] loss: 0.501\n",
            "[6,   453] loss: 0.621\n",
            "[6,   455] loss: 0.722\n",
            "[6,   457] loss: 0.540\n",
            "[6,   459] loss: 0.486\n",
            "[6,   461] loss: 0.775\n",
            "[6,   463] loss: 0.810\n",
            "[6,   465] loss: 1.043\n",
            "[6,   467] loss: 0.880\n",
            "Epoch 5, Train Loss: 314.6924129649997, Val Loss: 1.1059883151013972, Val Acc: 76.42015005359058\n",
            "Epoch 5, Test Acc: 74.8663101604278\n",
            "[7,     1] loss: 0.264\n",
            "[7,     3] loss: 0.709\n",
            "[7,     5] loss: 0.939\n",
            "[7,     7] loss: 1.022\n",
            "[7,     9] loss: 0.389\n",
            "[7,    11] loss: 0.481\n",
            "[7,    13] loss: 0.806\n",
            "[7,    15] loss: 0.946\n",
            "[7,    17] loss: 0.749\n",
            "[7,    19] loss: 1.013\n",
            "[7,    21] loss: 0.859\n",
            "[7,    23] loss: 0.458\n",
            "[7,    25] loss: 0.659\n",
            "[7,    27] loss: 0.416\n",
            "[7,    29] loss: 0.890\n",
            "[7,    31] loss: 0.312\n",
            "[7,    33] loss: 0.482\n",
            "[7,    35] loss: 0.730\n",
            "[7,    37] loss: 0.304\n",
            "[7,    39] loss: 0.830\n",
            "[7,    41] loss: 0.483\n",
            "[7,    43] loss: 0.525\n",
            "[7,    45] loss: 0.452\n",
            "[7,    47] loss: 0.536\n",
            "[7,    49] loss: 0.302\n",
            "[7,    51] loss: 0.418\n",
            "[7,    53] loss: 0.556\n",
            "[7,    55] loss: 0.301\n",
            "[7,    57] loss: 0.284\n",
            "[7,    59] loss: 0.521\n",
            "[7,    61] loss: 0.516\n",
            "[7,    63] loss: 0.708\n",
            "[7,    65] loss: 0.353\n",
            "[7,    67] loss: 0.377\n",
            "[7,    69] loss: 1.136\n",
            "[7,    71] loss: 0.363\n",
            "[7,    73] loss: 0.394\n",
            "[7,    75] loss: 0.327\n",
            "[7,    77] loss: 0.699\n",
            "[7,    79] loss: 0.604\n",
            "[7,    81] loss: 0.284\n",
            "[7,    83] loss: 1.520\n",
            "[7,    85] loss: 0.408\n",
            "[7,    87] loss: 0.567\n",
            "[7,    89] loss: 0.297\n",
            "[7,    91] loss: 0.256\n",
            "[7,    93] loss: 0.706\n",
            "[7,    95] loss: 0.386\n",
            "[7,    97] loss: 0.432\n",
            "[7,    99] loss: 1.829\n",
            "[7,   101] loss: 0.332\n",
            "[7,   103] loss: 0.884\n",
            "[7,   105] loss: 0.920\n",
            "[7,   107] loss: 0.784\n",
            "[7,   109] loss: 0.608\n",
            "[7,   111] loss: 0.614\n",
            "[7,   113] loss: 0.502\n",
            "[7,   115] loss: 1.480\n",
            "[7,   117] loss: 0.282\n",
            "[7,   119] loss: 0.700\n",
            "[7,   121] loss: 0.575\n",
            "[7,   123] loss: 0.834\n",
            "[7,   125] loss: 0.831\n",
            "[7,   127] loss: 0.753\n",
            "[7,   129] loss: 0.598\n",
            "[7,   131] loss: 1.124\n",
            "[7,   133] loss: 0.365\n",
            "[7,   135] loss: 0.924\n",
            "[7,   137] loss: 0.415\n",
            "[7,   139] loss: 0.642\n",
            "[7,   141] loss: 0.591\n",
            "[7,   143] loss: 0.428\n",
            "[7,   145] loss: 0.339\n",
            "[7,   147] loss: 0.399\n",
            "[7,   149] loss: 0.702\n",
            "[7,   151] loss: 0.348\n",
            "[7,   153] loss: 0.402\n",
            "[7,   155] loss: 0.832\n",
            "[7,   157] loss: 0.814\n",
            "[7,   159] loss: 0.794\n",
            "[7,   161] loss: 0.820\n",
            "[7,   163] loss: 0.711\n",
            "[7,   165] loss: 0.628\n",
            "[7,   167] loss: 0.369\n",
            "[7,   169] loss: 0.538\n",
            "[7,   171] loss: 0.400\n",
            "[7,   173] loss: 0.217\n",
            "[7,   175] loss: 0.365\n",
            "[7,   177] loss: 0.527\n",
            "[7,   179] loss: 0.652\n",
            "[7,   181] loss: 0.257\n",
            "[7,   183] loss: 0.396\n",
            "[7,   185] loss: 0.842\n",
            "[7,   187] loss: 1.137\n",
            "[7,   189] loss: 0.613\n",
            "[7,   191] loss: 0.343\n",
            "[7,   193] loss: 0.462\n",
            "[7,   195] loss: 0.196\n",
            "[7,   197] loss: 0.574\n",
            "[7,   199] loss: 0.377\n",
            "[7,   201] loss: 0.309\n",
            "[7,   203] loss: 0.501\n",
            "[7,   205] loss: 0.242\n",
            "[7,   207] loss: 0.498\n",
            "[7,   209] loss: 0.279\n",
            "[7,   211] loss: 0.964\n",
            "[7,   213] loss: 0.502\n",
            "[7,   215] loss: 0.468\n",
            "[7,   217] loss: 0.387\n",
            "[7,   219] loss: 0.563\n",
            "[7,   221] loss: 0.307\n",
            "[7,   223] loss: 0.426\n",
            "[7,   225] loss: 0.251\n",
            "[7,   227] loss: 0.193\n",
            "[7,   229] loss: 0.520\n",
            "[7,   231] loss: 0.262\n",
            "[7,   233] loss: 0.315\n",
            "[7,   235] loss: 0.443\n",
            "[7,   237] loss: 0.660\n",
            "[7,   239] loss: 0.213\n",
            "[7,   241] loss: 0.096\n",
            "[7,   243] loss: 0.308\n",
            "[7,   245] loss: 0.138\n",
            "[7,   247] loss: 0.519\n",
            "[7,   249] loss: 0.595\n",
            "[7,   251] loss: 0.488\n",
            "[7,   253] loss: 0.840\n",
            "[7,   255] loss: 0.379\n",
            "[7,   257] loss: 0.336\n",
            "[7,   259] loss: 0.721\n",
            "[7,   261] loss: 0.733\n",
            "[7,   263] loss: 0.282\n",
            "[7,   265] loss: 0.394\n",
            "[7,   267] loss: 0.597\n",
            "[7,   269] loss: 0.795\n",
            "[7,   271] loss: 0.861\n",
            "[7,   273] loss: 1.077\n",
            "[7,   275] loss: 0.465\n",
            "[7,   277] loss: 1.051\n",
            "[7,   279] loss: 0.494\n",
            "[7,   281] loss: 0.563\n",
            "[7,   283] loss: 0.488\n",
            "[7,   285] loss: 0.588\n",
            "[7,   287] loss: 0.374\n",
            "[7,   289] loss: 0.531\n",
            "[7,   291] loss: 0.295\n",
            "[7,   293] loss: 0.863\n",
            "[7,   295] loss: 0.284\n",
            "[7,   297] loss: 0.489\n",
            "[7,   299] loss: 0.686\n",
            "[7,   301] loss: 0.620\n",
            "[7,   303] loss: 0.360\n",
            "[7,   305] loss: 0.893\n",
            "[7,   307] loss: 1.034\n",
            "[7,   309] loss: 1.054\n",
            "[7,   311] loss: 0.712\n",
            "[7,   313] loss: 1.000\n",
            "[7,   315] loss: 0.373\n",
            "[7,   317] loss: 0.579\n",
            "[7,   319] loss: 0.389\n",
            "[7,   321] loss: 1.063\n",
            "[7,   323] loss: 0.543\n",
            "[7,   325] loss: 0.708\n",
            "[7,   327] loss: 0.862\n",
            "[7,   329] loss: 1.015\n",
            "[7,   331] loss: 0.425\n",
            "[7,   333] loss: 0.426\n",
            "[7,   335] loss: 0.420\n",
            "[7,   337] loss: 0.365\n",
            "[7,   339] loss: 0.762\n",
            "[7,   341] loss: 0.910\n",
            "[7,   343] loss: 1.158\n",
            "[7,   345] loss: 2.164\n",
            "[7,   347] loss: 0.287\n",
            "[7,   349] loss: 0.355\n",
            "[7,   351] loss: 0.645\n",
            "[7,   353] loss: 0.282\n",
            "[7,   355] loss: 1.911\n",
            "[7,   357] loss: 0.340\n",
            "[7,   359] loss: 0.443\n",
            "[7,   361] loss: 0.928\n",
            "[7,   363] loss: 0.358\n",
            "[7,   365] loss: 0.502\n",
            "[7,   367] loss: 0.423\n",
            "[7,   369] loss: 0.430\n",
            "[7,   371] loss: 0.542\n",
            "[7,   373] loss: 0.729\n",
            "[7,   375] loss: 1.067\n",
            "[7,   377] loss: 0.529\n",
            "[7,   379] loss: 0.649\n",
            "[7,   381] loss: 1.247\n",
            "[7,   383] loss: 0.403\n",
            "[7,   385] loss: 0.482\n",
            "[7,   387] loss: 0.223\n",
            "[7,   389] loss: 0.914\n",
            "[7,   391] loss: 0.590\n",
            "[7,   393] loss: 0.471\n",
            "[7,   395] loss: 0.389\n",
            "[7,   397] loss: 0.618\n",
            "[7,   399] loss: 0.640\n",
            "[7,   401] loss: 0.402\n",
            "[7,   403] loss: 0.458\n",
            "[7,   405] loss: 0.369\n",
            "[7,   407] loss: 0.726\n",
            "[7,   409] loss: 0.807\n",
            "[7,   411] loss: 0.610\n",
            "[7,   413] loss: 0.609\n",
            "[7,   415] loss: 0.801\n",
            "[7,   417] loss: 0.640\n",
            "[7,   419] loss: 0.629\n",
            "[7,   421] loss: 0.437\n",
            "[7,   423] loss: 0.397\n",
            "[7,   425] loss: 0.411\n",
            "[7,   427] loss: 0.384\n",
            "[7,   429] loss: 0.680\n",
            "[7,   431] loss: 0.880\n",
            "[7,   433] loss: 0.345\n",
            "[7,   435] loss: 0.593\n",
            "[7,   437] loss: 0.251\n",
            "[7,   439] loss: 0.231\n",
            "[7,   441] loss: 0.442\n",
            "[7,   443] loss: 0.386\n",
            "[7,   445] loss: 0.606\n",
            "[7,   447] loss: 0.351\n",
            "[7,   449] loss: 0.404\n",
            "[7,   451] loss: 0.470\n",
            "[7,   453] loss: 1.065\n",
            "[7,   455] loss: 0.781\n",
            "[7,   457] loss: 0.573\n",
            "[7,   459] loss: 0.793\n",
            "[7,   461] loss: 0.638\n",
            "[7,   463] loss: 0.610\n",
            "[7,   465] loss: 0.665\n",
            "[7,   467] loss: 0.710\n",
            "Epoch 6, Train Loss: 276.11629854142666, Val Loss: 2.494042586977199, Val Acc: 78.13504823151125\n",
            "Epoch 6, Test Acc: 77.86096256684492\n",
            "[8,     1] loss: 0.535\n",
            "[8,     3] loss: 0.885\n",
            "[8,     5] loss: 0.883\n",
            "[8,     7] loss: 1.538\n",
            "[8,     9] loss: 1.283\n",
            "[8,    11] loss: 0.635\n",
            "[8,    13] loss: 1.210\n",
            "[8,    15] loss: 0.847\n",
            "[8,    17] loss: 0.642\n",
            "[8,    19] loss: 0.733\n",
            "[8,    21] loss: 0.671\n",
            "[8,    23] loss: 0.799\n",
            "[8,    25] loss: 0.899\n",
            "[8,    27] loss: 0.681\n",
            "[8,    29] loss: 0.483\n",
            "[8,    31] loss: 0.412\n",
            "[8,    33] loss: 0.378\n",
            "[8,    35] loss: 0.614\n",
            "[8,    37] loss: 0.711\n",
            "[8,    39] loss: 0.097\n",
            "[8,    41] loss: 0.329\n",
            "[8,    43] loss: 0.172\n",
            "[8,    45] loss: 0.297\n",
            "[8,    47] loss: 0.534\n",
            "[8,    49] loss: 0.220\n",
            "[8,    51] loss: 0.536\n",
            "[8,    53] loss: 0.244\n",
            "[8,    55] loss: 0.377\n",
            "[8,    57] loss: 0.409\n",
            "[8,    59] loss: 1.012\n",
            "[8,    61] loss: 0.747\n",
            "[8,    63] loss: 0.241\n",
            "[8,    65] loss: 0.322\n",
            "[8,    67] loss: 0.412\n",
            "[8,    69] loss: 0.498\n",
            "[8,    71] loss: 0.455\n",
            "[8,    73] loss: 0.746\n",
            "[8,    75] loss: 0.449\n",
            "[8,    77] loss: 0.360\n",
            "[8,    79] loss: 0.256\n",
            "[8,    81] loss: 0.505\n",
            "[8,    83] loss: 0.929\n",
            "[8,    85] loss: 0.379\n",
            "[8,    87] loss: 1.398\n",
            "[8,    89] loss: 1.195\n",
            "[8,    91] loss: 1.028\n",
            "[8,    93] loss: 1.187\n",
            "[8,    95] loss: 0.388\n",
            "[8,    97] loss: 1.005\n",
            "[8,    99] loss: 1.007\n",
            "[8,   101] loss: 1.528\n",
            "[8,   103] loss: 0.525\n",
            "[8,   105] loss: 0.338\n",
            "[8,   107] loss: 0.324\n",
            "[8,   109] loss: 0.656\n",
            "[8,   111] loss: 0.725\n",
            "[8,   113] loss: 0.713\n",
            "[8,   115] loss: 0.450\n",
            "[8,   117] loss: 0.399\n",
            "[8,   119] loss: 0.485\n",
            "[8,   121] loss: 0.791\n",
            "[8,   123] loss: 0.608\n",
            "[8,   125] loss: 0.321\n",
            "[8,   127] loss: 0.531\n",
            "[8,   129] loss: 0.645\n",
            "[8,   131] loss: 0.444\n",
            "[8,   133] loss: 0.895\n",
            "[8,   135] loss: 0.384\n",
            "[8,   137] loss: 0.438\n",
            "[8,   139] loss: 0.650\n",
            "[8,   141] loss: 0.815\n",
            "[8,   143] loss: 1.293\n",
            "[8,   145] loss: 0.685\n",
            "[8,   147] loss: 0.960\n",
            "[8,   149] loss: 0.937\n",
            "[8,   151] loss: 0.653\n",
            "[8,   153] loss: 1.280\n",
            "[8,   155] loss: 0.821\n",
            "[8,   157] loss: 0.441\n",
            "[8,   159] loss: 0.948\n",
            "[8,   161] loss: 0.702\n",
            "[8,   163] loss: 1.082\n",
            "[8,   165] loss: 1.296\n",
            "[8,   167] loss: 0.372\n",
            "[8,   169] loss: 1.330\n",
            "[8,   171] loss: 0.509\n",
            "[8,   173] loss: 1.391\n",
            "[8,   175] loss: 0.875\n",
            "[8,   177] loss: 0.960\n",
            "[8,   179] loss: 0.629\n",
            "[8,   181] loss: 0.573\n",
            "[8,   183] loss: 0.501\n",
            "[8,   185] loss: 0.781\n",
            "[8,   187] loss: 0.812\n",
            "[8,   189] loss: 0.998\n",
            "[8,   191] loss: 0.494\n",
            "[8,   193] loss: 0.686\n",
            "[8,   195] loss: 0.622\n",
            "[8,   197] loss: 0.672\n",
            "[8,   199] loss: 0.782\n",
            "[8,   201] loss: 0.499\n",
            "[8,   203] loss: 0.748\n",
            "[8,   205] loss: 1.029\n",
            "[8,   207] loss: 0.989\n",
            "[8,   209] loss: 0.847\n",
            "[8,   211] loss: 0.622\n",
            "[8,   213] loss: 0.681\n",
            "[8,   215] loss: 0.779\n",
            "[8,   217] loss: 0.589\n",
            "[8,   219] loss: 1.292\n",
            "[8,   221] loss: 1.117\n",
            "[8,   223] loss: 1.501\n",
            "[8,   225] loss: 1.151\n",
            "[8,   227] loss: 1.422\n",
            "[8,   229] loss: 1.214\n",
            "[8,   231] loss: 2.017\n",
            "[8,   233] loss: 1.161\n",
            "[8,   235] loss: 1.397\n",
            "[8,   237] loss: 1.093\n",
            "[8,   239] loss: 1.217\n",
            "[8,   241] loss: 0.518\n",
            "[8,   243] loss: 0.675\n",
            "[8,   245] loss: 0.786\n",
            "[8,   247] loss: 0.911\n",
            "[8,   249] loss: 0.768\n",
            "[8,   251] loss: 1.142\n",
            "[8,   253] loss: 0.575\n",
            "[8,   255] loss: 0.778\n",
            "[8,   257] loss: 0.623\n",
            "[8,   259] loss: 1.157\n",
            "[8,   261] loss: 0.868\n",
            "[8,   263] loss: 0.772\n",
            "[8,   265] loss: 0.905\n",
            "[8,   267] loss: 1.077\n",
            "[8,   269] loss: 0.817\n",
            "[8,   271] loss: 0.541\n",
            "[8,   273] loss: 1.131\n",
            "[8,   275] loss: 1.317\n",
            "[8,   277] loss: 0.894\n",
            "[8,   279] loss: 1.172\n",
            "[8,   281] loss: 0.527\n",
            "[8,   283] loss: 0.689\n",
            "[8,   285] loss: 1.733\n",
            "[8,   287] loss: 0.856\n",
            "[8,   289] loss: 0.350\n",
            "[8,   291] loss: 1.073\n",
            "[8,   293] loss: 1.279\n",
            "[8,   295] loss: 1.298\n",
            "[8,   297] loss: 0.877\n",
            "[8,   299] loss: 1.355\n",
            "[8,   301] loss: 0.882\n",
            "[8,   303] loss: 0.910\n",
            "[8,   305] loss: 0.988\n",
            "[8,   307] loss: 0.825\n",
            "[8,   309] loss: 0.538\n",
            "[8,   311] loss: 1.091\n",
            "[8,   313] loss: 0.721\n",
            "[8,   315] loss: 0.987\n",
            "[8,   317] loss: 0.753\n",
            "[8,   319] loss: 0.378\n",
            "[8,   321] loss: 0.864\n",
            "[8,   323] loss: 1.658\n",
            "[8,   325] loss: 0.947\n",
            "[8,   327] loss: 1.805\n",
            "[8,   329] loss: 0.522\n",
            "[8,   331] loss: 0.599\n",
            "[8,   333] loss: 0.850\n",
            "[8,   335] loss: 0.864\n",
            "[8,   337] loss: 0.554\n",
            "[8,   339] loss: 1.223\n",
            "[8,   341] loss: 0.915\n",
            "[8,   343] loss: 0.883\n",
            "[8,   345] loss: 0.738\n",
            "[8,   347] loss: 0.702\n",
            "[8,   349] loss: 0.961\n",
            "[8,   351] loss: 0.468\n",
            "[8,   353] loss: 0.790\n",
            "[8,   355] loss: 0.951\n",
            "[8,   357] loss: 0.919\n",
            "[8,   359] loss: 0.951\n",
            "[8,   361] loss: 0.459\n",
            "[8,   363] loss: 1.126\n",
            "[8,   365] loss: 0.791\n",
            "[8,   367] loss: 0.860\n",
            "[8,   369] loss: 0.770\n",
            "[8,   371] loss: 0.700\n",
            "[8,   373] loss: 0.720\n",
            "[8,   375] loss: 1.272\n",
            "[8,   377] loss: 0.640\n",
            "[8,   379] loss: 0.815\n",
            "[8,   381] loss: 1.346\n",
            "[8,   383] loss: 0.749\n",
            "[8,   385] loss: 0.873\n",
            "[8,   387] loss: 1.130\n",
            "[8,   389] loss: 1.015\n",
            "[8,   391] loss: 0.503\n",
            "[8,   393] loss: 0.648\n",
            "[8,   395] loss: 0.903\n",
            "[8,   397] loss: 0.896\n",
            "[8,   399] loss: 0.967\n",
            "[8,   401] loss: 0.604\n",
            "[8,   403] loss: 0.734\n",
            "[8,   405] loss: 0.989\n",
            "[8,   407] loss: 0.738\n",
            "[8,   409] loss: 0.613\n",
            "[8,   411] loss: 0.871\n",
            "[8,   413] loss: 0.689\n",
            "[8,   415] loss: 0.334\n",
            "[8,   417] loss: 1.489\n",
            "[8,   419] loss: 0.423\n",
            "[8,   421] loss: 1.618\n",
            "[8,   423] loss: 0.761\n",
            "[8,   425] loss: 1.448\n",
            "[8,   427] loss: 0.566\n",
            "[8,   429] loss: 1.258\n",
            "[8,   431] loss: 0.398\n",
            "[8,   433] loss: 0.715\n",
            "[8,   435] loss: 1.127\n",
            "[8,   437] loss: 0.705\n",
            "[8,   439] loss: 0.497\n",
            "[8,   441] loss: 0.905\n",
            "[8,   443] loss: 0.803\n",
            "[8,   445] loss: 0.631\n",
            "[8,   447] loss: 0.421\n",
            "[8,   449] loss: 0.331\n",
            "[8,   451] loss: 0.912\n",
            "[8,   453] loss: 0.997\n",
            "[8,   455] loss: 0.501\n",
            "[8,   457] loss: 0.689\n",
            "[8,   459] loss: 0.616\n",
            "[8,   461] loss: 0.743\n",
            "[8,   463] loss: 0.818\n",
            "[8,   465] loss: 0.558\n",
            "[8,   467] loss: 0.410\n",
            "Epoch 7, Train Loss: 374.95906649529934, Val Loss: 1.0069720447063446, Val Acc: 82.7438370846731\n",
            "Epoch 7, Test Acc: 80.10695187165776\n",
            "[9,     1] loss: 0.582\n",
            "[9,     3] loss: 0.693\n",
            "[9,     5] loss: 1.239\n",
            "[9,     7] loss: 0.592\n",
            "[9,     9] loss: 0.435\n",
            "[9,    11] loss: 0.461\n",
            "[9,    13] loss: 0.714\n",
            "[9,    15] loss: 0.533\n",
            "[9,    17] loss: 0.722\n",
            "[9,    19] loss: 0.732\n",
            "[9,    21] loss: 0.638\n",
            "[9,    23] loss: 0.603\n",
            "[9,    25] loss: 0.665\n",
            "[9,    27] loss: 0.994\n",
            "[9,    29] loss: 0.579\n",
            "[9,    31] loss: 0.974\n",
            "[9,    33] loss: 0.795\n",
            "[9,    35] loss: 0.799\n",
            "[9,    37] loss: 0.917\n",
            "[9,    39] loss: 0.528\n",
            "[9,    41] loss: 0.781\n",
            "[9,    43] loss: 0.475\n",
            "[9,    45] loss: 0.455\n",
            "[9,    47] loss: 0.647\n",
            "[9,    49] loss: 0.572\n",
            "[9,    51] loss: 0.696\n",
            "[9,    53] loss: 0.534\n",
            "[9,    55] loss: 0.898\n",
            "[9,    57] loss: 0.404\n",
            "[9,    59] loss: 0.593\n",
            "[9,    61] loss: 0.562\n",
            "[9,    63] loss: 0.324\n",
            "[9,    65] loss: 0.816\n",
            "[9,    67] loss: 0.808\n",
            "[9,    69] loss: 0.578\n",
            "[9,    71] loss: 0.570\n",
            "[9,    73] loss: 0.554\n",
            "[9,    75] loss: 0.624\n",
            "[9,    77] loss: 0.649\n",
            "[9,    79] loss: 0.936\n",
            "[9,    81] loss: 0.534\n",
            "[9,    83] loss: 0.596\n",
            "[9,    85] loss: 0.761\n",
            "[9,    87] loss: 0.189\n",
            "[9,    89] loss: 0.336\n",
            "[9,    91] loss: 0.450\n",
            "[9,    93] loss: 0.666\n",
            "[9,    95] loss: 0.414\n",
            "[9,    97] loss: 0.552\n",
            "[9,    99] loss: 1.081\n",
            "[9,   101] loss: 0.524\n",
            "[9,   103] loss: 0.425\n",
            "[9,   105] loss: 0.619\n",
            "[9,   107] loss: 0.606\n",
            "[9,   109] loss: 0.446\n",
            "[9,   111] loss: 0.302\n",
            "[9,   113] loss: 0.695\n",
            "[9,   115] loss: 0.597\n",
            "[9,   117] loss: 0.311\n",
            "[9,   119] loss: 0.436\n",
            "[9,   121] loss: 0.477\n",
            "[9,   123] loss: 0.245\n",
            "[9,   125] loss: 0.414\n",
            "[9,   127] loss: 0.358\n",
            "[9,   129] loss: 0.366\n",
            "[9,   131] loss: 0.573\n",
            "[9,   133] loss: 0.672\n",
            "[9,   135] loss: 0.307\n",
            "[9,   137] loss: 0.396\n",
            "[9,   139] loss: 0.568\n",
            "[9,   141] loss: 0.217\n",
            "[9,   143] loss: 0.330\n",
            "[9,   145] loss: 0.461\n",
            "[9,   147] loss: 0.549\n",
            "[9,   149] loss: 0.186\n",
            "[9,   151] loss: 1.035\n",
            "[9,   153] loss: 1.226\n",
            "[9,   155] loss: 0.624\n",
            "[9,   157] loss: 0.711\n",
            "[9,   159] loss: 0.366\n",
            "[9,   161] loss: 0.629\n",
            "[9,   163] loss: 1.009\n",
            "[9,   165] loss: 0.331\n",
            "[9,   167] loss: 0.839\n",
            "[9,   169] loss: 1.442\n",
            "[9,   171] loss: 0.431\n",
            "[9,   173] loss: 0.293\n",
            "[9,   175] loss: 0.511\n",
            "[9,   177] loss: 0.216\n",
            "[9,   179] loss: 0.319\n",
            "[9,   181] loss: 0.401\n",
            "[9,   183] loss: 0.694\n",
            "[9,   185] loss: 0.585\n",
            "[9,   187] loss: 0.512\n",
            "[9,   189] loss: 0.219\n",
            "[9,   191] loss: 0.591\n",
            "[9,   193] loss: 0.312\n",
            "[9,   195] loss: 0.482\n",
            "[9,   197] loss: 0.511\n",
            "[9,   199] loss: 0.439\n",
            "[9,   201] loss: 0.365\n",
            "[9,   203] loss: 0.344\n",
            "[9,   205] loss: 0.498\n",
            "[9,   207] loss: 0.370\n",
            "[9,   209] loss: 0.467\n",
            "[9,   211] loss: 0.343\n",
            "[9,   213] loss: 0.278\n",
            "[9,   215] loss: 0.686\n",
            "[9,   217] loss: 0.733\n",
            "[9,   219] loss: 0.384\n",
            "[9,   221] loss: 0.394\n",
            "[9,   223] loss: 0.254\n",
            "[9,   225] loss: 0.331\n",
            "[9,   227] loss: 0.375\n",
            "[9,   229] loss: 0.392\n",
            "[9,   231] loss: 0.342\n",
            "[9,   233] loss: 0.467\n",
            "[9,   235] loss: 0.763\n",
            "[9,   237] loss: 0.379\n",
            "[9,   239] loss: 0.932\n",
            "[9,   241] loss: 0.429\n",
            "[9,   243] loss: 0.434\n",
            "[9,   245] loss: 0.380\n",
            "[9,   247] loss: 0.242\n",
            "[9,   249] loss: 0.684\n",
            "[9,   251] loss: 0.291\n",
            "[9,   253] loss: 0.417\n",
            "[9,   255] loss: 0.273\n",
            "[9,   257] loss: 0.482\n",
            "[9,   259] loss: 0.518\n",
            "[9,   261] loss: 0.500\n",
            "[9,   263] loss: 0.393\n",
            "[9,   265] loss: 0.610\n",
            "[9,   267] loss: 0.382\n",
            "[9,   269] loss: 0.522\n",
            "[9,   271] loss: 0.645\n",
            "[9,   273] loss: 0.894\n",
            "[9,   275] loss: 0.293\n",
            "[9,   277] loss: 0.810\n",
            "[9,   279] loss: 0.443\n",
            "[9,   281] loss: 0.784\n",
            "[9,   283] loss: 0.708\n",
            "[9,   285] loss: 0.601\n",
            "[9,   287] loss: 0.373\n",
            "[9,   289] loss: 0.297\n",
            "[9,   291] loss: 0.566\n",
            "[9,   293] loss: 0.634\n",
            "[9,   295] loss: 0.334\n",
            "[9,   297] loss: 0.566\n",
            "[9,   299] loss: 0.460\n",
            "[9,   301] loss: 0.498\n",
            "[9,   303] loss: 0.417\n",
            "[9,   305] loss: 0.175\n",
            "[9,   307] loss: 0.113\n",
            "[9,   309] loss: 0.544\n",
            "[9,   311] loss: 0.926\n",
            "[9,   313] loss: 0.448\n",
            "[9,   315] loss: 0.884\n",
            "[9,   317] loss: 0.702\n",
            "[9,   319] loss: 0.799\n",
            "[9,   321] loss: 0.814\n",
            "[9,   323] loss: 0.589\n",
            "[9,   325] loss: 0.629\n",
            "[9,   327] loss: 0.277\n",
            "[9,   329] loss: 0.288\n",
            "[9,   331] loss: 0.718\n",
            "[9,   333] loss: 0.327\n",
            "[9,   335] loss: 0.787\n",
            "[9,   337] loss: 0.368\n",
            "[9,   339] loss: 0.530\n",
            "[9,   341] loss: 0.672\n",
            "[9,   343] loss: 0.399\n",
            "[9,   345] loss: 0.299\n",
            "[9,   347] loss: 0.504\n",
            "[9,   349] loss: 0.426\n",
            "[9,   351] loss: 0.592\n",
            "[9,   353] loss: 0.394\n",
            "[9,   355] loss: 0.651\n",
            "[9,   357] loss: 0.490\n",
            "[9,   359] loss: 0.363\n",
            "[9,   361] loss: 0.505\n",
            "[9,   363] loss: 0.277\n",
            "[9,   365] loss: 0.698\n",
            "[9,   367] loss: 0.726\n",
            "[9,   369] loss: 0.256\n",
            "[9,   371] loss: 0.692\n",
            "[9,   373] loss: 0.708\n",
            "[9,   375] loss: 0.429\n",
            "[9,   377] loss: 0.372\n",
            "[9,   379] loss: 1.039\n",
            "[9,   381] loss: 0.496\n",
            "[9,   383] loss: 0.420\n",
            "[9,   385] loss: 0.341\n",
            "[9,   387] loss: 0.375\n",
            "[9,   389] loss: 0.519\n",
            "[9,   391] loss: 0.603\n",
            "[9,   393] loss: 0.271\n",
            "[9,   395] loss: 0.816\n",
            "[9,   397] loss: 0.258\n",
            "[9,   399] loss: 0.138\n",
            "[9,   401] loss: 0.585\n",
            "[9,   403] loss: 0.540\n",
            "[9,   405] loss: 0.718\n",
            "[9,   407] loss: 0.657\n",
            "[9,   409] loss: 0.531\n",
            "[9,   411] loss: 0.112\n",
            "[9,   413] loss: 0.365\n",
            "[9,   415] loss: 0.538\n",
            "[9,   417] loss: 0.568\n",
            "[9,   419] loss: 0.465\n",
            "[9,   421] loss: 0.465\n",
            "[9,   423] loss: 0.210\n",
            "[9,   425] loss: 0.184\n",
            "[9,   427] loss: 0.609\n",
            "[9,   429] loss: 0.404\n",
            "[9,   431] loss: 0.550\n",
            "[9,   433] loss: 0.241\n",
            "[9,   435] loss: 0.343\n",
            "[9,   437] loss: 0.628\n",
            "[9,   439] loss: 0.696\n",
            "[9,   441] loss: 0.953\n",
            "[9,   443] loss: 0.193\n",
            "[9,   445] loss: 0.410\n",
            "[9,   447] loss: 0.257\n",
            "[9,   449] loss: 0.397\n",
            "[9,   451] loss: 0.388\n",
            "[9,   453] loss: 0.566\n",
            "[9,   455] loss: 0.776\n",
            "[9,   457] loss: 0.523\n",
            "[9,   459] loss: 0.354\n",
            "[9,   461] loss: 0.286\n",
            "[9,   463] loss: 0.463\n",
            "[9,   465] loss: 0.654\n",
            "[9,   467] loss: 0.335\n",
            "Epoch 8, Train Loss: 247.29607278853655, Val Loss: 6.778888165856064, Val Acc: 85.95927116827438\n",
            "Epoch 8, Test Acc: 86.63101604278074\n",
            "[10,     1] loss: 0.311\n",
            "[10,     3] loss: 0.308\n",
            "[10,     5] loss: 0.313\n",
            "[10,     7] loss: 0.403\n",
            "[10,     9] loss: 0.267\n",
            "[10,    11] loss: 0.625\n",
            "[10,    13] loss: 0.230\n",
            "[10,    15] loss: 0.303\n",
            "[10,    17] loss: 0.183\n",
            "[10,    19] loss: 0.348\n",
            "[10,    21] loss: 0.610\n",
            "[10,    23] loss: 0.290\n",
            "[10,    25] loss: 0.440\n",
            "[10,    27] loss: 0.235\n",
            "[10,    29] loss: 1.127\n",
            "[10,    31] loss: 0.292\n",
            "[10,    33] loss: 0.239\n",
            "[10,    35] loss: 0.794\n",
            "[10,    37] loss: 0.605\n",
            "[10,    39] loss: 0.284\n",
            "[10,    41] loss: 0.452\n",
            "[10,    43] loss: 0.798\n",
            "[10,    45] loss: 0.391\n",
            "[10,    47] loss: 0.267\n",
            "[10,    49] loss: 0.601\n",
            "[10,    51] loss: 0.275\n",
            "[10,    53] loss: 0.345\n",
            "[10,    55] loss: 0.392\n",
            "[10,    57] loss: 0.228\n",
            "[10,    59] loss: 0.435\n",
            "[10,    61] loss: 0.182\n",
            "[10,    63] loss: 0.308\n",
            "[10,    65] loss: 0.351\n",
            "[10,    67] loss: 0.266\n",
            "[10,    69] loss: 0.318\n",
            "[10,    71] loss: 0.399\n",
            "[10,    73] loss: 0.266\n",
            "[10,    75] loss: 0.256\n",
            "[10,    77] loss: 0.264\n",
            "[10,    79] loss: 0.285\n",
            "[10,    81] loss: 0.508\n",
            "[10,    83] loss: 0.395\n",
            "[10,    85] loss: 0.277\n",
            "[10,    87] loss: 0.175\n",
            "[10,    89] loss: 0.387\n",
            "[10,    91] loss: 0.328\n",
            "[10,    93] loss: 0.500\n",
            "[10,    95] loss: 0.609\n",
            "[10,    97] loss: 0.848\n",
            "[10,    99] loss: 0.531\n",
            "[10,   101] loss: 0.397\n",
            "[10,   103] loss: 0.269\n",
            "[10,   105] loss: 0.764\n",
            "[10,   107] loss: 0.321\n",
            "[10,   109] loss: 0.363\n",
            "[10,   111] loss: 0.089\n",
            "[10,   113] loss: 1.072\n",
            "[10,   115] loss: 0.286\n",
            "[10,   117] loss: 0.647\n",
            "[10,   119] loss: 0.401\n",
            "[10,   121] loss: 0.169\n",
            "[10,   123] loss: 0.528\n",
            "[10,   125] loss: 0.202\n",
            "[10,   127] loss: 0.508\n",
            "[10,   129] loss: 0.374\n",
            "[10,   131] loss: 0.583\n",
            "[10,   133] loss: 0.322\n",
            "[10,   135] loss: 0.767\n",
            "[10,   137] loss: 0.234\n",
            "[10,   139] loss: 0.247\n",
            "[10,   141] loss: 0.691\n",
            "[10,   143] loss: 0.310\n",
            "[10,   145] loss: 0.135\n",
            "[10,   147] loss: 0.252\n",
            "[10,   149] loss: 0.584\n",
            "[10,   151] loss: 0.397\n",
            "[10,   153] loss: 0.764\n",
            "[10,   155] loss: 0.260\n",
            "[10,   157] loss: 0.502\n",
            "[10,   159] loss: 0.307\n",
            "[10,   161] loss: 0.547\n",
            "[10,   163] loss: 0.217\n",
            "[10,   165] loss: 0.358\n",
            "[10,   167] loss: 0.502\n",
            "[10,   169] loss: 0.632\n",
            "[10,   171] loss: 0.254\n",
            "[10,   173] loss: 0.533\n",
            "[10,   175] loss: 0.177\n",
            "[10,   177] loss: 0.090\n",
            "[10,   179] loss: 0.266\n",
            "[10,   181] loss: 0.580\n",
            "[10,   183] loss: 0.349\n",
            "[10,   185] loss: 0.458\n",
            "[10,   187] loss: 0.145\n",
            "[10,   189] loss: 0.279\n",
            "[10,   191] loss: 0.459\n",
            "[10,   193] loss: 0.367\n",
            "[10,   195] loss: 0.456\n",
            "[10,   197] loss: 0.678\n",
            "[10,   199] loss: 0.518\n",
            "[10,   201] loss: 0.464\n",
            "[10,   203] loss: 0.306\n",
            "[10,   205] loss: 0.391\n",
            "[10,   207] loss: 0.495\n",
            "[10,   209] loss: 0.365\n",
            "[10,   211] loss: 0.362\n",
            "[10,   213] loss: 0.203\n",
            "[10,   215] loss: 0.318\n",
            "[10,   217] loss: 0.535\n",
            "[10,   219] loss: 0.220\n",
            "[10,   221] loss: 0.211\n",
            "[10,   223] loss: 0.295\n",
            "[10,   225] loss: 0.352\n",
            "[10,   227] loss: 0.350\n",
            "[10,   229] loss: 0.156\n",
            "[10,   231] loss: 0.422\n",
            "[10,   233] loss: 0.378\n",
            "[10,   235] loss: 0.248\n",
            "[10,   237] loss: 0.428\n",
            "[10,   239] loss: 0.463\n",
            "[10,   241] loss: 0.631\n",
            "[10,   243] loss: 0.809\n",
            "[10,   245] loss: 0.369\n",
            "[10,   247] loss: 0.701\n",
            "[10,   249] loss: 0.470\n",
            "[10,   251] loss: 0.354\n",
            "[10,   253] loss: 0.357\n",
            "[10,   255] loss: 0.378\n",
            "[10,   257] loss: 1.046\n",
            "[10,   259] loss: 1.333\n",
            "[10,   261] loss: 0.227\n",
            "[10,   263] loss: 0.266\n",
            "[10,   265] loss: 0.431\n",
            "[10,   267] loss: 0.208\n",
            "[10,   269] loss: 0.232\n",
            "[10,   271] loss: 0.315\n",
            "[10,   273] loss: 0.279\n",
            "[10,   275] loss: 0.424\n",
            "[10,   277] loss: 0.224\n",
            "[10,   279] loss: 0.812\n",
            "[10,   281] loss: 0.766\n",
            "[10,   283] loss: 0.450\n",
            "[10,   285] loss: 0.340\n",
            "[10,   287] loss: 0.884\n",
            "[10,   289] loss: 0.405\n",
            "[10,   291] loss: 0.179\n",
            "[10,   293] loss: 0.536\n",
            "[10,   295] loss: 0.351\n",
            "[10,   297] loss: 0.244\n",
            "[10,   299] loss: 0.536\n",
            "[10,   301] loss: 0.428\n",
            "[10,   303] loss: 0.060\n",
            "[10,   305] loss: 0.439\n",
            "[10,   307] loss: 0.416\n",
            "[10,   309] loss: 0.233\n",
            "[10,   311] loss: 0.269\n",
            "[10,   313] loss: 0.469\n",
            "[10,   315] loss: 0.483\n",
            "[10,   317] loss: 0.334\n",
            "[10,   319] loss: 0.316\n",
            "[10,   321] loss: 0.163\n",
            "[10,   323] loss: 0.256\n",
            "[10,   325] loss: 0.134\n",
            "[10,   327] loss: 0.209\n",
            "[10,   329] loss: 0.645\n",
            "[10,   331] loss: 0.736\n",
            "[10,   333] loss: 0.301\n",
            "[10,   335] loss: 0.271\n",
            "[10,   337] loss: 0.411\n",
            "[10,   339] loss: 0.664\n",
            "[10,   341] loss: 0.459\n",
            "[10,   343] loss: 0.595\n",
            "[10,   345] loss: 0.371\n",
            "[10,   347] loss: 0.277\n",
            "[10,   349] loss: 0.088\n",
            "[10,   351] loss: 0.889\n",
            "[10,   353] loss: 0.628\n",
            "[10,   355] loss: 0.591\n",
            "[10,   357] loss: 0.711\n",
            "[10,   359] loss: 0.173\n",
            "[10,   361] loss: 0.328\n",
            "[10,   363] loss: 0.092\n",
            "[10,   365] loss: 0.274\n",
            "[10,   367] loss: 0.409\n",
            "[10,   369] loss: 0.161\n",
            "[10,   371] loss: 0.162\n",
            "[10,   373] loss: 0.833\n",
            "[10,   375] loss: 0.308\n",
            "[10,   377] loss: 0.487\n",
            "[10,   379] loss: 0.278\n",
            "[10,   381] loss: 0.427\n",
            "[10,   383] loss: 0.342\n",
            "[10,   385] loss: 0.155\n",
            "[10,   387] loss: 0.714\n",
            "[10,   389] loss: 0.174\n",
            "[10,   391] loss: 0.347\n",
            "[10,   393] loss: 0.813\n",
            "[10,   395] loss: 0.071\n",
            "[10,   397] loss: 0.464\n",
            "[10,   399] loss: 0.112\n",
            "[10,   401] loss: 0.387\n",
            "[10,   403] loss: 0.683\n",
            "[10,   405] loss: 0.357\n",
            "[10,   407] loss: 0.276\n",
            "[10,   409] loss: 0.485\n",
            "[10,   411] loss: 0.293\n",
            "[10,   413] loss: 0.447\n",
            "[10,   415] loss: 0.367\n",
            "[10,   417] loss: 0.671\n",
            "[10,   419] loss: 0.323\n",
            "[10,   421] loss: 0.492\n",
            "[10,   423] loss: 0.315\n",
            "[10,   425] loss: 0.333\n",
            "[10,   427] loss: 0.196\n",
            "[10,   429] loss: 0.589\n",
            "[10,   431] loss: 0.603\n",
            "[10,   433] loss: 0.909\n",
            "[10,   435] loss: 0.379\n",
            "[10,   437] loss: 0.486\n",
            "[10,   439] loss: 0.620\n",
            "[10,   441] loss: 0.277\n",
            "[10,   443] loss: 0.746\n",
            "[10,   445] loss: 0.527\n",
            "[10,   447] loss: 0.360\n",
            "[10,   449] loss: 0.722\n",
            "[10,   451] loss: 0.428\n",
            "[10,   453] loss: 0.526\n",
            "[10,   455] loss: 0.375\n",
            "[10,   457] loss: 0.662\n",
            "[10,   459] loss: 0.201\n",
            "[10,   461] loss: 0.329\n",
            "[10,   463] loss: 0.396\n",
            "[10,   465] loss: 0.298\n",
            "[10,   467] loss: 0.685\n",
            "Epoch 9, Train Loss: 193.37937603984028, Val Loss: 0.31975295902940176, Val Acc: 93.14040728831726\n",
            "Epoch 9, Test Acc: 92.83422459893048\n",
            "[11,     1] loss: 0.186\n",
            "[11,     3] loss: 0.393\n",
            "[11,     5] loss: 0.163\n",
            "[11,     7] loss: 0.396\n",
            "[11,     9] loss: 0.184\n",
            "[11,    11] loss: 0.423\n",
            "[11,    13] loss: 0.464\n",
            "[11,    15] loss: 0.287\n",
            "[11,    17] loss: 0.518\n",
            "[11,    19] loss: 0.459\n",
            "[11,    21] loss: 0.491\n",
            "[11,    23] loss: 0.287\n",
            "[11,    25] loss: 0.276\n",
            "[11,    27] loss: 0.250\n",
            "[11,    29] loss: 0.230\n",
            "[11,    31] loss: 0.541\n",
            "[11,    33] loss: 0.359\n",
            "[11,    35] loss: 0.212\n",
            "[11,    37] loss: 0.408\n",
            "[11,    39] loss: 0.193\n",
            "[11,    41] loss: 0.164\n",
            "[11,    43] loss: 0.107\n",
            "[11,    45] loss: 0.343\n",
            "[11,    47] loss: 0.510\n",
            "[11,    49] loss: 0.265\n",
            "[11,    51] loss: 0.355\n",
            "[11,    53] loss: 0.440\n",
            "[11,    55] loss: 0.409\n",
            "[11,    57] loss: 0.355\n",
            "[11,    59] loss: 0.712\n",
            "[11,    61] loss: 0.318\n",
            "[11,    63] loss: 0.172\n",
            "[11,    65] loss: 0.393\n",
            "[11,    67] loss: 0.788\n",
            "[11,    69] loss: 0.235\n",
            "[11,    71] loss: 0.661\n",
            "[11,    73] loss: 0.200\n",
            "[11,    75] loss: 0.217\n",
            "[11,    77] loss: 0.209\n",
            "[11,    79] loss: 0.385\n",
            "[11,    81] loss: 0.238\n",
            "[11,    83] loss: 0.713\n",
            "[11,    85] loss: 0.195\n",
            "[11,    87] loss: 0.171\n",
            "[11,    89] loss: 0.176\n",
            "[11,    91] loss: 0.151\n",
            "[11,    93] loss: 0.222\n",
            "[11,    95] loss: 0.205\n",
            "[11,    97] loss: 0.418\n",
            "[11,    99] loss: 0.120\n",
            "[11,   101] loss: 0.186\n",
            "[11,   103] loss: 0.443\n",
            "[11,   105] loss: 0.232\n",
            "[11,   107] loss: 0.298\n",
            "[11,   109] loss: 0.226\n",
            "[11,   111] loss: 0.261\n",
            "[11,   113] loss: 0.235\n",
            "[11,   115] loss: 0.498\n",
            "[11,   117] loss: 0.923\n",
            "[11,   119] loss: 0.097\n",
            "[11,   121] loss: 0.657\n",
            "[11,   123] loss: 0.604\n",
            "[11,   125] loss: 0.553\n",
            "[11,   127] loss: 0.452\n",
            "[11,   129] loss: 0.530\n",
            "[11,   131] loss: 0.289\n",
            "[11,   133] loss: 0.277\n",
            "[11,   135] loss: 0.249\n",
            "[11,   137] loss: 0.644\n",
            "[11,   139] loss: 0.365\n",
            "[11,   141] loss: 0.596\n",
            "[11,   143] loss: 0.055\n",
            "[11,   145] loss: 0.080\n",
            "[11,   147] loss: 0.967\n",
            "[11,   149] loss: 0.160\n",
            "[11,   151] loss: 0.436\n",
            "[11,   153] loss: 0.204\n",
            "[11,   155] loss: 0.423\n",
            "[11,   157] loss: 0.302\n",
            "[11,   159] loss: 0.280\n",
            "[11,   161] loss: 0.271\n",
            "[11,   163] loss: 0.206\n",
            "[11,   165] loss: 0.436\n",
            "[11,   167] loss: 0.461\n",
            "[11,   169] loss: 0.272\n",
            "[11,   171] loss: 0.229\n",
            "[11,   173] loss: 0.816\n",
            "[11,   175] loss: 0.128\n",
            "[11,   177] loss: 0.409\n",
            "[11,   179] loss: 0.048\n",
            "[11,   181] loss: 0.365\n",
            "[11,   183] loss: 0.425\n",
            "[11,   185] loss: 0.474\n",
            "[11,   187] loss: 0.189\n",
            "[11,   189] loss: 0.177\n",
            "[11,   191] loss: 0.276\n",
            "[11,   193] loss: 0.752\n",
            "[11,   195] loss: 1.007\n",
            "[11,   197] loss: 0.197\n",
            "[11,   199] loss: 0.097\n",
            "[11,   201] loss: 0.379\n",
            "[11,   203] loss: 0.318\n",
            "[11,   205] loss: 0.056\n",
            "[11,   207] loss: 0.563\n",
            "[11,   209] loss: 0.474\n",
            "[11,   211] loss: 0.106\n",
            "[11,   213] loss: 0.109\n",
            "[11,   215] loss: 0.671\n",
            "[11,   217] loss: 0.330\n",
            "[11,   219] loss: 0.691\n",
            "[11,   221] loss: 0.391\n",
            "[11,   223] loss: 0.250\n",
            "[11,   225] loss: 0.323\n",
            "[11,   227] loss: 0.610\n",
            "[11,   229] loss: 0.613\n",
            "[11,   231] loss: 0.583\n",
            "[11,   233] loss: 0.240\n",
            "[11,   235] loss: 0.059\n",
            "[11,   237] loss: 0.884\n",
            "[11,   239] loss: 0.249\n",
            "[11,   241] loss: 0.118\n",
            "[11,   243] loss: 0.175\n",
            "[11,   245] loss: 0.109\n",
            "[11,   247] loss: 0.401\n",
            "[11,   249] loss: 0.516\n",
            "[11,   251] loss: 0.364\n",
            "[11,   253] loss: 0.215\n",
            "[11,   255] loss: 0.315\n",
            "[11,   257] loss: 0.117\n",
            "[11,   259] loss: 0.305\n",
            "[11,   261] loss: 0.367\n",
            "[11,   263] loss: 0.286\n",
            "[11,   265] loss: 0.298\n",
            "[11,   267] loss: 0.258\n",
            "[11,   269] loss: 0.330\n",
            "[11,   271] loss: 0.568\n",
            "[11,   273] loss: 0.256\n",
            "[11,   275] loss: 0.492\n",
            "[11,   277] loss: 0.451\n",
            "[11,   279] loss: 0.314\n",
            "[11,   281] loss: 0.302\n",
            "[11,   283] loss: 1.011\n",
            "[11,   285] loss: 0.294\n",
            "[11,   287] loss: 0.313\n",
            "[11,   289] loss: 0.076\n",
            "[11,   291] loss: 0.388\n",
            "[11,   293] loss: 0.286\n",
            "[11,   295] loss: 0.528\n",
            "[11,   297] loss: 0.349\n",
            "[11,   299] loss: 0.276\n",
            "[11,   301] loss: 0.499\n",
            "[11,   303] loss: 0.561\n",
            "[11,   305] loss: 0.561\n",
            "[11,   307] loss: 0.207\n",
            "[11,   309] loss: 0.291\n",
            "[11,   311] loss: 0.284\n",
            "[11,   313] loss: 0.652\n",
            "[11,   315] loss: 0.295\n",
            "[11,   317] loss: 0.158\n",
            "[11,   319] loss: 0.545\n",
            "[11,   321] loss: 0.277\n",
            "[11,   323] loss: 0.229\n",
            "[11,   325] loss: 0.194\n",
            "[11,   327] loss: 0.320\n",
            "[11,   329] loss: 0.480\n",
            "[11,   331] loss: 0.280\n",
            "[11,   333] loss: 0.494\n",
            "[11,   335] loss: 0.203\n",
            "[11,   337] loss: 0.100\n",
            "[11,   339] loss: 0.256\n",
            "[11,   341] loss: 0.755\n",
            "[11,   343] loss: 0.277\n",
            "[11,   345] loss: 0.559\n",
            "[11,   347] loss: 0.289\n",
            "[11,   349] loss: 0.093\n",
            "[11,   351] loss: 0.471\n",
            "[11,   353] loss: 0.185\n",
            "[11,   355] loss: 1.144\n",
            "[11,   357] loss: 0.240\n",
            "[11,   359] loss: 0.313\n",
            "[11,   361] loss: 0.154\n",
            "[11,   363] loss: 0.264\n",
            "[11,   365] loss: 0.351\n",
            "[11,   367] loss: 0.201\n",
            "[11,   369] loss: 0.355\n",
            "[11,   371] loss: 0.117\n",
            "[11,   373] loss: 0.300\n",
            "[11,   375] loss: 0.095\n",
            "[11,   377] loss: 0.546\n",
            "[11,   379] loss: 0.115\n",
            "[11,   381] loss: 0.268\n",
            "[11,   383] loss: 0.451\n",
            "[11,   385] loss: 0.465\n",
            "[11,   387] loss: 0.496\n",
            "[11,   389] loss: 0.293\n",
            "[11,   391] loss: 0.555\n",
            "[11,   393] loss: 0.167\n",
            "[11,   395] loss: 0.291\n",
            "[11,   397] loss: 0.228\n",
            "[11,   399] loss: 0.332\n",
            "[11,   401] loss: 0.156\n",
            "[11,   403] loss: 0.344\n",
            "[11,   405] loss: 0.437\n",
            "[11,   407] loss: 0.539\n",
            "[11,   409] loss: 0.328\n",
            "[11,   411] loss: 0.356\n",
            "[11,   413] loss: 0.214\n",
            "[11,   415] loss: 0.263\n",
            "[11,   417] loss: 0.224\n",
            "[11,   419] loss: 0.242\n",
            "[11,   421] loss: 0.495\n",
            "[11,   423] loss: 0.397\n",
            "[11,   425] loss: 0.222\n",
            "[11,   427] loss: 0.526\n",
            "[11,   429] loss: 0.308\n",
            "[11,   431] loss: 0.183\n",
            "[11,   433] loss: 0.629\n",
            "[11,   435] loss: 0.150\n",
            "[11,   437] loss: 0.339\n",
            "[11,   439] loss: 0.456\n",
            "[11,   441] loss: 0.476\n",
            "[11,   443] loss: 0.282\n",
            "[11,   445] loss: 0.129\n",
            "[11,   447] loss: 0.180\n",
            "[11,   449] loss: 0.434\n",
            "[11,   451] loss: 0.502\n",
            "[11,   453] loss: 0.623\n",
            "[11,   455] loss: 0.210\n",
            "[11,   457] loss: 0.509\n",
            "[11,   459] loss: 0.442\n",
            "[11,   461] loss: 0.371\n",
            "[11,   463] loss: 0.220\n",
            "[11,   465] loss: 0.156\n",
            "[11,   467] loss: 0.626\n",
            "Epoch 10, Train Loss: 165.0475406125188, Val Loss: 0.46617048296888, Val Acc: 87.67416934619507\n",
            "Epoch 10, Test Acc: 87.70053475935829\n",
            "[12,     1] loss: 0.011\n",
            "[12,     3] loss: 0.776\n",
            "[12,     5] loss: 0.527\n",
            "[12,     7] loss: 0.373\n",
            "[12,     9] loss: 0.184\n",
            "[12,    11] loss: 0.279\n",
            "[12,    13] loss: 0.329\n",
            "[12,    15] loss: 1.097\n",
            "[12,    17] loss: 0.504\n",
            "[12,    19] loss: 0.895\n",
            "[12,    21] loss: 0.401\n",
            "[12,    23] loss: 0.656\n",
            "[12,    25] loss: 0.580\n",
            "[12,    27] loss: 0.640\n",
            "[12,    29] loss: 0.389\n",
            "[12,    31] loss: 0.251\n",
            "[12,    33] loss: 0.055\n",
            "[12,    35] loss: 0.653\n",
            "[12,    37] loss: 0.236\n",
            "[12,    39] loss: 0.312\n",
            "[12,    41] loss: 0.225\n",
            "[12,    43] loss: 0.199\n",
            "[12,    45] loss: 0.255\n",
            "[12,    47] loss: 0.554\n",
            "[12,    49] loss: 0.176\n",
            "[12,    51] loss: 0.381\n",
            "[12,    53] loss: 0.223\n",
            "[12,    55] loss: 0.131\n",
            "[12,    57] loss: 0.531\n",
            "[12,    59] loss: 0.110\n",
            "[12,    61] loss: 0.104\n",
            "[12,    63] loss: 0.228\n",
            "[12,    65] loss: 0.147\n",
            "[12,    67] loss: 0.170\n",
            "[12,    69] loss: 0.298\n",
            "[12,    71] loss: 0.293\n",
            "[12,    73] loss: 0.152\n",
            "[12,    75] loss: 0.620\n",
            "[12,    77] loss: 0.197\n",
            "[12,    79] loss: 0.771\n",
            "[12,    81] loss: 0.658\n",
            "[12,    83] loss: 0.312\n",
            "[12,    85] loss: 0.248\n",
            "[12,    87] loss: 0.372\n",
            "[12,    89] loss: 0.288\n",
            "[12,    91] loss: 0.290\n",
            "[12,    93] loss: 0.310\n",
            "[12,    95] loss: 0.259\n",
            "[12,    97] loss: 0.421\n",
            "[12,    99] loss: 0.315\n",
            "[12,   101] loss: 0.613\n",
            "[12,   103] loss: 0.673\n",
            "[12,   105] loss: 0.240\n",
            "[12,   107] loss: 0.274\n",
            "[12,   109] loss: 0.591\n",
            "[12,   111] loss: 0.537\n",
            "[12,   113] loss: 0.250\n",
            "[12,   115] loss: 0.516\n",
            "[12,   117] loss: 0.167\n",
            "[12,   119] loss: 0.275\n",
            "[12,   121] loss: 0.261\n",
            "[12,   123] loss: 0.250\n",
            "[12,   125] loss: 0.276\n",
            "[12,   127] loss: 0.183\n",
            "[12,   129] loss: 0.171\n",
            "[12,   131] loss: 0.127\n",
            "[12,   133] loss: 0.214\n",
            "[12,   135] loss: 0.287\n",
            "[12,   137] loss: 0.059\n",
            "[12,   139] loss: 0.469\n",
            "[12,   141] loss: 0.315\n",
            "[12,   143] loss: 0.338\n",
            "[12,   145] loss: 0.437\n",
            "[12,   147] loss: 0.236\n",
            "[12,   149] loss: 0.308\n",
            "[12,   151] loss: 0.581\n",
            "[12,   153] loss: 0.895\n",
            "[12,   155] loss: 0.075\n",
            "[12,   157] loss: 0.048\n",
            "[12,   159] loss: 0.138\n",
            "[12,   161] loss: 0.114\n",
            "[12,   163] loss: 0.127\n",
            "[12,   165] loss: 0.225\n",
            "[12,   167] loss: 0.359\n",
            "[12,   169] loss: 0.661\n",
            "[12,   171] loss: 0.206\n",
            "[12,   173] loss: 0.206\n",
            "[12,   175] loss: 0.086\n",
            "[12,   177] loss: 0.416\n",
            "[12,   179] loss: 0.146\n",
            "[12,   181] loss: 0.389\n",
            "[12,   183] loss: 0.238\n",
            "[12,   185] loss: 0.584\n",
            "[12,   187] loss: 0.692\n",
            "[12,   189] loss: 0.321\n",
            "[12,   191] loss: 0.454\n",
            "[12,   193] loss: 0.095\n",
            "[12,   195] loss: 0.167\n",
            "[12,   197] loss: 0.278\n",
            "[12,   199] loss: 0.593\n",
            "[12,   201] loss: 0.768\n",
            "[12,   203] loss: 0.139\n",
            "[12,   205] loss: 0.195\n",
            "[12,   207] loss: 0.646\n",
            "[12,   209] loss: 0.889\n",
            "[12,   211] loss: 0.251\n",
            "[12,   213] loss: 0.632\n",
            "[12,   215] loss: 0.287\n",
            "[12,   217] loss: 0.544\n",
            "[12,   219] loss: 0.272\n",
            "[12,   221] loss: 0.969\n",
            "[12,   223] loss: 0.150\n",
            "[12,   225] loss: 0.514\n",
            "[12,   227] loss: 0.687\n",
            "[12,   229] loss: 0.453\n",
            "[12,   231] loss: 0.138\n",
            "[12,   233] loss: 0.870\n",
            "[12,   235] loss: 0.487\n",
            "[12,   237] loss: 0.518\n",
            "[12,   239] loss: 0.629\n",
            "[12,   241] loss: 0.229\n",
            "[12,   243] loss: 0.434\n",
            "[12,   245] loss: 0.232\n",
            "[12,   247] loss: 0.226\n",
            "[12,   249] loss: 0.154\n",
            "[12,   251] loss: 0.424\n",
            "[12,   253] loss: 0.192\n",
            "[12,   255] loss: 0.498\n",
            "[12,   257] loss: 0.325\n",
            "[12,   259] loss: 0.206\n",
            "[12,   261] loss: 0.339\n",
            "[12,   263] loss: 0.266\n",
            "[12,   265] loss: 0.382\n",
            "[12,   267] loss: 0.251\n",
            "[12,   269] loss: 0.531\n",
            "[12,   271] loss: 0.189\n",
            "[12,   273] loss: 0.376\n",
            "[12,   275] loss: 0.535\n",
            "[12,   277] loss: 0.842\n",
            "[12,   279] loss: 0.531\n",
            "[12,   281] loss: 0.314\n",
            "[12,   283] loss: 0.318\n",
            "[12,   285] loss: 0.252\n",
            "[12,   287] loss: 0.231\n",
            "[12,   289] loss: 0.466\n",
            "[12,   291] loss: 0.304\n",
            "[12,   293] loss: 0.178\n",
            "[12,   295] loss: 0.308\n",
            "[12,   297] loss: 0.367\n",
            "[12,   299] loss: 0.262\n",
            "[12,   301] loss: 0.242\n",
            "[12,   303] loss: 0.421\n",
            "[12,   305] loss: 0.188\n",
            "[12,   307] loss: 0.334\n",
            "[12,   309] loss: 0.345\n",
            "[12,   311] loss: 0.271\n",
            "[12,   313] loss: 0.431\n",
            "[12,   315] loss: 0.126\n",
            "[12,   317] loss: 0.200\n",
            "[12,   319] loss: 0.177\n",
            "[12,   321] loss: 0.352\n",
            "[12,   323] loss: 0.764\n",
            "[12,   325] loss: 0.289\n",
            "[12,   327] loss: 0.212\n",
            "[12,   329] loss: 0.207\n",
            "[12,   331] loss: 0.243\n",
            "[12,   333] loss: 0.166\n",
            "[12,   335] loss: 0.288\n",
            "[12,   337] loss: 0.171\n",
            "[12,   339] loss: 0.221\n",
            "[12,   341] loss: 0.658\n",
            "[12,   343] loss: 0.387\n",
            "[12,   345] loss: 0.587\n",
            "[12,   347] loss: 0.439\n",
            "[12,   349] loss: 1.174\n",
            "[12,   351] loss: 0.224\n",
            "[12,   353] loss: 0.236\n",
            "[12,   355] loss: 0.148\n",
            "[12,   357] loss: 0.517\n",
            "[12,   359] loss: 0.435\n",
            "[12,   361] loss: 0.400\n",
            "[12,   363] loss: 0.522\n",
            "[12,   365] loss: 0.199\n",
            "[12,   367] loss: 0.429\n",
            "[12,   369] loss: 0.589\n",
            "[12,   371] loss: 0.157\n",
            "[12,   373] loss: 0.305\n",
            "[12,   375] loss: 0.527\n",
            "[12,   377] loss: 0.336\n",
            "[12,   379] loss: 0.084\n",
            "[12,   381] loss: 0.261\n",
            "[12,   383] loss: 0.502\n",
            "[12,   385] loss: 0.284\n",
            "[12,   387] loss: 0.579\n",
            "[12,   389] loss: 0.332\n",
            "[12,   391] loss: 0.104\n",
            "[12,   393] loss: 0.635\n",
            "[12,   395] loss: 0.356\n",
            "[12,   397] loss: 0.211\n",
            "[12,   399] loss: 0.176\n",
            "[12,   401] loss: 0.409\n",
            "[12,   403] loss: 0.456\n",
            "[12,   405] loss: 0.480\n",
            "[12,   407] loss: 0.249\n",
            "[12,   409] loss: 0.223\n",
            "[12,   411] loss: 0.385\n",
            "[12,   413] loss: 0.173\n",
            "[12,   415] loss: 0.432\n",
            "[12,   417] loss: 0.767\n",
            "[12,   419] loss: 0.510\n",
            "[12,   421] loss: 0.732\n",
            "[12,   423] loss: 0.376\n",
            "[12,   425] loss: 0.195\n",
            "[12,   427] loss: 0.177\n",
            "[12,   429] loss: 0.299\n",
            "[12,   431] loss: 0.357\n",
            "[12,   433] loss: 0.303\n",
            "[12,   435] loss: 0.147\n",
            "[12,   437] loss: 0.294\n",
            "[12,   439] loss: 0.517\n",
            "[12,   441] loss: 0.235\n",
            "[12,   443] loss: 0.453\n",
            "[12,   445] loss: 0.160\n",
            "[12,   447] loss: 0.305\n",
            "[12,   449] loss: 0.673\n",
            "[12,   451] loss: 0.404\n",
            "[12,   453] loss: 0.198\n",
            "[12,   455] loss: 0.505\n",
            "[12,   457] loss: 0.576\n",
            "[12,   459] loss: 0.587\n",
            "[12,   461] loss: 0.212\n",
            "[12,   463] loss: 0.483\n",
            "[12,   465] loss: 0.199\n",
            "[12,   467] loss: 0.440\n",
            "Epoch 11, Train Loss: 170.20313681662083, Val Loss: 0.5085498812782058, Val Acc: 88.53161843515541\n",
            "Epoch 11, Test Acc: 88.66310160427807\n",
            "[13,     1] loss: 0.193\n",
            "[13,     3] loss: 0.817\n",
            "[13,     5] loss: 0.206\n",
            "[13,     7] loss: 0.364\n",
            "[13,     9] loss: 0.304\n",
            "[13,    11] loss: 0.401\n",
            "[13,    13] loss: 0.484\n",
            "[13,    15] loss: 0.599\n",
            "[13,    17] loss: 0.106\n",
            "[13,    19] loss: 0.314\n",
            "[13,    21] loss: 0.457\n",
            "[13,    23] loss: 0.229\n",
            "[13,    25] loss: 0.350\n",
            "[13,    27] loss: 0.352\n",
            "[13,    29] loss: 0.276\n",
            "[13,    31] loss: 0.207\n",
            "[13,    33] loss: 0.783\n",
            "[13,    35] loss: 0.383\n",
            "[13,    37] loss: 0.384\n",
            "[13,    39] loss: 0.215\n",
            "[13,    41] loss: 0.287\n",
            "[13,    43] loss: 0.273\n",
            "[13,    45] loss: 0.146\n",
            "[13,    47] loss: 0.332\n",
            "[13,    49] loss: 0.316\n",
            "[13,    51] loss: 0.368\n",
            "[13,    53] loss: 0.208\n",
            "[13,    55] loss: 0.190\n",
            "[13,    57] loss: 0.377\n",
            "[13,    59] loss: 0.325\n",
            "[13,    61] loss: 0.102\n",
            "[13,    63] loss: 0.152\n",
            "[13,    65] loss: 0.258\n",
            "[13,    67] loss: 0.097\n",
            "[13,    69] loss: 0.084\n",
            "[13,    71] loss: 0.131\n",
            "[13,    73] loss: 0.122\n",
            "[13,    75] loss: 0.274\n",
            "[13,    77] loss: 0.335\n",
            "[13,    79] loss: 0.319\n",
            "[13,    81] loss: 0.078\n",
            "[13,    83] loss: 0.139\n",
            "[13,    85] loss: 0.530\n",
            "[13,    87] loss: 0.165\n",
            "[13,    89] loss: 0.127\n",
            "[13,    91] loss: 0.412\n",
            "[13,    93] loss: 0.088\n",
            "[13,    95] loss: 0.134\n",
            "[13,    97] loss: 0.216\n",
            "[13,    99] loss: 0.592\n",
            "[13,   101] loss: 0.042\n",
            "[13,   103] loss: 0.432\n",
            "[13,   105] loss: 0.278\n",
            "[13,   107] loss: 0.226\n",
            "[13,   109] loss: 0.276\n",
            "[13,   111] loss: 0.666\n",
            "[13,   113] loss: 0.229\n",
            "[13,   115] loss: 0.671\n",
            "[13,   117] loss: 0.096\n",
            "[13,   119] loss: 0.369\n",
            "[13,   121] loss: 0.885\n",
            "[13,   123] loss: 0.750\n",
            "[13,   125] loss: 0.556\n",
            "[13,   127] loss: 0.128\n",
            "[13,   129] loss: 0.595\n",
            "[13,   131] loss: 0.378\n",
            "[13,   133] loss: 0.306\n",
            "[13,   135] loss: 0.651\n",
            "[13,   137] loss: 0.634\n",
            "[13,   139] loss: 0.036\n",
            "[13,   141] loss: 0.220\n",
            "[13,   143] loss: 0.322\n",
            "[13,   145] loss: 0.639\n",
            "[13,   147] loss: 0.226\n",
            "[13,   149] loss: 0.182\n",
            "[13,   151] loss: 0.711\n",
            "[13,   153] loss: 0.072\n",
            "[13,   155] loss: 0.641\n",
            "[13,   157] loss: 0.240\n",
            "[13,   159] loss: 0.216\n",
            "[13,   161] loss: 0.711\n",
            "[13,   163] loss: 0.082\n",
            "[13,   165] loss: 0.266\n",
            "[13,   167] loss: 0.243\n",
            "[13,   169] loss: 0.872\n",
            "[13,   171] loss: 0.382\n",
            "[13,   173] loss: 0.468\n",
            "[13,   175] loss: 0.416\n",
            "[13,   177] loss: 0.357\n",
            "[13,   179] loss: 0.242\n",
            "[13,   181] loss: 0.367\n",
            "[13,   183] loss: 0.298\n",
            "[13,   185] loss: 0.327\n",
            "[13,   187] loss: 0.329\n",
            "[13,   189] loss: 0.686\n",
            "[13,   191] loss: 0.564\n",
            "[13,   193] loss: 0.181\n",
            "[13,   195] loss: 0.310\n",
            "[13,   197] loss: 0.073\n",
            "[13,   199] loss: 0.439\n",
            "[13,   201] loss: 0.529\n",
            "[13,   203] loss: 0.280\n",
            "[13,   205] loss: 0.447\n",
            "[13,   207] loss: 0.360\n",
            "[13,   209] loss: 0.187\n",
            "[13,   211] loss: 0.129\n",
            "[13,   213] loss: 0.473\n",
            "[13,   215] loss: 0.136\n",
            "[13,   217] loss: 0.233\n",
            "[13,   219] loss: 0.077\n",
            "[13,   221] loss: 0.295\n",
            "[13,   223] loss: 0.087\n",
            "[13,   225] loss: 0.373\n",
            "[13,   227] loss: 0.259\n",
            "[13,   229] loss: 0.721\n",
            "[13,   231] loss: 0.295\n",
            "[13,   233] loss: 0.089\n",
            "[13,   235] loss: 0.248\n",
            "[13,   237] loss: 0.443\n",
            "[13,   239] loss: 0.286\n",
            "[13,   241] loss: 0.134\n",
            "[13,   243] loss: 0.316\n",
            "[13,   245] loss: 0.678\n",
            "[13,   247] loss: 0.191\n",
            "[13,   249] loss: 0.035\n",
            "[13,   251] loss: 0.219\n",
            "[13,   253] loss: 0.185\n",
            "[13,   255] loss: 0.387\n",
            "[13,   257] loss: 0.291\n",
            "[13,   259] loss: 0.325\n",
            "[13,   261] loss: 0.511\n",
            "[13,   263] loss: 0.378\n",
            "[13,   265] loss: 0.562\n",
            "[13,   267] loss: 0.406\n",
            "[13,   269] loss: 0.529\n",
            "[13,   271] loss: 0.475\n",
            "[13,   273] loss: 0.366\n",
            "[13,   275] loss: 0.582\n",
            "[13,   277] loss: 0.058\n",
            "[13,   279] loss: 0.136\n",
            "[13,   281] loss: 0.205\n",
            "[13,   283] loss: 0.820\n",
            "[13,   285] loss: 0.711\n",
            "[13,   287] loss: 0.082\n",
            "[13,   289] loss: 0.228\n",
            "[13,   291] loss: 0.229\n",
            "[13,   293] loss: 0.287\n",
            "[13,   295] loss: 0.156\n",
            "[13,   297] loss: 0.328\n",
            "[13,   299] loss: 0.302\n",
            "[13,   301] loss: 0.648\n",
            "[13,   303] loss: 0.117\n",
            "[13,   305] loss: 0.190\n",
            "[13,   307] loss: 0.268\n",
            "[13,   309] loss: 0.213\n",
            "[13,   311] loss: 0.219\n",
            "[13,   313] loss: 0.600\n",
            "[13,   315] loss: 0.314\n",
            "[13,   317] loss: 0.477\n",
            "[13,   319] loss: 0.432\n",
            "[13,   321] loss: 0.246\n",
            "[13,   323] loss: 0.203\n",
            "[13,   325] loss: 0.818\n",
            "[13,   327] loss: 0.231\n",
            "[13,   329] loss: 0.318\n",
            "[13,   331] loss: 0.072\n",
            "[13,   333] loss: 0.187\n",
            "[13,   335] loss: 0.306\n",
            "[13,   337] loss: 0.449\n",
            "[13,   339] loss: 0.276\n",
            "[13,   341] loss: 0.192\n",
            "[13,   343] loss: 0.281\n",
            "[13,   345] loss: 0.419\n",
            "[13,   347] loss: 0.688\n",
            "[13,   349] loss: 0.384\n",
            "[13,   351] loss: 0.218\n",
            "[13,   353] loss: 0.219\n",
            "[13,   355] loss: 0.230\n",
            "[13,   357] loss: 0.359\n",
            "[13,   359] loss: 0.346\n",
            "[13,   361] loss: 0.315\n",
            "[13,   363] loss: 0.058\n",
            "[13,   365] loss: 0.213\n",
            "[13,   367] loss: 0.203\n",
            "[13,   369] loss: 0.286\n",
            "[13,   371] loss: 0.339\n",
            "[13,   373] loss: 0.262\n",
            "[13,   375] loss: 0.325\n",
            "[13,   377] loss: 0.451\n",
            "[13,   379] loss: 0.306\n",
            "[13,   381] loss: 0.316\n",
            "[13,   383] loss: 0.764\n",
            "[13,   385] loss: 0.204\n",
            "[13,   387] loss: 0.317\n",
            "[13,   389] loss: 0.421\n",
            "[13,   391] loss: 0.224\n",
            "[13,   393] loss: 0.148\n",
            "[13,   395] loss: 0.559\n",
            "[13,   397] loss: 0.238\n",
            "[13,   399] loss: 0.532\n",
            "[13,   401] loss: 0.454\n",
            "[13,   403] loss: 0.146\n",
            "[13,   405] loss: 0.457\n",
            "[13,   407] loss: 0.371\n",
            "[13,   409] loss: 0.454\n",
            "[13,   411] loss: 0.392\n",
            "[13,   413] loss: 0.484\n",
            "[13,   415] loss: 0.288\n",
            "[13,   417] loss: 0.452\n",
            "[13,   419] loss: 0.140\n",
            "[13,   421] loss: 0.178\n",
            "[13,   423] loss: 0.474\n",
            "[13,   425] loss: 0.819\n",
            "[13,   427] loss: 0.276\n",
            "[13,   429] loss: 0.140\n",
            "[13,   431] loss: 0.467\n",
            "[13,   433] loss: 0.270\n",
            "[13,   435] loss: 0.118\n",
            "[13,   437] loss: 0.420\n",
            "[13,   439] loss: 0.289\n",
            "[13,   441] loss: 0.344\n",
            "[13,   443] loss: 0.280\n",
            "[13,   445] loss: 0.217\n",
            "[13,   447] loss: 0.233\n",
            "[13,   449] loss: 0.266\n",
            "[13,   451] loss: 0.302\n",
            "[13,   453] loss: 0.961\n",
            "[13,   455] loss: 0.171\n",
            "[13,   457] loss: 1.241\n",
            "[13,   459] loss: 0.094\n",
            "[13,   461] loss: 0.213\n",
            "[13,   463] loss: 0.507\n",
            "[13,   465] loss: 0.237\n",
            "[13,   467] loss: 0.377\n",
            "Epoch 12, Train Loss: 157.77255008090287, Val Loss: 0.22391431342999815, Val Acc: 92.92604501607717\n",
            "Epoch 12, Test Acc: 91.76470588235294\n",
            "[14,     1] loss: 0.062\n",
            "[14,     3] loss: 0.487\n",
            "[14,     5] loss: 0.098\n",
            "[14,     7] loss: 0.157\n",
            "[14,     9] loss: 0.555\n",
            "[14,    11] loss: 0.575\n",
            "[14,    13] loss: 0.285\n",
            "[14,    15] loss: 0.471\n",
            "[14,    17] loss: 0.370\n",
            "[14,    19] loss: 0.568\n",
            "[14,    21] loss: 0.247\n",
            "[14,    23] loss: 0.186\n",
            "[14,    25] loss: 0.614\n",
            "[14,    27] loss: 0.726\n",
            "[14,    29] loss: 0.368\n",
            "[14,    31] loss: 0.398\n",
            "[14,    33] loss: 0.252\n",
            "[14,    35] loss: 0.053\n",
            "[14,    37] loss: 0.165\n",
            "[14,    39] loss: 0.741\n",
            "[14,    41] loss: 0.338\n",
            "[14,    43] loss: 0.347\n",
            "[14,    45] loss: 0.718\n",
            "[14,    47] loss: 0.181\n",
            "[14,    49] loss: 0.282\n",
            "[14,    51] loss: 0.264\n",
            "[14,    53] loss: 0.387\n",
            "[14,    55] loss: 0.072\n",
            "[14,    57] loss: 0.391\n",
            "[14,    59] loss: 0.123\n",
            "[14,    61] loss: 0.830\n",
            "[14,    63] loss: 0.094\n",
            "[14,    65] loss: 0.536\n",
            "[14,    67] loss: 0.301\n",
            "[14,    69] loss: 0.181\n",
            "[14,    71] loss: 0.641\n",
            "[14,    73] loss: 0.238\n",
            "[14,    75] loss: 0.736\n",
            "[14,    77] loss: 0.585\n",
            "[14,    79] loss: 0.162\n",
            "[14,    81] loss: 0.413\n",
            "[14,    83] loss: 0.636\n",
            "[14,    85] loss: 0.732\n",
            "[14,    87] loss: 0.293\n",
            "[14,    89] loss: 0.168\n",
            "[14,    91] loss: 0.247\n",
            "[14,    93] loss: 0.798\n",
            "[14,    95] loss: 0.157\n",
            "[14,    97] loss: 0.523\n",
            "[14,    99] loss: 0.537\n",
            "[14,   101] loss: 0.481\n",
            "[14,   103] loss: 0.290\n",
            "[14,   105] loss: 0.104\n",
            "[14,   107] loss: 0.499\n",
            "[14,   109] loss: 1.034\n",
            "[14,   111] loss: 0.330\n",
            "[14,   113] loss: 0.425\n",
            "[14,   115] loss: 0.156\n",
            "[14,   117] loss: 0.233\n",
            "[14,   119] loss: 0.254\n",
            "[14,   121] loss: 0.667\n",
            "[14,   123] loss: 0.050\n",
            "[14,   125] loss: 0.291\n",
            "[14,   127] loss: 0.225\n",
            "[14,   129] loss: 0.165\n",
            "[14,   131] loss: 1.326\n",
            "[14,   133] loss: 0.180\n",
            "[14,   135] loss: 0.220\n",
            "[14,   137] loss: 0.240\n",
            "[14,   139] loss: 0.263\n",
            "[14,   141] loss: 0.387\n",
            "[14,   143] loss: 0.198\n",
            "[14,   145] loss: 0.267\n",
            "[14,   147] loss: 0.278\n",
            "[14,   149] loss: 0.187\n",
            "[14,   151] loss: 0.126\n",
            "[14,   153] loss: 0.324\n",
            "[14,   155] loss: 0.331\n",
            "[14,   157] loss: 0.341\n",
            "[14,   159] loss: 0.281\n",
            "[14,   161] loss: 0.796\n",
            "[14,   163] loss: 0.610\n",
            "[14,   165] loss: 0.357\n",
            "[14,   167] loss: 0.526\n",
            "[14,   169] loss: 0.129\n",
            "[14,   171] loss: 0.207\n",
            "[14,   173] loss: 0.191\n",
            "[14,   175] loss: 0.340\n",
            "[14,   177] loss: 0.097\n",
            "[14,   179] loss: 0.343\n",
            "[14,   181] loss: 0.221\n",
            "[14,   183] loss: 0.381\n",
            "[14,   185] loss: 0.089\n",
            "[14,   187] loss: 0.220\n",
            "[14,   189] loss: 0.285\n",
            "[14,   191] loss: 0.727\n",
            "[14,   193] loss: 0.143\n",
            "[14,   195] loss: 0.897\n",
            "[14,   197] loss: 0.312\n",
            "[14,   199] loss: 1.069\n",
            "[14,   201] loss: 0.740\n",
            "[14,   203] loss: 0.469\n",
            "[14,   205] loss: 0.417\n",
            "[14,   207] loss: 0.716\n",
            "[14,   209] loss: 0.400\n",
            "[14,   211] loss: 0.298\n",
            "[14,   213] loss: 0.452\n",
            "[14,   215] loss: 0.477\n",
            "[14,   217] loss: 0.123\n",
            "[14,   219] loss: 0.218\n",
            "[14,   221] loss: 0.454\n",
            "[14,   223] loss: 0.094\n",
            "[14,   225] loss: 0.062\n",
            "[14,   227] loss: 0.484\n",
            "[14,   229] loss: 0.097\n",
            "[14,   231] loss: 0.228\n",
            "[14,   233] loss: 0.263\n",
            "[14,   235] loss: 0.213\n",
            "[14,   237] loss: 0.254\n",
            "[14,   239] loss: 0.222\n",
            "[14,   241] loss: 0.085\n",
            "[14,   243] loss: 0.181\n",
            "[14,   245] loss: 0.073\n",
            "[14,   247] loss: 0.301\n",
            "[14,   249] loss: 0.244\n",
            "[14,   251] loss: 0.476\n",
            "[14,   253] loss: 0.453\n",
            "[14,   255] loss: 0.106\n",
            "[14,   257] loss: 0.414\n",
            "[14,   259] loss: 0.509\n",
            "[14,   261] loss: 0.327\n",
            "[14,   263] loss: 0.382\n",
            "[14,   265] loss: 0.356\n",
            "[14,   267] loss: 0.320\n",
            "[14,   269] loss: 0.121\n",
            "[14,   271] loss: 0.392\n",
            "[14,   273] loss: 0.271\n",
            "[14,   275] loss: 0.273\n",
            "[14,   277] loss: 0.249\n",
            "[14,   279] loss: 0.177\n",
            "[14,   281] loss: 0.244\n",
            "[14,   283] loss: 0.367\n",
            "[14,   285] loss: 0.470\n",
            "[14,   287] loss: 0.556\n",
            "[14,   289] loss: 0.173\n",
            "[14,   291] loss: 0.186\n",
            "[14,   293] loss: 0.261\n",
            "[14,   295] loss: 0.314\n",
            "[14,   297] loss: 0.212\n",
            "[14,   299] loss: 0.210\n",
            "[14,   301] loss: 0.479\n",
            "[14,   303] loss: 0.208\n",
            "[14,   305] loss: 0.435\n",
            "[14,   307] loss: 0.106\n",
            "[14,   309] loss: 0.332\n",
            "[14,   311] loss: 0.414\n",
            "[14,   313] loss: 0.569\n",
            "[14,   315] loss: 0.125\n",
            "[14,   317] loss: 0.461\n",
            "[14,   319] loss: 0.438\n",
            "[14,   321] loss: 0.241\n",
            "[14,   323] loss: 0.439\n",
            "[14,   325] loss: 0.367\n",
            "[14,   327] loss: 0.249\n",
            "[14,   329] loss: 0.414\n",
            "[14,   331] loss: 0.218\n",
            "[14,   333] loss: 0.635\n",
            "[14,   335] loss: 0.697\n",
            "[14,   337] loss: 0.525\n",
            "[14,   339] loss: 0.488\n",
            "[14,   341] loss: 0.621\n",
            "[14,   343] loss: 0.646\n",
            "[14,   345] loss: 0.523\n",
            "[14,   347] loss: 0.540\n",
            "[14,   349] loss: 0.231\n",
            "[14,   351] loss: 0.841\n",
            "[14,   353] loss: 0.479\n",
            "[14,   355] loss: 0.218\n",
            "[14,   357] loss: 0.241\n",
            "[14,   359] loss: 0.413\n",
            "[14,   361] loss: 0.433\n",
            "[14,   363] loss: 0.458\n",
            "[14,   365] loss: 0.907\n",
            "[14,   367] loss: 0.536\n",
            "[14,   369] loss: 0.802\n",
            "[14,   371] loss: 0.447\n",
            "[14,   373] loss: 0.057\n",
            "[14,   375] loss: 0.335\n",
            "[14,   377] loss: 0.336\n",
            "[14,   379] loss: 0.524\n",
            "[14,   381] loss: 0.291\n",
            "[14,   383] loss: 0.486\n",
            "[14,   385] loss: 0.244\n",
            "[14,   387] loss: 0.283\n",
            "[14,   389] loss: 0.218\n",
            "[14,   391] loss: 0.360\n",
            "[14,   393] loss: 0.139\n",
            "[14,   395] loss: 0.321\n",
            "[14,   397] loss: 0.580\n",
            "[14,   399] loss: 0.434\n",
            "[14,   401] loss: 0.320\n",
            "[14,   403] loss: 0.881\n",
            "[14,   405] loss: 0.283\n",
            "[14,   407] loss: 0.807\n",
            "[14,   409] loss: 0.691\n",
            "[14,   411] loss: 0.222\n",
            "[14,   413] loss: 0.589\n",
            "[14,   415] loss: 0.194\n",
            "[14,   417] loss: 0.370\n",
            "[14,   419] loss: 0.266\n",
            "[14,   421] loss: 0.475\n",
            "[14,   423] loss: 0.521\n",
            "[14,   425] loss: 0.553\n",
            "[14,   427] loss: 0.670\n",
            "[14,   429] loss: 0.289\n",
            "[14,   431] loss: 0.208\n",
            "[14,   433] loss: 0.302\n",
            "[14,   435] loss: 0.323\n",
            "[14,   437] loss: 0.320\n",
            "[14,   439] loss: 0.456\n",
            "[14,   441] loss: 0.446\n",
            "[14,   443] loss: 0.549\n",
            "[14,   445] loss: 1.630\n",
            "[14,   447] loss: 0.368\n",
            "[14,   449] loss: 0.200\n",
            "[14,   451] loss: 0.233\n",
            "[14,   453] loss: 0.217\n",
            "[14,   455] loss: 0.122\n",
            "[14,   457] loss: 0.225\n",
            "[14,   459] loss: 0.315\n",
            "[14,   461] loss: 0.112\n",
            "[14,   463] loss: 0.125\n",
            "[14,   465] loss: 0.058\n",
            "[14,   467] loss: 0.376\n",
            "Epoch 13, Train Loss: 174.52935641538352, Val Loss: 0.484834491164755, Val Acc: 85.95927116827438\n",
            "Epoch 13, Test Acc: 83.63636363636364\n",
            "[15,     1] loss: 0.233\n",
            "[15,     3] loss: 0.527\n",
            "[15,     5] loss: 0.561\n",
            "[15,     7] loss: 0.861\n",
            "[15,     9] loss: 0.683\n",
            "[15,    11] loss: 0.488\n",
            "[15,    13] loss: 0.359\n",
            "[15,    15] loss: 0.334\n",
            "[15,    17] loss: 0.333\n",
            "[15,    19] loss: 0.144\n",
            "[15,    21] loss: 0.638\n",
            "[15,    23] loss: 0.367\n",
            "[15,    25] loss: 0.164\n",
            "[15,    27] loss: 0.363\n",
            "[15,    29] loss: 0.098\n",
            "[15,    31] loss: 0.210\n",
            "[15,    33] loss: 0.309\n",
            "[15,    35] loss: 0.307\n",
            "[15,    37] loss: 0.142\n",
            "[15,    39] loss: 0.392\n",
            "[15,    41] loss: 0.305\n",
            "[15,    43] loss: 0.075\n",
            "[15,    45] loss: 0.241\n",
            "[15,    47] loss: 0.428\n",
            "[15,    49] loss: 0.093\n",
            "[15,    51] loss: 0.230\n",
            "[15,    53] loss: 0.220\n",
            "[15,    55] loss: 0.495\n",
            "[15,    57] loss: 0.417\n",
            "[15,    59] loss: 0.349\n",
            "[15,    61] loss: 0.143\n",
            "[15,    63] loss: 0.189\n",
            "[15,    65] loss: 0.278\n",
            "[15,    67] loss: 0.585\n",
            "[15,    69] loss: 0.222\n",
            "[15,    71] loss: 0.663\n",
            "[15,    73] loss: 0.255\n",
            "[15,    75] loss: 0.082\n",
            "[15,    77] loss: 0.253\n",
            "[15,    79] loss: 0.249\n",
            "[15,    81] loss: 0.178\n",
            "[15,    83] loss: 0.273\n",
            "[15,    85] loss: 0.125\n",
            "[15,    87] loss: 0.735\n",
            "[15,    89] loss: 0.381\n",
            "[15,    91] loss: 0.681\n",
            "[15,    93] loss: 0.286\n",
            "[15,    95] loss: 0.233\n",
            "[15,    97] loss: 0.389\n",
            "[15,    99] loss: 0.481\n",
            "[15,   101] loss: 0.722\n",
            "[15,   103] loss: 0.493\n",
            "[15,   105] loss: 0.318\n",
            "[15,   107] loss: 0.139\n",
            "[15,   109] loss: 0.124\n",
            "[15,   111] loss: 0.285\n",
            "[15,   113] loss: 0.418\n",
            "[15,   115] loss: 0.634\n",
            "[15,   117] loss: 0.550\n",
            "[15,   119] loss: 0.246\n",
            "[15,   121] loss: 0.151\n",
            "[15,   123] loss: 0.190\n",
            "[15,   125] loss: 0.236\n",
            "[15,   127] loss: 0.161\n",
            "[15,   129] loss: 0.312\n",
            "[15,   131] loss: 0.249\n",
            "[15,   133] loss: 0.094\n",
            "[15,   135] loss: 0.449\n",
            "[15,   137] loss: 0.347\n",
            "[15,   139] loss: 0.465\n",
            "[15,   141] loss: 0.252\n",
            "[15,   143] loss: 0.262\n",
            "[15,   145] loss: 0.102\n",
            "[15,   147] loss: 0.058\n",
            "[15,   149] loss: 0.138\n",
            "[15,   151] loss: 0.042\n",
            "[15,   153] loss: 0.217\n",
            "[15,   155] loss: 0.379\n",
            "[15,   157] loss: 0.217\n",
            "[15,   159] loss: 0.083\n",
            "[15,   161] loss: 0.691\n",
            "[15,   163] loss: 0.276\n",
            "[15,   165] loss: 0.141\n",
            "[15,   167] loss: 0.552\n",
            "[15,   169] loss: 0.132\n",
            "[15,   171] loss: 0.236\n",
            "[15,   173] loss: 0.160\n",
            "[15,   175] loss: 0.191\n",
            "[15,   177] loss: 0.222\n",
            "[15,   179] loss: 0.300\n",
            "[15,   181] loss: 0.091\n",
            "[15,   183] loss: 0.220\n",
            "[15,   185] loss: 0.301\n",
            "[15,   187] loss: 0.135\n",
            "[15,   189] loss: 0.431\n",
            "[15,   191] loss: 0.251\n",
            "[15,   193] loss: 0.195\n",
            "[15,   195] loss: 0.182\n",
            "[15,   197] loss: 0.190\n",
            "[15,   199] loss: 0.164\n",
            "[15,   201] loss: 0.540\n",
            "[15,   203] loss: 0.376\n",
            "[15,   205] loss: 0.551\n",
            "[15,   207] loss: 0.594\n",
            "[15,   209] loss: 0.120\n",
            "[15,   211] loss: 0.143\n",
            "[15,   213] loss: 0.019\n",
            "[15,   215] loss: 1.087\n",
            "[15,   217] loss: 0.469\n",
            "[15,   219] loss: 0.199\n",
            "[15,   221] loss: 0.356\n",
            "[15,   223] loss: 0.173\n",
            "[15,   225] loss: 0.033\n",
            "[15,   227] loss: 0.175\n",
            "[15,   229] loss: 0.110\n",
            "[15,   231] loss: 0.301\n",
            "[15,   233] loss: 0.417\n",
            "[15,   235] loss: 0.082\n",
            "[15,   237] loss: 0.637\n",
            "[15,   239] loss: 0.315\n",
            "[15,   241] loss: 0.455\n",
            "[15,   243] loss: 0.160\n",
            "[15,   245] loss: 0.667\n",
            "[15,   247] loss: 0.352\n",
            "[15,   249] loss: 0.425\n",
            "[15,   251] loss: 0.166\n",
            "[15,   253] loss: 0.723\n",
            "[15,   255] loss: 0.291\n",
            "[15,   257] loss: 0.249\n",
            "[15,   259] loss: 0.123\n",
            "[15,   261] loss: 0.284\n",
            "[15,   263] loss: 0.716\n",
            "[15,   265] loss: 0.811\n",
            "[15,   267] loss: 0.404\n",
            "[15,   269] loss: 0.139\n",
            "[15,   271] loss: 0.316\n",
            "[15,   273] loss: 0.087\n",
            "[15,   275] loss: 0.172\n",
            "[15,   277] loss: 0.252\n",
            "[15,   279] loss: 0.195\n",
            "[15,   281] loss: 0.547\n",
            "[15,   283] loss: 0.403\n",
            "[15,   285] loss: 0.935\n",
            "[15,   287] loss: 0.425\n",
            "[15,   289] loss: 0.515\n",
            "[15,   291] loss: 0.372\n",
            "[15,   293] loss: 0.091\n",
            "[15,   295] loss: 0.233\n",
            "[15,   297] loss: 0.358\n",
            "[15,   299] loss: 0.225\n",
            "[15,   301] loss: 0.358\n",
            "[15,   303] loss: 0.207\n",
            "[15,   305] loss: 0.314\n",
            "[15,   307] loss: 0.436\n",
            "[15,   309] loss: 0.064\n",
            "[15,   311] loss: 0.157\n",
            "[15,   313] loss: 0.792\n",
            "[15,   315] loss: 0.287\n",
            "[15,   317] loss: 0.436\n",
            "[15,   319] loss: 0.272\n",
            "[15,   321] loss: 0.780\n",
            "[15,   323] loss: 0.434\n",
            "[15,   325] loss: 0.275\n",
            "[15,   327] loss: 0.112\n",
            "[15,   329] loss: 0.415\n",
            "[15,   331] loss: 0.447\n",
            "[15,   333] loss: 0.474\n",
            "[15,   335] loss: 0.486\n",
            "[15,   337] loss: 0.538\n",
            "[15,   339] loss: 0.283\n",
            "[15,   341] loss: 0.351\n",
            "[15,   343] loss: 0.476\n",
            "[15,   345] loss: 0.303\n",
            "[15,   347] loss: 0.766\n",
            "[15,   349] loss: 0.706\n",
            "[15,   351] loss: 0.406\n",
            "[15,   353] loss: 0.081\n",
            "[15,   355] loss: 0.385\n",
            "[15,   357] loss: 0.452\n",
            "[15,   359] loss: 0.425\n",
            "[15,   361] loss: 0.188\n",
            "[15,   363] loss: 0.274\n",
            "[15,   365] loss: 0.130\n",
            "[15,   367] loss: 0.852\n",
            "[15,   369] loss: 0.316\n",
            "[15,   371] loss: 0.168\n",
            "[15,   373] loss: 0.251\n",
            "[15,   375] loss: 0.545\n",
            "[15,   377] loss: 0.226\n",
            "[15,   379] loss: 0.198\n",
            "[15,   381] loss: 0.453\n",
            "[15,   383] loss: 0.389\n",
            "[15,   385] loss: 0.191\n",
            "[15,   387] loss: 0.335\n",
            "[15,   389] loss: 0.174\n",
            "[15,   391] loss: 0.698\n",
            "[15,   393] loss: 0.451\n",
            "[15,   395] loss: 0.350\n",
            "[15,   397] loss: 0.162\n",
            "[15,   399] loss: 0.231\n",
            "[15,   401] loss: 0.446\n",
            "[15,   403] loss: 0.151\n",
            "[15,   405] loss: 0.209\n",
            "[15,   407] loss: 0.179\n",
            "[15,   409] loss: 0.546\n",
            "[15,   411] loss: 0.539\n",
            "[15,   413] loss: 0.286\n",
            "[15,   415] loss: 0.263\n",
            "[15,   417] loss: 0.458\n",
            "[15,   419] loss: 0.383\n",
            "[15,   421] loss: 0.502\n",
            "[15,   423] loss: 0.353\n",
            "[15,   425] loss: 0.564\n",
            "[15,   427] loss: 0.526\n",
            "[15,   429] loss: 0.374\n",
            "[15,   431] loss: 0.363\n",
            "[15,   433] loss: 0.268\n",
            "[15,   435] loss: 1.011\n",
            "[15,   437] loss: 0.378\n",
            "[15,   439] loss: 0.177\n",
            "[15,   441] loss: 0.575\n",
            "[15,   443] loss: 0.399\n",
            "[15,   445] loss: 0.364\n",
            "[15,   447] loss: 0.267\n",
            "[15,   449] loss: 1.585\n",
            "[15,   451] loss: 0.178\n",
            "[15,   453] loss: 0.351\n",
            "[15,   455] loss: 1.187\n",
            "[15,   457] loss: 0.284\n",
            "[15,   459] loss: 0.129\n",
            "[15,   461] loss: 0.263\n",
            "[15,   463] loss: 0.173\n",
            "[15,   465] loss: 0.175\n",
            "[15,   467] loss: 0.232\n",
            "Epoch 14, Train Loss: 161.7285013590008, Val Loss: 0.41410746348043115, Val Acc: 92.49732047159699\n",
            "Epoch 14, Test Acc: 89.73262032085562\n",
            "[16,     1] loss: 0.134\n",
            "[16,     3] loss: 0.409\n",
            "[16,     5] loss: 0.383\n",
            "[16,     7] loss: 0.393\n",
            "[16,     9] loss: 0.108\n",
            "[16,    11] loss: 0.522\n",
            "[16,    13] loss: 0.290\n",
            "[16,    15] loss: 0.310\n",
            "[16,    17] loss: 0.239\n",
            "[16,    19] loss: 0.030\n",
            "[16,    21] loss: 0.301\n",
            "[16,    23] loss: 0.243\n",
            "[16,    25] loss: 0.172\n",
            "[16,    27] loss: 0.269\n",
            "[16,    29] loss: 0.317\n",
            "[16,    31] loss: 0.061\n",
            "[16,    33] loss: 0.564\n",
            "[16,    35] loss: 0.769\n",
            "[16,    37] loss: 0.586\n",
            "[16,    39] loss: 0.162\n",
            "[16,    41] loss: 0.300\n",
            "[16,    43] loss: 0.264\n",
            "[16,    45] loss: 0.798\n",
            "[16,    47] loss: 0.222\n",
            "[16,    49] loss: 0.145\n",
            "[16,    51] loss: 0.518\n",
            "[16,    53] loss: 0.291\n",
            "[16,    55] loss: 0.297\n",
            "[16,    57] loss: 0.597\n",
            "[16,    59] loss: 0.341\n",
            "[16,    61] loss: 0.222\n",
            "[16,    63] loss: 0.576\n",
            "[16,    65] loss: 0.413\n",
            "[16,    67] loss: 0.040\n",
            "[16,    69] loss: 0.026\n",
            "[16,    71] loss: 0.136\n",
            "[16,    73] loss: 0.096\n",
            "[16,    75] loss: 0.383\n",
            "[16,    77] loss: 0.022\n",
            "[16,    79] loss: 0.131\n",
            "[16,    81] loss: 0.260\n",
            "[16,    83] loss: 0.064\n",
            "[16,    85] loss: 0.049\n",
            "[16,    87] loss: 0.312\n",
            "[16,    89] loss: 0.160\n",
            "[16,    91] loss: 0.237\n",
            "[16,    93] loss: 0.138\n",
            "[16,    95] loss: 0.247\n",
            "[16,    97] loss: 0.167\n",
            "[16,    99] loss: 0.209\n",
            "[16,   101] loss: 0.130\n",
            "[16,   103] loss: 0.452\n",
            "[16,   105] loss: 0.629\n",
            "[16,   107] loss: 0.161\n",
            "[16,   109] loss: 0.193\n",
            "[16,   111] loss: 0.370\n",
            "[16,   113] loss: 0.363\n",
            "[16,   115] loss: 0.097\n",
            "[16,   117] loss: 0.266\n",
            "[16,   119] loss: 0.199\n",
            "[16,   121] loss: 0.069\n",
            "[16,   123] loss: 0.277\n",
            "[16,   125] loss: 0.165\n",
            "[16,   127] loss: 0.708\n",
            "[16,   129] loss: 0.508\n",
            "[16,   131] loss: 0.158\n",
            "[16,   133] loss: 0.141\n",
            "[16,   135] loss: 0.284\n",
            "[16,   137] loss: 0.111\n",
            "[16,   139] loss: 0.225\n",
            "[16,   141] loss: 0.203\n",
            "[16,   143] loss: 0.137\n",
            "[16,   145] loss: 0.152\n",
            "[16,   147] loss: 0.198\n",
            "[16,   149] loss: 0.160\n",
            "[16,   151] loss: 0.283\n",
            "[16,   153] loss: 0.485\n",
            "[16,   155] loss: 0.198\n",
            "[16,   157] loss: 0.212\n",
            "[16,   159] loss: 0.117\n",
            "[16,   161] loss: 0.320\n",
            "[16,   163] loss: 0.246\n",
            "[16,   165] loss: 0.243\n",
            "[16,   167] loss: 0.267\n",
            "[16,   169] loss: 0.241\n",
            "[16,   171] loss: 0.167\n",
            "[16,   173] loss: 0.130\n",
            "[16,   175] loss: 0.079\n",
            "[16,   177] loss: 0.060\n",
            "[16,   179] loss: 0.229\n",
            "[16,   181] loss: 0.510\n",
            "[16,   183] loss: 0.189\n",
            "[16,   185] loss: 0.252\n",
            "[16,   187] loss: 0.172\n",
            "[16,   189] loss: 0.282\n",
            "[16,   191] loss: 0.102\n",
            "[16,   193] loss: 0.227\n",
            "[16,   195] loss: 0.144\n",
            "[16,   197] loss: 0.331\n",
            "[16,   199] loss: 0.123\n",
            "[16,   201] loss: 0.172\n",
            "[16,   203] loss: 0.113\n",
            "[16,   205] loss: 0.330\n",
            "[16,   207] loss: 0.300\n",
            "[16,   209] loss: 0.128\n",
            "[16,   211] loss: 0.110\n",
            "[16,   213] loss: 0.399\n",
            "[16,   215] loss: 0.265\n",
            "[16,   217] loss: 0.285\n",
            "[16,   219] loss: 0.133\n",
            "[16,   221] loss: 0.227\n",
            "[16,   223] loss: 0.322\n",
            "[16,   225] loss: 0.078\n",
            "[16,   227] loss: 0.149\n",
            "[16,   229] loss: 0.360\n",
            "[16,   231] loss: 0.240\n",
            "[16,   233] loss: 0.345\n",
            "[16,   235] loss: 0.645\n",
            "[16,   237] loss: 0.144\n",
            "[16,   239] loss: 0.229\n",
            "[16,   241] loss: 0.254\n",
            "[16,   243] loss: 0.205\n",
            "[16,   245] loss: 0.220\n",
            "[16,   247] loss: 0.598\n",
            "[16,   249] loss: 0.163\n",
            "[16,   251] loss: 0.261\n",
            "[16,   253] loss: 0.319\n",
            "[16,   255] loss: 0.361\n",
            "[16,   257] loss: 0.507\n",
            "[16,   259] loss: 0.193\n",
            "[16,   261] loss: 0.276\n",
            "[16,   263] loss: 0.311\n",
            "[16,   265] loss: 0.395\n",
            "[16,   267] loss: 0.328\n",
            "[16,   269] loss: 0.228\n",
            "[16,   271] loss: 0.218\n",
            "[16,   273] loss: 0.293\n",
            "[16,   275] loss: 0.215\n",
            "[16,   277] loss: 0.216\n",
            "[16,   279] loss: 0.233\n",
            "[16,   281] loss: 0.137\n",
            "[16,   283] loss: 0.689\n",
            "[16,   285] loss: 0.165\n",
            "[16,   287] loss: 0.157\n",
            "[16,   289] loss: 0.341\n",
            "[16,   291] loss: 0.131\n",
            "[16,   293] loss: 0.155\n",
            "[16,   295] loss: 0.395\n",
            "[16,   297] loss: 0.241\n",
            "[16,   299] loss: 0.295\n",
            "[16,   301] loss: 0.201\n",
            "[16,   303] loss: 0.066\n",
            "[16,   305] loss: 0.295\n",
            "[16,   307] loss: 0.038\n",
            "[16,   309] loss: 0.538\n",
            "[16,   311] loss: 0.462\n",
            "[16,   313] loss: 0.203\n",
            "[16,   315] loss: 0.040\n",
            "[16,   317] loss: 0.162\n",
            "[16,   319] loss: 0.139\n",
            "[16,   321] loss: 0.263\n",
            "[16,   323] loss: 0.254\n",
            "[16,   325] loss: 0.335\n",
            "[16,   327] loss: 0.253\n",
            "[16,   329] loss: 0.204\n",
            "[16,   331] loss: 0.605\n",
            "[16,   333] loss: 0.375\n",
            "[16,   335] loss: 0.252\n",
            "[16,   337] loss: 0.389\n",
            "[16,   339] loss: 0.445\n",
            "[16,   341] loss: 0.171\n",
            "[16,   343] loss: 0.535\n",
            "[16,   345] loss: 0.248\n",
            "[16,   347] loss: 0.307\n",
            "[16,   349] loss: 0.507\n",
            "[16,   351] loss: 0.613\n",
            "[16,   353] loss: 0.297\n",
            "[16,   355] loss: 0.181\n",
            "[16,   357] loss: 0.122\n",
            "[16,   359] loss: 0.311\n",
            "[16,   361] loss: 0.109\n",
            "[16,   363] loss: 0.310\n",
            "[16,   365] loss: 0.093\n",
            "[16,   367] loss: 0.439\n",
            "[16,   369] loss: 0.197\n",
            "[16,   371] loss: 0.154\n",
            "[16,   373] loss: 0.332\n",
            "[16,   375] loss: 0.035\n",
            "[16,   377] loss: 0.294\n",
            "[16,   379] loss: 0.345\n",
            "[16,   381] loss: 0.538\n",
            "[16,   383] loss: 0.280\n",
            "[16,   385] loss: 0.086\n",
            "[16,   387] loss: 0.250\n",
            "[16,   389] loss: 0.135\n",
            "[16,   391] loss: 0.262\n",
            "[16,   393] loss: 0.255\n",
            "[16,   395] loss: 0.092\n",
            "[16,   397] loss: 0.269\n",
            "[16,   399] loss: 0.336\n",
            "[16,   401] loss: 0.185\n",
            "[16,   403] loss: 0.185\n",
            "[16,   405] loss: 0.053\n",
            "[16,   407] loss: 0.471\n",
            "[16,   409] loss: 0.258\n",
            "[16,   411] loss: 0.169\n",
            "[16,   413] loss: 0.344\n",
            "[16,   415] loss: 0.189\n",
            "[16,   417] loss: 0.103\n",
            "[16,   419] loss: 0.243\n",
            "[16,   421] loss: 0.272\n",
            "[16,   423] loss: 0.280\n",
            "[16,   425] loss: 0.194\n",
            "[16,   427] loss: 0.268\n",
            "[16,   429] loss: 0.426\n",
            "[16,   431] loss: 0.279\n",
            "[16,   433] loss: 0.231\n",
            "[16,   435] loss: 0.110\n",
            "[16,   437] loss: 0.250\n",
            "[16,   439] loss: 0.222\n",
            "[16,   441] loss: 0.188\n",
            "[16,   443] loss: 0.222\n",
            "[16,   445] loss: 0.286\n",
            "[16,   447] loss: 0.316\n",
            "[16,   449] loss: 0.332\n",
            "[16,   451] loss: 0.694\n",
            "[16,   453] loss: 0.529\n",
            "[16,   455] loss: 1.423\n",
            "[16,   457] loss: 0.226\n",
            "[16,   459] loss: 0.142\n",
            "[16,   461] loss: 0.187\n",
            "[16,   463] loss: 0.191\n",
            "[16,   465] loss: 0.826\n",
            "[16,   467] loss: 0.378\n",
            "Epoch 15, Train Loss: 127.31347623188049, Val Loss: 0.39980737568210745, Val Acc: 89.17470525187566\n",
            "Epoch 15, Test Acc: 90.80213903743315\n",
            "[17,     1] loss: 0.294\n",
            "[17,     3] loss: 0.330\n",
            "[17,     5] loss: 0.260\n",
            "[17,     7] loss: 0.728\n",
            "[17,     9] loss: 0.076\n",
            "[17,    11] loss: 0.263\n",
            "[17,    13] loss: 0.225\n",
            "[17,    15] loss: 0.183\n",
            "[17,    17] loss: 0.130\n",
            "[17,    19] loss: 0.142\n",
            "[17,    21] loss: 0.443\n",
            "[17,    23] loss: 0.410\n",
            "[17,    25] loss: 0.373\n",
            "[17,    27] loss: 0.185\n",
            "[17,    29] loss: 0.331\n",
            "[17,    31] loss: 0.097\n",
            "[17,    33] loss: 0.138\n",
            "[17,    35] loss: 0.183\n",
            "[17,    37] loss: 0.061\n",
            "[17,    39] loss: 0.368\n",
            "[17,    41] loss: 0.134\n",
            "[17,    43] loss: 0.139\n",
            "[17,    45] loss: 0.792\n",
            "[17,    47] loss: 0.283\n",
            "[17,    49] loss: 0.171\n",
            "[17,    51] loss: 0.133\n",
            "[17,    53] loss: 0.181\n",
            "[17,    55] loss: 0.102\n",
            "[17,    57] loss: 0.292\n",
            "[17,    59] loss: 0.233\n",
            "[17,    61] loss: 0.056\n",
            "[17,    63] loss: 0.182\n",
            "[17,    65] loss: 0.122\n",
            "[17,    67] loss: 0.168\n",
            "[17,    69] loss: 0.232\n",
            "[17,    71] loss: 0.406\n",
            "[17,    73] loss: 0.234\n",
            "[17,    75] loss: 0.275\n",
            "[17,    77] loss: 0.200\n",
            "[17,    79] loss: 0.095\n",
            "[17,    81] loss: 0.305\n",
            "[17,    83] loss: 0.296\n",
            "[17,    85] loss: 0.255\n",
            "[17,    87] loss: 0.270\n",
            "[17,    89] loss: 0.310\n",
            "[17,    91] loss: 0.207\n",
            "[17,    93] loss: 0.104\n",
            "[17,    95] loss: 0.020\n",
            "[17,    97] loss: 0.268\n",
            "[17,    99] loss: 0.122\n",
            "[17,   101] loss: 0.277\n",
            "[17,   103] loss: 0.269\n",
            "[17,   105] loss: 0.244\n",
            "[17,   107] loss: 0.043\n",
            "[17,   109] loss: 0.163\n",
            "[17,   111] loss: 0.299\n",
            "[17,   113] loss: 0.232\n",
            "[17,   115] loss: 0.300\n",
            "[17,   117] loss: 0.188\n",
            "[17,   119] loss: 0.264\n",
            "[17,   121] loss: 0.072\n",
            "[17,   123] loss: 0.121\n",
            "[17,   125] loss: 0.399\n",
            "[17,   127] loss: 0.035\n",
            "[17,   129] loss: 0.386\n",
            "[17,   131] loss: 0.547\n",
            "[17,   133] loss: 0.166\n",
            "[17,   135] loss: 0.034\n",
            "[17,   137] loss: 0.301\n",
            "[17,   139] loss: 0.240\n",
            "[17,   141] loss: 0.412\n",
            "[17,   143] loss: 0.197\n",
            "[17,   145] loss: 0.273\n",
            "[17,   147] loss: 0.048\n",
            "[17,   149] loss: 0.385\n",
            "[17,   151] loss: 0.076\n",
            "[17,   153] loss: 0.238\n",
            "[17,   155] loss: 0.375\n",
            "[17,   157] loss: 0.308\n",
            "[17,   159] loss: 0.598\n",
            "[17,   161] loss: 0.126\n",
            "[17,   163] loss: 0.155\n",
            "[17,   165] loss: 0.077\n",
            "[17,   167] loss: 0.165\n",
            "[17,   169] loss: 0.134\n",
            "[17,   171] loss: 0.270\n",
            "[17,   173] loss: 0.273\n",
            "[17,   175] loss: 0.158\n",
            "[17,   177] loss: 0.314\n",
            "[17,   179] loss: 0.639\n",
            "[17,   181] loss: 0.100\n",
            "[17,   183] loss: 0.265\n",
            "[17,   185] loss: 0.186\n",
            "[17,   187] loss: 0.234\n",
            "[17,   189] loss: 0.347\n",
            "[17,   191] loss: 0.183\n",
            "[17,   193] loss: 0.205\n",
            "[17,   195] loss: 0.409\n",
            "[17,   197] loss: 0.111\n",
            "[17,   199] loss: 0.202\n",
            "[17,   201] loss: 0.384\n",
            "[17,   203] loss: 0.098\n",
            "[17,   205] loss: 0.149\n",
            "[17,   207] loss: 0.319\n",
            "[17,   209] loss: 0.078\n",
            "[17,   211] loss: 0.062\n",
            "[17,   213] loss: 0.125\n",
            "[17,   215] loss: 0.095\n",
            "[17,   217] loss: 0.341\n",
            "[17,   219] loss: 0.163\n",
            "[17,   221] loss: 0.113\n",
            "[17,   223] loss: 0.371\n",
            "[17,   225] loss: 0.189\n",
            "[17,   227] loss: 0.146\n",
            "[17,   229] loss: 0.153\n",
            "[17,   231] loss: 0.143\n",
            "[17,   233] loss: 0.132\n",
            "[17,   235] loss: 0.234\n",
            "[17,   237] loss: 0.153\n",
            "[17,   239] loss: 0.233\n",
            "[17,   241] loss: 0.220\n",
            "[17,   243] loss: 0.172\n",
            "[17,   245] loss: 0.132\n",
            "[17,   247] loss: 0.125\n",
            "[17,   249] loss: 0.577\n",
            "[17,   251] loss: 0.214\n",
            "[17,   253] loss: 0.214\n",
            "[17,   255] loss: 0.172\n",
            "[17,   257] loss: 0.199\n",
            "[17,   259] loss: 0.352\n",
            "[17,   261] loss: 0.130\n",
            "[17,   263] loss: 0.250\n",
            "[17,   265] loss: 0.336\n",
            "[17,   267] loss: 0.395\n",
            "[17,   269] loss: 0.099\n",
            "[17,   271] loss: 0.296\n",
            "[17,   273] loss: 0.808\n",
            "[17,   275] loss: 0.158\n",
            "[17,   277] loss: 0.323\n",
            "[17,   279] loss: 0.114\n",
            "[17,   281] loss: 0.146\n",
            "[17,   283] loss: 0.435\n",
            "[17,   285] loss: 0.154\n",
            "[17,   287] loss: 0.168\n",
            "[17,   289] loss: 0.056\n",
            "[17,   291] loss: 0.193\n",
            "[17,   293] loss: 0.181\n",
            "[17,   295] loss: 0.457\n",
            "[17,   297] loss: 0.215\n",
            "[17,   299] loss: 0.212\n",
            "[17,   301] loss: 0.097\n",
            "[17,   303] loss: 0.556\n",
            "[17,   305] loss: 0.098\n",
            "[17,   307] loss: 0.181\n",
            "[17,   309] loss: 0.305\n",
            "[17,   311] loss: 0.181\n",
            "[17,   313] loss: 0.141\n",
            "[17,   315] loss: 0.250\n",
            "[17,   317] loss: 0.261\n",
            "[17,   319] loss: 0.280\n",
            "[17,   321] loss: 0.157\n",
            "[17,   323] loss: 0.308\n",
            "[17,   325] loss: 0.099\n",
            "[17,   327] loss: 0.210\n",
            "[17,   329] loss: 0.132\n",
            "[17,   331] loss: 0.291\n",
            "[17,   333] loss: 0.137\n",
            "[17,   335] loss: 0.472\n",
            "[17,   337] loss: 0.310\n",
            "[17,   339] loss: 0.041\n",
            "[17,   341] loss: 0.334\n",
            "[17,   343] loss: 0.301\n",
            "[17,   345] loss: 0.665\n",
            "[17,   347] loss: 0.219\n",
            "[17,   349] loss: 0.152\n",
            "[17,   351] loss: 0.074\n",
            "[17,   353] loss: 0.378\n",
            "[17,   355] loss: 0.385\n",
            "[17,   357] loss: 0.342\n",
            "[17,   359] loss: 0.191\n",
            "[17,   361] loss: 0.378\n",
            "[17,   363] loss: 0.503\n",
            "[17,   365] loss: 0.066\n",
            "[17,   367] loss: 0.347\n",
            "[17,   369] loss: 0.119\n",
            "[17,   371] loss: 0.229\n",
            "[17,   373] loss: 0.203\n",
            "[17,   375] loss: 0.400\n",
            "[17,   377] loss: 0.318\n",
            "[17,   379] loss: 0.423\n",
            "[17,   381] loss: 0.202\n",
            "[17,   383] loss: 0.323\n",
            "[17,   385] loss: 0.485\n",
            "[17,   387] loss: 0.329\n",
            "[17,   389] loss: 0.583\n",
            "[17,   391] loss: 0.055\n",
            "[17,   393] loss: 0.149\n",
            "[17,   395] loss: 0.225\n",
            "[17,   397] loss: 0.259\n",
            "[17,   399] loss: 0.343\n",
            "[17,   401] loss: 0.055\n",
            "[17,   403] loss: 0.047\n",
            "[17,   405] loss: 0.310\n",
            "[17,   407] loss: 0.425\n",
            "[17,   409] loss: 0.216\n",
            "[17,   411] loss: 0.638\n",
            "[17,   413] loss: 0.102\n",
            "[17,   415] loss: 0.128\n",
            "[17,   417] loss: 0.344\n",
            "[17,   419] loss: 0.411\n",
            "[17,   421] loss: 0.277\n",
            "[17,   423] loss: 0.520\n",
            "[17,   425] loss: 0.140\n",
            "[17,   427] loss: 0.321\n",
            "[17,   429] loss: 0.501\n",
            "[17,   431] loss: 0.077\n",
            "[17,   433] loss: 0.347\n",
            "[17,   435] loss: 0.496\n",
            "[17,   437] loss: 0.522\n",
            "[17,   439] loss: 0.146\n",
            "[17,   441] loss: 0.811\n",
            "[17,   443] loss: 0.312\n",
            "[17,   445] loss: 0.283\n",
            "[17,   447] loss: 0.187\n",
            "[17,   449] loss: 0.508\n",
            "[17,   451] loss: 0.168\n",
            "[17,   453] loss: 0.670\n",
            "[17,   455] loss: 0.322\n",
            "[17,   457] loss: 0.960\n",
            "[17,   459] loss: 0.175\n",
            "[17,   461] loss: 0.210\n",
            "[17,   463] loss: 0.555\n",
            "[17,   465] loss: 0.430\n",
            "[17,   467] loss: 0.470\n",
            "Epoch 16, Train Loss: 121.20066885184497, Val Loss: 0.33876772898138835, Val Acc: 89.92497320471597\n",
            "Epoch 16, Test Acc: 90.16042780748663\n",
            "[18,     1] loss: 0.619\n",
            "[18,     3] loss: 0.463\n",
            "[18,     5] loss: 0.461\n",
            "[18,     7] loss: 0.786\n",
            "[18,     9] loss: 0.487\n",
            "[18,    11] loss: 0.558\n",
            "[18,    13] loss: 0.550\n",
            "[18,    15] loss: 0.419\n",
            "[18,    17] loss: 0.551\n",
            "[18,    19] loss: 0.195\n",
            "[18,    21] loss: 0.147\n",
            "[18,    23] loss: 0.232\n",
            "[18,    25] loss: 0.258\n",
            "[18,    27] loss: 0.228\n",
            "[18,    29] loss: 0.342\n",
            "[18,    31] loss: 0.257\n",
            "[18,    33] loss: 0.672\n",
            "[18,    35] loss: 0.194\n",
            "[18,    37] loss: 0.156\n",
            "[18,    39] loss: 0.335\n",
            "[18,    41] loss: 0.277\n",
            "[18,    43] loss: 0.190\n",
            "[18,    45] loss: 0.527\n",
            "[18,    47] loss: 0.589\n",
            "[18,    49] loss: 0.038\n",
            "[18,    51] loss: 0.269\n",
            "[18,    53] loss: 0.082\n",
            "[18,    55] loss: 0.042\n",
            "[18,    57] loss: 0.160\n",
            "[18,    59] loss: 0.080\n",
            "[18,    61] loss: 0.556\n",
            "[18,    63] loss: 0.141\n",
            "[18,    65] loss: 0.101\n",
            "[18,    67] loss: 0.227\n",
            "[18,    69] loss: 0.368\n",
            "[18,    71] loss: 0.381\n",
            "[18,    73] loss: 0.272\n",
            "[18,    75] loss: 0.480\n",
            "[18,    77] loss: 0.306\n",
            "[18,    79] loss: 0.420\n",
            "[18,    81] loss: 0.596\n",
            "[18,    83] loss: 0.285\n",
            "[18,    85] loss: 0.092\n",
            "[18,    87] loss: 0.283\n",
            "[18,    89] loss: 0.375\n",
            "[18,    91] loss: 0.630\n",
            "[18,    93] loss: 0.402\n",
            "[18,    95] loss: 0.407\n",
            "[18,    97] loss: 0.093\n",
            "[18,    99] loss: 0.373\n",
            "[18,   101] loss: 0.222\n",
            "[18,   103] loss: 0.162\n",
            "[18,   105] loss: 0.295\n",
            "[18,   107] loss: 0.671\n",
            "[18,   109] loss: 0.203\n",
            "[18,   111] loss: 0.158\n",
            "[18,   113] loss: 0.182\n",
            "[18,   115] loss: 0.049\n",
            "[18,   117] loss: 0.118\n",
            "[18,   119] loss: 0.380\n",
            "[18,   121] loss: 0.340\n",
            "[18,   123] loss: 0.075\n",
            "[18,   125] loss: 0.141\n",
            "[18,   127] loss: 0.321\n",
            "[18,   129] loss: 0.274\n",
            "[18,   131] loss: 0.228\n",
            "[18,   133] loss: 0.163\n",
            "[18,   135] loss: 0.135\n",
            "[18,   137] loss: 0.024\n",
            "[18,   139] loss: 0.084\n",
            "[18,   141] loss: 0.218\n",
            "[18,   143] loss: 0.253\n",
            "[18,   145] loss: 0.089\n",
            "[18,   147] loss: 0.234\n",
            "[18,   149] loss: 0.062\n",
            "[18,   151] loss: 0.298\n",
            "[18,   153] loss: 0.090\n",
            "[18,   155] loss: 0.053\n",
            "[18,   157] loss: 0.543\n",
            "[18,   159] loss: 0.206\n",
            "[18,   161] loss: 0.158\n",
            "[18,   163] loss: 0.207\n",
            "[18,   165] loss: 0.180\n",
            "[18,   167] loss: 0.181\n",
            "[18,   169] loss: 0.404\n",
            "[18,   171] loss: 0.333\n",
            "[18,   173] loss: 0.169\n",
            "[18,   175] loss: 0.319\n",
            "[18,   177] loss: 0.235\n",
            "[18,   179] loss: 0.357\n",
            "[18,   181] loss: 0.290\n",
            "[18,   183] loss: 0.368\n",
            "[18,   185] loss: 0.498\n",
            "[18,   187] loss: 0.210\n",
            "[18,   189] loss: 0.454\n",
            "[18,   191] loss: 0.046\n",
            "[18,   193] loss: 0.242\n",
            "[18,   195] loss: 0.639\n",
            "[18,   197] loss: 0.215\n",
            "[18,   199] loss: 0.243\n",
            "[18,   201] loss: 0.096\n",
            "[18,   203] loss: 0.754\n",
            "[18,   205] loss: 0.048\n",
            "[18,   207] loss: 0.439\n",
            "[18,   209] loss: 0.310\n",
            "[18,   211] loss: 0.065\n",
            "[18,   213] loss: 0.045\n",
            "[18,   215] loss: 0.378\n",
            "[18,   217] loss: 0.058\n",
            "[18,   219] loss: 0.362\n",
            "[18,   221] loss: 0.619\n",
            "[18,   223] loss: 0.278\n",
            "[18,   225] loss: 0.192\n",
            "[18,   227] loss: 0.162\n",
            "[18,   229] loss: 0.223\n",
            "[18,   231] loss: 0.101\n",
            "[18,   233] loss: 0.113\n",
            "[18,   235] loss: 0.398\n",
            "[18,   237] loss: 0.150\n",
            "[18,   239] loss: 0.283\n",
            "[18,   241] loss: 0.067\n",
            "[18,   243] loss: 0.303\n",
            "[18,   245] loss: 0.556\n",
            "[18,   247] loss: 0.128\n",
            "[18,   249] loss: 0.124\n",
            "[18,   251] loss: 0.169\n",
            "[18,   253] loss: 0.005\n",
            "[18,   255] loss: 0.163\n",
            "[18,   257] loss: 0.532\n",
            "[18,   259] loss: 0.577\n",
            "[18,   261] loss: 0.312\n",
            "[18,   263] loss: 0.096\n",
            "[18,   265] loss: 0.637\n",
            "[18,   267] loss: 0.273\n",
            "[18,   269] loss: 0.202\n",
            "[18,   271] loss: 0.269\n",
            "[18,   273] loss: 0.194\n",
            "[18,   275] loss: 0.060\n",
            "[18,   277] loss: 0.027\n",
            "[18,   279] loss: 0.196\n",
            "[18,   281] loss: 0.092\n",
            "[18,   283] loss: 0.452\n",
            "[18,   285] loss: 0.689\n",
            "[18,   287] loss: 0.087\n",
            "[18,   289] loss: 0.165\n",
            "[18,   291] loss: 0.019\n",
            "[18,   293] loss: 0.192\n",
            "[18,   295] loss: 0.329\n",
            "[18,   297] loss: 0.278\n",
            "[18,   299] loss: 0.170\n",
            "[18,   301] loss: 0.248\n",
            "[18,   303] loss: 0.873\n",
            "[18,   305] loss: 0.169\n",
            "[18,   307] loss: 0.557\n",
            "[18,   309] loss: 0.122\n",
            "[18,   311] loss: 0.111\n",
            "[18,   313] loss: 0.490\n",
            "[18,   315] loss: 0.843\n",
            "[18,   317] loss: 0.239\n",
            "[18,   319] loss: 0.281\n",
            "[18,   321] loss: 0.208\n",
            "[18,   323] loss: 0.188\n",
            "[18,   325] loss: 0.749\n",
            "[18,   327] loss: 0.305\n",
            "[18,   329] loss: 0.627\n",
            "[18,   331] loss: 0.180\n",
            "[18,   333] loss: 0.249\n",
            "[18,   335] loss: 0.298\n",
            "[18,   337] loss: 0.307\n",
            "[18,   339] loss: 0.328\n",
            "[18,   341] loss: 0.122\n",
            "[18,   343] loss: 0.421\n",
            "[18,   345] loss: 0.215\n",
            "[18,   347] loss: 0.492\n",
            "[18,   349] loss: 0.092\n",
            "[18,   351] loss: 0.105\n",
            "[18,   353] loss: 0.226\n",
            "[18,   355] loss: 0.024\n",
            "[18,   357] loss: 0.184\n",
            "[18,   359] loss: 0.358\n",
            "[18,   361] loss: 0.396\n",
            "[18,   363] loss: 0.060\n",
            "[18,   365] loss: 0.065\n",
            "[18,   367] loss: 0.071\n",
            "[18,   369] loss: 0.214\n",
            "[18,   371] loss: 0.253\n",
            "[18,   373] loss: 0.233\n",
            "[18,   375] loss: 0.343\n",
            "[18,   377] loss: 0.263\n",
            "[18,   379] loss: 0.439\n",
            "[18,   381] loss: 0.213\n",
            "[18,   383] loss: 0.210\n",
            "[18,   385] loss: 0.400\n",
            "[18,   387] loss: 0.381\n",
            "[18,   389] loss: 0.407\n",
            "[18,   391] loss: 0.665\n",
            "[18,   393] loss: 0.986\n",
            "[18,   395] loss: 0.342\n",
            "[18,   397] loss: 0.842\n",
            "[18,   399] loss: 0.432\n",
            "[18,   401] loss: 0.681\n",
            "[18,   403] loss: 0.213\n",
            "[18,   405] loss: 0.591\n",
            "[18,   407] loss: 0.557\n",
            "[18,   409] loss: 0.273\n",
            "[18,   411] loss: 0.178\n",
            "[18,   413] loss: 0.590\n",
            "[18,   415] loss: 0.250\n",
            "[18,   417] loss: 0.140\n",
            "[18,   419] loss: 0.063\n",
            "[18,   421] loss: 0.200\n",
            "[18,   423] loss: 0.185\n",
            "[18,   425] loss: 0.129\n",
            "[18,   427] loss: 0.118\n",
            "[18,   429] loss: 0.054\n",
            "[18,   431] loss: 0.227\n",
            "[18,   433] loss: 0.231\n",
            "[18,   435] loss: 0.089\n",
            "[18,   437] loss: 0.101\n",
            "[18,   439] loss: 0.041\n",
            "[18,   441] loss: 0.244\n",
            "[18,   443] loss: 0.229\n",
            "[18,   445] loss: 0.245\n",
            "[18,   447] loss: 0.315\n",
            "[18,   449] loss: 0.036\n",
            "[18,   451] loss: 0.396\n",
            "[18,   453] loss: 0.099\n",
            "[18,   455] loss: 0.060\n",
            "[18,   457] loss: 0.166\n",
            "[18,   459] loss: 0.261\n",
            "[18,   461] loss: 0.449\n",
            "[18,   463] loss: 0.193\n",
            "[18,   465] loss: 0.327\n",
            "[18,   467] loss: 0.140\n",
            "Epoch 17, Train Loss: 132.61109537724406, Val Loss: 2.5486974539393086, Val Acc: 72.56162915326902\n",
            "Epoch 17, Test Acc: 72.40641711229947\n",
            "[19,     1] loss: 0.129\n",
            "[19,     3] loss: 0.363\n",
            "[19,     5] loss: 0.434\n",
            "[19,     7] loss: 0.547\n",
            "[19,     9] loss: 0.207\n",
            "[19,    11] loss: 0.189\n",
            "[19,    13] loss: 0.171\n",
            "[19,    15] loss: 0.091\n",
            "[19,    17] loss: 0.208\n",
            "[19,    19] loss: 0.622\n",
            "[19,    21] loss: 0.088\n",
            "[19,    23] loss: 0.147\n",
            "[19,    25] loss: 0.202\n",
            "[19,    27] loss: 0.278\n",
            "[19,    29] loss: 0.367\n",
            "[19,    31] loss: 0.111\n",
            "[19,    33] loss: 0.269\n",
            "[19,    35] loss: 0.048\n",
            "[19,    37] loss: 0.130\n",
            "[19,    39] loss: 0.333\n",
            "[19,    41] loss: 0.220\n",
            "[19,    43] loss: 0.071\n",
            "[19,    45] loss: 0.170\n",
            "[19,    47] loss: 0.182\n",
            "[19,    49] loss: 0.191\n",
            "[19,    51] loss: 0.083\n",
            "[19,    53] loss: 0.272\n",
            "[19,    55] loss: 0.179\n",
            "[19,    57] loss: 0.227\n",
            "[19,    59] loss: 0.146\n",
            "[19,    61] loss: 0.129\n",
            "[19,    63] loss: 0.355\n",
            "[19,    65] loss: 0.158\n",
            "[19,    67] loss: 0.088\n",
            "[19,    69] loss: 0.277\n",
            "[19,    71] loss: 0.111\n",
            "[19,    73] loss: 0.551\n",
            "[19,    75] loss: 0.093\n",
            "[19,    77] loss: 0.367\n",
            "[19,    79] loss: 0.057\n",
            "[19,    81] loss: 0.166\n",
            "[19,    83] loss: 0.296\n",
            "[19,    85] loss: 0.141\n",
            "[19,    87] loss: 0.109\n",
            "[19,    89] loss: 0.306\n",
            "[19,    91] loss: 0.171\n",
            "[19,    93] loss: 0.159\n",
            "[19,    95] loss: 0.159\n",
            "[19,    97] loss: 0.053\n",
            "[19,    99] loss: 0.190\n",
            "[19,   101] loss: 0.171\n",
            "[19,   103] loss: 0.489\n",
            "[19,   105] loss: 0.095\n",
            "[19,   107] loss: 0.092\n",
            "[19,   109] loss: 0.379\n",
            "[19,   111] loss: 0.095\n",
            "[19,   113] loss: 0.213\n",
            "[19,   115] loss: 0.046\n",
            "[19,   117] loss: 0.285\n",
            "[19,   119] loss: 0.145\n",
            "[19,   121] loss: 0.149\n",
            "[19,   123] loss: 0.054\n",
            "[19,   125] loss: 0.213\n",
            "[19,   127] loss: 0.201\n",
            "[19,   129] loss: 0.311\n",
            "[19,   131] loss: 0.219\n",
            "[19,   133] loss: 0.167\n",
            "[19,   135] loss: 0.318\n",
            "[19,   137] loss: 0.296\n",
            "[19,   139] loss: 0.096\n",
            "[19,   141] loss: 0.115\n",
            "[19,   143] loss: 0.049\n",
            "[19,   145] loss: 0.375\n",
            "[19,   147] loss: 0.554\n",
            "[19,   149] loss: 0.145\n",
            "[19,   151] loss: 0.048\n",
            "[19,   153] loss: 0.099\n",
            "[19,   155] loss: 0.478\n",
            "[19,   157] loss: 0.104\n",
            "[19,   159] loss: 0.299\n",
            "[19,   161] loss: 0.523\n",
            "[19,   163] loss: 0.219\n",
            "[19,   165] loss: 0.033\n",
            "[19,   167] loss: 0.161\n",
            "[19,   169] loss: 0.176\n",
            "[19,   171] loss: 0.286\n",
            "[19,   173] loss: 0.071\n",
            "[19,   175] loss: 0.244\n",
            "[19,   177] loss: 0.150\n",
            "[19,   179] loss: 0.307\n",
            "[19,   181] loss: 0.111\n",
            "[19,   183] loss: 0.414\n",
            "[19,   185] loss: 0.176\n",
            "[19,   187] loss: 0.258\n",
            "[19,   189] loss: 0.254\n",
            "[19,   191] loss: 0.194\n",
            "[19,   193] loss: 0.251\n",
            "[19,   195] loss: 0.146\n",
            "[19,   197] loss: 0.146\n",
            "[19,   199] loss: 0.070\n",
            "[19,   201] loss: 0.277\n",
            "[19,   203] loss: 0.287\n",
            "[19,   205] loss: 0.389\n",
            "[19,   207] loss: 0.174\n",
            "[19,   209] loss: 0.143\n",
            "[19,   211] loss: 0.551\n",
            "[19,   213] loss: 0.256\n",
            "[19,   215] loss: 0.222\n",
            "[19,   217] loss: 0.041\n",
            "[19,   219] loss: 0.442\n",
            "[19,   221] loss: 0.153\n",
            "[19,   223] loss: 0.234\n",
            "[19,   225] loss: 0.268\n",
            "[19,   227] loss: 0.608\n",
            "[19,   229] loss: 0.257\n",
            "[19,   231] loss: 0.135\n",
            "[19,   233] loss: 0.210\n",
            "[19,   235] loss: 0.271\n",
            "[19,   237] loss: 0.291\n",
            "[19,   239] loss: 0.027\n",
            "[19,   241] loss: 0.265\n",
            "[19,   243] loss: 0.089\n",
            "[19,   245] loss: 0.041\n",
            "[19,   247] loss: 0.069\n",
            "[19,   249] loss: 0.550\n",
            "[19,   251] loss: 0.046\n",
            "[19,   253] loss: 0.012\n",
            "[19,   255] loss: 0.199\n",
            "[19,   257] loss: 0.197\n",
            "[19,   259] loss: 0.520\n",
            "[19,   261] loss: 0.020\n",
            "[19,   263] loss: 0.132\n",
            "[19,   265] loss: 0.452\n",
            "[19,   267] loss: 0.054\n",
            "[19,   269] loss: 0.299\n",
            "[19,   271] loss: 0.073\n",
            "[19,   273] loss: 0.240\n",
            "[19,   275] loss: 0.223\n",
            "[19,   277] loss: 0.216\n",
            "[19,   279] loss: 0.267\n",
            "[19,   281] loss: 0.788\n",
            "[19,   283] loss: 0.413\n",
            "[19,   285] loss: 0.057\n",
            "[19,   287] loss: 0.196\n",
            "[19,   289] loss: 0.382\n",
            "[19,   291] loss: 0.023\n",
            "[19,   293] loss: 0.442\n",
            "[19,   295] loss: 0.046\n",
            "[19,   297] loss: 0.128\n",
            "[19,   299] loss: 0.084\n",
            "[19,   301] loss: 0.109\n",
            "[19,   303] loss: 0.055\n",
            "[19,   305] loss: 0.318\n",
            "[19,   307] loss: 0.353\n",
            "[19,   309] loss: 0.080\n",
            "[19,   311] loss: 0.706\n",
            "[19,   313] loss: 0.475\n",
            "[19,   315] loss: 0.096\n",
            "[19,   317] loss: 0.443\n",
            "[19,   319] loss: 0.372\n",
            "[19,   321] loss: 0.420\n",
            "[19,   323] loss: 0.495\n",
            "[19,   325] loss: 0.117\n",
            "[19,   327] loss: 0.198\n",
            "[19,   329] loss: 0.139\n",
            "[19,   331] loss: 0.115\n",
            "[19,   333] loss: 0.015\n",
            "[19,   335] loss: 0.095\n",
            "[19,   337] loss: 0.051\n",
            "[19,   339] loss: 0.240\n",
            "[19,   341] loss: 0.046\n",
            "[19,   343] loss: 0.136\n",
            "[19,   345] loss: 0.222\n",
            "[19,   347] loss: 0.189\n",
            "[19,   349] loss: 0.558\n",
            "[19,   351] loss: 0.041\n",
            "[19,   353] loss: 0.264\n",
            "[19,   355] loss: 0.052\n",
            "[19,   357] loss: 0.820\n",
            "[19,   359] loss: 0.028\n",
            "[19,   361] loss: 0.162\n",
            "[19,   363] loss: 0.293\n",
            "[19,   365] loss: 0.021\n",
            "[19,   367] loss: 0.166\n",
            "[19,   369] loss: 0.222\n",
            "[19,   371] loss: 0.042\n",
            "[19,   373] loss: 0.045\n",
            "[19,   375] loss: 0.540\n",
            "[19,   377] loss: 0.532\n",
            "[19,   379] loss: 0.304\n",
            "[19,   381] loss: 0.295\n",
            "[19,   383] loss: 0.091\n",
            "[19,   385] loss: 0.237\n",
            "[19,   387] loss: 0.561\n",
            "[19,   389] loss: 0.121\n",
            "[19,   391] loss: 0.176\n",
            "[19,   393] loss: 0.265\n",
            "[19,   395] loss: 0.161\n",
            "[19,   397] loss: 0.254\n",
            "[19,   399] loss: 0.237\n",
            "[19,   401] loss: 0.285\n",
            "[19,   403] loss: 0.185\n",
            "[19,   405] loss: 0.204\n",
            "[19,   407] loss: 0.226\n",
            "[19,   409] loss: 0.379\n",
            "[19,   411] loss: 0.295\n",
            "[19,   413] loss: 0.679\n",
            "[19,   415] loss: 0.185\n",
            "[19,   417] loss: 0.289\n",
            "[19,   419] loss: 0.293\n",
            "[19,   421] loss: 0.463\n",
            "[19,   423] loss: 0.419\n",
            "[19,   425] loss: 0.221\n",
            "[19,   427] loss: 0.216\n",
            "[19,   429] loss: 0.148\n",
            "[19,   431] loss: 0.413\n",
            "[19,   433] loss: 0.339\n",
            "[19,   435] loss: 0.484\n",
            "[19,   437] loss: 0.250\n",
            "[19,   439] loss: 0.265\n",
            "[19,   441] loss: 0.220\n",
            "[19,   443] loss: 0.051\n",
            "[19,   445] loss: 0.205\n",
            "[19,   447] loss: 0.033\n",
            "[19,   449] loss: 0.365\n",
            "[19,   451] loss: 0.187\n",
            "[19,   453] loss: 0.131\n",
            "[19,   455] loss: 0.155\n",
            "[19,   457] loss: 0.132\n",
            "[19,   459] loss: 0.050\n",
            "[19,   461] loss: 0.275\n",
            "[19,   463] loss: 0.259\n",
            "[19,   465] loss: 0.158\n",
            "[19,   467] loss: 0.165\n",
            "Epoch 18, Train Loss: 106.91810674336739, Val Loss: 0.1654896056753094, Val Acc: 95.06966773847803\n",
            "Epoch 18, Test Acc: 95.72192513368984\n",
            "[20,     1] loss: 0.322\n",
            "[20,     3] loss: 0.274\n",
            "[20,     5] loss: 0.254\n",
            "[20,     7] loss: 0.192\n",
            "[20,     9] loss: 0.124\n",
            "[20,    11] loss: 0.295\n",
            "[20,    13] loss: 0.238\n",
            "[20,    15] loss: 0.297\n",
            "[20,    17] loss: 0.165\n",
            "[20,    19] loss: 0.190\n",
            "[20,    21] loss: 0.179\n",
            "[20,    23] loss: 0.149\n",
            "[20,    25] loss: 0.159\n",
            "[20,    27] loss: 0.276\n",
            "[20,    29] loss: 0.248\n",
            "[20,    31] loss: 0.314\n",
            "[20,    33] loss: 0.262\n",
            "[20,    35] loss: 0.181\n",
            "[20,    37] loss: 0.168\n",
            "[20,    39] loss: 0.307\n",
            "[20,    41] loss: 0.103\n",
            "[20,    43] loss: 0.183\n",
            "[20,    45] loss: 0.203\n",
            "[20,    47] loss: 0.036\n",
            "[20,    49] loss: 0.199\n",
            "[20,    51] loss: 0.048\n",
            "[20,    53] loss: 0.009\n",
            "[20,    55] loss: 0.022\n",
            "[20,    57] loss: 0.207\n",
            "[20,    59] loss: 0.252\n",
            "[20,    61] loss: 0.091\n",
            "[20,    63] loss: 0.152\n",
            "[20,    65] loss: 0.304\n",
            "[20,    67] loss: 0.348\n",
            "[20,    69] loss: 0.252\n",
            "[20,    71] loss: 0.204\n",
            "[20,    73] loss: 0.827\n",
            "[20,    75] loss: 0.029\n",
            "[20,    77] loss: 0.669\n",
            "[20,    79] loss: 0.132\n",
            "[20,    81] loss: 0.214\n",
            "[20,    83] loss: 0.361\n",
            "[20,    85] loss: 0.285\n",
            "[20,    87] loss: 0.288\n",
            "[20,    89] loss: 0.233\n",
            "[20,    91] loss: 0.021\n",
            "[20,    93] loss: 0.440\n",
            "[20,    95] loss: 0.189\n",
            "[20,    97] loss: 0.250\n",
            "[20,    99] loss: 0.085\n",
            "[20,   101] loss: 0.205\n",
            "[20,   103] loss: 0.096\n",
            "[20,   105] loss: 0.198\n",
            "[20,   107] loss: 0.057\n",
            "[20,   109] loss: 0.582\n",
            "[20,   111] loss: 0.164\n",
            "[20,   113] loss: 0.195\n",
            "[20,   115] loss: 0.086\n",
            "[20,   117] loss: 0.466\n",
            "[20,   119] loss: 0.357\n",
            "[20,   121] loss: 0.171\n",
            "[20,   123] loss: 0.166\n",
            "[20,   125] loss: 0.154\n",
            "[20,   127] loss: 0.230\n",
            "[20,   129] loss: 0.066\n",
            "[20,   131] loss: 0.166\n",
            "[20,   133] loss: 0.052\n",
            "[20,   135] loss: 0.260\n",
            "[20,   137] loss: 0.311\n",
            "[20,   139] loss: 0.143\n",
            "[20,   141] loss: 0.559\n",
            "[20,   143] loss: 0.083\n",
            "[20,   145] loss: 0.156\n",
            "[20,   147] loss: 0.053\n",
            "[20,   149] loss: 0.036\n",
            "[20,   151] loss: 0.182\n",
            "[20,   153] loss: 0.234\n",
            "[20,   155] loss: 0.065\n",
            "[20,   157] loss: 0.045\n",
            "[20,   159] loss: 0.094\n",
            "[20,   161] loss: 0.163\n",
            "[20,   163] loss: 0.193\n",
            "[20,   165] loss: 0.108\n",
            "[20,   167] loss: 0.157\n",
            "[20,   169] loss: 0.148\n",
            "[20,   171] loss: 0.214\n",
            "[20,   173] loss: 0.124\n",
            "[20,   175] loss: 0.114\n",
            "[20,   177] loss: 0.143\n",
            "[20,   179] loss: 0.358\n",
            "[20,   181] loss: 0.093\n",
            "[20,   183] loss: 0.178\n",
            "[20,   185] loss: 0.148\n",
            "[20,   187] loss: 0.089\n",
            "[20,   189] loss: 0.327\n",
            "[20,   191] loss: 0.069\n",
            "[20,   193] loss: 0.200\n",
            "[20,   195] loss: 0.200\n",
            "[20,   197] loss: 0.152\n",
            "[20,   199] loss: 0.117\n",
            "[20,   201] loss: 0.274\n",
            "[20,   203] loss: 0.048\n",
            "[20,   205] loss: 0.358\n",
            "[20,   207] loss: 0.014\n",
            "[20,   209] loss: 0.048\n",
            "[20,   211] loss: 0.174\n",
            "[20,   213] loss: 0.037\n",
            "[20,   215] loss: 0.607\n",
            "[20,   217] loss: 0.195\n",
            "[20,   219] loss: 0.470\n",
            "[20,   221] loss: 0.054\n",
            "[20,   223] loss: 0.449\n",
            "[20,   225] loss: 0.173\n",
            "[20,   227] loss: 0.137\n",
            "[20,   229] loss: 0.188\n",
            "[20,   231] loss: 0.108\n",
            "[20,   233] loss: 0.480\n",
            "[20,   235] loss: 0.321\n",
            "[20,   237] loss: 0.078\n",
            "[20,   239] loss: 0.062\n",
            "[20,   241] loss: 0.240\n",
            "[20,   243] loss: 0.945\n",
            "[20,   245] loss: 0.515\n",
            "[20,   247] loss: 0.005\n",
            "[20,   249] loss: 0.149\n",
            "[20,   251] loss: 0.367\n",
            "[20,   253] loss: 0.237\n",
            "[20,   255] loss: 0.235\n",
            "[20,   257] loss: 0.151\n",
            "[20,   259] loss: 0.330\n",
            "[20,   261] loss: 0.165\n",
            "[20,   263] loss: 0.140\n",
            "[20,   265] loss: 0.191\n",
            "[20,   267] loss: 0.214\n",
            "[20,   269] loss: 0.629\n",
            "[20,   271] loss: 0.064\n",
            "[20,   273] loss: 0.192\n",
            "[20,   275] loss: 0.176\n",
            "[20,   277] loss: 0.130\n",
            "[20,   279] loss: 0.142\n",
            "[20,   281] loss: 0.349\n",
            "[20,   283] loss: 0.130\n",
            "[20,   285] loss: 0.200\n",
            "[20,   287] loss: 0.169\n",
            "[20,   289] loss: 0.044\n",
            "[20,   291] loss: 0.040\n",
            "[20,   293] loss: 0.354\n",
            "[20,   295] loss: 0.029\n",
            "[20,   297] loss: 0.322\n",
            "[20,   299] loss: 0.709\n",
            "[20,   301] loss: 0.196\n",
            "[20,   303] loss: 0.495\n",
            "[20,   305] loss: 0.242\n",
            "[20,   307] loss: 0.057\n",
            "[20,   309] loss: 0.092\n",
            "[20,   311] loss: 0.338\n",
            "[20,   313] loss: 0.135\n",
            "[20,   315] loss: 0.127\n",
            "[20,   317] loss: 0.136\n",
            "[20,   319] loss: 0.075\n",
            "[20,   321] loss: 0.064\n",
            "[20,   323] loss: 0.057\n",
            "[20,   325] loss: 0.230\n",
            "[20,   327] loss: 0.201\n",
            "[20,   329] loss: 0.304\n",
            "[20,   331] loss: 0.223\n",
            "[20,   333] loss: 0.130\n",
            "[20,   335] loss: 0.125\n",
            "[20,   337] loss: 0.300\n",
            "[20,   339] loss: 0.354\n",
            "[20,   341] loss: 0.163\n",
            "[20,   343] loss: 0.316\n",
            "[20,   345] loss: 0.157\n",
            "[20,   347] loss: 0.163\n",
            "[20,   349] loss: 0.149\n",
            "[20,   351] loss: 0.050\n",
            "[20,   353] loss: 0.073\n",
            "[20,   355] loss: 0.168\n",
            "[20,   357] loss: 0.096\n",
            "[20,   359] loss: 0.097\n",
            "[20,   361] loss: 0.111\n",
            "[20,   363] loss: 0.094\n",
            "[20,   365] loss: 0.248\n",
            "[20,   367] loss: 0.101\n",
            "[20,   369] loss: 0.060\n",
            "[20,   371] loss: 0.167\n",
            "[20,   373] loss: 0.198\n",
            "[20,   375] loss: 0.581\n",
            "[20,   377] loss: 0.165\n",
            "[20,   379] loss: 0.545\n",
            "[20,   381] loss: 0.253\n",
            "[20,   383] loss: 0.215\n",
            "[20,   385] loss: 0.456\n",
            "[20,   387] loss: 0.201\n",
            "[20,   389] loss: 0.201\n",
            "[20,   391] loss: 0.120\n",
            "[20,   393] loss: 0.210\n",
            "[20,   395] loss: 0.386\n",
            "[20,   397] loss: 0.156\n",
            "[20,   399] loss: 0.022\n",
            "[20,   401] loss: 0.578\n",
            "[20,   403] loss: 0.279\n",
            "[20,   405] loss: 0.058\n",
            "[20,   407] loss: 0.251\n",
            "[20,   409] loss: 0.051\n",
            "[20,   411] loss: 0.210\n",
            "[20,   413] loss: 0.171\n",
            "[20,   415] loss: 0.199\n",
            "[20,   417] loss: 0.167\n",
            "[20,   419] loss: 0.248\n",
            "[20,   421] loss: 0.029\n",
            "[20,   423] loss: 0.218\n",
            "[20,   425] loss: 0.091\n",
            "[20,   427] loss: 0.229\n",
            "[20,   429] loss: 0.187\n",
            "[20,   431] loss: 0.193\n",
            "[20,   433] loss: 0.256\n",
            "[20,   435] loss: 0.116\n",
            "[20,   437] loss: 0.238\n",
            "[20,   439] loss: 0.032\n",
            "[20,   441] loss: 0.501\n",
            "[20,   443] loss: 0.250\n",
            "[20,   445] loss: 0.190\n",
            "[20,   447] loss: 0.203\n",
            "[20,   449] loss: 0.104\n",
            "[20,   451] loss: 0.158\n",
            "[20,   453] loss: 0.378\n",
            "[20,   455] loss: 0.362\n",
            "[20,   457] loss: 0.457\n",
            "[20,   459] loss: 0.399\n",
            "[20,   461] loss: 0.046\n",
            "[20,   463] loss: 0.154\n",
            "[20,   465] loss: 0.214\n",
            "[20,   467] loss: 0.051\n",
            "Epoch 19, Train Loss: 98.12484163569752, Val Loss: 0.22520272933357097, Val Acc: 91.7470525187567\n",
            "Epoch 19, Test Acc: 93.475935828877\n",
            "[21,     1] loss: 0.044\n",
            "[21,     3] loss: 0.338\n",
            "[21,     5] loss: 0.099\n",
            "[21,     7] loss: 0.139\n",
            "[21,     9] loss: 0.246\n",
            "[21,    11] loss: 0.543\n",
            "[21,    13] loss: 0.670\n",
            "[21,    15] loss: 0.330\n",
            "[21,    17] loss: 0.488\n",
            "[21,    19] loss: 0.415\n",
            "[21,    21] loss: 0.137\n",
            "[21,    23] loss: 0.041\n",
            "[21,    25] loss: 0.327\n",
            "[21,    27] loss: 0.360\n",
            "[21,    29] loss: 0.327\n",
            "[21,    31] loss: 0.123\n",
            "[21,    33] loss: 0.267\n",
            "[21,    35] loss: 0.184\n",
            "[21,    37] loss: 0.171\n",
            "[21,    39] loss: 0.060\n",
            "[21,    41] loss: 0.070\n",
            "[21,    43] loss: 0.053\n",
            "[21,    45] loss: 0.127\n",
            "[21,    47] loss: 0.073\n",
            "[21,    49] loss: 0.175\n",
            "[21,    51] loss: 0.290\n",
            "[21,    53] loss: 0.197\n",
            "[21,    55] loss: 0.189\n",
            "[21,    57] loss: 0.262\n",
            "[21,    59] loss: 0.301\n",
            "[21,    61] loss: 0.199\n",
            "[21,    63] loss: 0.274\n",
            "[21,    65] loss: 0.250\n",
            "[21,    67] loss: 0.289\n",
            "[21,    69] loss: 0.256\n",
            "[21,    71] loss: 0.217\n",
            "[21,    73] loss: 0.223\n",
            "[21,    75] loss: 0.177\n",
            "[21,    77] loss: 0.222\n",
            "[21,    79] loss: 0.085\n",
            "[21,    81] loss: 0.556\n",
            "[21,    83] loss: 0.247\n",
            "[21,    85] loss: 0.372\n",
            "[21,    87] loss: 0.207\n",
            "[21,    89] loss: 0.071\n",
            "[21,    91] loss: 0.224\n",
            "[21,    93] loss: 0.055\n",
            "[21,    95] loss: 0.084\n",
            "[21,    97] loss: 0.067\n",
            "[21,    99] loss: 0.373\n",
            "[21,   101] loss: 0.022\n",
            "[21,   103] loss: 0.360\n",
            "[21,   105] loss: 0.075\n",
            "[21,   107] loss: 0.379\n",
            "[21,   109] loss: 0.179\n",
            "[21,   111] loss: 0.254\n",
            "[21,   113] loss: 0.401\n",
            "[21,   115] loss: 0.144\n",
            "[21,   117] loss: 0.045\n",
            "[21,   119] loss: 0.265\n",
            "[21,   121] loss: 0.062\n",
            "[21,   123] loss: 0.660\n",
            "[21,   125] loss: 0.094\n",
            "[21,   127] loss: 0.078\n",
            "[21,   129] loss: 0.141\n",
            "[21,   131] loss: 0.068\n",
            "[21,   133] loss: 0.285\n",
            "[21,   135] loss: 0.066\n",
            "[21,   137] loss: 0.062\n",
            "[21,   139] loss: 0.332\n",
            "[21,   141] loss: 0.182\n",
            "[21,   143] loss: 0.512\n",
            "[21,   145] loss: 0.305\n",
            "[21,   147] loss: 0.339\n",
            "[21,   149] loss: 0.169\n",
            "[21,   151] loss: 0.320\n",
            "[21,   153] loss: 0.213\n",
            "[21,   155] loss: 0.442\n",
            "[21,   157] loss: 0.188\n",
            "[21,   159] loss: 0.165\n",
            "[21,   161] loss: 0.531\n",
            "[21,   163] loss: 0.187\n",
            "[21,   165] loss: 0.112\n",
            "[21,   167] loss: 0.313\n",
            "[21,   169] loss: 0.378\n",
            "[21,   171] loss: 0.104\n",
            "[21,   173] loss: 0.332\n",
            "[21,   175] loss: 0.312\n",
            "[21,   177] loss: 0.034\n",
            "[21,   179] loss: 0.118\n",
            "[21,   181] loss: 0.053\n",
            "[21,   183] loss: 0.038\n",
            "[21,   185] loss: 0.218\n",
            "[21,   187] loss: 0.263\n",
            "[21,   189] loss: 0.234\n",
            "[21,   191] loss: 0.026\n",
            "[21,   193] loss: 0.143\n",
            "[21,   195] loss: 0.136\n",
            "[21,   197] loss: 0.303\n",
            "[21,   199] loss: 0.266\n",
            "[21,   201] loss: 0.431\n",
            "[21,   203] loss: 0.241\n",
            "[21,   205] loss: 0.452\n",
            "[21,   207] loss: 0.218\n",
            "[21,   209] loss: 0.341\n",
            "[21,   211] loss: 0.383\n",
            "[21,   213] loss: 0.117\n",
            "[21,   215] loss: 0.502\n",
            "[21,   217] loss: 0.173\n",
            "[21,   219] loss: 0.331\n",
            "[21,   221] loss: 0.302\n",
            "[21,   223] loss: 0.590\n",
            "[21,   225] loss: 0.386\n",
            "[21,   227] loss: 0.321\n",
            "[21,   229] loss: 0.313\n",
            "[21,   231] loss: 0.270\n",
            "[21,   233] loss: 0.130\n",
            "[21,   235] loss: 0.014\n",
            "[21,   237] loss: 0.413\n",
            "[21,   239] loss: 0.058\n",
            "[21,   241] loss: 0.171\n",
            "[21,   243] loss: 0.284\n",
            "[21,   245] loss: 0.238\n",
            "[21,   247] loss: 0.624\n",
            "[21,   249] loss: 0.195\n",
            "[21,   251] loss: 0.094\n",
            "[21,   253] loss: 0.246\n",
            "[21,   255] loss: 0.219\n",
            "[21,   257] loss: 0.319\n",
            "[21,   259] loss: 0.196\n",
            "[21,   261] loss: 0.055\n",
            "[21,   263] loss: 0.241\n",
            "[21,   265] loss: 0.018\n",
            "[21,   267] loss: 0.048\n",
            "[21,   269] loss: 0.207\n",
            "[21,   271] loss: 0.217\n",
            "[21,   273] loss: 0.063\n",
            "[21,   275] loss: 0.240\n",
            "[21,   277] loss: 0.506\n",
            "[21,   279] loss: 0.408\n",
            "[21,   281] loss: 0.133\n",
            "[21,   283] loss: 0.185\n",
            "[21,   285] loss: 0.217\n",
            "[21,   287] loss: 0.494\n",
            "[21,   289] loss: 0.231\n",
            "[21,   291] loss: 0.065\n",
            "[21,   293] loss: 0.037\n",
            "[21,   295] loss: 0.168\n",
            "[21,   297] loss: 0.412\n",
            "[21,   299] loss: 0.167\n",
            "[21,   301] loss: 0.744\n",
            "[21,   303] loss: 0.113\n",
            "[21,   305] loss: 0.064\n",
            "[21,   307] loss: 0.058\n",
            "[21,   309] loss: 0.441\n",
            "[21,   311] loss: 0.187\n",
            "[21,   313] loss: 0.232\n",
            "[21,   315] loss: 0.315\n",
            "[21,   317] loss: 0.211\n",
            "[21,   319] loss: 0.117\n",
            "[21,   321] loss: 0.122\n",
            "[21,   323] loss: 0.140\n",
            "[21,   325] loss: 0.193\n",
            "[21,   327] loss: 0.206\n",
            "[21,   329] loss: 0.206\n",
            "[21,   331] loss: 0.124\n",
            "[21,   333] loss: 0.139\n",
            "[21,   335] loss: 0.223\n",
            "[21,   337] loss: 0.310\n",
            "[21,   339] loss: 0.649\n",
            "[21,   341] loss: 0.247\n",
            "[21,   343] loss: 0.214\n",
            "[21,   345] loss: 0.190\n",
            "[21,   347] loss: 0.217\n",
            "[21,   349] loss: 0.139\n",
            "[21,   351] loss: 0.201\n",
            "[21,   353] loss: 0.290\n",
            "[21,   355] loss: 0.119\n",
            "[21,   357] loss: 0.247\n",
            "[21,   359] loss: 0.669\n",
            "[21,   361] loss: 0.187\n",
            "[21,   363] loss: 0.139\n",
            "[21,   365] loss: 0.437\n",
            "[21,   367] loss: 0.306\n",
            "[21,   369] loss: 0.470\n",
            "[21,   371] loss: 0.712\n",
            "[21,   373] loss: 0.086\n",
            "[21,   375] loss: 0.137\n",
            "[21,   377] loss: 0.197\n",
            "[21,   379] loss: 0.577\n",
            "[21,   381] loss: 0.190\n",
            "[21,   383] loss: 0.418\n",
            "[21,   385] loss: 0.307\n",
            "[21,   387] loss: 0.276\n",
            "[21,   389] loss: 0.261\n",
            "[21,   391] loss: 0.323\n",
            "[21,   393] loss: 0.184\n",
            "[21,   395] loss: 0.314\n",
            "[21,   397] loss: 0.259\n",
            "[21,   399] loss: 0.458\n",
            "[21,   401] loss: 0.140\n",
            "[21,   403] loss: 0.357\n",
            "[21,   405] loss: 0.189\n",
            "[21,   407] loss: 0.191\n",
            "[21,   409] loss: 0.357\n",
            "[21,   411] loss: 0.265\n",
            "[21,   413] loss: 0.427\n",
            "[21,   415] loss: 0.007\n",
            "[21,   417] loss: 0.167\n",
            "[21,   419] loss: 0.147\n",
            "[21,   421] loss: 0.147\n",
            "[21,   423] loss: 0.177\n",
            "[21,   425] loss: 0.178\n",
            "[21,   427] loss: 0.222\n",
            "[21,   429] loss: 0.111\n",
            "[21,   431] loss: 0.451\n",
            "[21,   433] loss: 0.123\n",
            "[21,   435] loss: 0.238\n",
            "[21,   437] loss: 0.433\n",
            "[21,   439] loss: 0.126\n",
            "[21,   441] loss: 0.173\n",
            "[21,   443] loss: 0.253\n",
            "[21,   445] loss: 0.299\n",
            "[21,   447] loss: 0.303\n",
            "[21,   449] loss: 0.233\n",
            "[21,   451] loss: 0.249\n",
            "[21,   453] loss: 0.016\n",
            "[21,   455] loss: 0.179\n",
            "[21,   457] loss: 0.202\n",
            "[21,   459] loss: 0.116\n",
            "[21,   461] loss: 0.022\n",
            "[21,   463] loss: 0.214\n",
            "[21,   465] loss: 0.132\n",
            "[21,   467] loss: 0.086\n",
            "Epoch 20, Train Loss: 111.15727668209001, Val Loss: 0.7939607101981923, Val Acc: 80.38585209003216\n",
            "Epoch 20, Test Acc: 80.32085561497327\n",
            "[22,     1] loss: 0.199\n",
            "[22,     3] loss: 0.189\n",
            "[22,     5] loss: 0.246\n",
            "[22,     7] loss: 0.737\n",
            "[22,     9] loss: 0.492\n",
            "[22,    11] loss: 0.047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-360f62f4e01b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tx87lo0AV3r",
        "colab_type": "text"
      },
      "source": [
        "### Add\n",
        "* 1. epoch, loss, test_acc 등 값 저장 \n",
        "* 2. Graph 출력 및 저장\n",
        "* <code>-20.09.16.Wed. pm12:--</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI7ZylP_A5DO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "f4cd695a-4af4-466c-d4aa-fee5e6f79794"
      },
      "source": [
        "# plot\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "# ===== Loss Function ====== #\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax1.plot(list_epoch, list_train_loss, label='train_loss')\n",
        "ax1.plot(list_epoch, list_val_loss, '--', label='val_loss')\n",
        "ax1.set_xlabel('epoch')\n",
        "ax1.set_ylabel('loss')\n",
        "ax1.grid()\n",
        "ax1.legend()\n",
        "ax1.set_title('epoch vs loss')\n",
        "\n",
        "# ====== Metric Fluctuation ===== #\n",
        "ax2 = fig.add_subplot(1, 2, 2)\n",
        "ax2.plot(list_acc_epoch, list_test_acc, marker='x', label='Accuracy metric')\n",
        "ax2.set_xlabel('epoch')\n",
        "ax2.set_ylabel('Acc')\n",
        "ax2.grid()\n",
        "ax2.legend()\n",
        "ax2.set_title('epoch vs Accuracy')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuXogTLCCu9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save loss values\n",
        "# save all train, test results\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# save model\n",
        "save_model_path = './results_ResNet-VAE_Exp01-Classification_Test'  \n",
        "\n",
        "\n",
        "train_losses = list_train_loss\n",
        "val_losses = list_val_loss\n",
        "test_acc = list_test_acc\n",
        "\n",
        "np.save(os.path.join(save_model_path, 'ResNet-152_Training_loss.npy'), train_losses)\n",
        "np.save(os.path.join(save_model_path, 'ResNet-152_Test_loss.npy'), val_losses)\n",
        "np.save(os.path.join(save_model_path, 'ResNet-152_Test_Acc.npy'), test_acc)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thfttTjF8-Wm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5dac9343-c821-4dc2-b23b-749c3d6008c6"
      },
      "source": [
        "import argparse\n",
        "\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "args.net = resnet152\n",
        "args.criterion = criterion\n",
        "args.optim = optimizer\n",
        "\n",
        "args.train_loader = train_loader\n",
        "args.val_loader = valid_loader\n",
        "args.test_loader = test_loader\n",
        "\n",
        "# args.n_layer = 5\n",
        "# args.in_dim = 3072\n",
        "# args.out_dim = 10\n",
        "# args.hid_dim = 100\n",
        "# args.act = 'relu'\n",
        "\n",
        "args.lr = 0.001\n",
        "args.mm = 0.9\n",
        "args.epoch = 100\n",
        "\n",
        "print(args.epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB51INQhAxTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7c22bfb7-df40-4f11-da1a-e98adec0038e"
      },
      "source": [
        "experiment(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Train Loss: 985.2862548828125, Val Loss: 2091.101662643885, Val Acc: 34.833869239013936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(985.2862548828125, 2091.101662643885, 34.833869239013936, 31.764705882352942)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHhsevzzx1VG",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXtaJBsMVnki",
        "colab_type": "text"
      },
      "source": [
        "## 03. Model Architecture\n",
        "\n",
        "* argparser\n",
        "* save model parameters\n",
        "* loss function\n",
        "* define hyperthesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gupryt18d2Pk",
        "colab_type": "text"
      },
      "source": [
        "✅ Have to-do <br>\n",
        "> 1. Train ResNet-VAE\n",
        "> 2. 📌 Hyperparameter Optimization >>> argparser...!!\n",
        "> 3. 📌 Save Model's Parameters\n",
        "> 4. 📌 Plot & Save values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmCALY3ogDOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save model\n",
        "save_model_path = './results_ResNet-VAE_Exp01'  # save_model parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57uBwQXKgUsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_mkdir(dir_name):\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.mkdir(dir_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzUcKJYGgivD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    # MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    MSE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return MSE + KLD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzJXvtlnmvUb",
        "colab_type": "text"
      },
      "source": [
        "model from this repository: https://github.com/hsinyilin19/ResNetVAE/blob/master/modules.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diJLO0clmrL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet_VAE(nn.Module):\n",
        "    def __init__(self, fc_hidden1=1024, fc_hidden2=768, drop_p=0.3, CNN_embed_dim=256):\n",
        "        super(ResNet_VAE, self).__init__()\n",
        "\n",
        "        self.fc_hidden1, self.fc_hidden2, self.CNN_embed_dim = fc_hidden1, fc_hidden2, CNN_embed_dim\n",
        "\n",
        "        # CNN architechtures\n",
        "        self.ch1, self.ch2, self.ch3, self.ch4 = 16, 32, 64, 128\n",
        "        self.k1, self.k2, self.k3, self.k4 = (5, 5), (3, 3), (3, 3), (3, 3)      # 2d kernal size\n",
        "        self.s1, self.s2, self.s3, self.s4 = (2, 2), (2, 2), (2, 2), (2, 2)      # 2d strides\n",
        "        self.pd1, self.pd2, self.pd3, self.pd4 = (0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\n",
        "\n",
        "        # encoding components\n",
        "        resnet = models.resnet152(pretrained=True)  # ResNet-VAE -20.09.15.Tue- pm5:00\n",
        "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.fc1 = nn.Linear(resnet.fc.in_features, self.fc_hidden1)\n",
        "        self.bn1 = nn.BatchNorm1d(self.fc_hidden1, momentum=0.01)\n",
        "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
        "        self.bn2 = nn.BatchNorm1d(self.fc_hidden2, momentum=0.01)\n",
        "        # Latent vectors mu and sigma\n",
        "        self.fc3_mu = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)      # output = CNN embedding latent variables\n",
        "        self.fc3_logvar = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)  # output = CNN embedding latent variables\n",
        "\n",
        "        # Sampling vector\n",
        "        self.fc4 = nn.Linear(self.CNN_embed_dim, self.fc_hidden2)\n",
        "        self.fc_bn4 = nn.BatchNorm1d(self.fc_hidden2)\n",
        "        self.fc5 = nn.Linear(self.fc_hidden2, 64 * 4 * 4)\n",
        "        self.fc_bn5 = nn.BatchNorm1d(64 * 4 * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Decoder\n",
        "        self.convTrans6 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=self.k4, stride=self.s4,\n",
        "                               padding=self.pd4),\n",
        "            nn.BatchNorm2d(32, momentum=0.01),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.convTrans7 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=32, out_channels=8, kernel_size=self.k3, stride=self.s3,\n",
        "                               padding=self.pd3),\n",
        "            nn.BatchNorm2d(8, momentum=0.01),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.convTrans8 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=self.k2, stride=self.s2,\n",
        "                               padding=self.pd2),\n",
        "            nn.BatchNorm2d(3, momentum=0.01),\n",
        "            nn.Sigmoid()    # y = (y1, y2, y3) \\in [0 ,1]^3\n",
        "        )\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.resnet(x)  # ResNet\n",
        "        x = x.view(x.size(0), -1)  # flatten output of conv\n",
        "\n",
        "        # FC layers\n",
        "        x = self.bn1(self.fc1(x))\n",
        "        x = self.relu(x)\n",
        "        x = self.bn2(self.fc2(x))\n",
        "        x = self.relu(x)\n",
        "        # x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "        mu, logvar = self.fc3_mu(x), self.fc3_logvar(x)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = Variable(std.data.new(std.size()).normal_())\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def decode(self, z):\n",
        "        x = self.relu(self.fc_bn4(self.fc4(z)))\n",
        "        x = self.relu(self.fc_bn5(self.fc5(x))).view(-1, 64, 4, 4)\n",
        "        x = self.convTrans6(x)\n",
        "        x = self.convTrans7(x)\n",
        "        x = self.convTrans8(x)\n",
        "        x = F.interpolate(x, size=(224, 224), mode='bilinear')\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_reconst = self.decode(z)\n",
        "\n",
        "        return x_reconst, z, mu, logvar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuGbzAY_Vp7g",
        "colab_type": "text"
      },
      "source": [
        "## 04. Define Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dfNGY24giRq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a86a3158-9cd0-4bec-fb56-866b28a1f153"
      },
      "source": [
        "# Detect devices\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mDXrJ4bgfpM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ecf199e1-f75e-42b8-e7fc-1743f3ea8d2c"
      },
      "source": [
        "'''\n",
        "### If you want to use pre-trained model ####\n",
        "pre_saved_model_path = './results_Malimg'\n",
        "epoch=20\n",
        "'''\n",
        "\n",
        "# Create model\n",
        "resnet_vae = ResNet_VAE(fc_hidden1=args.CNN_fc_hidden1, fc_hidden2=args.CNN_fc_hidden2, drop_p=args.dropout_p, CNN_embed_dim=args.CNN_embed_dim).to(device)\n",
        "'''\n",
        "### If you want to use pre-trained model ####\n",
        "resnet_vae.load_state_dict(torch.load(os.path.join(pre_saved_model_path, 'model_epoch{}.pth'.format(epoch))))\n",
        "'''\n",
        "print(resnet_vae)\n",
        "print('Number of {} parameters'.format(sum(p.numel() for p in resnet_vae.parameters() if p.requires_grad)))\n",
        "print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n",
        "model_params = list(resnet_vae.parameters())\n",
        "optimizer = torch.optim.Adam(model_params, lr=args.learning_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet_VAE(\n",
            "  (resnet): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (6): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (7): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (6): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (7): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (8): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (9): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (10): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (11): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (12): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (13): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (14): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (15): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (16): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (17): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (18): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (19): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (20): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (21): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (22): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (23): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (24): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (25): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (26): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (27): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (28): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (29): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (30): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (31): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (32): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (33): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (34): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (35): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
            "  (fc3_mu): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (fc3_logvar): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (fc4): Linear(in_features=256, out_features=1024, bias=True)\n",
            "  (fc_bn4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc_bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (convTrans6): Sequential(\n",
            "    (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (convTrans7): Sequential(\n",
            "    (0): ConvTranspose2d(32, 8, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (convTrans8): Sequential(\n",
            "    (0): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
            "    (2): Sigmoid()\n",
            "  )\n",
            ")\n",
            "Number of 63158425 parameters\n",
            "Using 1 GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh1Ve1PdgjAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
        "    # set model as training mode\n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "    all_y, all_z, all_mu, all_logvar = [], [], [], []\n",
        "    N_count = 0   # counting total trained sample in one epoch\n",
        "    for batch_idx, (X, y) in enumerate(train_loader):\n",
        "        # distribute data to device\n",
        "        X, y = X.to(device), y.to(device).view(-1, )\n",
        "        N_count += X.size(0)  # count batch_size sample\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        X_reconst, z, mu, logvar = model(X)  # VAE\n",
        "        loss = loss_function(X_reconst, X, mu, logvar)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        all_y.extend(y.data.cpu().numpy())\n",
        "        all_z.extend(z.data.cpu().numpy())\n",
        "        all_mu.extend(mu.data.cpu().numpy())\n",
        "        all_logvar.extend(logvar.data.cpu().numpy())\n",
        "\n",
        "        # show information\n",
        "        if (batch_idx + 1) % log_interval == 0:  # if batch_size = 16 => 160\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "    # calculate train_loss\n",
        "    losses /= len(train_loader.dataset)\n",
        "    all_y = np.stack(all_y, axis=0)\n",
        "    all_z = np.stack(all_z, axis=0)\n",
        "    all_mu = np.stack(all_mu, axis=0)\n",
        "    all_logvar = np.stack(all_logvar, axis=0)\n",
        "\n",
        "    # save Pytorch models of best record\n",
        "    torch.save(model.state_dict(), os.path.join(save_model_path, 'model_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
        "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
        "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
        "\n",
        "    return X.data.cpu().numpy(), all_y, all_z, all_mu, all_logvar, losses\n",
        "\n",
        "\n",
        "def validation(model, device, optimizer, test_loader):\n",
        "    # set model as testing mode\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    all_y, all_z, all_mu, all_logvar = [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y) in enumerate(test_loader):\n",
        "            # distribute data to device\n",
        "            X, y = X.to(device), y.to(device).view(-1, )\n",
        "            X_reconst, z, mu, logvar = model(X)\n",
        "\n",
        "            loss = loss_function(X_reconst, X, mu, logvar)\n",
        "            test_loss += loss.item()  # sum up batch loss\n",
        "\n",
        "            all_y.extend(y.data.cpu().numpy())\n",
        "            all_z.extend(z.data.cpu().numpy())\n",
        "            all_mu.extend(mu.data.cpu().numpy())\n",
        "            all_logvar.extend(logvar.data.cpu().numpy())\n",
        "            \n",
        "        ## Save_Recon_Malimg\n",
        "            if i == 0:\n",
        "                n = min(X.size(0), 8)\n",
        "                comparison = torch.cat([X[:n],\n",
        "                                    X_reconst.view(16, 3, 224, 224)[:n]])  # Recon 4 Images\n",
        "                save_image(comparison.cpu(),\n",
        "                        './results_ResNet-VAE_Exp01/recon_sampling/reconstruction_' + str(epoch + 1) + '.png', nrow=n)\n",
        "                # save_image(comparison.cpu(),\n",
        "                #         os.path.join(save_model_path, '/recon_sampling/reconstruction{}.png'.format(epoch + 1)))\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    all_y = np.stack(all_y, axis=0)\n",
        "    all_z = np.stack(all_z, axis=0)\n",
        "    all_mu = np.stack(all_mu, axis=0)\n",
        "    all_logvar = np.stack(all_logvar, axis=0)\n",
        "\n",
        "    # show information\n",
        "    print('\\nTest set ({:d} samples): Average loss: {:.4f}\\n'.format(len(test_loader.dataset), test_loss))\n",
        "    return X.data.cpu().numpy(), all_y, all_z, all_mu, all_logvar, test_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWrOAYCHw-Dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(resnet_vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGZkel39Vo0L",
        "colab_type": "text"
      },
      "source": [
        "### Argparse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o89ggU79exTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "972732d1-92fa-40f7-a9c0-e9ed3ea90d5f"
      },
      "source": [
        "import argparse\n",
        "\n",
        "seed = 123\n",
        "np.random.seed(seed)  # Reseed a legacy MT19937 BitGenerator\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "args.CNN_fc_hidden1 = 1024\n",
        "args.CNN_fc_hidden2 = 1024\n",
        "args.CNN_embed_dim = 256  # latent dim extracted by 2D CNN\n",
        "args.res_size = 224       # ResNet Image size\n",
        "args.dropout_p = 0.2           # dropout probability\n",
        "\n",
        "# training parameters\n",
        "args.epochs = 20\n",
        "args.batch_size = 50\n",
        "args.learning_rate = 1e-3\n",
        "args.log_interval = 10  # interval for displaying training info\n",
        "\n",
        "print(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(CNN_embed_dim=256, CNN_fc_hidden1=1024, CNN_fc_hidden2=1024, batch_size=50, dropout_p=0.2, epochs=20, learning_rate=0.001, log_interval=10, res_size=224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFuyo3QTrfJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "10bbf9e7-0578-4e2c-f8a8-5276ef6e8c57"
      },
      "source": [
        "print(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(CNN_embed_dim=256, CNN_fc_hidden1=1024, CNN_fc_hidden2=1024, batch_size=50, dropout_p=0.2, epochs=20, learning_rate=0.001, log_interval=10, res_size=224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMvQL9RPtkPM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "4430fbad-b7b1-4aa6-f7c6-a4b4319b7742"
      },
      "source": [
        "args.epochs = 100\n",
        "print(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(CNN_embed_dim=256, CNN_fc_hidden1=1024, CNN_fc_hidden2=1024, batch_size=50, dropout_p=0.2, epochs=100, learning_rate=0.001, log_interval=10, res_size=224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2-9ZnrDVuoI",
        "colab_type": "text"
      },
      "source": [
        "## 05. Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdrpoBUQVwz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dad3eb7f-992f-4b19-988e-162f9ab52081"
      },
      "source": [
        "# start training\n",
        "\n",
        "list_epoch=[]\n",
        "epoch_train_losses = []\n",
        "epoch_test_losses = []\n",
        "list_acc = []\n",
        "list_acc_epoch =[]\n",
        "check_mkdir(save_model_path)\n",
        "\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "\n",
        "    # train, test model\n",
        "    X_train, y_train, z_train, mu_train, logvar_train, epoch_train_loss = train(args.log_interval, resnet_vae, device, train_loader, optimizer, epoch)\n",
        "    X_test, y_test, z_test, mu_test, logvar_test, epoch_test_loss = validation(resnet_vae, device, optimizer, valid_loader)\n",
        "\n",
        "    # save results\n",
        "    list_epoch.append(epoch)\n",
        "    epoch_train_losses.append(epoch_train_loss)\n",
        "    epoch_test_losses.append(epoch_test_loss)\n",
        "\n",
        "    \n",
        "    # save all train test results\n",
        "    # A = np.array(all_train_losses)\n",
        "    B = np.array(epoch_train_losses)\n",
        "    C = np.array(epoch_test_losses)\n",
        "    \n",
        "    # np.save(os.path.join(save_model_path, 'ResNet_VAE_training_loss_all.npy'), A)\n",
        "    np.save(os.path.join(save_model_path, 'ResNet_VAE_training_loss.npy'), B)\n",
        "    np.save(os.path.join(save_model_path, 'ResNet_VAE_test_loss.npy'), C)\n",
        "\n",
        "    np.save(os.path.join(save_model_path, 'X_Malimg_train_epoch{}.npy'.format(epoch + 1)), X_train) #save last batch\n",
        "    np.save(os.path.join(save_model_path, 'y_Malimg_train_epoch{}.npy'.format(epoch + 1)), y_train)\n",
        "    np.save(os.path.join(save_model_path, 'z_Malimg_train_epoch{}.npy'.format(epoch + 1)), z_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Train Epoch: 1 [320/7471 (4%)]\tLoss: 1663872.000000\n",
            "Train Epoch: 1 [480/7471 (6%)]\tLoss: 1673245.250000\n",
            "Train Epoch: 1 [640/7471 (9%)]\tLoss: 1666329.125000\n",
            "Train Epoch: 1 [800/7471 (11%)]\tLoss: 1655970.125000\n",
            "Train Epoch: 1 [960/7471 (13%)]\tLoss: 1652011.250000\n",
            "Train Epoch: 1 [1120/7471 (15%)]\tLoss: 1653168.625000\n",
            "Train Epoch: 1 [1280/7471 (17%)]\tLoss: 1664516.125000\n",
            "Train Epoch: 1 [1440/7471 (19%)]\tLoss: 1650637.125000\n",
            "Train Epoch: 1 [1600/7471 (21%)]\tLoss: 1638597.250000\n",
            "Train Epoch: 1 [1760/7471 (24%)]\tLoss: 1656080.250000\n",
            "Train Epoch: 1 [1920/7471 (26%)]\tLoss: 1664741.750000\n",
            "Train Epoch: 1 [2080/7471 (28%)]\tLoss: 1627625.750000\n",
            "Train Epoch: 1 [2240/7471 (30%)]\tLoss: 1633431.375000\n",
            "Train Epoch: 1 [2400/7471 (32%)]\tLoss: 1643345.000000\n",
            "Train Epoch: 1 [2560/7471 (34%)]\tLoss: 1627399.750000\n",
            "Train Epoch: 1 [2720/7471 (36%)]\tLoss: 1617293.500000\n",
            "Train Epoch: 1 [2880/7471 (39%)]\tLoss: 1625719.125000\n",
            "Train Epoch: 1 [3040/7471 (41%)]\tLoss: 1655760.625000\n",
            "Train Epoch: 1 [3200/7471 (43%)]\tLoss: 1654996.000000\n",
            "Train Epoch: 1 [3360/7471 (45%)]\tLoss: 1645986.000000\n",
            "Train Epoch: 1 [3520/7471 (47%)]\tLoss: 1671683.250000\n",
            "Train Epoch: 1 [3680/7471 (49%)]\tLoss: 1647754.625000\n",
            "Train Epoch: 1 [3840/7471 (51%)]\tLoss: 1618821.125000\n",
            "Train Epoch: 1 [4000/7471 (54%)]\tLoss: 1630413.125000\n",
            "Train Epoch: 1 [4160/7471 (56%)]\tLoss: 1627947.875000\n",
            "Train Epoch: 1 [4320/7471 (58%)]\tLoss: 1643958.500000\n",
            "Train Epoch: 1 [4480/7471 (60%)]\tLoss: 1651149.375000\n",
            "Train Epoch: 1 [4640/7471 (62%)]\tLoss: 1650797.375000\n",
            "Train Epoch: 1 [4800/7471 (64%)]\tLoss: 1614956.750000\n",
            "Train Epoch: 1 [4960/7471 (66%)]\tLoss: 1621760.125000\n",
            "Train Epoch: 1 [5120/7471 (69%)]\tLoss: 1614780.500000\n",
            "Train Epoch: 1 [5280/7471 (71%)]\tLoss: 1595116.250000\n",
            "Train Epoch: 1 [5440/7471 (73%)]\tLoss: 1627965.375000\n",
            "Train Epoch: 1 [5600/7471 (75%)]\tLoss: 1627943.625000\n",
            "Train Epoch: 1 [5760/7471 (77%)]\tLoss: 1606273.250000\n",
            "Train Epoch: 1 [5920/7471 (79%)]\tLoss: 1610396.875000\n",
            "Train Epoch: 1 [6080/7471 (81%)]\tLoss: 1643688.000000\n",
            "Train Epoch: 1 [6240/7471 (84%)]\tLoss: 1628070.000000\n",
            "Train Epoch: 1 [6400/7471 (86%)]\tLoss: 1648356.250000\n",
            "Train Epoch: 1 [6560/7471 (88%)]\tLoss: 1565806.625000\n",
            "Train Epoch: 1 [6720/7471 (90%)]\tLoss: 1600846.125000\n",
            "Train Epoch: 1 [6880/7471 (92%)]\tLoss: 1642413.625000\n",
            "Train Epoch: 1 [7040/7471 (94%)]\tLoss: 1615052.500000\n",
            "Train Epoch: 1 [7200/7471 (96%)]\tLoss: 1624421.500000\n",
            "Train Epoch: 1 [7360/7471 (99%)]\tLoss: 1595021.000000\n",
            "Epoch 1 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 2 [160/7471 (2%)]\tLoss: 1638124.750000\n",
            "Train Epoch: 2 [320/7471 (4%)]\tLoss: 1630460.625000\n",
            "Train Epoch: 2 [480/7471 (6%)]\tLoss: 1615888.125000\n",
            "Train Epoch: 2 [640/7471 (9%)]\tLoss: 1600745.625000\n",
            "Train Epoch: 2 [800/7471 (11%)]\tLoss: 1608715.375000\n",
            "Train Epoch: 2 [960/7471 (13%)]\tLoss: 1652586.500000\n",
            "Train Epoch: 2 [1120/7471 (15%)]\tLoss: 1606889.500000\n",
            "Train Epoch: 2 [1280/7471 (17%)]\tLoss: 1625643.000000\n",
            "Train Epoch: 2 [1440/7471 (19%)]\tLoss: 1599501.875000\n",
            "Train Epoch: 2 [1600/7471 (21%)]\tLoss: 1660855.125000\n",
            "Train Epoch: 2 [1760/7471 (24%)]\tLoss: 1617220.000000\n",
            "Train Epoch: 2 [1920/7471 (26%)]\tLoss: 1623305.125000\n",
            "Train Epoch: 2 [2080/7471 (28%)]\tLoss: 1650226.750000\n",
            "Train Epoch: 2 [2240/7471 (30%)]\tLoss: 1642594.375000\n",
            "Train Epoch: 2 [2400/7471 (32%)]\tLoss: 1618370.375000\n",
            "Train Epoch: 2 [2560/7471 (34%)]\tLoss: 1617079.500000\n",
            "Train Epoch: 2 [2720/7471 (36%)]\tLoss: 1633970.875000\n",
            "Train Epoch: 2 [2880/7471 (39%)]\tLoss: 1638984.375000\n",
            "Train Epoch: 2 [3040/7471 (41%)]\tLoss: 1640539.375000\n",
            "Train Epoch: 2 [3200/7471 (43%)]\tLoss: 1595387.500000\n",
            "Train Epoch: 2 [3360/7471 (45%)]\tLoss: 1612598.250000\n",
            "Train Epoch: 2 [3520/7471 (47%)]\tLoss: 1587785.125000\n",
            "Train Epoch: 2 [3680/7471 (49%)]\tLoss: 1618284.250000\n",
            "Train Epoch: 2 [3840/7471 (51%)]\tLoss: 1670260.125000\n",
            "Train Epoch: 2 [4000/7471 (54%)]\tLoss: 1639710.750000\n",
            "Train Epoch: 2 [4160/7471 (56%)]\tLoss: 1585709.875000\n",
            "Train Epoch: 2 [4320/7471 (58%)]\tLoss: 1619067.625000\n",
            "Train Epoch: 2 [4480/7471 (60%)]\tLoss: 1600471.875000\n",
            "Train Epoch: 2 [4640/7471 (62%)]\tLoss: 1581970.000000\n",
            "Train Epoch: 2 [4800/7471 (64%)]\tLoss: 1568222.500000\n",
            "Train Epoch: 2 [4960/7471 (66%)]\tLoss: 1625650.250000\n",
            "Train Epoch: 2 [5120/7471 (69%)]\tLoss: 1637322.250000\n",
            "Train Epoch: 2 [5280/7471 (71%)]\tLoss: 1579114.375000\n",
            "Train Epoch: 2 [5440/7471 (73%)]\tLoss: 1614714.875000\n",
            "Train Epoch: 2 [5600/7471 (75%)]\tLoss: 1600310.375000\n",
            "Train Epoch: 2 [5760/7471 (77%)]\tLoss: 1613841.375000\n",
            "Train Epoch: 2 [5920/7471 (79%)]\tLoss: 1589390.250000\n",
            "Train Epoch: 2 [6080/7471 (81%)]\tLoss: 1622623.750000\n",
            "Train Epoch: 2 [6240/7471 (84%)]\tLoss: 1533361.375000\n",
            "Train Epoch: 2 [6400/7471 (86%)]\tLoss: 1606695.125000\n",
            "Train Epoch: 2 [6560/7471 (88%)]\tLoss: 1623617.625000\n",
            "Train Epoch: 2 [6720/7471 (90%)]\tLoss: 1636675.000000\n",
            "Train Epoch: 2 [6880/7471 (92%)]\tLoss: 1632004.625000\n",
            "Train Epoch: 2 [7040/7471 (94%)]\tLoss: 1614112.875000\n",
            "Train Epoch: 2 [7200/7471 (96%)]\tLoss: 1614774.625000\n",
            "Train Epoch: 2 [7360/7471 (99%)]\tLoss: 1649852.000000\n",
            "Epoch 2 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 3 [160/7471 (2%)]\tLoss: 1646686.750000\n",
            "Train Epoch: 3 [320/7471 (4%)]\tLoss: 1577615.375000\n",
            "Train Epoch: 3 [480/7471 (6%)]\tLoss: 1621973.125000\n",
            "Train Epoch: 3 [640/7471 (9%)]\tLoss: 1622402.375000\n",
            "Train Epoch: 3 [800/7471 (11%)]\tLoss: 1615995.000000\n",
            "Train Epoch: 3 [960/7471 (13%)]\tLoss: 1646218.750000\n",
            "Train Epoch: 3 [1120/7471 (15%)]\tLoss: 1610477.250000\n",
            "Train Epoch: 3 [1280/7471 (17%)]\tLoss: 1621476.875000\n",
            "Train Epoch: 3 [1440/7471 (19%)]\tLoss: 1632998.500000\n",
            "Train Epoch: 3 [1600/7471 (21%)]\tLoss: 1621830.625000\n",
            "Train Epoch: 3 [1760/7471 (24%)]\tLoss: 1574590.375000\n",
            "Train Epoch: 3 [1920/7471 (26%)]\tLoss: 1622255.000000\n",
            "Train Epoch: 3 [2080/7471 (28%)]\tLoss: 1627478.750000\n",
            "Train Epoch: 3 [2240/7471 (30%)]\tLoss: 1588174.750000\n",
            "Train Epoch: 3 [2400/7471 (32%)]\tLoss: 1647936.500000\n",
            "Train Epoch: 3 [2560/7471 (34%)]\tLoss: 1600886.875000\n",
            "Train Epoch: 3 [2720/7471 (36%)]\tLoss: 1608709.000000\n",
            "Train Epoch: 3 [2880/7471 (39%)]\tLoss: 1609930.250000\n",
            "Train Epoch: 3 [3040/7471 (41%)]\tLoss: 1589301.250000\n",
            "Train Epoch: 3 [3200/7471 (43%)]\tLoss: 1601460.375000\n",
            "Train Epoch: 3 [3360/7471 (45%)]\tLoss: 1615997.375000\n",
            "Train Epoch: 3 [3520/7471 (47%)]\tLoss: 1613724.750000\n",
            "Train Epoch: 3 [3680/7471 (49%)]\tLoss: 1559322.000000\n",
            "Train Epoch: 3 [3840/7471 (51%)]\tLoss: 1565618.250000\n",
            "Train Epoch: 3 [4000/7471 (54%)]\tLoss: 1623344.750000\n",
            "Train Epoch: 3 [4160/7471 (56%)]\tLoss: 1550898.500000\n",
            "Train Epoch: 3 [4320/7471 (58%)]\tLoss: 1611786.000000\n",
            "Train Epoch: 3 [4480/7471 (60%)]\tLoss: 1613765.250000\n",
            "Train Epoch: 3 [4640/7471 (62%)]\tLoss: 1593745.875000\n",
            "Train Epoch: 3 [4800/7471 (64%)]\tLoss: 1566163.625000\n",
            "Train Epoch: 3 [4960/7471 (66%)]\tLoss: 1576930.750000\n",
            "Train Epoch: 3 [5120/7471 (69%)]\tLoss: 1643413.000000\n",
            "Train Epoch: 3 [5280/7471 (71%)]\tLoss: 1674588.750000\n",
            "Train Epoch: 3 [5440/7471 (73%)]\tLoss: 1603321.500000\n",
            "Train Epoch: 3 [5600/7471 (75%)]\tLoss: 1615882.875000\n",
            "Train Epoch: 3 [5760/7471 (77%)]\tLoss: 1574501.250000\n",
            "Train Epoch: 3 [5920/7471 (79%)]\tLoss: 1629088.375000\n",
            "Train Epoch: 3 [6080/7471 (81%)]\tLoss: 1629943.125000\n",
            "Train Epoch: 3 [6240/7471 (84%)]\tLoss: 1585108.125000\n",
            "Train Epoch: 3 [6400/7471 (86%)]\tLoss: 1592669.875000\n",
            "Train Epoch: 3 [6560/7471 (88%)]\tLoss: 1621371.875000\n",
            "Train Epoch: 3 [6720/7471 (90%)]\tLoss: 1655093.875000\n",
            "Train Epoch: 3 [6880/7471 (92%)]\tLoss: 1583736.375000\n",
            "Train Epoch: 3 [7040/7471 (94%)]\tLoss: 1615828.625000\n",
            "Train Epoch: 3 [7200/7471 (96%)]\tLoss: 1631256.625000\n",
            "Train Epoch: 3 [7360/7471 (99%)]\tLoss: 1578683.250000\n",
            "Epoch 3 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 4 [160/7471 (2%)]\tLoss: 1577907.625000\n",
            "Train Epoch: 4 [320/7471 (4%)]\tLoss: 1617395.625000\n",
            "Train Epoch: 4 [480/7471 (6%)]\tLoss: 1601049.125000\n",
            "Train Epoch: 4 [640/7471 (9%)]\tLoss: 1608837.000000\n",
            "Train Epoch: 4 [800/7471 (11%)]\tLoss: 1601950.125000\n",
            "Train Epoch: 4 [960/7471 (13%)]\tLoss: 1585947.375000\n",
            "Train Epoch: 4 [1120/7471 (15%)]\tLoss: 1618168.250000\n",
            "Train Epoch: 4 [1280/7471 (17%)]\tLoss: 1652667.625000\n",
            "Train Epoch: 4 [1440/7471 (19%)]\tLoss: 1620717.875000\n",
            "Train Epoch: 4 [1600/7471 (21%)]\tLoss: 1611458.250000\n",
            "Train Epoch: 4 [1760/7471 (24%)]\tLoss: 1633973.500000\n",
            "Train Epoch: 4 [1920/7471 (26%)]\tLoss: 1623466.375000\n",
            "Train Epoch: 4 [2080/7471 (28%)]\tLoss: 1628139.625000\n",
            "Train Epoch: 4 [2240/7471 (30%)]\tLoss: 1622696.375000\n",
            "Train Epoch: 4 [2400/7471 (32%)]\tLoss: 1609970.375000\n",
            "Train Epoch: 4 [2560/7471 (34%)]\tLoss: 1602367.875000\n",
            "Train Epoch: 4 [2720/7471 (36%)]\tLoss: 1623455.500000\n",
            "Train Epoch: 4 [2880/7471 (39%)]\tLoss: 1652228.000000\n",
            "Train Epoch: 4 [3040/7471 (41%)]\tLoss: 1612147.125000\n",
            "Train Epoch: 4 [3200/7471 (43%)]\tLoss: 1620009.375000\n",
            "Train Epoch: 4 [3360/7471 (45%)]\tLoss: 1582172.250000\n",
            "Train Epoch: 4 [3520/7471 (47%)]\tLoss: 1576030.125000\n",
            "Train Epoch: 4 [3680/7471 (49%)]\tLoss: 1594783.500000\n",
            "Train Epoch: 4 [3840/7471 (51%)]\tLoss: 1520140.500000\n",
            "Train Epoch: 4 [4000/7471 (54%)]\tLoss: 1591723.375000\n",
            "Train Epoch: 4 [4160/7471 (56%)]\tLoss: 1590871.250000\n",
            "Train Epoch: 4 [4320/7471 (58%)]\tLoss: 1648433.875000\n",
            "Train Epoch: 4 [4480/7471 (60%)]\tLoss: 1643392.000000\n",
            "Train Epoch: 4 [4640/7471 (62%)]\tLoss: 1545399.250000\n",
            "Train Epoch: 4 [4800/7471 (64%)]\tLoss: 1607695.000000\n",
            "Train Epoch: 4 [4960/7471 (66%)]\tLoss: 1596125.625000\n",
            "Train Epoch: 4 [5120/7471 (69%)]\tLoss: 1630229.500000\n",
            "Train Epoch: 4 [5280/7471 (71%)]\tLoss: 1618706.625000\n",
            "Train Epoch: 4 [5440/7471 (73%)]\tLoss: 1677364.125000\n",
            "Train Epoch: 4 [5600/7471 (75%)]\tLoss: 1647619.000000\n",
            "Train Epoch: 4 [5760/7471 (77%)]\tLoss: 1609320.500000\n",
            "Train Epoch: 4 [5920/7471 (79%)]\tLoss: 1620627.750000\n",
            "Train Epoch: 4 [6080/7471 (81%)]\tLoss: 1642723.625000\n",
            "Train Epoch: 4 [6240/7471 (84%)]\tLoss: 1590775.500000\n",
            "Train Epoch: 4 [6400/7471 (86%)]\tLoss: 1604084.750000\n",
            "Train Epoch: 4 [6560/7471 (88%)]\tLoss: 1620020.375000\n",
            "Train Epoch: 4 [6720/7471 (90%)]\tLoss: 1617686.000000\n",
            "Train Epoch: 4 [6880/7471 (92%)]\tLoss: 1579645.125000\n",
            "Train Epoch: 4 [7040/7471 (94%)]\tLoss: 1627866.875000\n",
            "Train Epoch: 4 [7200/7471 (96%)]\tLoss: 1575168.625000\n",
            "Train Epoch: 4 [7360/7471 (99%)]\tLoss: 1554660.875000\n",
            "Epoch 4 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 5 [160/7471 (2%)]\tLoss: 1636329.875000\n",
            "Train Epoch: 5 [320/7471 (4%)]\tLoss: 1620082.625000\n",
            "Train Epoch: 5 [480/7471 (6%)]\tLoss: 1629063.375000\n",
            "Train Epoch: 5 [640/7471 (9%)]\tLoss: 1657729.000000\n",
            "Train Epoch: 5 [800/7471 (11%)]\tLoss: 1624493.500000\n",
            "Train Epoch: 5 [960/7471 (13%)]\tLoss: 1631253.125000\n",
            "Train Epoch: 5 [1120/7471 (15%)]\tLoss: 1599759.875000\n",
            "Train Epoch: 5 [1280/7471 (17%)]\tLoss: 1583858.625000\n",
            "Train Epoch: 5 [1440/7471 (19%)]\tLoss: 1564886.250000\n",
            "Train Epoch: 5 [1600/7471 (21%)]\tLoss: 1639682.125000\n",
            "Train Epoch: 5 [1760/7471 (24%)]\tLoss: 1600521.875000\n",
            "Train Epoch: 5 [1920/7471 (26%)]\tLoss: 1596378.750000\n",
            "Train Epoch: 5 [2080/7471 (28%)]\tLoss: 1595632.125000\n",
            "Train Epoch: 5 [2240/7471 (30%)]\tLoss: 1627861.375000\n",
            "Train Epoch: 5 [2400/7471 (32%)]\tLoss: 1610138.500000\n",
            "Train Epoch: 5 [2560/7471 (34%)]\tLoss: 1633927.250000\n",
            "Train Epoch: 5 [2720/7471 (36%)]\tLoss: 1587774.000000\n",
            "Train Epoch: 5 [2880/7471 (39%)]\tLoss: 1564040.875000\n",
            "Train Epoch: 5 [3040/7471 (41%)]\tLoss: 1603307.750000\n",
            "Train Epoch: 5 [3200/7471 (43%)]\tLoss: 1569016.750000\n",
            "Train Epoch: 5 [3360/7471 (45%)]\tLoss: 1614844.875000\n",
            "Train Epoch: 5 [3520/7471 (47%)]\tLoss: 1607187.250000\n",
            "Train Epoch: 5 [3680/7471 (49%)]\tLoss: 1566551.375000\n",
            "Train Epoch: 5 [3840/7471 (51%)]\tLoss: 1618628.125000\n",
            "Train Epoch: 5 [4000/7471 (54%)]\tLoss: 1602401.125000\n",
            "Train Epoch: 5 [4160/7471 (56%)]\tLoss: 1624442.625000\n",
            "Train Epoch: 5 [4320/7471 (58%)]\tLoss: 1612315.875000\n",
            "Train Epoch: 5 [4480/7471 (60%)]\tLoss: 1563085.500000\n",
            "Train Epoch: 5 [4640/7471 (62%)]\tLoss: 1599928.375000\n",
            "Train Epoch: 5 [4800/7471 (64%)]\tLoss: 1636901.875000\n",
            "Train Epoch: 5 [4960/7471 (66%)]\tLoss: 1653554.375000\n",
            "Train Epoch: 5 [5120/7471 (69%)]\tLoss: 1592159.375000\n",
            "Train Epoch: 5 [5280/7471 (71%)]\tLoss: 1601088.375000\n",
            "Train Epoch: 5 [5440/7471 (73%)]\tLoss: 1664219.375000\n",
            "Train Epoch: 5 [5600/7471 (75%)]\tLoss: 1658508.625000\n",
            "Train Epoch: 5 [5760/7471 (77%)]\tLoss: 1620997.000000\n",
            "Train Epoch: 5 [5920/7471 (79%)]\tLoss: 1627283.750000\n",
            "Train Epoch: 5 [6080/7471 (81%)]\tLoss: 1582777.500000\n",
            "Train Epoch: 5 [6240/7471 (84%)]\tLoss: 1603502.625000\n",
            "Train Epoch: 5 [6400/7471 (86%)]\tLoss: 1601128.625000\n",
            "Train Epoch: 5 [6560/7471 (88%)]\tLoss: 1590542.500000\n",
            "Train Epoch: 5 [6720/7471 (90%)]\tLoss: 1649821.625000\n",
            "Train Epoch: 5 [6880/7471 (92%)]\tLoss: 1533051.500000\n",
            "Train Epoch: 5 [7040/7471 (94%)]\tLoss: 1621246.875000\n",
            "Train Epoch: 5 [7200/7471 (96%)]\tLoss: 1565614.500000\n",
            "Train Epoch: 5 [7360/7471 (99%)]\tLoss: 1586562.375000\n",
            "Epoch 5 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 2043397937569089.7500\n",
            "\n",
            "Train Epoch: 6 [160/7471 (2%)]\tLoss: 1646461.750000\n",
            "Train Epoch: 6 [320/7471 (4%)]\tLoss: 1590788.625000\n",
            "Train Epoch: 6 [480/7471 (6%)]\tLoss: 1610837.750000\n",
            "Train Epoch: 6 [640/7471 (9%)]\tLoss: 1647338.500000\n",
            "Train Epoch: 6 [800/7471 (11%)]\tLoss: 1610491.500000\n",
            "Train Epoch: 6 [960/7471 (13%)]\tLoss: 1555603.375000\n",
            "Train Epoch: 6 [1120/7471 (15%)]\tLoss: 1588647.500000\n",
            "Train Epoch: 6 [1280/7471 (17%)]\tLoss: 1630242.000000\n",
            "Train Epoch: 6 [1440/7471 (19%)]\tLoss: 1598875.250000\n",
            "Train Epoch: 6 [1600/7471 (21%)]\tLoss: 1641983.000000\n",
            "Train Epoch: 6 [1760/7471 (24%)]\tLoss: 1622674.750000\n",
            "Train Epoch: 6 [1920/7471 (26%)]\tLoss: 1584188.375000\n",
            "Train Epoch: 6 [2080/7471 (28%)]\tLoss: 1562322.500000\n",
            "Train Epoch: 6 [2240/7471 (30%)]\tLoss: 1632849.250000\n",
            "Train Epoch: 6 [2400/7471 (32%)]\tLoss: 1611517.500000\n",
            "Train Epoch: 6 [2560/7471 (34%)]\tLoss: 1632644.000000\n",
            "Train Epoch: 6 [2720/7471 (36%)]\tLoss: 1620246.250000\n",
            "Train Epoch: 6 [2880/7471 (39%)]\tLoss: 1567492.000000\n",
            "Train Epoch: 6 [3040/7471 (41%)]\tLoss: 1627843.375000\n",
            "Train Epoch: 6 [3200/7471 (43%)]\tLoss: 1569358.750000\n",
            "Train Epoch: 6 [3360/7471 (45%)]\tLoss: 1623988.125000\n",
            "Train Epoch: 6 [3520/7471 (47%)]\tLoss: 1543995.750000\n",
            "Train Epoch: 6 [3680/7471 (49%)]\tLoss: 1589607.625000\n",
            "Train Epoch: 6 [3840/7471 (51%)]\tLoss: 1608273.625000\n",
            "Train Epoch: 6 [4000/7471 (54%)]\tLoss: 1624285.250000\n",
            "Train Epoch: 6 [4160/7471 (56%)]\tLoss: 1596753.750000\n",
            "Train Epoch: 6 [4320/7471 (58%)]\tLoss: 1627423.375000\n",
            "Train Epoch: 6 [4480/7471 (60%)]\tLoss: 1585216.375000\n",
            "Train Epoch: 6 [4640/7471 (62%)]\tLoss: 1590045.250000\n",
            "Train Epoch: 6 [4800/7471 (64%)]\tLoss: 1632040.125000\n",
            "Train Epoch: 6 [4960/7471 (66%)]\tLoss: 1627175.500000\n",
            "Train Epoch: 6 [5120/7471 (69%)]\tLoss: 1581858.250000\n",
            "Train Epoch: 6 [5280/7471 (71%)]\tLoss: 1615847.625000\n",
            "Train Epoch: 6 [5440/7471 (73%)]\tLoss: 1664251.500000\n",
            "Train Epoch: 6 [5600/7471 (75%)]\tLoss: 1655505.375000\n",
            "Train Epoch: 6 [5760/7471 (77%)]\tLoss: 1579515.625000\n",
            "Train Epoch: 6 [5920/7471 (79%)]\tLoss: 1585560.875000\n",
            "Train Epoch: 6 [6080/7471 (81%)]\tLoss: 1573832.500000\n",
            "Train Epoch: 6 [6240/7471 (84%)]\tLoss: 1645162.750000\n",
            "Train Epoch: 6 [6400/7471 (86%)]\tLoss: 1597319.500000\n",
            "Train Epoch: 6 [6560/7471 (88%)]\tLoss: 1577627.500000\n",
            "Train Epoch: 6 [6720/7471 (90%)]\tLoss: 1579047.500000\n",
            "Train Epoch: 6 [6880/7471 (92%)]\tLoss: 1626357.750000\n",
            "Train Epoch: 6 [7040/7471 (94%)]\tLoss: 1644365.500000\n",
            "Train Epoch: 6 [7200/7471 (96%)]\tLoss: 1656482.375000\n",
            "Train Epoch: 6 [7360/7471 (99%)]\tLoss: 1639596.375000\n",
            "Epoch 6 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 7 [160/7471 (2%)]\tLoss: 1617760.125000\n",
            "Train Epoch: 7 [320/7471 (4%)]\tLoss: 1580578.625000\n",
            "Train Epoch: 7 [480/7471 (6%)]\tLoss: 1636048.125000\n",
            "Train Epoch: 7 [640/7471 (9%)]\tLoss: 1605419.250000\n",
            "Train Epoch: 7 [800/7471 (11%)]\tLoss: 1633288.000000\n",
            "Train Epoch: 7 [960/7471 (13%)]\tLoss: 1581317.000000\n",
            "Train Epoch: 7 [1120/7471 (15%)]\tLoss: 1637222.875000\n",
            "Train Epoch: 7 [1280/7471 (17%)]\tLoss: 1605729.500000\n",
            "Train Epoch: 7 [1440/7471 (19%)]\tLoss: 1617294.375000\n",
            "Train Epoch: 7 [1600/7471 (21%)]\tLoss: 1563806.250000\n",
            "Train Epoch: 7 [1760/7471 (24%)]\tLoss: 1598177.750000\n",
            "Train Epoch: 7 [1920/7471 (26%)]\tLoss: 1582031.750000\n",
            "Train Epoch: 7 [2080/7471 (28%)]\tLoss: 1604806.000000\n",
            "Train Epoch: 7 [2240/7471 (30%)]\tLoss: 1610064.375000\n",
            "Train Epoch: 7 [2400/7471 (32%)]\tLoss: 1578898.125000\n",
            "Train Epoch: 7 [2560/7471 (34%)]\tLoss: 1616955.875000\n",
            "Train Epoch: 7 [2720/7471 (36%)]\tLoss: 1524218.250000\n",
            "Train Epoch: 7 [2880/7471 (39%)]\tLoss: 1632576.750000\n",
            "Train Epoch: 7 [3040/7471 (41%)]\tLoss: 1637747.500000\n",
            "Train Epoch: 7 [3200/7471 (43%)]\tLoss: 1589013.875000\n",
            "Train Epoch: 7 [3360/7471 (45%)]\tLoss: 1596517.250000\n",
            "Train Epoch: 7 [3520/7471 (47%)]\tLoss: 1629036.125000\n",
            "Train Epoch: 7 [3680/7471 (49%)]\tLoss: 1621452.125000\n",
            "Train Epoch: 7 [3840/7471 (51%)]\tLoss: 1611401.125000\n",
            "Train Epoch: 7 [4000/7471 (54%)]\tLoss: 1612265.000000\n",
            "Train Epoch: 7 [4160/7471 (56%)]\tLoss: 1611193.625000\n",
            "Train Epoch: 7 [4320/7471 (58%)]\tLoss: 1663525.625000\n",
            "Train Epoch: 7 [4480/7471 (60%)]\tLoss: 1537043.500000\n",
            "Train Epoch: 7 [4640/7471 (62%)]\tLoss: 1615439.375000\n",
            "Train Epoch: 7 [4800/7471 (64%)]\tLoss: 1634931.875000\n",
            "Train Epoch: 7 [4960/7471 (66%)]\tLoss: 1634702.625000\n",
            "Train Epoch: 7 [5120/7471 (69%)]\tLoss: 1639354.000000\n",
            "Train Epoch: 7 [5280/7471 (71%)]\tLoss: 1613216.875000\n",
            "Train Epoch: 7 [5440/7471 (73%)]\tLoss: 1623273.375000\n",
            "Train Epoch: 7 [5600/7471 (75%)]\tLoss: 1568277.500000\n",
            "Train Epoch: 7 [5760/7471 (77%)]\tLoss: 1575440.125000\n",
            "Train Epoch: 7 [5920/7471 (79%)]\tLoss: 1632654.625000\n",
            "Train Epoch: 7 [6080/7471 (81%)]\tLoss: 1637149.250000\n",
            "Train Epoch: 7 [6240/7471 (84%)]\tLoss: 1645262.000000\n",
            "Train Epoch: 7 [6400/7471 (86%)]\tLoss: 1601657.125000\n",
            "Train Epoch: 7 [6560/7471 (88%)]\tLoss: 1619254.750000\n",
            "Train Epoch: 7 [6720/7471 (90%)]\tLoss: 1572113.250000\n",
            "Train Epoch: 7 [6880/7471 (92%)]\tLoss: 1614074.000000\n",
            "Train Epoch: 7 [7040/7471 (94%)]\tLoss: 1620417.750000\n",
            "Train Epoch: 7 [7200/7471 (96%)]\tLoss: 1560626.750000\n",
            "Train Epoch: 7 [7360/7471 (99%)]\tLoss: 1593482.000000\n",
            "Epoch 7 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 101770.8230\n",
            "\n",
            "Train Epoch: 8 [160/7471 (2%)]\tLoss: 1627113.625000\n",
            "Train Epoch: 8 [320/7471 (4%)]\tLoss: 1606368.875000\n",
            "Train Epoch: 8 [480/7471 (6%)]\tLoss: 1578488.500000\n",
            "Train Epoch: 8 [640/7471 (9%)]\tLoss: 1596773.875000\n",
            "Train Epoch: 8 [800/7471 (11%)]\tLoss: 1564150.750000\n",
            "Train Epoch: 8 [960/7471 (13%)]\tLoss: 1607177.125000\n",
            "Train Epoch: 8 [1120/7471 (15%)]\tLoss: 1634424.750000\n",
            "Train Epoch: 8 [1280/7471 (17%)]\tLoss: 1593597.250000\n",
            "Train Epoch: 8 [1440/7471 (19%)]\tLoss: 1564774.000000\n",
            "Train Epoch: 8 [1600/7471 (21%)]\tLoss: 1609698.750000\n",
            "Train Epoch: 8 [1760/7471 (24%)]\tLoss: 1606271.875000\n",
            "Train Epoch: 8 [1920/7471 (26%)]\tLoss: 1627314.375000\n",
            "Train Epoch: 8 [2080/7471 (28%)]\tLoss: 1591004.375000\n",
            "Train Epoch: 8 [2240/7471 (30%)]\tLoss: 1668009.250000\n",
            "Train Epoch: 8 [2400/7471 (32%)]\tLoss: 1597317.875000\n",
            "Train Epoch: 8 [2560/7471 (34%)]\tLoss: 1603816.750000\n",
            "Train Epoch: 8 [2720/7471 (36%)]\tLoss: 1616842.125000\n",
            "Train Epoch: 8 [2880/7471 (39%)]\tLoss: 1597892.875000\n",
            "Train Epoch: 8 [3040/7471 (41%)]\tLoss: 1553766.125000\n",
            "Train Epoch: 8 [3200/7471 (43%)]\tLoss: 1601841.125000\n",
            "Train Epoch: 8 [3360/7471 (45%)]\tLoss: 1636914.000000\n",
            "Train Epoch: 8 [3520/7471 (47%)]\tLoss: 1633996.000000\n",
            "Train Epoch: 8 [3680/7471 (49%)]\tLoss: 1579913.875000\n",
            "Train Epoch: 8 [3840/7471 (51%)]\tLoss: 1625016.625000\n",
            "Train Epoch: 8 [4000/7471 (54%)]\tLoss: 1637213.250000\n",
            "Train Epoch: 8 [4160/7471 (56%)]\tLoss: 1589665.125000\n",
            "Train Epoch: 8 [4320/7471 (58%)]\tLoss: 1598753.250000\n",
            "Train Epoch: 8 [4480/7471 (60%)]\tLoss: 1563494.625000\n",
            "Train Epoch: 8 [4640/7471 (62%)]\tLoss: 1649750.250000\n",
            "Train Epoch: 8 [4800/7471 (64%)]\tLoss: 1597230.875000\n",
            "Train Epoch: 8 [4960/7471 (66%)]\tLoss: 1578616.875000\n",
            "Train Epoch: 8 [5120/7471 (69%)]\tLoss: 1552191.250000\n",
            "Train Epoch: 8 [5280/7471 (71%)]\tLoss: 1627956.250000\n",
            "Train Epoch: 8 [5440/7471 (73%)]\tLoss: 1595706.250000\n",
            "Train Epoch: 8 [5600/7471 (75%)]\tLoss: 1608619.875000\n",
            "Train Epoch: 8 [5760/7471 (77%)]\tLoss: 1627565.250000\n",
            "Train Epoch: 8 [5920/7471 (79%)]\tLoss: 1644788.875000\n",
            "Train Epoch: 8 [6080/7471 (81%)]\tLoss: 1646253.000000\n",
            "Train Epoch: 8 [6240/7471 (84%)]\tLoss: 1636913.375000\n",
            "Train Epoch: 8 [6400/7471 (86%)]\tLoss: 1604534.000000\n",
            "Train Epoch: 8 [6560/7471 (88%)]\tLoss: 1535052.250000\n",
            "Train Epoch: 8 [6720/7471 (90%)]\tLoss: 1583749.500000\n",
            "Train Epoch: 8 [6880/7471 (92%)]\tLoss: 1496856.125000\n",
            "Train Epoch: 8 [7040/7471 (94%)]\tLoss: 1599821.125000\n",
            "Train Epoch: 8 [7200/7471 (96%)]\tLoss: 1612866.750000\n",
            "Train Epoch: 8 [7360/7471 (99%)]\tLoss: 1643911.000000\n",
            "Epoch 8 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 100493.5306\n",
            "\n",
            "Train Epoch: 9 [160/7471 (2%)]\tLoss: 1591276.750000\n",
            "Train Epoch: 9 [320/7471 (4%)]\tLoss: 1574161.125000\n",
            "Train Epoch: 9 [480/7471 (6%)]\tLoss: 1587974.250000\n",
            "Train Epoch: 9 [640/7471 (9%)]\tLoss: 1571128.000000\n",
            "Train Epoch: 9 [800/7471 (11%)]\tLoss: 1579048.000000\n",
            "Train Epoch: 9 [960/7471 (13%)]\tLoss: 1597728.875000\n",
            "Train Epoch: 9 [1120/7471 (15%)]\tLoss: 1583069.250000\n",
            "Train Epoch: 9 [1280/7471 (17%)]\tLoss: 1572368.875000\n",
            "Train Epoch: 9 [1440/7471 (19%)]\tLoss: 1638794.500000\n",
            "Train Epoch: 9 [1600/7471 (21%)]\tLoss: 1625061.125000\n",
            "Train Epoch: 9 [1760/7471 (24%)]\tLoss: 1611392.500000\n",
            "Train Epoch: 9 [1920/7471 (26%)]\tLoss: 1633471.625000\n",
            "Train Epoch: 9 [2080/7471 (28%)]\tLoss: 1583158.875000\n",
            "Train Epoch: 9 [2240/7471 (30%)]\tLoss: 1628403.125000\n",
            "Train Epoch: 9 [2400/7471 (32%)]\tLoss: 1611244.250000\n",
            "Train Epoch: 9 [2560/7471 (34%)]\tLoss: 1616632.375000\n",
            "Train Epoch: 9 [2720/7471 (36%)]\tLoss: 1583725.250000\n",
            "Train Epoch: 9 [2880/7471 (39%)]\tLoss: 1590799.625000\n",
            "Train Epoch: 9 [3040/7471 (41%)]\tLoss: 1616859.125000\n",
            "Train Epoch: 9 [3200/7471 (43%)]\tLoss: 1623124.750000\n",
            "Train Epoch: 9 [3360/7471 (45%)]\tLoss: 1525932.375000\n",
            "Train Epoch: 9 [3520/7471 (47%)]\tLoss: 1590273.875000\n",
            "Train Epoch: 9 [3680/7471 (49%)]\tLoss: 1591966.000000\n",
            "Train Epoch: 9 [3840/7471 (51%)]\tLoss: 1584940.125000\n",
            "Train Epoch: 9 [4000/7471 (54%)]\tLoss: 1569780.750000\n",
            "Train Epoch: 9 [4160/7471 (56%)]\tLoss: 1547131.625000\n",
            "Train Epoch: 9 [4320/7471 (58%)]\tLoss: 1588752.250000\n",
            "Train Epoch: 9 [4480/7471 (60%)]\tLoss: 1560256.000000\n",
            "Train Epoch: 9 [4640/7471 (62%)]\tLoss: 1535226.625000\n",
            "Train Epoch: 9 [4800/7471 (64%)]\tLoss: 1617287.625000\n",
            "Train Epoch: 9 [4960/7471 (66%)]\tLoss: 1579919.750000\n",
            "Train Epoch: 9 [5120/7471 (69%)]\tLoss: 1598268.375000\n",
            "Train Epoch: 9 [5280/7471 (71%)]\tLoss: 1576143.875000\n",
            "Train Epoch: 9 [5440/7471 (73%)]\tLoss: 1568704.875000\n",
            "Train Epoch: 9 [5600/7471 (75%)]\tLoss: 1490757.250000\n",
            "Train Epoch: 9 [5760/7471 (77%)]\tLoss: 1635608.125000\n",
            "Train Epoch: 9 [5920/7471 (79%)]\tLoss: 1547085.875000\n",
            "Train Epoch: 9 [6080/7471 (81%)]\tLoss: 1547617.125000\n",
            "Train Epoch: 9 [6240/7471 (84%)]\tLoss: 1621658.875000\n",
            "Train Epoch: 9 [6400/7471 (86%)]\tLoss: 1620033.625000\n",
            "Train Epoch: 9 [6560/7471 (88%)]\tLoss: 1599629.500000\n",
            "Train Epoch: 9 [6720/7471 (90%)]\tLoss: 1629686.000000\n",
            "Train Epoch: 9 [6880/7471 (92%)]\tLoss: 1628076.125000\n",
            "Train Epoch: 9 [7040/7471 (94%)]\tLoss: 1627341.875000\n",
            "Train Epoch: 9 [7200/7471 (96%)]\tLoss: 1611895.625000\n",
            "Train Epoch: 9 [7360/7471 (99%)]\tLoss: 1581065.250000\n",
            "Epoch 9 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 10 [160/7471 (2%)]\tLoss: 1617140.625000\n",
            "Train Epoch: 10 [320/7471 (4%)]\tLoss: 1550745.875000\n",
            "Train Epoch: 10 [480/7471 (6%)]\tLoss: 1566244.375000\n",
            "Train Epoch: 10 [640/7471 (9%)]\tLoss: 1597871.375000\n",
            "Train Epoch: 10 [800/7471 (11%)]\tLoss: 1525755.625000\n",
            "Train Epoch: 10 [960/7471 (13%)]\tLoss: 1621986.875000\n",
            "Train Epoch: 10 [1120/7471 (15%)]\tLoss: 1594872.000000\n",
            "Train Epoch: 10 [1280/7471 (17%)]\tLoss: 1624306.500000\n",
            "Train Epoch: 10 [1440/7471 (19%)]\tLoss: 1580050.250000\n",
            "Train Epoch: 10 [1600/7471 (21%)]\tLoss: 1617848.125000\n",
            "Train Epoch: 10 [1760/7471 (24%)]\tLoss: 1557918.625000\n",
            "Train Epoch: 10 [1920/7471 (26%)]\tLoss: 1640531.500000\n",
            "Train Epoch: 10 [2080/7471 (28%)]\tLoss: 1570439.875000\n",
            "Train Epoch: 10 [2240/7471 (30%)]\tLoss: 1660403.875000\n",
            "Train Epoch: 10 [2400/7471 (32%)]\tLoss: 1562102.375000\n",
            "Train Epoch: 10 [2560/7471 (34%)]\tLoss: 1607533.875000\n",
            "Train Epoch: 10 [2720/7471 (36%)]\tLoss: 1543178.125000\n",
            "Train Epoch: 10 [2880/7471 (39%)]\tLoss: 1551943.875000\n",
            "Train Epoch: 10 [3040/7471 (41%)]\tLoss: 1636552.375000\n",
            "Train Epoch: 10 [3200/7471 (43%)]\tLoss: 1632708.625000\n",
            "Train Epoch: 10 [3360/7471 (45%)]\tLoss: 1622049.000000\n",
            "Train Epoch: 10 [3520/7471 (47%)]\tLoss: 1563728.500000\n",
            "Train Epoch: 10 [3680/7471 (49%)]\tLoss: 1587369.250000\n",
            "Train Epoch: 10 [3840/7471 (51%)]\tLoss: 1591968.250000\n",
            "Train Epoch: 10 [4000/7471 (54%)]\tLoss: 1606621.000000\n",
            "Train Epoch: 10 [4160/7471 (56%)]\tLoss: 1619672.125000\n",
            "Train Epoch: 10 [4320/7471 (58%)]\tLoss: 1621953.875000\n",
            "Train Epoch: 10 [4480/7471 (60%)]\tLoss: 1557957.000000\n",
            "Train Epoch: 10 [4640/7471 (62%)]\tLoss: 1568729.750000\n",
            "Train Epoch: 10 [4800/7471 (64%)]\tLoss: 1484590.375000\n",
            "Train Epoch: 10 [4960/7471 (66%)]\tLoss: 1605514.375000\n",
            "Train Epoch: 10 [5120/7471 (69%)]\tLoss: 1576060.000000\n",
            "Train Epoch: 10 [5280/7471 (71%)]\tLoss: 1648159.625000\n",
            "Train Epoch: 10 [5440/7471 (73%)]\tLoss: 1574701.625000\n",
            "Train Epoch: 10 [5600/7471 (75%)]\tLoss: 1587709.375000\n",
            "Train Epoch: 10 [5760/7471 (77%)]\tLoss: 1637279.375000\n",
            "Train Epoch: 10 [5920/7471 (79%)]\tLoss: 1613054.000000\n",
            "Train Epoch: 10 [6080/7471 (81%)]\tLoss: 1601194.250000\n",
            "Train Epoch: 10 [6240/7471 (84%)]\tLoss: 1581760.500000\n",
            "Train Epoch: 10 [6400/7471 (86%)]\tLoss: 1575330.000000\n",
            "Train Epoch: 10 [6560/7471 (88%)]\tLoss: 1570170.625000\n",
            "Train Epoch: 10 [6720/7471 (90%)]\tLoss: 1621502.500000\n",
            "Train Epoch: 10 [6880/7471 (92%)]\tLoss: 1585563.875000\n",
            "Train Epoch: 10 [7040/7471 (94%)]\tLoss: 1601013.500000\n",
            "Train Epoch: 10 [7200/7471 (96%)]\tLoss: 1599401.875000\n",
            "Train Epoch: 10 [7360/7471 (99%)]\tLoss: 1652462.500000\n",
            "Epoch 10 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 11 [160/7471 (2%)]\tLoss: 1526891.250000\n",
            "Train Epoch: 11 [320/7471 (4%)]\tLoss: 1598266.000000\n",
            "Train Epoch: 11 [480/7471 (6%)]\tLoss: 1581917.000000\n",
            "Train Epoch: 11 [640/7471 (9%)]\tLoss: 1644553.250000\n",
            "Train Epoch: 11 [800/7471 (11%)]\tLoss: 1603105.250000\n",
            "Train Epoch: 11 [960/7471 (13%)]\tLoss: 1620001.875000\n",
            "Train Epoch: 11 [1120/7471 (15%)]\tLoss: 1608315.500000\n",
            "Train Epoch: 11 [1280/7471 (17%)]\tLoss: 1625671.875000\n",
            "Train Epoch: 11 [1440/7471 (19%)]\tLoss: 1632931.375000\n",
            "Train Epoch: 11 [1600/7471 (21%)]\tLoss: 1577659.250000\n",
            "Train Epoch: 11 [1760/7471 (24%)]\tLoss: 1606280.250000\n",
            "Train Epoch: 11 [1920/7471 (26%)]\tLoss: 1600284.750000\n",
            "Train Epoch: 11 [2080/7471 (28%)]\tLoss: 1577747.250000\n",
            "Train Epoch: 11 [2240/7471 (30%)]\tLoss: 1593128.500000\n",
            "Train Epoch: 11 [2400/7471 (32%)]\tLoss: 1609523.000000\n",
            "Train Epoch: 11 [2560/7471 (34%)]\tLoss: 1623756.875000\n",
            "Train Epoch: 11 [2720/7471 (36%)]\tLoss: 1591125.000000\n",
            "Train Epoch: 11 [2880/7471 (39%)]\tLoss: 1609310.125000\n",
            "Train Epoch: 11 [3040/7471 (41%)]\tLoss: 1622996.125000\n",
            "Train Epoch: 11 [3200/7471 (43%)]\tLoss: 1633673.250000\n",
            "Train Epoch: 11 [3360/7471 (45%)]\tLoss: 1623186.000000\n",
            "Train Epoch: 11 [3520/7471 (47%)]\tLoss: 1603090.500000\n",
            "Train Epoch: 11 [3680/7471 (49%)]\tLoss: 1619301.500000\n",
            "Train Epoch: 11 [3840/7471 (51%)]\tLoss: 1639240.500000\n",
            "Train Epoch: 11 [4000/7471 (54%)]\tLoss: 1561738.875000\n",
            "Train Epoch: 11 [4160/7471 (56%)]\tLoss: 1585055.500000\n",
            "Train Epoch: 11 [4320/7471 (58%)]\tLoss: 1567187.625000\n",
            "Train Epoch: 11 [4480/7471 (60%)]\tLoss: 1571996.125000\n",
            "Train Epoch: 11 [4640/7471 (62%)]\tLoss: 1612093.625000\n",
            "Train Epoch: 11 [4800/7471 (64%)]\tLoss: 1572187.375000\n",
            "Train Epoch: 11 [4960/7471 (66%)]\tLoss: 1570021.125000\n",
            "Train Epoch: 11 [5120/7471 (69%)]\tLoss: 1618451.500000\n",
            "Train Epoch: 11 [5280/7471 (71%)]\tLoss: 1539592.750000\n",
            "Train Epoch: 11 [5440/7471 (73%)]\tLoss: 1547196.625000\n",
            "Train Epoch: 11 [5600/7471 (75%)]\tLoss: 1560873.500000\n",
            "Train Epoch: 11 [5760/7471 (77%)]\tLoss: 1595088.375000\n",
            "Train Epoch: 11 [5920/7471 (79%)]\tLoss: 1636530.125000\n",
            "Train Epoch: 11 [6080/7471 (81%)]\tLoss: 1604775.875000\n",
            "Train Epoch: 11 [6240/7471 (84%)]\tLoss: 1573878.375000\n",
            "Train Epoch: 11 [6400/7471 (86%)]\tLoss: 1614799.750000\n",
            "Train Epoch: 11 [6560/7471 (88%)]\tLoss: 1567784.625000\n",
            "Train Epoch: 11 [6720/7471 (90%)]\tLoss: 1610238.375000\n",
            "Train Epoch: 11 [6880/7471 (92%)]\tLoss: 1575593.125000\n",
            "Train Epoch: 11 [7040/7471 (94%)]\tLoss: 1623169.625000\n",
            "Train Epoch: 11 [7200/7471 (96%)]\tLoss: 1655590.625000\n",
            "Train Epoch: 11 [7360/7471 (99%)]\tLoss: 1627673.250000\n",
            "Epoch 11 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 99515.1711\n",
            "\n",
            "Train Epoch: 12 [160/7471 (2%)]\tLoss: 1591741.250000\n",
            "Train Epoch: 12 [320/7471 (4%)]\tLoss: 1623391.625000\n",
            "Train Epoch: 12 [480/7471 (6%)]\tLoss: 1558865.375000\n",
            "Train Epoch: 12 [640/7471 (9%)]\tLoss: 1584075.375000\n",
            "Train Epoch: 12 [800/7471 (11%)]\tLoss: 1597124.625000\n",
            "Train Epoch: 12 [960/7471 (13%)]\tLoss: 1622183.250000\n",
            "Train Epoch: 12 [1120/7471 (15%)]\tLoss: 1602825.750000\n",
            "Train Epoch: 12 [1280/7471 (17%)]\tLoss: 1539896.250000\n",
            "Train Epoch: 12 [1440/7471 (19%)]\tLoss: 1565424.375000\n",
            "Train Epoch: 12 [1600/7471 (21%)]\tLoss: 1620013.625000\n",
            "Train Epoch: 12 [1760/7471 (24%)]\tLoss: 1566347.125000\n",
            "Train Epoch: 12 [1920/7471 (26%)]\tLoss: 1616167.750000\n",
            "Train Epoch: 12 [2080/7471 (28%)]\tLoss: 1562404.000000\n",
            "Train Epoch: 12 [2240/7471 (30%)]\tLoss: 1634954.375000\n",
            "Train Epoch: 12 [2400/7471 (32%)]\tLoss: 1619961.125000\n",
            "Train Epoch: 12 [2560/7471 (34%)]\tLoss: 1608059.750000\n",
            "Train Epoch: 12 [2720/7471 (36%)]\tLoss: 1530705.875000\n",
            "Train Epoch: 12 [2880/7471 (39%)]\tLoss: 1600595.500000\n",
            "Train Epoch: 12 [3040/7471 (41%)]\tLoss: 1584016.875000\n",
            "Train Epoch: 12 [3200/7471 (43%)]\tLoss: 1586017.750000\n",
            "Train Epoch: 12 [3360/7471 (45%)]\tLoss: 1604691.500000\n",
            "Train Epoch: 12 [3520/7471 (47%)]\tLoss: 1613466.125000\n",
            "Train Epoch: 12 [3680/7471 (49%)]\tLoss: 1602365.000000\n",
            "Train Epoch: 12 [3840/7471 (51%)]\tLoss: 1600627.000000\n",
            "Train Epoch: 12 [4000/7471 (54%)]\tLoss: 1621291.625000\n",
            "Train Epoch: 12 [4160/7471 (56%)]\tLoss: 1576439.000000\n",
            "Train Epoch: 12 [4320/7471 (58%)]\tLoss: 1583792.500000\n",
            "Train Epoch: 12 [4480/7471 (60%)]\tLoss: 1539381.625000\n",
            "Train Epoch: 12 [4640/7471 (62%)]\tLoss: 1622748.125000\n",
            "Train Epoch: 12 [4800/7471 (64%)]\tLoss: 1589426.500000\n",
            "Train Epoch: 12 [4960/7471 (66%)]\tLoss: 1585651.625000\n",
            "Train Epoch: 12 [5120/7471 (69%)]\tLoss: 1577569.500000\n",
            "Train Epoch: 12 [5280/7471 (71%)]\tLoss: 1616269.250000\n",
            "Train Epoch: 12 [5440/7471 (73%)]\tLoss: 1597055.250000\n",
            "Train Epoch: 12 [5600/7471 (75%)]\tLoss: 1609540.125000\n",
            "Train Epoch: 12 [5760/7471 (77%)]\tLoss: 1589482.000000\n",
            "Train Epoch: 12 [5920/7471 (79%)]\tLoss: 1584022.875000\n",
            "Train Epoch: 12 [6080/7471 (81%)]\tLoss: 1584818.750000\n",
            "Train Epoch: 12 [6240/7471 (84%)]\tLoss: 1652080.500000\n",
            "Train Epoch: 12 [6400/7471 (86%)]\tLoss: 1593448.500000\n",
            "Train Epoch: 12 [6560/7471 (88%)]\tLoss: 1618563.375000\n",
            "Train Epoch: 12 [6720/7471 (90%)]\tLoss: 1625835.625000\n",
            "Train Epoch: 12 [6880/7471 (92%)]\tLoss: 1663843.375000\n",
            "Train Epoch: 12 [7040/7471 (94%)]\tLoss: 1582280.875000\n",
            "Train Epoch: 12 [7200/7471 (96%)]\tLoss: 1565758.125000\n",
            "Train Epoch: 12 [7360/7471 (99%)]\tLoss: 1627608.500000\n",
            "Epoch 12 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 13 [160/7471 (2%)]\tLoss: 1544594.250000\n",
            "Train Epoch: 13 [320/7471 (4%)]\tLoss: 1620878.000000\n",
            "Train Epoch: 13 [480/7471 (6%)]\tLoss: 1581865.500000\n",
            "Train Epoch: 13 [640/7471 (9%)]\tLoss: 1578402.000000\n",
            "Train Epoch: 13 [800/7471 (11%)]\tLoss: 1603594.375000\n",
            "Train Epoch: 13 [960/7471 (13%)]\tLoss: 1596538.375000\n",
            "Train Epoch: 13 [1120/7471 (15%)]\tLoss: 1615045.000000\n",
            "Train Epoch: 13 [1280/7471 (17%)]\tLoss: 1644850.125000\n",
            "Train Epoch: 13 [1440/7471 (19%)]\tLoss: 1621869.875000\n",
            "Train Epoch: 13 [1600/7471 (21%)]\tLoss: 1581102.625000\n",
            "Train Epoch: 13 [1760/7471 (24%)]\tLoss: 1632906.250000\n",
            "Train Epoch: 13 [1920/7471 (26%)]\tLoss: 1564247.625000\n",
            "Train Epoch: 13 [2080/7471 (28%)]\tLoss: 1612766.250000\n",
            "Train Epoch: 13 [2240/7471 (30%)]\tLoss: 1586361.500000\n",
            "Train Epoch: 13 [2400/7471 (32%)]\tLoss: 1572988.750000\n",
            "Train Epoch: 13 [2560/7471 (34%)]\tLoss: 1599654.625000\n",
            "Train Epoch: 13 [2720/7471 (36%)]\tLoss: 1632457.000000\n",
            "Train Epoch: 13 [2880/7471 (39%)]\tLoss: 1608829.125000\n",
            "Train Epoch: 13 [3040/7471 (41%)]\tLoss: 1646266.625000\n",
            "Train Epoch: 13 [3200/7471 (43%)]\tLoss: 1637689.375000\n",
            "Train Epoch: 13 [3360/7471 (45%)]\tLoss: 1612534.875000\n",
            "Train Epoch: 13 [3520/7471 (47%)]\tLoss: 1640371.250000\n",
            "Train Epoch: 13 [3680/7471 (49%)]\tLoss: 1586847.375000\n",
            "Train Epoch: 13 [3840/7471 (51%)]\tLoss: 1601380.500000\n",
            "Train Epoch: 13 [4000/7471 (54%)]\tLoss: 1620791.125000\n",
            "Train Epoch: 13 [4160/7471 (56%)]\tLoss: 1607049.750000\n",
            "Train Epoch: 13 [4320/7471 (58%)]\tLoss: 1620527.625000\n",
            "Train Epoch: 13 [4480/7471 (60%)]\tLoss: 1632870.250000\n",
            "Train Epoch: 13 [4640/7471 (62%)]\tLoss: 1626277.875000\n",
            "Train Epoch: 13 [4800/7471 (64%)]\tLoss: 1631748.500000\n",
            "Train Epoch: 13 [4960/7471 (66%)]\tLoss: 1577885.375000\n",
            "Train Epoch: 13 [5120/7471 (69%)]\tLoss: 1626241.625000\n",
            "Train Epoch: 13 [5280/7471 (71%)]\tLoss: 1573533.250000\n",
            "Train Epoch: 13 [5440/7471 (73%)]\tLoss: 1583372.375000\n",
            "Train Epoch: 13 [5600/7471 (75%)]\tLoss: 1616025.750000\n",
            "Train Epoch: 13 [5760/7471 (77%)]\tLoss: 1590710.500000\n",
            "Train Epoch: 13 [5920/7471 (79%)]\tLoss: 1597758.125000\n",
            "Train Epoch: 13 [6080/7471 (81%)]\tLoss: 1548348.125000\n",
            "Train Epoch: 13 [6240/7471 (84%)]\tLoss: 1561913.500000\n",
            "Train Epoch: 13 [6400/7471 (86%)]\tLoss: 1607364.625000\n",
            "Train Epoch: 13 [6560/7471 (88%)]\tLoss: 1598832.750000\n",
            "Train Epoch: 13 [6720/7471 (90%)]\tLoss: 1599590.250000\n",
            "Train Epoch: 13 [6880/7471 (92%)]\tLoss: 1542201.875000\n",
            "Train Epoch: 13 [7040/7471 (94%)]\tLoss: 1627410.375000\n",
            "Train Epoch: 13 [7200/7471 (96%)]\tLoss: 1616421.500000\n",
            "Train Epoch: 13 [7360/7471 (99%)]\tLoss: 1580130.625000\n",
            "Epoch 13 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 14 [160/7471 (2%)]\tLoss: 1573064.625000\n",
            "Train Epoch: 14 [320/7471 (4%)]\tLoss: 1570829.250000\n",
            "Train Epoch: 14 [480/7471 (6%)]\tLoss: 1579697.875000\n",
            "Train Epoch: 14 [640/7471 (9%)]\tLoss: 1573308.375000\n",
            "Train Epoch: 14 [800/7471 (11%)]\tLoss: 1642841.625000\n",
            "Train Epoch: 14 [960/7471 (13%)]\tLoss: 1626822.375000\n",
            "Train Epoch: 14 [1120/7471 (15%)]\tLoss: 1516003.250000\n",
            "Train Epoch: 14 [1280/7471 (17%)]\tLoss: 1616090.875000\n",
            "Train Epoch: 14 [1440/7471 (19%)]\tLoss: 1564093.750000\n",
            "Train Epoch: 14 [1600/7471 (21%)]\tLoss: 1593028.250000\n",
            "Train Epoch: 14 [1760/7471 (24%)]\tLoss: 1588522.750000\n",
            "Train Epoch: 14 [1920/7471 (26%)]\tLoss: 1589749.375000\n",
            "Train Epoch: 14 [2080/7471 (28%)]\tLoss: 1607672.000000\n",
            "Train Epoch: 14 [2240/7471 (30%)]\tLoss: 1596477.375000\n",
            "Train Epoch: 14 [2400/7471 (32%)]\tLoss: 1591850.000000\n",
            "Train Epoch: 14 [2560/7471 (34%)]\tLoss: 1586639.000000\n",
            "Train Epoch: 14 [2720/7471 (36%)]\tLoss: 1555976.875000\n",
            "Train Epoch: 14 [2880/7471 (39%)]\tLoss: 1498918.750000\n",
            "Train Epoch: 14 [3040/7471 (41%)]\tLoss: 1615206.250000\n",
            "Train Epoch: 14 [3200/7471 (43%)]\tLoss: 1595183.125000\n",
            "Train Epoch: 14 [3360/7471 (45%)]\tLoss: 1637917.625000\n",
            "Train Epoch: 14 [3520/7471 (47%)]\tLoss: 1629636.625000\n",
            "Train Epoch: 14 [3680/7471 (49%)]\tLoss: 1600554.500000\n",
            "Train Epoch: 14 [3840/7471 (51%)]\tLoss: 1500670.625000\n",
            "Train Epoch: 14 [4000/7471 (54%)]\tLoss: 1515715.000000\n",
            "Train Epoch: 14 [4160/7471 (56%)]\tLoss: 1599331.500000\n",
            "Train Epoch: 14 [4320/7471 (58%)]\tLoss: 1547939.375000\n",
            "Train Epoch: 14 [4480/7471 (60%)]\tLoss: 1631396.500000\n",
            "Train Epoch: 14 [4640/7471 (62%)]\tLoss: 1575332.875000\n",
            "Train Epoch: 14 [4800/7471 (64%)]\tLoss: 1608873.125000\n",
            "Train Epoch: 14 [4960/7471 (66%)]\tLoss: 1519020.750000\n",
            "Train Epoch: 14 [5120/7471 (69%)]\tLoss: 1585971.500000\n",
            "Train Epoch: 14 [5280/7471 (71%)]\tLoss: 1592318.000000\n",
            "Train Epoch: 14 [5440/7471 (73%)]\tLoss: 1602054.875000\n",
            "Train Epoch: 14 [5600/7471 (75%)]\tLoss: 1585735.500000\n",
            "Train Epoch: 14 [5760/7471 (77%)]\tLoss: 1525147.250000\n",
            "Train Epoch: 14 [5920/7471 (79%)]\tLoss: 1587630.625000\n",
            "Train Epoch: 14 [6080/7471 (81%)]\tLoss: 1602884.875000\n",
            "Train Epoch: 14 [6240/7471 (84%)]\tLoss: 1585550.500000\n",
            "Train Epoch: 14 [6400/7471 (86%)]\tLoss: 1653593.250000\n",
            "Train Epoch: 14 [6560/7471 (88%)]\tLoss: 1627971.750000\n",
            "Train Epoch: 14 [6720/7471 (90%)]\tLoss: 1586406.125000\n",
            "Train Epoch: 14 [6880/7471 (92%)]\tLoss: 1631766.875000\n",
            "Train Epoch: 14 [7040/7471 (94%)]\tLoss: 1617407.875000\n",
            "Train Epoch: 14 [7200/7471 (96%)]\tLoss: 1569229.750000\n",
            "Train Epoch: 14 [7360/7471 (99%)]\tLoss: 1526912.500000\n",
            "Epoch 14 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 15 [160/7471 (2%)]\tLoss: 1528888.375000\n",
            "Train Epoch: 15 [320/7471 (4%)]\tLoss: 1553042.250000\n",
            "Train Epoch: 15 [480/7471 (6%)]\tLoss: 1613986.875000\n",
            "Train Epoch: 15 [640/7471 (9%)]\tLoss: 1575144.875000\n",
            "Train Epoch: 15 [800/7471 (11%)]\tLoss: 1598630.375000\n",
            "Train Epoch: 15 [960/7471 (13%)]\tLoss: 1542250.875000\n",
            "Train Epoch: 15 [1120/7471 (15%)]\tLoss: 1581870.375000\n",
            "Train Epoch: 15 [1280/7471 (17%)]\tLoss: 1605282.875000\n",
            "Train Epoch: 15 [1440/7471 (19%)]\tLoss: 1585246.500000\n",
            "Train Epoch: 15 [1600/7471 (21%)]\tLoss: 1562331.375000\n",
            "Train Epoch: 15 [1760/7471 (24%)]\tLoss: 1604539.750000\n",
            "Train Epoch: 15 [1920/7471 (26%)]\tLoss: 1652528.375000\n",
            "Train Epoch: 15 [2080/7471 (28%)]\tLoss: 1620499.750000\n",
            "Train Epoch: 15 [2240/7471 (30%)]\tLoss: 1613464.000000\n",
            "Train Epoch: 15 [2400/7471 (32%)]\tLoss: 1520155.875000\n",
            "Train Epoch: 15 [2560/7471 (34%)]\tLoss: 1626181.875000\n",
            "Train Epoch: 15 [2720/7471 (36%)]\tLoss: 1592427.625000\n",
            "Train Epoch: 15 [2880/7471 (39%)]\tLoss: 1610230.125000\n",
            "Train Epoch: 15 [3040/7471 (41%)]\tLoss: 1624797.500000\n",
            "Train Epoch: 15 [3200/7471 (43%)]\tLoss: 1563354.625000\n",
            "Train Epoch: 15 [3360/7471 (45%)]\tLoss: 1588234.750000\n",
            "Train Epoch: 15 [3520/7471 (47%)]\tLoss: 1519803.375000\n",
            "Train Epoch: 15 [3680/7471 (49%)]\tLoss: 1513910.125000\n",
            "Train Epoch: 15 [3840/7471 (51%)]\tLoss: 1559364.750000\n",
            "Train Epoch: 15 [4000/7471 (54%)]\tLoss: 1564931.500000\n",
            "Train Epoch: 15 [4160/7471 (56%)]\tLoss: 1536549.875000\n",
            "Train Epoch: 15 [4320/7471 (58%)]\tLoss: 1597941.875000\n",
            "Train Epoch: 15 [4480/7471 (60%)]\tLoss: 1591725.375000\n",
            "Train Epoch: 15 [4640/7471 (62%)]\tLoss: 1612613.125000\n",
            "Train Epoch: 15 [4800/7471 (64%)]\tLoss: 1597326.250000\n",
            "Train Epoch: 15 [4960/7471 (66%)]\tLoss: 1635943.750000\n",
            "Train Epoch: 15 [5120/7471 (69%)]\tLoss: 1562647.875000\n",
            "Train Epoch: 15 [5280/7471 (71%)]\tLoss: 1565306.625000\n",
            "Train Epoch: 15 [5440/7471 (73%)]\tLoss: 1582150.250000\n",
            "Train Epoch: 15 [5600/7471 (75%)]\tLoss: 1586887.625000\n",
            "Train Epoch: 15 [5760/7471 (77%)]\tLoss: 1588524.875000\n",
            "Train Epoch: 15 [5920/7471 (79%)]\tLoss: 1567206.625000\n",
            "Train Epoch: 15 [6080/7471 (81%)]\tLoss: 1520679.750000\n",
            "Train Epoch: 15 [6240/7471 (84%)]\tLoss: 1589724.750000\n",
            "Train Epoch: 15 [6400/7471 (86%)]\tLoss: 1588518.625000\n",
            "Train Epoch: 15 [6560/7471 (88%)]\tLoss: 1574492.250000\n",
            "Train Epoch: 15 [6720/7471 (90%)]\tLoss: 1584410.500000\n",
            "Train Epoch: 15 [6880/7471 (92%)]\tLoss: 1601717.625000\n",
            "Train Epoch: 15 [7040/7471 (94%)]\tLoss: 1593180.625000\n",
            "Train Epoch: 15 [7200/7471 (96%)]\tLoss: 1556955.500000\n",
            "Train Epoch: 15 [7360/7471 (99%)]\tLoss: 1592908.875000\n",
            "Epoch 15 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 16 [160/7471 (2%)]\tLoss: 1599833.250000\n",
            "Train Epoch: 16 [320/7471 (4%)]\tLoss: 1544388.000000\n",
            "Train Epoch: 16 [480/7471 (6%)]\tLoss: 1579525.125000\n",
            "Train Epoch: 16 [640/7471 (9%)]\tLoss: 1516537.250000\n",
            "Train Epoch: 16 [800/7471 (11%)]\tLoss: 1646375.125000\n",
            "Train Epoch: 16 [960/7471 (13%)]\tLoss: 1635067.875000\n",
            "Train Epoch: 16 [1120/7471 (15%)]\tLoss: 1596071.000000\n",
            "Train Epoch: 16 [1280/7471 (17%)]\tLoss: 1572007.250000\n",
            "Train Epoch: 16 [1440/7471 (19%)]\tLoss: 1619227.000000\n",
            "Train Epoch: 16 [1600/7471 (21%)]\tLoss: 1509570.125000\n",
            "Train Epoch: 16 [1760/7471 (24%)]\tLoss: 1631570.750000\n",
            "Train Epoch: 16 [1920/7471 (26%)]\tLoss: 1513674.875000\n",
            "Train Epoch: 16 [2080/7471 (28%)]\tLoss: 1569701.625000\n",
            "Train Epoch: 16 [2240/7471 (30%)]\tLoss: 1593284.250000\n",
            "Train Epoch: 16 [2400/7471 (32%)]\tLoss: 1631089.750000\n",
            "Train Epoch: 16 [2560/7471 (34%)]\tLoss: 1532320.000000\n",
            "Train Epoch: 16 [2720/7471 (36%)]\tLoss: 1568948.500000\n",
            "Train Epoch: 16 [2880/7471 (39%)]\tLoss: 1627271.750000\n",
            "Train Epoch: 16 [3040/7471 (41%)]\tLoss: 1594059.875000\n",
            "Train Epoch: 16 [3200/7471 (43%)]\tLoss: 1618603.250000\n",
            "Train Epoch: 16 [3360/7471 (45%)]\tLoss: 1566061.375000\n",
            "Train Epoch: 16 [3520/7471 (47%)]\tLoss: 1599313.125000\n",
            "Train Epoch: 16 [3680/7471 (49%)]\tLoss: 1599686.625000\n",
            "Train Epoch: 16 [3840/7471 (51%)]\tLoss: 1632684.125000\n",
            "Train Epoch: 16 [4000/7471 (54%)]\tLoss: 1614397.125000\n",
            "Train Epoch: 16 [4160/7471 (56%)]\tLoss: 1578766.625000\n",
            "Train Epoch: 16 [4320/7471 (58%)]\tLoss: 1579415.750000\n",
            "Train Epoch: 16 [4480/7471 (60%)]\tLoss: 1615901.375000\n",
            "Train Epoch: 16 [4640/7471 (62%)]\tLoss: 1626552.000000\n",
            "Train Epoch: 16 [4800/7471 (64%)]\tLoss: 1621608.125000\n",
            "Train Epoch: 16 [4960/7471 (66%)]\tLoss: 1506755.875000\n",
            "Train Epoch: 16 [5120/7471 (69%)]\tLoss: 1559007.875000\n",
            "Train Epoch: 16 [5280/7471 (71%)]\tLoss: 1589881.000000\n",
            "Train Epoch: 16 [5440/7471 (73%)]\tLoss: 1554719.125000\n",
            "Train Epoch: 16 [5600/7471 (75%)]\tLoss: 1582615.875000\n",
            "Train Epoch: 16 [5760/7471 (77%)]\tLoss: 1611325.250000\n",
            "Train Epoch: 16 [5920/7471 (79%)]\tLoss: 1596269.250000\n",
            "Train Epoch: 16 [6080/7471 (81%)]\tLoss: 1623965.375000\n",
            "Train Epoch: 16 [6240/7471 (84%)]\tLoss: 1595731.000000\n",
            "Train Epoch: 16 [6400/7471 (86%)]\tLoss: 1580244.250000\n",
            "Train Epoch: 16 [6560/7471 (88%)]\tLoss: 1600511.875000\n",
            "Train Epoch: 16 [6720/7471 (90%)]\tLoss: 1529677.625000\n",
            "Train Epoch: 16 [6880/7471 (92%)]\tLoss: 1617535.250000\n",
            "Train Epoch: 16 [7040/7471 (94%)]\tLoss: 1617337.500000\n",
            "Train Epoch: 16 [7200/7471 (96%)]\tLoss: 1576537.000000\n",
            "Train Epoch: 16 [7360/7471 (99%)]\tLoss: 1598889.125000\n",
            "Epoch 16 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 17 [160/7471 (2%)]\tLoss: 1590434.500000\n",
            "Train Epoch: 17 [320/7471 (4%)]\tLoss: 1535442.625000\n",
            "Train Epoch: 17 [480/7471 (6%)]\tLoss: 1634152.125000\n",
            "Train Epoch: 17 [640/7471 (9%)]\tLoss: 1612819.875000\n",
            "Train Epoch: 17 [800/7471 (11%)]\tLoss: 1562790.125000\n",
            "Train Epoch: 17 [960/7471 (13%)]\tLoss: 1611723.500000\n",
            "Train Epoch: 17 [1120/7471 (15%)]\tLoss: 1525289.250000\n",
            "Train Epoch: 17 [1280/7471 (17%)]\tLoss: 1618976.625000\n",
            "Train Epoch: 17 [1440/7471 (19%)]\tLoss: 1554516.125000\n",
            "Train Epoch: 17 [1600/7471 (21%)]\tLoss: 1608997.000000\n",
            "Train Epoch: 17 [1760/7471 (24%)]\tLoss: 1604459.625000\n",
            "Train Epoch: 17 [1920/7471 (26%)]\tLoss: 1599694.250000\n",
            "Train Epoch: 17 [2080/7471 (28%)]\tLoss: 1581349.875000\n",
            "Train Epoch: 17 [2240/7471 (30%)]\tLoss: 1611130.250000\n",
            "Train Epoch: 17 [2400/7471 (32%)]\tLoss: 1638294.375000\n",
            "Train Epoch: 17 [2560/7471 (34%)]\tLoss: 1594746.375000\n",
            "Train Epoch: 17 [2720/7471 (36%)]\tLoss: 1553897.875000\n",
            "Train Epoch: 17 [2880/7471 (39%)]\tLoss: 1536002.625000\n",
            "Train Epoch: 17 [3040/7471 (41%)]\tLoss: 1519994.875000\n",
            "Train Epoch: 17 [3200/7471 (43%)]\tLoss: 1576896.000000\n",
            "Train Epoch: 17 [3360/7471 (45%)]\tLoss: 1561177.250000\n",
            "Train Epoch: 17 [3520/7471 (47%)]\tLoss: 1622485.875000\n",
            "Train Epoch: 17 [3680/7471 (49%)]\tLoss: 1587866.125000\n",
            "Train Epoch: 17 [3840/7471 (51%)]\tLoss: 1569649.250000\n",
            "Train Epoch: 17 [4000/7471 (54%)]\tLoss: 1572786.875000\n",
            "Train Epoch: 17 [4160/7471 (56%)]\tLoss: 1648237.875000\n",
            "Train Epoch: 17 [4320/7471 (58%)]\tLoss: 1500256.750000\n",
            "Train Epoch: 17 [4480/7471 (60%)]\tLoss: 1557519.625000\n",
            "Train Epoch: 17 [4640/7471 (62%)]\tLoss: 1628349.750000\n",
            "Train Epoch: 17 [4800/7471 (64%)]\tLoss: 1572941.375000\n",
            "Train Epoch: 17 [4960/7471 (66%)]\tLoss: 1637931.250000\n",
            "Train Epoch: 17 [5120/7471 (69%)]\tLoss: 1607256.625000\n",
            "Train Epoch: 17 [5280/7471 (71%)]\tLoss: 1530232.250000\n",
            "Train Epoch: 17 [5440/7471 (73%)]\tLoss: 1648159.000000\n",
            "Train Epoch: 17 [5600/7471 (75%)]\tLoss: 1592714.750000\n",
            "Train Epoch: 17 [5760/7471 (77%)]\tLoss: 1580215.750000\n",
            "Train Epoch: 17 [5920/7471 (79%)]\tLoss: 1603960.250000\n",
            "Train Epoch: 17 [6080/7471 (81%)]\tLoss: 1615801.000000\n",
            "Train Epoch: 17 [6240/7471 (84%)]\tLoss: 1505148.500000\n",
            "Train Epoch: 17 [6400/7471 (86%)]\tLoss: 1599358.750000\n",
            "Train Epoch: 17 [6560/7471 (88%)]\tLoss: 1628091.750000\n",
            "Train Epoch: 17 [6720/7471 (90%)]\tLoss: 1517679.625000\n",
            "Train Epoch: 17 [6880/7471 (92%)]\tLoss: 1578753.500000\n",
            "Train Epoch: 17 [7040/7471 (94%)]\tLoss: 1533023.875000\n",
            "Train Epoch: 17 [7200/7471 (96%)]\tLoss: 1607119.125000\n",
            "Train Epoch: 17 [7360/7471 (99%)]\tLoss: 1584719.750000\n",
            "Epoch 17 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 18 [160/7471 (2%)]\tLoss: 1564283.000000\n",
            "Train Epoch: 18 [320/7471 (4%)]\tLoss: 1639349.250000\n",
            "Train Epoch: 18 [480/7471 (6%)]\tLoss: 1515995.625000\n",
            "Train Epoch: 18 [640/7471 (9%)]\tLoss: 1615937.125000\n",
            "Train Epoch: 18 [800/7471 (11%)]\tLoss: 1562218.500000\n",
            "Train Epoch: 18 [960/7471 (13%)]\tLoss: 1586647.750000\n",
            "Train Epoch: 18 [1120/7471 (15%)]\tLoss: 1592447.875000\n",
            "Train Epoch: 18 [1280/7471 (17%)]\tLoss: 1561535.875000\n",
            "Train Epoch: 18 [1440/7471 (19%)]\tLoss: 1618942.375000\n",
            "Train Epoch: 18 [1600/7471 (21%)]\tLoss: 1622005.500000\n",
            "Train Epoch: 18 [1760/7471 (24%)]\tLoss: 1510490.375000\n",
            "Train Epoch: 18 [1920/7471 (26%)]\tLoss: 1570144.875000\n",
            "Train Epoch: 18 [2080/7471 (28%)]\tLoss: 1601686.625000\n",
            "Train Epoch: 18 [2240/7471 (30%)]\tLoss: 1618635.875000\n",
            "Train Epoch: 18 [2400/7471 (32%)]\tLoss: 1615793.750000\n",
            "Train Epoch: 18 [2560/7471 (34%)]\tLoss: 1544174.750000\n",
            "Train Epoch: 18 [2720/7471 (36%)]\tLoss: 1597196.750000\n",
            "Train Epoch: 18 [2880/7471 (39%)]\tLoss: 1629967.875000\n",
            "Train Epoch: 18 [3040/7471 (41%)]\tLoss: 1616295.125000\n",
            "Train Epoch: 18 [3200/7471 (43%)]\tLoss: 1591234.500000\n",
            "Train Epoch: 18 [3360/7471 (45%)]\tLoss: 1599927.875000\n",
            "Train Epoch: 18 [3520/7471 (47%)]\tLoss: 1605646.625000\n",
            "Train Epoch: 18 [3680/7471 (49%)]\tLoss: 1634355.750000\n",
            "Train Epoch: 18 [3840/7471 (51%)]\tLoss: 1548907.875000\n",
            "Train Epoch: 18 [4000/7471 (54%)]\tLoss: 1584318.750000\n",
            "Train Epoch: 18 [4160/7471 (56%)]\tLoss: 1654431.125000\n",
            "Train Epoch: 18 [4320/7471 (58%)]\tLoss: 1575183.125000\n",
            "Train Epoch: 18 [4480/7471 (60%)]\tLoss: 1603570.125000\n",
            "Train Epoch: 18 [4640/7471 (62%)]\tLoss: 1582381.500000\n",
            "Train Epoch: 18 [4800/7471 (64%)]\tLoss: 1568323.250000\n",
            "Train Epoch: 18 [4960/7471 (66%)]\tLoss: 1565656.125000\n",
            "Train Epoch: 18 [5120/7471 (69%)]\tLoss: 1632784.375000\n",
            "Train Epoch: 18 [5280/7471 (71%)]\tLoss: 1595303.375000\n",
            "Train Epoch: 18 [5440/7471 (73%)]\tLoss: 1538241.500000\n",
            "Train Epoch: 18 [5600/7471 (75%)]\tLoss: 1603323.500000\n",
            "Train Epoch: 18 [5760/7471 (77%)]\tLoss: 1565687.000000\n",
            "Train Epoch: 18 [5920/7471 (79%)]\tLoss: 1563219.875000\n",
            "Train Epoch: 18 [6080/7471 (81%)]\tLoss: 1605352.250000\n",
            "Train Epoch: 18 [6240/7471 (84%)]\tLoss: 1585501.125000\n",
            "Train Epoch: 18 [6400/7471 (86%)]\tLoss: 1571841.000000\n",
            "Train Epoch: 18 [6560/7471 (88%)]\tLoss: 1573402.750000\n",
            "Train Epoch: 18 [6720/7471 (90%)]\tLoss: 1505004.500000\n",
            "Train Epoch: 18 [6880/7471 (92%)]\tLoss: 1586942.250000\n",
            "Train Epoch: 18 [7040/7471 (94%)]\tLoss: 1525106.500000\n",
            "Train Epoch: 18 [7200/7471 (96%)]\tLoss: 1642645.000000\n",
            "Train Epoch: 18 [7360/7471 (99%)]\tLoss: 1596565.125000\n",
            "Epoch 18 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 19 [160/7471 (2%)]\tLoss: 1633419.875000\n",
            "Train Epoch: 19 [320/7471 (4%)]\tLoss: 1583055.500000\n",
            "Train Epoch: 19 [480/7471 (6%)]\tLoss: 1610883.500000\n",
            "Train Epoch: 19 [640/7471 (9%)]\tLoss: 1627058.625000\n",
            "Train Epoch: 19 [800/7471 (11%)]\tLoss: 1533657.625000\n",
            "Train Epoch: 19 [960/7471 (13%)]\tLoss: 1559804.250000\n",
            "Train Epoch: 19 [1120/7471 (15%)]\tLoss: 1629110.625000\n",
            "Train Epoch: 19 [1280/7471 (17%)]\tLoss: 1626034.250000\n",
            "Train Epoch: 19 [1440/7471 (19%)]\tLoss: 1587477.000000\n",
            "Train Epoch: 19 [1600/7471 (21%)]\tLoss: 1554465.750000\n",
            "Train Epoch: 19 [1760/7471 (24%)]\tLoss: 1593343.250000\n",
            "Train Epoch: 19 [1920/7471 (26%)]\tLoss: 1599093.875000\n",
            "Train Epoch: 19 [2080/7471 (28%)]\tLoss: 1573266.875000\n",
            "Train Epoch: 19 [2240/7471 (30%)]\tLoss: 1575141.500000\n",
            "Train Epoch: 19 [2400/7471 (32%)]\tLoss: 1539422.000000\n",
            "Train Epoch: 19 [2560/7471 (34%)]\tLoss: 1592825.125000\n",
            "Train Epoch: 19 [2720/7471 (36%)]\tLoss: 1608234.250000\n",
            "Train Epoch: 19 [2880/7471 (39%)]\tLoss: 1590011.625000\n",
            "Train Epoch: 19 [3040/7471 (41%)]\tLoss: 1560321.250000\n",
            "Train Epoch: 19 [3200/7471 (43%)]\tLoss: 1592191.375000\n",
            "Train Epoch: 19 [3360/7471 (45%)]\tLoss: 1501503.750000\n",
            "Train Epoch: 19 [3520/7471 (47%)]\tLoss: 1558999.875000\n",
            "Train Epoch: 19 [3680/7471 (49%)]\tLoss: 1595751.375000\n",
            "Train Epoch: 19 [3840/7471 (51%)]\tLoss: 1622972.750000\n",
            "Train Epoch: 19 [4000/7471 (54%)]\tLoss: 1571406.125000\n",
            "Train Epoch: 19 [4160/7471 (56%)]\tLoss: 1558556.875000\n",
            "Train Epoch: 19 [4320/7471 (58%)]\tLoss: 1598188.250000\n",
            "Train Epoch: 19 [4480/7471 (60%)]\tLoss: 1562318.625000\n",
            "Train Epoch: 19 [4640/7471 (62%)]\tLoss: 1589510.875000\n",
            "Train Epoch: 19 [4800/7471 (64%)]\tLoss: 1619693.000000\n",
            "Train Epoch: 19 [4960/7471 (66%)]\tLoss: 1543296.625000\n",
            "Train Epoch: 19 [5120/7471 (69%)]\tLoss: 1583020.750000\n",
            "Train Epoch: 19 [5280/7471 (71%)]\tLoss: 1612315.875000\n",
            "Train Epoch: 19 [5440/7471 (73%)]\tLoss: 1607390.875000\n",
            "Train Epoch: 19 [5600/7471 (75%)]\tLoss: 1625227.875000\n",
            "Train Epoch: 19 [5760/7471 (77%)]\tLoss: 1529790.125000\n",
            "Train Epoch: 19 [5920/7471 (79%)]\tLoss: 1588839.125000\n",
            "Train Epoch: 19 [6080/7471 (81%)]\tLoss: 1573262.750000\n",
            "Train Epoch: 19 [6240/7471 (84%)]\tLoss: 1563539.250000\n",
            "Train Epoch: 19 [6400/7471 (86%)]\tLoss: 1588065.000000\n",
            "Train Epoch: 19 [6560/7471 (88%)]\tLoss: 1587790.750000\n",
            "Train Epoch: 19 [6720/7471 (90%)]\tLoss: 1560032.500000\n",
            "Train Epoch: 19 [6880/7471 (92%)]\tLoss: 1550940.875000\n",
            "Train Epoch: 19 [7040/7471 (94%)]\tLoss: 1519064.250000\n",
            "Train Epoch: 19 [7200/7471 (96%)]\tLoss: 1539587.250000\n",
            "Train Epoch: 19 [7360/7471 (99%)]\tLoss: 1621540.375000\n",
            "Epoch 19 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 20 [160/7471 (2%)]\tLoss: 1577982.875000\n",
            "Train Epoch: 20 [320/7471 (4%)]\tLoss: 1622913.125000\n",
            "Train Epoch: 20 [480/7471 (6%)]\tLoss: 1575464.875000\n",
            "Train Epoch: 20 [640/7471 (9%)]\tLoss: 1581490.375000\n",
            "Train Epoch: 20 [800/7471 (11%)]\tLoss: 1558964.375000\n",
            "Train Epoch: 20 [960/7471 (13%)]\tLoss: 1627836.250000\n",
            "Train Epoch: 20 [1120/7471 (15%)]\tLoss: 1624319.500000\n",
            "Train Epoch: 20 [1280/7471 (17%)]\tLoss: 1607160.250000\n",
            "Train Epoch: 20 [1440/7471 (19%)]\tLoss: 1571864.500000\n",
            "Train Epoch: 20 [1600/7471 (21%)]\tLoss: 1610226.000000\n",
            "Train Epoch: 20 [1760/7471 (24%)]\tLoss: 1624510.250000\n",
            "Train Epoch: 20 [1920/7471 (26%)]\tLoss: 1600905.875000\n",
            "Train Epoch: 20 [2080/7471 (28%)]\tLoss: 1592094.625000\n",
            "Train Epoch: 20 [2240/7471 (30%)]\tLoss: 1624593.000000\n",
            "Train Epoch: 20 [2400/7471 (32%)]\tLoss: 1569831.000000\n",
            "Train Epoch: 20 [2560/7471 (34%)]\tLoss: 1574768.750000\n",
            "Train Epoch: 20 [2720/7471 (36%)]\tLoss: 1558606.500000\n",
            "Train Epoch: 20 [2880/7471 (39%)]\tLoss: 1569061.375000\n",
            "Train Epoch: 20 [3040/7471 (41%)]\tLoss: 1517823.500000\n",
            "Train Epoch: 20 [3200/7471 (43%)]\tLoss: 1587861.250000\n",
            "Train Epoch: 20 [3360/7471 (45%)]\tLoss: 1600460.875000\n",
            "Train Epoch: 20 [3520/7471 (47%)]\tLoss: 1582791.375000\n",
            "Train Epoch: 20 [3680/7471 (49%)]\tLoss: 1580026.875000\n",
            "Train Epoch: 20 [3840/7471 (51%)]\tLoss: 1580632.875000\n",
            "Train Epoch: 20 [4000/7471 (54%)]\tLoss: 1554281.625000\n",
            "Train Epoch: 20 [4160/7471 (56%)]\tLoss: 1623739.375000\n",
            "Train Epoch: 20 [4320/7471 (58%)]\tLoss: 1572662.125000\n",
            "Train Epoch: 20 [4480/7471 (60%)]\tLoss: 1602064.250000\n",
            "Train Epoch: 20 [4640/7471 (62%)]\tLoss: 1605487.500000\n",
            "Train Epoch: 20 [4800/7471 (64%)]\tLoss: 1618845.500000\n",
            "Train Epoch: 20 [4960/7471 (66%)]\tLoss: 1517509.250000\n",
            "Train Epoch: 20 [5120/7471 (69%)]\tLoss: 1607085.625000\n",
            "Train Epoch: 20 [5280/7471 (71%)]\tLoss: 1616318.750000\n",
            "Train Epoch: 20 [5440/7471 (73%)]\tLoss: 1559470.750000\n",
            "Train Epoch: 20 [5600/7471 (75%)]\tLoss: 1596057.875000\n",
            "Train Epoch: 20 [5760/7471 (77%)]\tLoss: 1583369.500000\n",
            "Train Epoch: 20 [5920/7471 (79%)]\tLoss: 1570722.750000\n",
            "Train Epoch: 20 [6080/7471 (81%)]\tLoss: 1634027.625000\n",
            "Train Epoch: 20 [6240/7471 (84%)]\tLoss: 1583339.375000\n",
            "Train Epoch: 20 [6400/7471 (86%)]\tLoss: 1615657.875000\n",
            "Train Epoch: 20 [6560/7471 (88%)]\tLoss: 1541800.250000\n",
            "Train Epoch: 20 [6720/7471 (90%)]\tLoss: 1582159.875000\n",
            "Train Epoch: 20 [6880/7471 (92%)]\tLoss: 1557304.875000\n",
            "Train Epoch: 20 [7040/7471 (94%)]\tLoss: 1623610.250000\n",
            "Train Epoch: 20 [7200/7471 (96%)]\tLoss: 1659801.500000\n",
            "Train Epoch: 20 [7360/7471 (99%)]\tLoss: 1625108.375000\n",
            "Epoch 20 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 21 [160/7471 (2%)]\tLoss: 1600198.875000\n",
            "Train Epoch: 21 [320/7471 (4%)]\tLoss: 1559775.875000\n",
            "Train Epoch: 21 [480/7471 (6%)]\tLoss: 1590422.125000\n",
            "Train Epoch: 21 [640/7471 (9%)]\tLoss: 1536476.250000\n",
            "Train Epoch: 21 [800/7471 (11%)]\tLoss: 1568477.625000\n",
            "Train Epoch: 21 [960/7471 (13%)]\tLoss: 1628093.125000\n",
            "Train Epoch: 21 [1120/7471 (15%)]\tLoss: 1543172.250000\n",
            "Train Epoch: 21 [1280/7471 (17%)]\tLoss: 1540384.750000\n",
            "Train Epoch: 21 [1440/7471 (19%)]\tLoss: 1599039.875000\n",
            "Train Epoch: 21 [1600/7471 (21%)]\tLoss: 1579281.375000\n",
            "Train Epoch: 21 [1760/7471 (24%)]\tLoss: 1589906.250000\n",
            "Train Epoch: 21 [1920/7471 (26%)]\tLoss: 1586280.625000\n",
            "Train Epoch: 21 [2080/7471 (28%)]\tLoss: 1556365.250000\n",
            "Train Epoch: 21 [2240/7471 (30%)]\tLoss: 1553857.375000\n",
            "Train Epoch: 21 [2400/7471 (32%)]\tLoss: 1549776.250000\n",
            "Train Epoch: 21 [2560/7471 (34%)]\tLoss: 1522210.375000\n",
            "Train Epoch: 21 [2720/7471 (36%)]\tLoss: 1545779.125000\n",
            "Train Epoch: 21 [2880/7471 (39%)]\tLoss: 1560503.000000\n",
            "Train Epoch: 21 [3040/7471 (41%)]\tLoss: 1589248.500000\n",
            "Train Epoch: 21 [3200/7471 (43%)]\tLoss: 1522510.500000\n",
            "Train Epoch: 21 [3360/7471 (45%)]\tLoss: 1569947.750000\n",
            "Train Epoch: 21 [3520/7471 (47%)]\tLoss: 1592872.625000\n",
            "Train Epoch: 21 [3680/7471 (49%)]\tLoss: 1620902.875000\n",
            "Train Epoch: 21 [3840/7471 (51%)]\tLoss: 1607369.250000\n",
            "Train Epoch: 21 [4000/7471 (54%)]\tLoss: 1578272.875000\n",
            "Train Epoch: 21 [4160/7471 (56%)]\tLoss: 1622052.375000\n",
            "Train Epoch: 21 [4320/7471 (58%)]\tLoss: 1582494.250000\n",
            "Train Epoch: 21 [4480/7471 (60%)]\tLoss: 1583718.875000\n",
            "Train Epoch: 21 [4640/7471 (62%)]\tLoss: 1617800.625000\n",
            "Train Epoch: 21 [4800/7471 (64%)]\tLoss: 1649110.625000\n",
            "Train Epoch: 21 [4960/7471 (66%)]\tLoss: 1642672.250000\n",
            "Train Epoch: 21 [5120/7471 (69%)]\tLoss: 1605321.500000\n",
            "Train Epoch: 21 [5280/7471 (71%)]\tLoss: 1600239.750000\n",
            "Train Epoch: 21 [5440/7471 (73%)]\tLoss: 1533126.250000\n",
            "Train Epoch: 21 [5600/7471 (75%)]\tLoss: 1605064.125000\n",
            "Train Epoch: 21 [5760/7471 (77%)]\tLoss: 1616413.750000\n",
            "Train Epoch: 21 [5920/7471 (79%)]\tLoss: 1602956.500000\n",
            "Train Epoch: 21 [6080/7471 (81%)]\tLoss: 1521510.875000\n",
            "Train Epoch: 21 [6240/7471 (84%)]\tLoss: 1595190.625000\n",
            "Train Epoch: 21 [6400/7471 (86%)]\tLoss: 1613442.125000\n",
            "Train Epoch: 21 [6560/7471 (88%)]\tLoss: 1582714.625000\n",
            "Train Epoch: 21 [6720/7471 (90%)]\tLoss: 1596008.500000\n",
            "Train Epoch: 21 [6880/7471 (92%)]\tLoss: 1631695.250000\n",
            "Train Epoch: 21 [7040/7471 (94%)]\tLoss: 1561249.750000\n",
            "Train Epoch: 21 [7200/7471 (96%)]\tLoss: 1571953.125000\n",
            "Train Epoch: 21 [7360/7471 (99%)]\tLoss: 1611108.875000\n",
            "Epoch 21 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 22 [160/7471 (2%)]\tLoss: 1620010.500000\n",
            "Train Epoch: 22 [320/7471 (4%)]\tLoss: 1524320.000000\n",
            "Train Epoch: 22 [480/7471 (6%)]\tLoss: 1600991.750000\n",
            "Train Epoch: 22 [640/7471 (9%)]\tLoss: 1559866.000000\n",
            "Train Epoch: 22 [800/7471 (11%)]\tLoss: 1563888.875000\n",
            "Train Epoch: 22 [960/7471 (13%)]\tLoss: 1590811.000000\n",
            "Train Epoch: 22 [1120/7471 (15%)]\tLoss: 1620457.125000\n",
            "Train Epoch: 22 [1280/7471 (17%)]\tLoss: 1548393.000000\n",
            "Train Epoch: 22 [1440/7471 (19%)]\tLoss: 1614052.125000\n",
            "Train Epoch: 22 [1600/7471 (21%)]\tLoss: 1614091.625000\n",
            "Train Epoch: 22 [1760/7471 (24%)]\tLoss: 1628209.500000\n",
            "Train Epoch: 22 [1920/7471 (26%)]\tLoss: 1601094.750000\n",
            "Train Epoch: 22 [2080/7471 (28%)]\tLoss: 1564787.000000\n",
            "Train Epoch: 22 [2240/7471 (30%)]\tLoss: 1585848.750000\n",
            "Train Epoch: 22 [2400/7471 (32%)]\tLoss: 1540245.250000\n",
            "Train Epoch: 22 [2560/7471 (34%)]\tLoss: 1618077.000000\n",
            "Train Epoch: 22 [2720/7471 (36%)]\tLoss: 1507231.250000\n",
            "Train Epoch: 22 [2880/7471 (39%)]\tLoss: 1555737.500000\n",
            "Train Epoch: 22 [3040/7471 (41%)]\tLoss: 1571502.250000\n",
            "Train Epoch: 22 [3200/7471 (43%)]\tLoss: 1608562.500000\n",
            "Train Epoch: 22 [3360/7471 (45%)]\tLoss: 1537348.750000\n",
            "Train Epoch: 22 [3520/7471 (47%)]\tLoss: 1592820.125000\n",
            "Train Epoch: 22 [3680/7471 (49%)]\tLoss: 1621128.125000\n",
            "Train Epoch: 22 [3840/7471 (51%)]\tLoss: 1589111.875000\n",
            "Train Epoch: 22 [4000/7471 (54%)]\tLoss: 1617310.125000\n",
            "Train Epoch: 22 [4160/7471 (56%)]\tLoss: 1593601.250000\n",
            "Train Epoch: 22 [4320/7471 (58%)]\tLoss: 1594830.500000\n",
            "Train Epoch: 22 [4480/7471 (60%)]\tLoss: 1581277.875000\n",
            "Train Epoch: 22 [4640/7471 (62%)]\tLoss: 1605333.875000\n",
            "Train Epoch: 22 [4800/7471 (64%)]\tLoss: 1548155.250000\n",
            "Train Epoch: 22 [4960/7471 (66%)]\tLoss: 1573432.750000\n",
            "Train Epoch: 22 [5120/7471 (69%)]\tLoss: 1527966.000000\n",
            "Train Epoch: 22 [5280/7471 (71%)]\tLoss: 1610581.000000\n",
            "Train Epoch: 22 [5440/7471 (73%)]\tLoss: 1532848.000000\n",
            "Train Epoch: 22 [5600/7471 (75%)]\tLoss: 1605505.625000\n",
            "Train Epoch: 22 [5760/7471 (77%)]\tLoss: 1617943.000000\n",
            "Train Epoch: 22 [5920/7471 (79%)]\tLoss: 1556951.125000\n",
            "Train Epoch: 22 [6080/7471 (81%)]\tLoss: 1586122.500000\n",
            "Train Epoch: 22 [6240/7471 (84%)]\tLoss: 1589201.625000\n",
            "Train Epoch: 22 [6400/7471 (86%)]\tLoss: 1565871.125000\n",
            "Train Epoch: 22 [6560/7471 (88%)]\tLoss: 1555125.375000\n",
            "Train Epoch: 22 [6720/7471 (90%)]\tLoss: 1625029.625000\n",
            "Train Epoch: 22 [6880/7471 (92%)]\tLoss: 1545955.125000\n",
            "Train Epoch: 22 [7040/7471 (94%)]\tLoss: 1553168.750000\n",
            "Train Epoch: 22 [7200/7471 (96%)]\tLoss: 1631073.125000\n",
            "Train Epoch: 22 [7360/7471 (99%)]\tLoss: 1579036.500000\n",
            "Epoch 22 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 23 [160/7471 (2%)]\tLoss: 1583660.375000\n",
            "Train Epoch: 23 [320/7471 (4%)]\tLoss: 1609126.250000\n",
            "Train Epoch: 23 [480/7471 (6%)]\tLoss: 1544185.250000\n",
            "Train Epoch: 23 [640/7471 (9%)]\tLoss: 1585593.625000\n",
            "Train Epoch: 23 [800/7471 (11%)]\tLoss: 1551313.375000\n",
            "Train Epoch: 23 [960/7471 (13%)]\tLoss: 1624818.625000\n",
            "Train Epoch: 23 [1120/7471 (15%)]\tLoss: 1581563.250000\n",
            "Train Epoch: 23 [1280/7471 (17%)]\tLoss: 1635987.000000\n",
            "Train Epoch: 23 [1440/7471 (19%)]\tLoss: 1538021.500000\n",
            "Train Epoch: 23 [1600/7471 (21%)]\tLoss: 1572194.875000\n",
            "Train Epoch: 23 [1760/7471 (24%)]\tLoss: 1526350.375000\n",
            "Train Epoch: 23 [1920/7471 (26%)]\tLoss: 1599671.500000\n",
            "Train Epoch: 23 [2080/7471 (28%)]\tLoss: 1567018.500000\n",
            "Train Epoch: 23 [2240/7471 (30%)]\tLoss: 1555378.375000\n",
            "Train Epoch: 23 [2400/7471 (32%)]\tLoss: 1500394.250000\n",
            "Train Epoch: 23 [2560/7471 (34%)]\tLoss: 1574995.750000\n",
            "Train Epoch: 23 [2720/7471 (36%)]\tLoss: 1615068.625000\n",
            "Train Epoch: 23 [2880/7471 (39%)]\tLoss: 1543530.125000\n",
            "Train Epoch: 23 [3040/7471 (41%)]\tLoss: 1586244.000000\n",
            "Train Epoch: 23 [3200/7471 (43%)]\tLoss: 1601403.875000\n",
            "Train Epoch: 23 [3360/7471 (45%)]\tLoss: 1594387.500000\n",
            "Train Epoch: 23 [3520/7471 (47%)]\tLoss: 1590558.750000\n",
            "Train Epoch: 23 [3680/7471 (49%)]\tLoss: 1593239.500000\n",
            "Train Epoch: 23 [3840/7471 (51%)]\tLoss: 1591594.000000\n",
            "Train Epoch: 23 [4000/7471 (54%)]\tLoss: 1627522.875000\n",
            "Train Epoch: 23 [4160/7471 (56%)]\tLoss: 1637407.250000\n",
            "Train Epoch: 23 [4320/7471 (58%)]\tLoss: 1615813.500000\n",
            "Train Epoch: 23 [4480/7471 (60%)]\tLoss: 1616437.750000\n",
            "Train Epoch: 23 [4640/7471 (62%)]\tLoss: 1536719.500000\n",
            "Train Epoch: 23 [4800/7471 (64%)]\tLoss: 1614723.750000\n",
            "Train Epoch: 23 [4960/7471 (66%)]\tLoss: 1540469.500000\n",
            "Train Epoch: 23 [5120/7471 (69%)]\tLoss: 1634024.250000\n",
            "Train Epoch: 23 [5280/7471 (71%)]\tLoss: 1589573.250000\n",
            "Train Epoch: 23 [5440/7471 (73%)]\tLoss: 1617629.375000\n",
            "Train Epoch: 23 [5600/7471 (75%)]\tLoss: 1593918.875000\n",
            "Train Epoch: 23 [5760/7471 (77%)]\tLoss: 1640370.375000\n",
            "Train Epoch: 23 [5920/7471 (79%)]\tLoss: 1613511.250000\n",
            "Train Epoch: 23 [6080/7471 (81%)]\tLoss: 1568180.125000\n",
            "Train Epoch: 23 [6240/7471 (84%)]\tLoss: 1569082.875000\n",
            "Train Epoch: 23 [6400/7471 (86%)]\tLoss: 1596247.125000\n",
            "Train Epoch: 23 [6560/7471 (88%)]\tLoss: 1642165.000000\n",
            "Train Epoch: 23 [6720/7471 (90%)]\tLoss: 1585645.750000\n",
            "Train Epoch: 23 [6880/7471 (92%)]\tLoss: 1581405.000000\n",
            "Train Epoch: 23 [7040/7471 (94%)]\tLoss: 1632807.750000\n",
            "Train Epoch: 23 [7200/7471 (96%)]\tLoss: 1591224.250000\n",
            "Train Epoch: 23 [7360/7471 (99%)]\tLoss: 1523659.250000\n",
            "Epoch 23 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 24 [160/7471 (2%)]\tLoss: 1590561.125000\n",
            "Train Epoch: 24 [320/7471 (4%)]\tLoss: 1515764.625000\n",
            "Train Epoch: 24 [480/7471 (6%)]\tLoss: 1553047.000000\n",
            "Train Epoch: 24 [640/7471 (9%)]\tLoss: 1576052.125000\n",
            "Train Epoch: 24 [800/7471 (11%)]\tLoss: 1556732.625000\n",
            "Train Epoch: 24 [960/7471 (13%)]\tLoss: 1631955.125000\n",
            "Train Epoch: 24 [1120/7471 (15%)]\tLoss: 1580229.750000\n",
            "Train Epoch: 24 [1280/7471 (17%)]\tLoss: 1494567.750000\n",
            "Train Epoch: 24 [1440/7471 (19%)]\tLoss: 1622526.250000\n",
            "Train Epoch: 24 [1600/7471 (21%)]\tLoss: 1626084.125000\n",
            "Train Epoch: 24 [1760/7471 (24%)]\tLoss: 1625940.375000\n",
            "Train Epoch: 24 [1920/7471 (26%)]\tLoss: 1611241.250000\n",
            "Train Epoch: 24 [2080/7471 (28%)]\tLoss: 1624619.625000\n",
            "Train Epoch: 24 [2240/7471 (30%)]\tLoss: 1558172.125000\n",
            "Train Epoch: 24 [2400/7471 (32%)]\tLoss: 1615635.250000\n",
            "Train Epoch: 24 [2560/7471 (34%)]\tLoss: 1540068.625000\n",
            "Train Epoch: 24 [2720/7471 (36%)]\tLoss: 1561818.125000\n",
            "Train Epoch: 24 [2880/7471 (39%)]\tLoss: 1579490.250000\n",
            "Train Epoch: 24 [3040/7471 (41%)]\tLoss: 1572991.000000\n",
            "Train Epoch: 24 [3200/7471 (43%)]\tLoss: 1559473.125000\n",
            "Train Epoch: 24 [3360/7471 (45%)]\tLoss: 1629740.875000\n",
            "Train Epoch: 24 [3520/7471 (47%)]\tLoss: 1626872.000000\n",
            "Train Epoch: 24 [3680/7471 (49%)]\tLoss: 1557303.750000\n",
            "Train Epoch: 24 [3840/7471 (51%)]\tLoss: 1570463.500000\n",
            "Train Epoch: 24 [4000/7471 (54%)]\tLoss: 1617146.625000\n",
            "Train Epoch: 24 [4160/7471 (56%)]\tLoss: 1621117.125000\n",
            "Train Epoch: 24 [4320/7471 (58%)]\tLoss: 1537317.000000\n",
            "Train Epoch: 24 [4480/7471 (60%)]\tLoss: 1589717.875000\n",
            "Train Epoch: 24 [4640/7471 (62%)]\tLoss: 1517229.250000\n",
            "Train Epoch: 24 [4800/7471 (64%)]\tLoss: 1603392.250000\n",
            "Train Epoch: 24 [4960/7471 (66%)]\tLoss: 1610095.875000\n",
            "Train Epoch: 24 [5120/7471 (69%)]\tLoss: 1573447.875000\n",
            "Train Epoch: 24 [5280/7471 (71%)]\tLoss: 1557226.750000\n",
            "Train Epoch: 24 [5440/7471 (73%)]\tLoss: 1504964.125000\n",
            "Train Epoch: 24 [5600/7471 (75%)]\tLoss: 1581156.500000\n",
            "Train Epoch: 24 [5760/7471 (77%)]\tLoss: 1609508.250000\n",
            "Train Epoch: 24 [5920/7471 (79%)]\tLoss: 1576976.750000\n",
            "Train Epoch: 24 [6080/7471 (81%)]\tLoss: 1626739.500000\n",
            "Train Epoch: 24 [6240/7471 (84%)]\tLoss: 1591411.500000\n",
            "Train Epoch: 24 [6400/7471 (86%)]\tLoss: 1598469.000000\n",
            "Train Epoch: 24 [6560/7471 (88%)]\tLoss: 1586589.125000\n",
            "Train Epoch: 24 [6720/7471 (90%)]\tLoss: 1621330.750000\n",
            "Train Epoch: 24 [6880/7471 (92%)]\tLoss: 1601105.375000\n",
            "Train Epoch: 24 [7040/7471 (94%)]\tLoss: 1627032.750000\n",
            "Train Epoch: 24 [7200/7471 (96%)]\tLoss: 1599168.250000\n",
            "Train Epoch: 24 [7360/7471 (99%)]\tLoss: 1567331.625000\n",
            "Epoch 24 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 25 [160/7471 (2%)]\tLoss: 1616227.625000\n",
            "Train Epoch: 25 [320/7471 (4%)]\tLoss: 1610847.875000\n",
            "Train Epoch: 25 [480/7471 (6%)]\tLoss: 1639008.250000\n",
            "Train Epoch: 25 [640/7471 (9%)]\tLoss: 1576231.250000\n",
            "Train Epoch: 25 [800/7471 (11%)]\tLoss: 1543584.125000\n",
            "Train Epoch: 25 [960/7471 (13%)]\tLoss: 1570320.000000\n",
            "Train Epoch: 25 [1120/7471 (15%)]\tLoss: 1591498.375000\n",
            "Train Epoch: 25 [1280/7471 (17%)]\tLoss: 1641868.750000\n",
            "Train Epoch: 25 [1440/7471 (19%)]\tLoss: 1630250.375000\n",
            "Train Epoch: 25 [1600/7471 (21%)]\tLoss: 1553346.750000\n",
            "Train Epoch: 25 [1760/7471 (24%)]\tLoss: 1596944.500000\n",
            "Train Epoch: 25 [1920/7471 (26%)]\tLoss: 1625777.875000\n",
            "Train Epoch: 25 [2080/7471 (28%)]\tLoss: 1610605.625000\n",
            "Train Epoch: 25 [2240/7471 (30%)]\tLoss: 1597316.250000\n",
            "Train Epoch: 25 [2400/7471 (32%)]\tLoss: 1604403.875000\n",
            "Train Epoch: 25 [2560/7471 (34%)]\tLoss: 1591594.625000\n",
            "Train Epoch: 25 [2720/7471 (36%)]\tLoss: 1601729.125000\n",
            "Train Epoch: 25 [2880/7471 (39%)]\tLoss: 1625471.625000\n",
            "Train Epoch: 25 [3040/7471 (41%)]\tLoss: 1539560.625000\n",
            "Train Epoch: 25 [3200/7471 (43%)]\tLoss: 1577094.625000\n",
            "Train Epoch: 25 [3360/7471 (45%)]\tLoss: 1598244.625000\n",
            "Train Epoch: 25 [3520/7471 (47%)]\tLoss: 1584225.250000\n",
            "Train Epoch: 25 [3680/7471 (49%)]\tLoss: 1593768.750000\n",
            "Train Epoch: 25 [3840/7471 (51%)]\tLoss: 1603505.875000\n",
            "Train Epoch: 25 [4000/7471 (54%)]\tLoss: 1552096.250000\n",
            "Train Epoch: 25 [4160/7471 (56%)]\tLoss: 1550736.250000\n",
            "Train Epoch: 25 [4320/7471 (58%)]\tLoss: 1593719.750000\n",
            "Train Epoch: 25 [4480/7471 (60%)]\tLoss: 1634656.875000\n",
            "Train Epoch: 25 [4640/7471 (62%)]\tLoss: 1525160.750000\n",
            "Train Epoch: 25 [4800/7471 (64%)]\tLoss: 1582801.000000\n",
            "Train Epoch: 25 [4960/7471 (66%)]\tLoss: 1611222.625000\n",
            "Train Epoch: 25 [5120/7471 (69%)]\tLoss: 1615703.500000\n",
            "Train Epoch: 25 [5280/7471 (71%)]\tLoss: 1555102.125000\n",
            "Train Epoch: 25 [5440/7471 (73%)]\tLoss: 1566505.375000\n",
            "Train Epoch: 25 [5600/7471 (75%)]\tLoss: 1518353.750000\n",
            "Train Epoch: 25 [5760/7471 (77%)]\tLoss: 1526427.000000\n",
            "Train Epoch: 25 [5920/7471 (79%)]\tLoss: 1626920.750000\n",
            "Train Epoch: 25 [6080/7471 (81%)]\tLoss: 1611972.375000\n",
            "Train Epoch: 25 [6240/7471 (84%)]\tLoss: 1518000.125000\n",
            "Train Epoch: 25 [6400/7471 (86%)]\tLoss: 1625701.500000\n",
            "Train Epoch: 25 [6560/7471 (88%)]\tLoss: 1564003.250000\n",
            "Train Epoch: 25 [6720/7471 (90%)]\tLoss: 1566932.500000\n",
            "Train Epoch: 25 [6880/7471 (92%)]\tLoss: 1585277.500000\n",
            "Train Epoch: 25 [7040/7471 (94%)]\tLoss: 1575378.500000\n",
            "Train Epoch: 25 [7200/7471 (96%)]\tLoss: 1647869.750000\n",
            "Train Epoch: 25 [7360/7471 (99%)]\tLoss: 1566393.375000\n",
            "Epoch 25 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 99112.9865\n",
            "\n",
            "Train Epoch: 26 [160/7471 (2%)]\tLoss: 1585473.625000\n",
            "Train Epoch: 26 [320/7471 (4%)]\tLoss: 1558632.875000\n",
            "Train Epoch: 26 [480/7471 (6%)]\tLoss: 1604970.750000\n",
            "Train Epoch: 26 [640/7471 (9%)]\tLoss: 1606024.625000\n",
            "Train Epoch: 26 [800/7471 (11%)]\tLoss: 1619187.375000\n",
            "Train Epoch: 26 [960/7471 (13%)]\tLoss: 1617979.875000\n",
            "Train Epoch: 26 [1120/7471 (15%)]\tLoss: 1544862.625000\n",
            "Train Epoch: 26 [1280/7471 (17%)]\tLoss: 1595689.250000\n",
            "Train Epoch: 26 [1440/7471 (19%)]\tLoss: 1545747.500000\n",
            "Train Epoch: 26 [1600/7471 (21%)]\tLoss: 1610574.500000\n",
            "Train Epoch: 26 [1760/7471 (24%)]\tLoss: 1535779.500000\n",
            "Train Epoch: 26 [1920/7471 (26%)]\tLoss: 1633128.125000\n",
            "Train Epoch: 26 [2080/7471 (28%)]\tLoss: 1631964.125000\n",
            "Train Epoch: 26 [2240/7471 (30%)]\tLoss: 1563158.625000\n",
            "Train Epoch: 26 [2400/7471 (32%)]\tLoss: 1591300.375000\n",
            "Train Epoch: 26 [2560/7471 (34%)]\tLoss: 1613517.375000\n",
            "Train Epoch: 26 [2720/7471 (36%)]\tLoss: 1595263.750000\n",
            "Train Epoch: 26 [2880/7471 (39%)]\tLoss: 1507680.750000\n",
            "Train Epoch: 26 [3040/7471 (41%)]\tLoss: 1588715.500000\n",
            "Train Epoch: 26 [3200/7471 (43%)]\tLoss: 1593561.625000\n",
            "Train Epoch: 26 [3360/7471 (45%)]\tLoss: 1592788.875000\n",
            "Train Epoch: 26 [3520/7471 (47%)]\tLoss: 1599587.375000\n",
            "Train Epoch: 26 [3680/7471 (49%)]\tLoss: 1549175.125000\n",
            "Train Epoch: 26 [3840/7471 (51%)]\tLoss: 1600797.875000\n",
            "Train Epoch: 26 [4000/7471 (54%)]\tLoss: 1587960.500000\n",
            "Train Epoch: 26 [4160/7471 (56%)]\tLoss: 1582829.250000\n",
            "Train Epoch: 26 [4320/7471 (58%)]\tLoss: 1596240.625000\n",
            "Train Epoch: 26 [4480/7471 (60%)]\tLoss: 1599618.125000\n",
            "Train Epoch: 26 [4640/7471 (62%)]\tLoss: 1604410.750000\n",
            "Train Epoch: 26 [4800/7471 (64%)]\tLoss: 1619898.625000\n",
            "Train Epoch: 26 [4960/7471 (66%)]\tLoss: 1634936.750000\n",
            "Train Epoch: 26 [5120/7471 (69%)]\tLoss: 1633808.000000\n",
            "Train Epoch: 26 [5280/7471 (71%)]\tLoss: 1527734.750000\n",
            "Train Epoch: 26 [5440/7471 (73%)]\tLoss: 1613764.500000\n",
            "Train Epoch: 26 [5600/7471 (75%)]\tLoss: 1590489.000000\n",
            "Train Epoch: 26 [5760/7471 (77%)]\tLoss: 1570831.250000\n",
            "Train Epoch: 26 [5920/7471 (79%)]\tLoss: 1580298.250000\n",
            "Train Epoch: 26 [6080/7471 (81%)]\tLoss: 1592188.000000\n",
            "Train Epoch: 26 [6240/7471 (84%)]\tLoss: 1639271.125000\n",
            "Train Epoch: 26 [6400/7471 (86%)]\tLoss: 1582390.625000\n",
            "Train Epoch: 26 [6560/7471 (88%)]\tLoss: 1567006.500000\n",
            "Train Epoch: 26 [6720/7471 (90%)]\tLoss: 1619691.375000\n",
            "Train Epoch: 26 [6880/7471 (92%)]\tLoss: 1600030.375000\n",
            "Train Epoch: 26 [7040/7471 (94%)]\tLoss: 1620915.375000\n",
            "Train Epoch: 26 [7200/7471 (96%)]\tLoss: 1566296.875000\n",
            "Train Epoch: 26 [7360/7471 (99%)]\tLoss: 1584645.625000\n",
            "Epoch 26 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 27 [160/7471 (2%)]\tLoss: 1548505.500000\n",
            "Train Epoch: 27 [320/7471 (4%)]\tLoss: 1638204.500000\n",
            "Train Epoch: 27 [480/7471 (6%)]\tLoss: 1638069.375000\n",
            "Train Epoch: 27 [640/7471 (9%)]\tLoss: 1577613.875000\n",
            "Train Epoch: 27 [800/7471 (11%)]\tLoss: 1614765.375000\n",
            "Train Epoch: 27 [960/7471 (13%)]\tLoss: 1640597.000000\n",
            "Train Epoch: 27 [1120/7471 (15%)]\tLoss: 1562803.250000\n",
            "Train Epoch: 27 [1280/7471 (17%)]\tLoss: 1624816.750000\n",
            "Train Epoch: 27 [1440/7471 (19%)]\tLoss: 1594742.750000\n",
            "Train Epoch: 27 [1600/7471 (21%)]\tLoss: 1603284.375000\n",
            "Train Epoch: 27 [1760/7471 (24%)]\tLoss: 1647574.125000\n",
            "Train Epoch: 27 [1920/7471 (26%)]\tLoss: 1622895.000000\n",
            "Train Epoch: 27 [2080/7471 (28%)]\tLoss: 1651681.375000\n",
            "Train Epoch: 27 [2240/7471 (30%)]\tLoss: 1578373.875000\n",
            "Train Epoch: 27 [2400/7471 (32%)]\tLoss: 1603808.500000\n",
            "Train Epoch: 27 [2560/7471 (34%)]\tLoss: 1620005.125000\n",
            "Train Epoch: 27 [2720/7471 (36%)]\tLoss: 1579744.375000\n",
            "Train Epoch: 27 [2880/7471 (39%)]\tLoss: 1591931.625000\n",
            "Train Epoch: 27 [3040/7471 (41%)]\tLoss: 1605574.000000\n",
            "Train Epoch: 27 [3200/7471 (43%)]\tLoss: 1567395.375000\n",
            "Train Epoch: 27 [3360/7471 (45%)]\tLoss: 1612714.750000\n",
            "Train Epoch: 27 [3520/7471 (47%)]\tLoss: 1572816.500000\n",
            "Train Epoch: 27 [3680/7471 (49%)]\tLoss: 1536642.750000\n",
            "Train Epoch: 27 [3840/7471 (51%)]\tLoss: 1471127.500000\n",
            "Train Epoch: 27 [4000/7471 (54%)]\tLoss: 1515883.375000\n",
            "Train Epoch: 27 [4160/7471 (56%)]\tLoss: 1562123.625000\n",
            "Train Epoch: 27 [4320/7471 (58%)]\tLoss: 1616099.250000\n",
            "Train Epoch: 27 [4480/7471 (60%)]\tLoss: 1547074.000000\n",
            "Train Epoch: 27 [4640/7471 (62%)]\tLoss: 1546451.500000\n",
            "Train Epoch: 27 [4800/7471 (64%)]\tLoss: 1603909.750000\n",
            "Train Epoch: 27 [4960/7471 (66%)]\tLoss: 1613655.250000\n",
            "Train Epoch: 27 [5120/7471 (69%)]\tLoss: 1534848.125000\n",
            "Train Epoch: 27 [5280/7471 (71%)]\tLoss: 1560816.250000\n",
            "Train Epoch: 27 [5440/7471 (73%)]\tLoss: 1563315.375000\n",
            "Train Epoch: 27 [5600/7471 (75%)]\tLoss: 1586976.500000\n",
            "Train Epoch: 27 [5760/7471 (77%)]\tLoss: 1590745.875000\n",
            "Train Epoch: 27 [5920/7471 (79%)]\tLoss: 1580414.125000\n",
            "Train Epoch: 27 [6080/7471 (81%)]\tLoss: 1589181.625000\n",
            "Train Epoch: 27 [6240/7471 (84%)]\tLoss: 1615162.750000\n",
            "Train Epoch: 27 [6400/7471 (86%)]\tLoss: 1558023.750000\n",
            "Train Epoch: 27 [6560/7471 (88%)]\tLoss: 1572974.250000\n",
            "Train Epoch: 27 [6720/7471 (90%)]\tLoss: 1562101.000000\n",
            "Train Epoch: 27 [6880/7471 (92%)]\tLoss: 1588747.750000\n",
            "Train Epoch: 27 [7040/7471 (94%)]\tLoss: 1582402.750000\n",
            "Train Epoch: 27 [7200/7471 (96%)]\tLoss: 1643842.625000\n",
            "Train Epoch: 27 [7360/7471 (99%)]\tLoss: 1574922.250000\n",
            "Epoch 27 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 28 [160/7471 (2%)]\tLoss: 1552265.875000\n",
            "Train Epoch: 28 [320/7471 (4%)]\tLoss: 1597789.500000\n",
            "Train Epoch: 28 [480/7471 (6%)]\tLoss: 1631642.375000\n",
            "Train Epoch: 28 [640/7471 (9%)]\tLoss: 1616043.500000\n",
            "Train Epoch: 28 [800/7471 (11%)]\tLoss: 1589985.750000\n",
            "Train Epoch: 28 [960/7471 (13%)]\tLoss: 1567141.375000\n",
            "Train Epoch: 28 [1120/7471 (15%)]\tLoss: 1607521.750000\n",
            "Train Epoch: 28 [1280/7471 (17%)]\tLoss: 1498058.875000\n",
            "Train Epoch: 28 [1440/7471 (19%)]\tLoss: 1492814.625000\n",
            "Train Epoch: 28 [1600/7471 (21%)]\tLoss: 1573812.125000\n",
            "Train Epoch: 28 [1760/7471 (24%)]\tLoss: 1508839.625000\n",
            "Train Epoch: 28 [1920/7471 (26%)]\tLoss: 1597981.000000\n",
            "Train Epoch: 28 [2080/7471 (28%)]\tLoss: 1556848.125000\n",
            "Train Epoch: 28 [2240/7471 (30%)]\tLoss: 1566678.750000\n",
            "Train Epoch: 28 [2400/7471 (32%)]\tLoss: 1514197.250000\n",
            "Train Epoch: 28 [2560/7471 (34%)]\tLoss: 1532758.875000\n",
            "Train Epoch: 28 [2720/7471 (36%)]\tLoss: 1580755.875000\n",
            "Train Epoch: 28 [2880/7471 (39%)]\tLoss: 1622000.875000\n",
            "Train Epoch: 28 [3040/7471 (41%)]\tLoss: 1566369.125000\n",
            "Train Epoch: 28 [3200/7471 (43%)]\tLoss: 1563621.000000\n",
            "Train Epoch: 28 [3360/7471 (45%)]\tLoss: 1543423.500000\n",
            "Train Epoch: 28 [3520/7471 (47%)]\tLoss: 1588647.625000\n",
            "Train Epoch: 28 [3680/7471 (49%)]\tLoss: 1587070.625000\n",
            "Train Epoch: 28 [3840/7471 (51%)]\tLoss: 1610230.125000\n",
            "Train Epoch: 28 [4000/7471 (54%)]\tLoss: 1628174.375000\n",
            "Train Epoch: 28 [4160/7471 (56%)]\tLoss: 1586573.750000\n",
            "Train Epoch: 28 [4320/7471 (58%)]\tLoss: 1653734.875000\n",
            "Train Epoch: 28 [4480/7471 (60%)]\tLoss: 1557531.250000\n",
            "Train Epoch: 28 [4640/7471 (62%)]\tLoss: 1568882.500000\n",
            "Train Epoch: 28 [4800/7471 (64%)]\tLoss: 1613395.875000\n",
            "Train Epoch: 28 [4960/7471 (66%)]\tLoss: 1566433.625000\n",
            "Train Epoch: 28 [5120/7471 (69%)]\tLoss: 1546214.625000\n",
            "Train Epoch: 28 [5280/7471 (71%)]\tLoss: 1558105.250000\n",
            "Train Epoch: 28 [5440/7471 (73%)]\tLoss: 1502504.500000\n",
            "Train Epoch: 28 [5600/7471 (75%)]\tLoss: 1627041.500000\n",
            "Train Epoch: 28 [5760/7471 (77%)]\tLoss: 1589691.375000\n",
            "Train Epoch: 28 [5920/7471 (79%)]\tLoss: 1619381.250000\n",
            "Train Epoch: 28 [6080/7471 (81%)]\tLoss: 1562801.000000\n",
            "Train Epoch: 28 [6240/7471 (84%)]\tLoss: 1612377.750000\n",
            "Train Epoch: 28 [6400/7471 (86%)]\tLoss: 1566282.500000\n",
            "Train Epoch: 28 [6560/7471 (88%)]\tLoss: 1524566.000000\n",
            "Train Epoch: 28 [6720/7471 (90%)]\tLoss: 1536252.250000\n",
            "Train Epoch: 28 [6880/7471 (92%)]\tLoss: 1550928.625000\n",
            "Train Epoch: 28 [7040/7471 (94%)]\tLoss: 1535884.375000\n",
            "Train Epoch: 28 [7200/7471 (96%)]\tLoss: 1557496.375000\n",
            "Train Epoch: 28 [7360/7471 (99%)]\tLoss: 1593716.750000\n",
            "Epoch 28 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 29 [160/7471 (2%)]\tLoss: 1631484.250000\n",
            "Train Epoch: 29 [320/7471 (4%)]\tLoss: 1612445.750000\n",
            "Train Epoch: 29 [480/7471 (6%)]\tLoss: 1563539.500000\n",
            "Train Epoch: 29 [640/7471 (9%)]\tLoss: 1552536.250000\n",
            "Train Epoch: 29 [800/7471 (11%)]\tLoss: 1537642.125000\n",
            "Train Epoch: 29 [960/7471 (13%)]\tLoss: 1644382.250000\n",
            "Train Epoch: 29 [1120/7471 (15%)]\tLoss: 1638512.125000\n",
            "Train Epoch: 29 [1280/7471 (17%)]\tLoss: 1552298.500000\n",
            "Train Epoch: 29 [1440/7471 (19%)]\tLoss: 1559952.500000\n",
            "Train Epoch: 29 [1600/7471 (21%)]\tLoss: 1581292.500000\n",
            "Train Epoch: 29 [1760/7471 (24%)]\tLoss: 1552098.500000\n",
            "Train Epoch: 29 [1920/7471 (26%)]\tLoss: 1542109.375000\n",
            "Train Epoch: 29 [2080/7471 (28%)]\tLoss: 1601541.500000\n",
            "Train Epoch: 29 [2240/7471 (30%)]\tLoss: 1610233.750000\n",
            "Train Epoch: 29 [2400/7471 (32%)]\tLoss: 1572339.625000\n",
            "Train Epoch: 29 [2560/7471 (34%)]\tLoss: 1564902.500000\n",
            "Train Epoch: 29 [2720/7471 (36%)]\tLoss: 1612492.625000\n",
            "Train Epoch: 29 [2880/7471 (39%)]\tLoss: 1534305.625000\n",
            "Train Epoch: 29 [3040/7471 (41%)]\tLoss: 1575177.125000\n",
            "Train Epoch: 29 [3200/7471 (43%)]\tLoss: 1623259.125000\n",
            "Train Epoch: 29 [3360/7471 (45%)]\tLoss: 1573879.250000\n",
            "Train Epoch: 29 [3520/7471 (47%)]\tLoss: 1594026.000000\n",
            "Train Epoch: 29 [3680/7471 (49%)]\tLoss: 1604019.250000\n",
            "Train Epoch: 29 [3840/7471 (51%)]\tLoss: 1599413.250000\n",
            "Train Epoch: 29 [4000/7471 (54%)]\tLoss: 1551970.500000\n",
            "Train Epoch: 29 [4160/7471 (56%)]\tLoss: 1626112.125000\n",
            "Train Epoch: 29 [4320/7471 (58%)]\tLoss: 1552315.250000\n",
            "Train Epoch: 29 [4480/7471 (60%)]\tLoss: 1593775.250000\n",
            "Train Epoch: 29 [4640/7471 (62%)]\tLoss: 1617416.125000\n",
            "Train Epoch: 29 [4800/7471 (64%)]\tLoss: 1601348.375000\n",
            "Train Epoch: 29 [4960/7471 (66%)]\tLoss: 1532113.875000\n",
            "Train Epoch: 29 [5120/7471 (69%)]\tLoss: 1538123.375000\n",
            "Train Epoch: 29 [5280/7471 (71%)]\tLoss: 1615191.000000\n",
            "Train Epoch: 29 [5440/7471 (73%)]\tLoss: 1569751.250000\n",
            "Train Epoch: 29 [5600/7471 (75%)]\tLoss: 1635807.750000\n",
            "Train Epoch: 29 [5760/7471 (77%)]\tLoss: 1568726.750000\n",
            "Train Epoch: 29 [5920/7471 (79%)]\tLoss: 1598137.250000\n",
            "Train Epoch: 29 [6080/7471 (81%)]\tLoss: 1615723.250000\n",
            "Train Epoch: 29 [6240/7471 (84%)]\tLoss: 1554051.000000\n",
            "Train Epoch: 29 [6400/7471 (86%)]\tLoss: 1491685.750000\n",
            "Train Epoch: 29 [6560/7471 (88%)]\tLoss: 1635252.500000\n",
            "Train Epoch: 29 [6720/7471 (90%)]\tLoss: 1623542.250000\n",
            "Train Epoch: 29 [6880/7471 (92%)]\tLoss: 1589533.875000\n",
            "Train Epoch: 29 [7040/7471 (94%)]\tLoss: 1602172.250000\n",
            "Train Epoch: 29 [7200/7471 (96%)]\tLoss: 1558970.750000\n",
            "Train Epoch: 29 [7360/7471 (99%)]\tLoss: 1597274.750000\n",
            "Epoch 29 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 30 [160/7471 (2%)]\tLoss: 1578465.750000\n",
            "Train Epoch: 30 [320/7471 (4%)]\tLoss: 1618643.000000\n",
            "Train Epoch: 30 [480/7471 (6%)]\tLoss: 1631933.125000\n",
            "Train Epoch: 30 [640/7471 (9%)]\tLoss: 1548556.500000\n",
            "Train Epoch: 30 [800/7471 (11%)]\tLoss: 1568497.500000\n",
            "Train Epoch: 30 [960/7471 (13%)]\tLoss: 1532670.750000\n",
            "Train Epoch: 30 [1120/7471 (15%)]\tLoss: 1541585.375000\n",
            "Train Epoch: 30 [1280/7471 (17%)]\tLoss: 1574575.125000\n",
            "Train Epoch: 30 [1440/7471 (19%)]\tLoss: 1539603.375000\n",
            "Train Epoch: 30 [1600/7471 (21%)]\tLoss: 1592164.875000\n",
            "Train Epoch: 30 [1760/7471 (24%)]\tLoss: 1591782.625000\n",
            "Train Epoch: 30 [1920/7471 (26%)]\tLoss: 1624378.250000\n",
            "Train Epoch: 30 [2080/7471 (28%)]\tLoss: 1566449.500000\n",
            "Train Epoch: 30 [2240/7471 (30%)]\tLoss: 1550551.750000\n",
            "Train Epoch: 30 [2400/7471 (32%)]\tLoss: 1505334.875000\n",
            "Train Epoch: 30 [2560/7471 (34%)]\tLoss: 1574010.375000\n",
            "Train Epoch: 30 [2720/7471 (36%)]\tLoss: 1608979.625000\n",
            "Train Epoch: 30 [2880/7471 (39%)]\tLoss: 1561140.125000\n",
            "Train Epoch: 30 [3040/7471 (41%)]\tLoss: 1603429.375000\n",
            "Train Epoch: 30 [3200/7471 (43%)]\tLoss: 1570827.125000\n",
            "Train Epoch: 30 [3360/7471 (45%)]\tLoss: 1544406.375000\n",
            "Train Epoch: 30 [3520/7471 (47%)]\tLoss: 1596345.125000\n",
            "Train Epoch: 30 [3680/7471 (49%)]\tLoss: 1563632.250000\n",
            "Train Epoch: 30 [3840/7471 (51%)]\tLoss: 1629888.750000\n",
            "Train Epoch: 30 [4000/7471 (54%)]\tLoss: 1587790.500000\n",
            "Train Epoch: 30 [4160/7471 (56%)]\tLoss: 1571991.000000\n",
            "Train Epoch: 30 [4320/7471 (58%)]\tLoss: 1513766.750000\n",
            "Train Epoch: 30 [4480/7471 (60%)]\tLoss: 1604342.125000\n",
            "Train Epoch: 30 [4640/7471 (62%)]\tLoss: 1568022.625000\n",
            "Train Epoch: 30 [4800/7471 (64%)]\tLoss: 1601851.500000\n",
            "Train Epoch: 30 [4960/7471 (66%)]\tLoss: 1647777.875000\n",
            "Train Epoch: 30 [5120/7471 (69%)]\tLoss: 1556865.875000\n",
            "Train Epoch: 30 [5280/7471 (71%)]\tLoss: 1527313.750000\n",
            "Train Epoch: 30 [5440/7471 (73%)]\tLoss: 1570404.750000\n",
            "Train Epoch: 30 [5600/7471 (75%)]\tLoss: 1575758.875000\n",
            "Train Epoch: 30 [5760/7471 (77%)]\tLoss: 1590149.000000\n",
            "Train Epoch: 30 [5920/7471 (79%)]\tLoss: 1612281.500000\n",
            "Train Epoch: 30 [6080/7471 (81%)]\tLoss: 1529396.750000\n",
            "Train Epoch: 30 [6240/7471 (84%)]\tLoss: 1612901.875000\n",
            "Train Epoch: 30 [6400/7471 (86%)]\tLoss: 1551288.125000\n",
            "Train Epoch: 30 [6560/7471 (88%)]\tLoss: 1644063.750000\n",
            "Train Epoch: 30 [6720/7471 (90%)]\tLoss: 1521367.500000\n",
            "Train Epoch: 30 [6880/7471 (92%)]\tLoss: 1614062.375000\n",
            "Train Epoch: 30 [7040/7471 (94%)]\tLoss: 1604467.500000\n",
            "Train Epoch: 30 [7200/7471 (96%)]\tLoss: 1628192.500000\n",
            "Train Epoch: 30 [7360/7471 (99%)]\tLoss: 1553088.875000\n",
            "Epoch 30 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 31 [160/7471 (2%)]\tLoss: 1638215.000000\n",
            "Train Epoch: 31 [320/7471 (4%)]\tLoss: 1625368.125000\n",
            "Train Epoch: 31 [480/7471 (6%)]\tLoss: 1595051.875000\n",
            "Train Epoch: 31 [640/7471 (9%)]\tLoss: 1626051.875000\n",
            "Train Epoch: 31 [800/7471 (11%)]\tLoss: 1545758.500000\n",
            "Train Epoch: 31 [960/7471 (13%)]\tLoss: 1561526.875000\n",
            "Train Epoch: 31 [1120/7471 (15%)]\tLoss: 1560966.250000\n",
            "Train Epoch: 31 [1280/7471 (17%)]\tLoss: 1557192.000000\n",
            "Train Epoch: 31 [1440/7471 (19%)]\tLoss: 1571599.250000\n",
            "Train Epoch: 31 [1600/7471 (21%)]\tLoss: 1533409.375000\n",
            "Train Epoch: 31 [1760/7471 (24%)]\tLoss: 1601587.000000\n",
            "Train Epoch: 31 [1920/7471 (26%)]\tLoss: 1579533.000000\n",
            "Train Epoch: 31 [2080/7471 (28%)]\tLoss: 1557205.125000\n",
            "Train Epoch: 31 [2240/7471 (30%)]\tLoss: 1594633.125000\n",
            "Train Epoch: 31 [2400/7471 (32%)]\tLoss: 1557207.875000\n",
            "Train Epoch: 31 [2560/7471 (34%)]\tLoss: 1586236.625000\n",
            "Train Epoch: 31 [2720/7471 (36%)]\tLoss: 1634816.000000\n",
            "Train Epoch: 31 [2880/7471 (39%)]\tLoss: 1611049.250000\n",
            "Train Epoch: 31 [3040/7471 (41%)]\tLoss: 1572779.000000\n",
            "Train Epoch: 31 [3200/7471 (43%)]\tLoss: 1571362.875000\n",
            "Train Epoch: 31 [3360/7471 (45%)]\tLoss: 1553106.875000\n",
            "Train Epoch: 31 [3520/7471 (47%)]\tLoss: 1562040.750000\n",
            "Train Epoch: 31 [3680/7471 (49%)]\tLoss: 1627640.625000\n",
            "Train Epoch: 31 [3840/7471 (51%)]\tLoss: 1563142.875000\n",
            "Train Epoch: 31 [4000/7471 (54%)]\tLoss: 1574080.250000\n",
            "Train Epoch: 31 [4160/7471 (56%)]\tLoss: 1544412.000000\n",
            "Train Epoch: 31 [4320/7471 (58%)]\tLoss: 1523018.875000\n",
            "Train Epoch: 31 [4480/7471 (60%)]\tLoss: 1597814.000000\n",
            "Train Epoch: 31 [4640/7471 (62%)]\tLoss: 1605341.875000\n",
            "Train Epoch: 31 [4800/7471 (64%)]\tLoss: 1594135.250000\n",
            "Train Epoch: 31 [4960/7471 (66%)]\tLoss: 1553531.875000\n",
            "Train Epoch: 31 [5120/7471 (69%)]\tLoss: 1556373.250000\n",
            "Train Epoch: 31 [5280/7471 (71%)]\tLoss: 1635660.500000\n",
            "Train Epoch: 31 [5440/7471 (73%)]\tLoss: 1599184.375000\n",
            "Train Epoch: 31 [5600/7471 (75%)]\tLoss: 1611203.000000\n",
            "Train Epoch: 31 [5760/7471 (77%)]\tLoss: 1587424.875000\n",
            "Train Epoch: 31 [5920/7471 (79%)]\tLoss: 1599029.750000\n",
            "Train Epoch: 31 [6080/7471 (81%)]\tLoss: 1590155.500000\n",
            "Train Epoch: 31 [6240/7471 (84%)]\tLoss: 1612477.625000\n",
            "Train Epoch: 31 [6400/7471 (86%)]\tLoss: 1609666.625000\n",
            "Train Epoch: 31 [6560/7471 (88%)]\tLoss: 1555545.875000\n",
            "Train Epoch: 31 [6720/7471 (90%)]\tLoss: 1546606.375000\n",
            "Train Epoch: 31 [6880/7471 (92%)]\tLoss: 1594124.125000\n",
            "Train Epoch: 31 [7040/7471 (94%)]\tLoss: 1580784.875000\n",
            "Train Epoch: 31 [7200/7471 (96%)]\tLoss: 1610844.000000\n",
            "Train Epoch: 31 [7360/7471 (99%)]\tLoss: 1599699.500000\n",
            "Epoch 31 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 32 [160/7471 (2%)]\tLoss: 1583772.000000\n",
            "Train Epoch: 32 [320/7471 (4%)]\tLoss: 1542797.375000\n",
            "Train Epoch: 32 [480/7471 (6%)]\tLoss: 1559996.250000\n",
            "Train Epoch: 32 [640/7471 (9%)]\tLoss: 1512281.875000\n",
            "Train Epoch: 32 [800/7471 (11%)]\tLoss: 1598265.875000\n",
            "Train Epoch: 32 [960/7471 (13%)]\tLoss: 1620449.375000\n",
            "Train Epoch: 32 [1120/7471 (15%)]\tLoss: 1566640.875000\n",
            "Train Epoch: 32 [1280/7471 (17%)]\tLoss: 1614038.250000\n",
            "Train Epoch: 32 [1440/7471 (19%)]\tLoss: 1626531.750000\n",
            "Train Epoch: 32 [1600/7471 (21%)]\tLoss: 1562486.250000\n",
            "Train Epoch: 32 [1760/7471 (24%)]\tLoss: 1531558.000000\n",
            "Train Epoch: 32 [1920/7471 (26%)]\tLoss: 1574842.875000\n",
            "Train Epoch: 32 [2080/7471 (28%)]\tLoss: 1576484.750000\n",
            "Train Epoch: 32 [2240/7471 (30%)]\tLoss: 1592002.750000\n",
            "Train Epoch: 32 [2400/7471 (32%)]\tLoss: 1605065.750000\n",
            "Train Epoch: 32 [2560/7471 (34%)]\tLoss: 1593841.250000\n",
            "Train Epoch: 32 [2720/7471 (36%)]\tLoss: 1621075.250000\n",
            "Train Epoch: 32 [2880/7471 (39%)]\tLoss: 1583533.375000\n",
            "Train Epoch: 32 [3040/7471 (41%)]\tLoss: 1600093.500000\n",
            "Train Epoch: 32 [3200/7471 (43%)]\tLoss: 1619037.375000\n",
            "Train Epoch: 32 [3360/7471 (45%)]\tLoss: 1621077.750000\n",
            "Train Epoch: 32 [3520/7471 (47%)]\tLoss: 1608164.000000\n",
            "Train Epoch: 32 [3680/7471 (49%)]\tLoss: 1631745.875000\n",
            "Train Epoch: 32 [3840/7471 (51%)]\tLoss: 1602874.750000\n",
            "Train Epoch: 32 [4000/7471 (54%)]\tLoss: 1497706.375000\n",
            "Train Epoch: 32 [4160/7471 (56%)]\tLoss: 1608600.250000\n",
            "Train Epoch: 32 [4320/7471 (58%)]\tLoss: 1604146.375000\n",
            "Train Epoch: 32 [4480/7471 (60%)]\tLoss: 1626866.500000\n",
            "Train Epoch: 32 [4640/7471 (62%)]\tLoss: 1624606.375000\n",
            "Train Epoch: 32 [4800/7471 (64%)]\tLoss: 1569519.000000\n",
            "Train Epoch: 32 [4960/7471 (66%)]\tLoss: 1577886.500000\n",
            "Train Epoch: 32 [5120/7471 (69%)]\tLoss: 1616102.250000\n",
            "Train Epoch: 32 [5280/7471 (71%)]\tLoss: 1640917.875000\n",
            "Train Epoch: 32 [5440/7471 (73%)]\tLoss: 1584856.750000\n",
            "Train Epoch: 32 [5600/7471 (75%)]\tLoss: 1613092.625000\n",
            "Train Epoch: 32 [5760/7471 (77%)]\tLoss: 1576291.250000\n",
            "Train Epoch: 32 [5920/7471 (79%)]\tLoss: 1539006.375000\n",
            "Train Epoch: 32 [6080/7471 (81%)]\tLoss: 1595043.625000\n",
            "Train Epoch: 32 [6240/7471 (84%)]\tLoss: 1589357.125000\n",
            "Train Epoch: 32 [6400/7471 (86%)]\tLoss: 1557485.250000\n",
            "Train Epoch: 32 [6560/7471 (88%)]\tLoss: 1600017.000000\n",
            "Train Epoch: 32 [6720/7471 (90%)]\tLoss: 1594668.500000\n",
            "Train Epoch: 32 [6880/7471 (92%)]\tLoss: 1597166.375000\n",
            "Train Epoch: 32 [7040/7471 (94%)]\tLoss: 1587653.750000\n",
            "Train Epoch: 32 [7200/7471 (96%)]\tLoss: 1571494.125000\n",
            "Train Epoch: 32 [7360/7471 (99%)]\tLoss: 1612121.250000\n",
            "Epoch 32 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 33 [160/7471 (2%)]\tLoss: 1614972.125000\n",
            "Train Epoch: 33 [320/7471 (4%)]\tLoss: 1591135.375000\n",
            "Train Epoch: 33 [480/7471 (6%)]\tLoss: 1601891.750000\n",
            "Train Epoch: 33 [640/7471 (9%)]\tLoss: 1575779.375000\n",
            "Train Epoch: 33 [800/7471 (11%)]\tLoss: 1570269.250000\n",
            "Train Epoch: 33 [960/7471 (13%)]\tLoss: 1510083.000000\n",
            "Train Epoch: 33 [1120/7471 (15%)]\tLoss: 1551091.625000\n",
            "Train Epoch: 33 [1280/7471 (17%)]\tLoss: 1490609.250000\n",
            "Train Epoch: 33 [1440/7471 (19%)]\tLoss: 1620044.250000\n",
            "Train Epoch: 33 [1600/7471 (21%)]\tLoss: 1478997.875000\n",
            "Train Epoch: 33 [1760/7471 (24%)]\tLoss: 1585512.750000\n",
            "Train Epoch: 33 [1920/7471 (26%)]\tLoss: 1641792.625000\n",
            "Train Epoch: 33 [2080/7471 (28%)]\tLoss: 1552723.250000\n",
            "Train Epoch: 33 [2240/7471 (30%)]\tLoss: 1618048.375000\n",
            "Train Epoch: 33 [2400/7471 (32%)]\tLoss: 1598132.875000\n",
            "Train Epoch: 33 [2560/7471 (34%)]\tLoss: 1568924.625000\n",
            "Train Epoch: 33 [2720/7471 (36%)]\tLoss: 1569725.875000\n",
            "Train Epoch: 33 [2880/7471 (39%)]\tLoss: 1559426.500000\n",
            "Train Epoch: 33 [3040/7471 (41%)]\tLoss: 1534306.875000\n",
            "Train Epoch: 33 [3200/7471 (43%)]\tLoss: 1616234.625000\n",
            "Train Epoch: 33 [3360/7471 (45%)]\tLoss: 1578816.750000\n",
            "Train Epoch: 33 [3520/7471 (47%)]\tLoss: 1550154.250000\n",
            "Train Epoch: 33 [3680/7471 (49%)]\tLoss: 1641933.375000\n",
            "Train Epoch: 33 [3840/7471 (51%)]\tLoss: 1573523.375000\n",
            "Train Epoch: 33 [4000/7471 (54%)]\tLoss: 1597623.500000\n",
            "Train Epoch: 33 [4160/7471 (56%)]\tLoss: 1553280.875000\n",
            "Train Epoch: 33 [4320/7471 (58%)]\tLoss: 1622427.375000\n",
            "Train Epoch: 33 [4480/7471 (60%)]\tLoss: 1521105.375000\n",
            "Train Epoch: 33 [4640/7471 (62%)]\tLoss: 1623031.875000\n",
            "Train Epoch: 33 [4800/7471 (64%)]\tLoss: 1627304.750000\n",
            "Train Epoch: 33 [4960/7471 (66%)]\tLoss: 1564096.750000\n",
            "Train Epoch: 33 [5120/7471 (69%)]\tLoss: 1562700.250000\n",
            "Train Epoch: 33 [5280/7471 (71%)]\tLoss: 1572657.750000\n",
            "Train Epoch: 33 [5440/7471 (73%)]\tLoss: 1563743.375000\n",
            "Train Epoch: 33 [5600/7471 (75%)]\tLoss: 1620823.500000\n",
            "Train Epoch: 33 [5760/7471 (77%)]\tLoss: 1590950.875000\n",
            "Train Epoch: 33 [5920/7471 (79%)]\tLoss: 1620925.750000\n",
            "Train Epoch: 33 [6080/7471 (81%)]\tLoss: 1599591.000000\n",
            "Train Epoch: 33 [6240/7471 (84%)]\tLoss: 1611338.125000\n",
            "Train Epoch: 33 [6400/7471 (86%)]\tLoss: 1538228.500000\n",
            "Train Epoch: 33 [6560/7471 (88%)]\tLoss: 1613924.000000\n",
            "Train Epoch: 33 [6720/7471 (90%)]\tLoss: 1591221.875000\n",
            "Train Epoch: 33 [6880/7471 (92%)]\tLoss: 1578477.500000\n",
            "Train Epoch: 33 [7040/7471 (94%)]\tLoss: 1560292.375000\n",
            "Train Epoch: 33 [7200/7471 (96%)]\tLoss: 1617221.250000\n",
            "Train Epoch: 33 [7360/7471 (99%)]\tLoss: 1592354.250000\n",
            "Epoch 33 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 34 [160/7471 (2%)]\tLoss: 1610135.250000\n",
            "Train Epoch: 34 [320/7471 (4%)]\tLoss: 1587111.250000\n",
            "Train Epoch: 34 [480/7471 (6%)]\tLoss: 1583014.000000\n",
            "Train Epoch: 34 [640/7471 (9%)]\tLoss: 1561543.750000\n",
            "Train Epoch: 34 [800/7471 (11%)]\tLoss: 1587553.750000\n",
            "Train Epoch: 34 [960/7471 (13%)]\tLoss: 1625151.625000\n",
            "Train Epoch: 34 [1120/7471 (15%)]\tLoss: 1570262.625000\n",
            "Train Epoch: 34 [1280/7471 (17%)]\tLoss: 1584233.875000\n",
            "Train Epoch: 34 [1440/7471 (19%)]\tLoss: 1588959.125000\n",
            "Train Epoch: 34 [1600/7471 (21%)]\tLoss: 1590838.875000\n",
            "Train Epoch: 34 [1760/7471 (24%)]\tLoss: 1564437.875000\n",
            "Train Epoch: 34 [1920/7471 (26%)]\tLoss: 1620543.625000\n",
            "Train Epoch: 34 [2080/7471 (28%)]\tLoss: 1582874.875000\n",
            "Train Epoch: 34 [2240/7471 (30%)]\tLoss: 1603756.125000\n",
            "Train Epoch: 34 [2400/7471 (32%)]\tLoss: 1633785.875000\n",
            "Train Epoch: 34 [2560/7471 (34%)]\tLoss: 1635469.125000\n",
            "Train Epoch: 34 [2720/7471 (36%)]\tLoss: 1578507.750000\n",
            "Train Epoch: 34 [2880/7471 (39%)]\tLoss: 1527988.000000\n",
            "Train Epoch: 34 [3040/7471 (41%)]\tLoss: 1633098.375000\n",
            "Train Epoch: 34 [3200/7471 (43%)]\tLoss: 1609212.375000\n",
            "Train Epoch: 34 [3360/7471 (45%)]\tLoss: 1583686.000000\n",
            "Train Epoch: 34 [3520/7471 (47%)]\tLoss: 1552323.500000\n",
            "Train Epoch: 34 [3680/7471 (49%)]\tLoss: 1567115.500000\n",
            "Train Epoch: 34 [3840/7471 (51%)]\tLoss: 1619882.875000\n",
            "Train Epoch: 34 [4000/7471 (54%)]\tLoss: 1544029.250000\n",
            "Train Epoch: 34 [4160/7471 (56%)]\tLoss: 1628577.125000\n",
            "Train Epoch: 34 [4320/7471 (58%)]\tLoss: 1589134.375000\n",
            "Train Epoch: 34 [4480/7471 (60%)]\tLoss: 1622194.125000\n",
            "Train Epoch: 34 [4640/7471 (62%)]\tLoss: 1564861.750000\n",
            "Train Epoch: 34 [4800/7471 (64%)]\tLoss: 1554570.375000\n",
            "Train Epoch: 34 [4960/7471 (66%)]\tLoss: 1533003.875000\n",
            "Train Epoch: 34 [5120/7471 (69%)]\tLoss: 1577700.625000\n",
            "Train Epoch: 34 [5280/7471 (71%)]\tLoss: 1466727.000000\n",
            "Train Epoch: 34 [5440/7471 (73%)]\tLoss: 1564732.625000\n",
            "Train Epoch: 34 [5600/7471 (75%)]\tLoss: 1578875.375000\n",
            "Train Epoch: 34 [5760/7471 (77%)]\tLoss: 1599308.750000\n",
            "Train Epoch: 34 [5920/7471 (79%)]\tLoss: 1595599.625000\n",
            "Train Epoch: 34 [6080/7471 (81%)]\tLoss: 1603304.250000\n",
            "Train Epoch: 34 [6240/7471 (84%)]\tLoss: 1548152.625000\n",
            "Train Epoch: 34 [6400/7471 (86%)]\tLoss: 1614805.750000\n",
            "Train Epoch: 34 [6560/7471 (88%)]\tLoss: 1566336.250000\n",
            "Train Epoch: 34 [6720/7471 (90%)]\tLoss: 1536119.750000\n",
            "Train Epoch: 34 [6880/7471 (92%)]\tLoss: 1553986.500000\n",
            "Train Epoch: 34 [7040/7471 (94%)]\tLoss: 1591920.750000\n",
            "Train Epoch: 34 [7200/7471 (96%)]\tLoss: 1618896.500000\n",
            "Train Epoch: 34 [7360/7471 (99%)]\tLoss: 1595491.000000\n",
            "Epoch 34 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 35 [160/7471 (2%)]\tLoss: 1602654.000000\n",
            "Train Epoch: 35 [320/7471 (4%)]\tLoss: 1593845.875000\n",
            "Train Epoch: 35 [480/7471 (6%)]\tLoss: 1566622.875000\n",
            "Train Epoch: 35 [640/7471 (9%)]\tLoss: 1567717.000000\n",
            "Train Epoch: 35 [800/7471 (11%)]\tLoss: 1567530.875000\n",
            "Train Epoch: 35 [960/7471 (13%)]\tLoss: 1599161.625000\n",
            "Train Epoch: 35 [1120/7471 (15%)]\tLoss: 1542673.875000\n",
            "Train Epoch: 35 [1280/7471 (17%)]\tLoss: 1575246.875000\n",
            "Train Epoch: 35 [1440/7471 (19%)]\tLoss: 1572496.375000\n",
            "Train Epoch: 35 [1600/7471 (21%)]\tLoss: 1614162.000000\n",
            "Train Epoch: 35 [1760/7471 (24%)]\tLoss: 1567744.750000\n",
            "Train Epoch: 35 [1920/7471 (26%)]\tLoss: 1569052.250000\n",
            "Train Epoch: 35 [2080/7471 (28%)]\tLoss: 1572815.000000\n",
            "Train Epoch: 35 [2240/7471 (30%)]\tLoss: 1603309.000000\n",
            "Train Epoch: 35 [2400/7471 (32%)]\tLoss: 1550178.250000\n",
            "Train Epoch: 35 [2560/7471 (34%)]\tLoss: 1574753.250000\n",
            "Train Epoch: 35 [2720/7471 (36%)]\tLoss: 1589648.500000\n",
            "Train Epoch: 35 [2880/7471 (39%)]\tLoss: 1584663.500000\n",
            "Train Epoch: 35 [3040/7471 (41%)]\tLoss: 1621052.625000\n",
            "Train Epoch: 35 [3200/7471 (43%)]\tLoss: 1599831.750000\n",
            "Train Epoch: 35 [3360/7471 (45%)]\tLoss: 1612055.750000\n",
            "Train Epoch: 35 [3520/7471 (47%)]\tLoss: 1633655.375000\n",
            "Train Epoch: 35 [3680/7471 (49%)]\tLoss: 1559772.875000\n",
            "Train Epoch: 35 [3840/7471 (51%)]\tLoss: 1569829.125000\n",
            "Train Epoch: 35 [4000/7471 (54%)]\tLoss: 1554366.500000\n",
            "Train Epoch: 35 [4160/7471 (56%)]\tLoss: 1643785.250000\n",
            "Train Epoch: 35 [4320/7471 (58%)]\tLoss: 1549367.750000\n",
            "Train Epoch: 35 [4480/7471 (60%)]\tLoss: 1553232.875000\n",
            "Train Epoch: 35 [4640/7471 (62%)]\tLoss: 1572254.500000\n",
            "Train Epoch: 35 [4800/7471 (64%)]\tLoss: 1639961.625000\n",
            "Train Epoch: 35 [4960/7471 (66%)]\tLoss: 1561066.250000\n",
            "Train Epoch: 35 [5120/7471 (69%)]\tLoss: 1584760.875000\n",
            "Train Epoch: 35 [5280/7471 (71%)]\tLoss: 1500592.500000\n",
            "Train Epoch: 35 [5440/7471 (73%)]\tLoss: 1563905.750000\n",
            "Train Epoch: 35 [5600/7471 (75%)]\tLoss: 1563564.250000\n",
            "Train Epoch: 35 [5760/7471 (77%)]\tLoss: 1532179.125000\n",
            "Train Epoch: 35 [5920/7471 (79%)]\tLoss: 1586198.625000\n",
            "Train Epoch: 35 [6080/7471 (81%)]\tLoss: 1495896.750000\n",
            "Train Epoch: 35 [6240/7471 (84%)]\tLoss: 1536057.875000\n",
            "Train Epoch: 35 [6400/7471 (86%)]\tLoss: 1574138.000000\n",
            "Train Epoch: 35 [6560/7471 (88%)]\tLoss: 1542358.375000\n",
            "Train Epoch: 35 [6720/7471 (90%)]\tLoss: 1627076.375000\n",
            "Train Epoch: 35 [6880/7471 (92%)]\tLoss: 1558191.375000\n",
            "Train Epoch: 35 [7040/7471 (94%)]\tLoss: 1541981.500000\n",
            "Train Epoch: 35 [7200/7471 (96%)]\tLoss: 1606336.500000\n",
            "Train Epoch: 35 [7360/7471 (99%)]\tLoss: 1574970.000000\n",
            "Epoch 35 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 36 [160/7471 (2%)]\tLoss: 1591544.125000\n",
            "Train Epoch: 36 [320/7471 (4%)]\tLoss: 1573415.125000\n",
            "Train Epoch: 36 [480/7471 (6%)]\tLoss: 1608086.750000\n",
            "Train Epoch: 36 [640/7471 (9%)]\tLoss: 1579212.875000\n",
            "Train Epoch: 36 [800/7471 (11%)]\tLoss: 1568528.250000\n",
            "Train Epoch: 36 [960/7471 (13%)]\tLoss: 1607742.250000\n",
            "Train Epoch: 36 [1120/7471 (15%)]\tLoss: 1570543.375000\n",
            "Train Epoch: 36 [1280/7471 (17%)]\tLoss: 1530377.375000\n",
            "Train Epoch: 36 [1440/7471 (19%)]\tLoss: 1528669.750000\n",
            "Train Epoch: 36 [1600/7471 (21%)]\tLoss: 1595823.750000\n",
            "Train Epoch: 36 [1760/7471 (24%)]\tLoss: 1573791.750000\n",
            "Train Epoch: 36 [1920/7471 (26%)]\tLoss: 1570393.375000\n",
            "Train Epoch: 36 [2080/7471 (28%)]\tLoss: 1541139.250000\n",
            "Train Epoch: 36 [2240/7471 (30%)]\tLoss: 1573780.375000\n",
            "Train Epoch: 36 [2400/7471 (32%)]\tLoss: 1585793.625000\n",
            "Train Epoch: 36 [2560/7471 (34%)]\tLoss: 1529957.625000\n",
            "Train Epoch: 36 [2720/7471 (36%)]\tLoss: 1586210.875000\n",
            "Train Epoch: 36 [2880/7471 (39%)]\tLoss: 1529923.750000\n",
            "Train Epoch: 36 [3040/7471 (41%)]\tLoss: 1618334.500000\n",
            "Train Epoch: 36 [3200/7471 (43%)]\tLoss: 1561322.125000\n",
            "Train Epoch: 36 [3360/7471 (45%)]\tLoss: 1609900.625000\n",
            "Train Epoch: 36 [3520/7471 (47%)]\tLoss: 1586762.875000\n",
            "Train Epoch: 36 [3680/7471 (49%)]\tLoss: 1554119.125000\n",
            "Train Epoch: 36 [3840/7471 (51%)]\tLoss: 1531047.875000\n",
            "Train Epoch: 36 [4000/7471 (54%)]\tLoss: 1558265.500000\n",
            "Train Epoch: 36 [4160/7471 (56%)]\tLoss: 1645918.750000\n",
            "Train Epoch: 36 [4320/7471 (58%)]\tLoss: 1581476.750000\n",
            "Train Epoch: 36 [4480/7471 (60%)]\tLoss: 1536492.375000\n",
            "Train Epoch: 36 [4640/7471 (62%)]\tLoss: 1573340.625000\n",
            "Train Epoch: 36 [4800/7471 (64%)]\tLoss: 1622582.625000\n",
            "Train Epoch: 36 [4960/7471 (66%)]\tLoss: 1577932.750000\n",
            "Train Epoch: 36 [5120/7471 (69%)]\tLoss: 1617450.625000\n",
            "Train Epoch: 36 [5280/7471 (71%)]\tLoss: 1593010.375000\n",
            "Train Epoch: 36 [5440/7471 (73%)]\tLoss: 1572910.375000\n",
            "Train Epoch: 36 [5600/7471 (75%)]\tLoss: 1575023.250000\n",
            "Train Epoch: 36 [5760/7471 (77%)]\tLoss: 1561354.500000\n",
            "Train Epoch: 36 [5920/7471 (79%)]\tLoss: 1568918.500000\n",
            "Train Epoch: 36 [6080/7471 (81%)]\tLoss: 1595858.500000\n",
            "Train Epoch: 36 [6240/7471 (84%)]\tLoss: 1588432.750000\n",
            "Train Epoch: 36 [6400/7471 (86%)]\tLoss: 1584851.625000\n",
            "Train Epoch: 36 [6560/7471 (88%)]\tLoss: 1605520.250000\n",
            "Train Epoch: 36 [6720/7471 (90%)]\tLoss: 1563424.500000\n",
            "Train Epoch: 36 [6880/7471 (92%)]\tLoss: 1591328.875000\n",
            "Train Epoch: 36 [7040/7471 (94%)]\tLoss: 1587063.750000\n",
            "Train Epoch: 36 [7200/7471 (96%)]\tLoss: 1570373.000000\n",
            "Train Epoch: 36 [7360/7471 (99%)]\tLoss: 1588442.750000\n",
            "Epoch 36 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 37 [160/7471 (2%)]\tLoss: 1577080.250000\n",
            "Train Epoch: 37 [320/7471 (4%)]\tLoss: 1635886.500000\n",
            "Train Epoch: 37 [480/7471 (6%)]\tLoss: 1597608.875000\n",
            "Train Epoch: 37 [640/7471 (9%)]\tLoss: 1567954.375000\n",
            "Train Epoch: 37 [800/7471 (11%)]\tLoss: 1612215.125000\n",
            "Train Epoch: 37 [960/7471 (13%)]\tLoss: 1556079.625000\n",
            "Train Epoch: 37 [1120/7471 (15%)]\tLoss: 1634680.875000\n",
            "Train Epoch: 37 [1280/7471 (17%)]\tLoss: 1544069.750000\n",
            "Train Epoch: 37 [1440/7471 (19%)]\tLoss: 1612438.250000\n",
            "Train Epoch: 37 [1600/7471 (21%)]\tLoss: 1588642.875000\n",
            "Train Epoch: 37 [1760/7471 (24%)]\tLoss: 1558381.125000\n",
            "Train Epoch: 37 [1920/7471 (26%)]\tLoss: 1513681.125000\n",
            "Train Epoch: 37 [2080/7471 (28%)]\tLoss: 1593600.125000\n",
            "Train Epoch: 37 [2240/7471 (30%)]\tLoss: 1538439.000000\n",
            "Train Epoch: 37 [2400/7471 (32%)]\tLoss: 1600685.750000\n",
            "Train Epoch: 37 [2560/7471 (34%)]\tLoss: 1631997.875000\n",
            "Train Epoch: 37 [2720/7471 (36%)]\tLoss: 1541540.625000\n",
            "Train Epoch: 37 [2880/7471 (39%)]\tLoss: 1584719.875000\n",
            "Train Epoch: 37 [3040/7471 (41%)]\tLoss: 1621221.375000\n",
            "Train Epoch: 37 [3200/7471 (43%)]\tLoss: 1564343.000000\n",
            "Train Epoch: 37 [3360/7471 (45%)]\tLoss: 1611506.000000\n",
            "Train Epoch: 37 [3520/7471 (47%)]\tLoss: 1553181.875000\n",
            "Train Epoch: 37 [3680/7471 (49%)]\tLoss: 1636249.625000\n",
            "Train Epoch: 37 [3840/7471 (51%)]\tLoss: 1510271.125000\n",
            "Train Epoch: 37 [4000/7471 (54%)]\tLoss: 1621209.375000\n",
            "Train Epoch: 37 [4160/7471 (56%)]\tLoss: 1595234.375000\n",
            "Train Epoch: 37 [4320/7471 (58%)]\tLoss: 1546042.750000\n",
            "Train Epoch: 37 [4480/7471 (60%)]\tLoss: 1494812.375000\n",
            "Train Epoch: 37 [4640/7471 (62%)]\tLoss: 1632224.000000\n",
            "Train Epoch: 37 [4800/7471 (64%)]\tLoss: 1554654.000000\n",
            "Train Epoch: 37 [4960/7471 (66%)]\tLoss: 1568586.250000\n",
            "Train Epoch: 37 [5120/7471 (69%)]\tLoss: 1649292.000000\n",
            "Train Epoch: 37 [5280/7471 (71%)]\tLoss: 1629887.125000\n",
            "Train Epoch: 37 [5440/7471 (73%)]\tLoss: 1585930.250000\n",
            "Train Epoch: 37 [5600/7471 (75%)]\tLoss: 1541343.000000\n",
            "Train Epoch: 37 [5760/7471 (77%)]\tLoss: 1596631.500000\n",
            "Train Epoch: 37 [5920/7471 (79%)]\tLoss: 1571838.625000\n",
            "Train Epoch: 37 [6080/7471 (81%)]\tLoss: 1599777.750000\n",
            "Train Epoch: 37 [6240/7471 (84%)]\tLoss: 1565314.375000\n",
            "Train Epoch: 37 [6400/7471 (86%)]\tLoss: 1572580.375000\n",
            "Train Epoch: 37 [6560/7471 (88%)]\tLoss: 1576414.750000\n",
            "Train Epoch: 37 [6720/7471 (90%)]\tLoss: 1604391.625000\n",
            "Train Epoch: 37 [6880/7471 (92%)]\tLoss: 1577205.000000\n",
            "Train Epoch: 37 [7040/7471 (94%)]\tLoss: 1575501.000000\n",
            "Train Epoch: 37 [7200/7471 (96%)]\tLoss: 1587665.375000\n",
            "Train Epoch: 37 [7360/7471 (99%)]\tLoss: 1581613.250000\n",
            "Epoch 37 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 38 [160/7471 (2%)]\tLoss: 1487404.500000\n",
            "Train Epoch: 38 [320/7471 (4%)]\tLoss: 1607705.625000\n",
            "Train Epoch: 38 [480/7471 (6%)]\tLoss: 1619911.375000\n",
            "Train Epoch: 38 [640/7471 (9%)]\tLoss: 1623461.500000\n",
            "Train Epoch: 38 [800/7471 (11%)]\tLoss: 1559177.250000\n",
            "Train Epoch: 38 [960/7471 (13%)]\tLoss: 1574582.000000\n",
            "Train Epoch: 38 [1120/7471 (15%)]\tLoss: 1557700.500000\n",
            "Train Epoch: 38 [1280/7471 (17%)]\tLoss: 1621687.750000\n",
            "Train Epoch: 38 [1440/7471 (19%)]\tLoss: 1568685.500000\n",
            "Train Epoch: 38 [1600/7471 (21%)]\tLoss: 1537031.000000\n",
            "Train Epoch: 38 [1760/7471 (24%)]\tLoss: 1552193.250000\n",
            "Train Epoch: 38 [1920/7471 (26%)]\tLoss: 1516815.750000\n",
            "Train Epoch: 38 [2080/7471 (28%)]\tLoss: 1503304.750000\n",
            "Train Epoch: 38 [2240/7471 (30%)]\tLoss: 1540356.625000\n",
            "Train Epoch: 38 [2400/7471 (32%)]\tLoss: 1588452.875000\n",
            "Train Epoch: 38 [2560/7471 (34%)]\tLoss: 1600320.750000\n",
            "Train Epoch: 38 [2720/7471 (36%)]\tLoss: 1492796.250000\n",
            "Train Epoch: 38 [2880/7471 (39%)]\tLoss: 1594607.500000\n",
            "Train Epoch: 38 [3040/7471 (41%)]\tLoss: 1623568.375000\n",
            "Train Epoch: 38 [3200/7471 (43%)]\tLoss: 1550621.625000\n",
            "Train Epoch: 38 [3360/7471 (45%)]\tLoss: 1468897.500000\n",
            "Train Epoch: 38 [3520/7471 (47%)]\tLoss: 1556236.500000\n",
            "Train Epoch: 38 [3680/7471 (49%)]\tLoss: 1591226.375000\n",
            "Train Epoch: 38 [3840/7471 (51%)]\tLoss: 1592912.375000\n",
            "Train Epoch: 38 [4000/7471 (54%)]\tLoss: 1545655.750000\n",
            "Train Epoch: 38 [4160/7471 (56%)]\tLoss: 1536377.750000\n",
            "Train Epoch: 38 [4320/7471 (58%)]\tLoss: 1612977.125000\n",
            "Train Epoch: 38 [4480/7471 (60%)]\tLoss: 1616597.875000\n",
            "Train Epoch: 38 [4640/7471 (62%)]\tLoss: 1544913.750000\n",
            "Train Epoch: 38 [4800/7471 (64%)]\tLoss: 1602027.000000\n",
            "Train Epoch: 38 [4960/7471 (66%)]\tLoss: 1609984.625000\n",
            "Train Epoch: 38 [5120/7471 (69%)]\tLoss: 1627886.875000\n",
            "Train Epoch: 38 [5280/7471 (71%)]\tLoss: 1534940.500000\n",
            "Train Epoch: 38 [5440/7471 (73%)]\tLoss: 1589668.875000\n",
            "Train Epoch: 38 [5600/7471 (75%)]\tLoss: 1532972.000000\n",
            "Train Epoch: 38 [5760/7471 (77%)]\tLoss: 1603078.000000\n",
            "Train Epoch: 38 [5920/7471 (79%)]\tLoss: 1615971.875000\n",
            "Train Epoch: 38 [6080/7471 (81%)]\tLoss: 1542239.750000\n",
            "Train Epoch: 38 [6240/7471 (84%)]\tLoss: 1555319.000000\n",
            "Train Epoch: 38 [6400/7471 (86%)]\tLoss: 1577536.500000\n",
            "Train Epoch: 38 [6560/7471 (88%)]\tLoss: 1604092.625000\n",
            "Train Epoch: 38 [6720/7471 (90%)]\tLoss: 1551200.750000\n",
            "Train Epoch: 38 [6880/7471 (92%)]\tLoss: 1624101.125000\n",
            "Train Epoch: 38 [7040/7471 (94%)]\tLoss: 1620891.750000\n",
            "Train Epoch: 38 [7200/7471 (96%)]\tLoss: 1647330.625000\n",
            "Train Epoch: 38 [7360/7471 (99%)]\tLoss: 1643371.500000\n",
            "Epoch 38 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 39 [160/7471 (2%)]\tLoss: 1584199.875000\n",
            "Train Epoch: 39 [320/7471 (4%)]\tLoss: 1632724.625000\n",
            "Train Epoch: 39 [480/7471 (6%)]\tLoss: 1575262.875000\n",
            "Train Epoch: 39 [640/7471 (9%)]\tLoss: 1615697.875000\n",
            "Train Epoch: 39 [800/7471 (11%)]\tLoss: 1584572.750000\n",
            "Train Epoch: 39 [960/7471 (13%)]\tLoss: 1606917.500000\n",
            "Train Epoch: 39 [1120/7471 (15%)]\tLoss: 1600058.000000\n",
            "Train Epoch: 39 [1280/7471 (17%)]\tLoss: 1595191.375000\n",
            "Train Epoch: 39 [1440/7471 (19%)]\tLoss: 1598148.875000\n",
            "Train Epoch: 39 [1600/7471 (21%)]\tLoss: 1581320.500000\n",
            "Train Epoch: 39 [1760/7471 (24%)]\tLoss: 1642636.000000\n",
            "Train Epoch: 39 [1920/7471 (26%)]\tLoss: 1541178.000000\n",
            "Train Epoch: 39 [2080/7471 (28%)]\tLoss: 1614974.000000\n",
            "Train Epoch: 39 [2240/7471 (30%)]\tLoss: 1570415.625000\n",
            "Train Epoch: 39 [2400/7471 (32%)]\tLoss: 1626857.250000\n",
            "Train Epoch: 39 [2560/7471 (34%)]\tLoss: 1635724.625000\n",
            "Train Epoch: 39 [2720/7471 (36%)]\tLoss: 1609058.750000\n",
            "Train Epoch: 39 [2880/7471 (39%)]\tLoss: 1552864.875000\n",
            "Train Epoch: 39 [3040/7471 (41%)]\tLoss: 1620386.250000\n",
            "Train Epoch: 39 [3200/7471 (43%)]\tLoss: 1547463.000000\n",
            "Train Epoch: 39 [3360/7471 (45%)]\tLoss: 1517724.000000\n",
            "Train Epoch: 39 [3520/7471 (47%)]\tLoss: 1577359.250000\n",
            "Train Epoch: 39 [3680/7471 (49%)]\tLoss: 1556665.000000\n",
            "Train Epoch: 39 [3840/7471 (51%)]\tLoss: 1627121.500000\n",
            "Train Epoch: 39 [4000/7471 (54%)]\tLoss: 1620301.125000\n",
            "Train Epoch: 39 [4160/7471 (56%)]\tLoss: 1525976.625000\n",
            "Train Epoch: 39 [4320/7471 (58%)]\tLoss: 1577686.500000\n",
            "Train Epoch: 39 [4480/7471 (60%)]\tLoss: 1590472.750000\n",
            "Train Epoch: 39 [4640/7471 (62%)]\tLoss: 1571694.125000\n",
            "Train Epoch: 39 [4800/7471 (64%)]\tLoss: 1618903.500000\n",
            "Train Epoch: 39 [4960/7471 (66%)]\tLoss: 1594700.875000\n",
            "Train Epoch: 39 [5120/7471 (69%)]\tLoss: 1555693.625000\n",
            "Train Epoch: 39 [5280/7471 (71%)]\tLoss: 1570264.500000\n",
            "Train Epoch: 39 [5440/7471 (73%)]\tLoss: 1594777.750000\n",
            "Train Epoch: 39 [5600/7471 (75%)]\tLoss: 1574520.375000\n",
            "Train Epoch: 39 [5760/7471 (77%)]\tLoss: 1617313.125000\n",
            "Train Epoch: 39 [5920/7471 (79%)]\tLoss: 1599566.000000\n",
            "Train Epoch: 39 [6080/7471 (81%)]\tLoss: 1538314.875000\n",
            "Train Epoch: 39 [6240/7471 (84%)]\tLoss: 1571964.000000\n",
            "Train Epoch: 39 [6400/7471 (86%)]\tLoss: 1549481.375000\n",
            "Train Epoch: 39 [6560/7471 (88%)]\tLoss: 1618550.750000\n",
            "Train Epoch: 39 [6720/7471 (90%)]\tLoss: 1522901.875000\n",
            "Train Epoch: 39 [6880/7471 (92%)]\tLoss: 1580120.875000\n",
            "Train Epoch: 39 [7040/7471 (94%)]\tLoss: 1578921.375000\n",
            "Train Epoch: 39 [7200/7471 (96%)]\tLoss: 1575726.125000\n",
            "Train Epoch: 39 [7360/7471 (99%)]\tLoss: 1547848.250000\n",
            "Epoch 39 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 40 [160/7471 (2%)]\tLoss: 1515913.000000\n",
            "Train Epoch: 40 [320/7471 (4%)]\tLoss: 1609100.500000\n",
            "Train Epoch: 40 [480/7471 (6%)]\tLoss: 1527935.875000\n",
            "Train Epoch: 40 [640/7471 (9%)]\tLoss: 1591983.750000\n",
            "Train Epoch: 40 [800/7471 (11%)]\tLoss: 1574587.500000\n",
            "Train Epoch: 40 [960/7471 (13%)]\tLoss: 1535299.500000\n",
            "Train Epoch: 40 [1120/7471 (15%)]\tLoss: 1594549.375000\n",
            "Train Epoch: 40 [1280/7471 (17%)]\tLoss: 1647330.750000\n",
            "Train Epoch: 40 [1440/7471 (19%)]\tLoss: 1584410.125000\n",
            "Train Epoch: 40 [1600/7471 (21%)]\tLoss: 1615363.500000\n",
            "Train Epoch: 40 [1760/7471 (24%)]\tLoss: 1514393.625000\n",
            "Train Epoch: 40 [1920/7471 (26%)]\tLoss: 1515106.625000\n",
            "Train Epoch: 40 [2080/7471 (28%)]\tLoss: 1577202.375000\n",
            "Train Epoch: 40 [2240/7471 (30%)]\tLoss: 1551203.750000\n",
            "Train Epoch: 40 [2400/7471 (32%)]\tLoss: 1581491.250000\n",
            "Train Epoch: 40 [2560/7471 (34%)]\tLoss: 1621069.625000\n",
            "Train Epoch: 40 [2720/7471 (36%)]\tLoss: 1633496.000000\n",
            "Train Epoch: 40 [2880/7471 (39%)]\tLoss: 1576860.375000\n",
            "Train Epoch: 40 [3040/7471 (41%)]\tLoss: 1591822.000000\n",
            "Train Epoch: 40 [3200/7471 (43%)]\tLoss: 1522671.875000\n",
            "Train Epoch: 40 [3360/7471 (45%)]\tLoss: 1525350.250000\n",
            "Train Epoch: 40 [3520/7471 (47%)]\tLoss: 1531830.000000\n",
            "Train Epoch: 40 [3680/7471 (49%)]\tLoss: 1519172.750000\n",
            "Train Epoch: 40 [3840/7471 (51%)]\tLoss: 1568838.125000\n",
            "Train Epoch: 40 [4000/7471 (54%)]\tLoss: 1557292.750000\n",
            "Train Epoch: 40 [4160/7471 (56%)]\tLoss: 1622466.375000\n",
            "Train Epoch: 40 [4320/7471 (58%)]\tLoss: 1539611.500000\n",
            "Train Epoch: 40 [4480/7471 (60%)]\tLoss: 1602999.125000\n",
            "Train Epoch: 40 [4640/7471 (62%)]\tLoss: 1562748.250000\n",
            "Train Epoch: 40 [4800/7471 (64%)]\tLoss: 1597124.000000\n",
            "Train Epoch: 40 [4960/7471 (66%)]\tLoss: 1583933.375000\n",
            "Train Epoch: 40 [5120/7471 (69%)]\tLoss: 1584622.625000\n",
            "Train Epoch: 40 [5280/7471 (71%)]\tLoss: 1571642.000000\n",
            "Train Epoch: 40 [5440/7471 (73%)]\tLoss: 1580969.250000\n",
            "Train Epoch: 40 [5600/7471 (75%)]\tLoss: 1591375.375000\n",
            "Train Epoch: 40 [5760/7471 (77%)]\tLoss: 1587092.875000\n",
            "Train Epoch: 40 [5920/7471 (79%)]\tLoss: 1612994.375000\n",
            "Train Epoch: 40 [6080/7471 (81%)]\tLoss: 1598726.125000\n",
            "Train Epoch: 40 [6240/7471 (84%)]\tLoss: 1584525.000000\n",
            "Train Epoch: 40 [6400/7471 (86%)]\tLoss: 1572903.875000\n",
            "Train Epoch: 40 [6560/7471 (88%)]\tLoss: 1586291.375000\n",
            "Train Epoch: 40 [6720/7471 (90%)]\tLoss: 1586268.750000\n",
            "Train Epoch: 40 [6880/7471 (92%)]\tLoss: 1594089.125000\n",
            "Train Epoch: 40 [7040/7471 (94%)]\tLoss: 1605068.875000\n",
            "Train Epoch: 40 [7200/7471 (96%)]\tLoss: 1600151.750000\n",
            "Train Epoch: 40 [7360/7471 (99%)]\tLoss: 1592719.250000\n",
            "Epoch 40 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 41 [160/7471 (2%)]\tLoss: 1596354.000000\n",
            "Train Epoch: 41 [320/7471 (4%)]\tLoss: 1573093.625000\n",
            "Train Epoch: 41 [480/7471 (6%)]\tLoss: 1550367.750000\n",
            "Train Epoch: 41 [640/7471 (9%)]\tLoss: 1603727.750000\n",
            "Train Epoch: 41 [800/7471 (11%)]\tLoss: 1597140.000000\n",
            "Train Epoch: 41 [960/7471 (13%)]\tLoss: 1561171.875000\n",
            "Train Epoch: 41 [1120/7471 (15%)]\tLoss: 1617321.375000\n",
            "Train Epoch: 41 [1280/7471 (17%)]\tLoss: 1557603.250000\n",
            "Train Epoch: 41 [1440/7471 (19%)]\tLoss: 1536359.875000\n",
            "Train Epoch: 41 [1600/7471 (21%)]\tLoss: 1535952.875000\n",
            "Train Epoch: 41 [1760/7471 (24%)]\tLoss: 1607643.125000\n",
            "Train Epoch: 41 [1920/7471 (26%)]\tLoss: 1628559.750000\n",
            "Train Epoch: 41 [2080/7471 (28%)]\tLoss: 1625188.625000\n",
            "Train Epoch: 41 [2240/7471 (30%)]\tLoss: 1510601.875000\n",
            "Train Epoch: 41 [2400/7471 (32%)]\tLoss: 1577260.750000\n",
            "Train Epoch: 41 [2560/7471 (34%)]\tLoss: 1616540.375000\n",
            "Train Epoch: 41 [2720/7471 (36%)]\tLoss: 1527543.125000\n",
            "Train Epoch: 41 [2880/7471 (39%)]\tLoss: 1557345.875000\n",
            "Train Epoch: 41 [3040/7471 (41%)]\tLoss: 1586392.375000\n",
            "Train Epoch: 41 [3200/7471 (43%)]\tLoss: 1514268.000000\n",
            "Train Epoch: 41 [3360/7471 (45%)]\tLoss: 1571158.875000\n",
            "Train Epoch: 41 [3520/7471 (47%)]\tLoss: 1585434.250000\n",
            "Train Epoch: 41 [3680/7471 (49%)]\tLoss: 1616875.750000\n",
            "Train Epoch: 41 [3840/7471 (51%)]\tLoss: 1556308.375000\n",
            "Train Epoch: 41 [4000/7471 (54%)]\tLoss: 1487462.875000\n",
            "Train Epoch: 41 [4160/7471 (56%)]\tLoss: 1627140.250000\n",
            "Train Epoch: 41 [4320/7471 (58%)]\tLoss: 1602108.875000\n",
            "Train Epoch: 41 [4480/7471 (60%)]\tLoss: 1597927.250000\n",
            "Train Epoch: 41 [4640/7471 (62%)]\tLoss: 1545253.625000\n",
            "Train Epoch: 41 [4800/7471 (64%)]\tLoss: 1545019.875000\n",
            "Train Epoch: 41 [4960/7471 (66%)]\tLoss: 1555417.000000\n",
            "Train Epoch: 41 [5120/7471 (69%)]\tLoss: 1578382.000000\n",
            "Train Epoch: 41 [5280/7471 (71%)]\tLoss: 1633052.500000\n",
            "Train Epoch: 41 [5440/7471 (73%)]\tLoss: 1594752.125000\n",
            "Train Epoch: 41 [5600/7471 (75%)]\tLoss: 1579062.375000\n",
            "Train Epoch: 41 [5760/7471 (77%)]\tLoss: 1538280.500000\n",
            "Train Epoch: 41 [5920/7471 (79%)]\tLoss: 1622441.500000\n",
            "Train Epoch: 41 [6080/7471 (81%)]\tLoss: 1637004.125000\n",
            "Train Epoch: 41 [6240/7471 (84%)]\tLoss: 1579051.500000\n",
            "Train Epoch: 41 [6400/7471 (86%)]\tLoss: 1581515.500000\n",
            "Train Epoch: 41 [6560/7471 (88%)]\tLoss: 1599097.000000\n",
            "Train Epoch: 41 [6720/7471 (90%)]\tLoss: 1592709.125000\n",
            "Train Epoch: 41 [6880/7471 (92%)]\tLoss: 1506493.875000\n",
            "Train Epoch: 41 [7040/7471 (94%)]\tLoss: 1601089.000000\n",
            "Train Epoch: 41 [7200/7471 (96%)]\tLoss: 1575978.500000\n",
            "Train Epoch: 41 [7360/7471 (99%)]\tLoss: 1576498.125000\n",
            "Epoch 41 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 42 [160/7471 (2%)]\tLoss: 1579228.500000\n",
            "Train Epoch: 42 [320/7471 (4%)]\tLoss: 1536213.250000\n",
            "Train Epoch: 42 [480/7471 (6%)]\tLoss: 1575887.125000\n",
            "Train Epoch: 42 [640/7471 (9%)]\tLoss: 1630641.625000\n",
            "Train Epoch: 42 [800/7471 (11%)]\tLoss: 1621255.000000\n",
            "Train Epoch: 42 [960/7471 (13%)]\tLoss: 1534528.750000\n",
            "Train Epoch: 42 [1120/7471 (15%)]\tLoss: 1558416.500000\n",
            "Train Epoch: 42 [1280/7471 (17%)]\tLoss: 1579809.625000\n",
            "Train Epoch: 42 [1440/7471 (19%)]\tLoss: 1622578.250000\n",
            "Train Epoch: 42 [1600/7471 (21%)]\tLoss: 1566249.000000\n",
            "Train Epoch: 42 [1760/7471 (24%)]\tLoss: 1552914.375000\n",
            "Train Epoch: 42 [1920/7471 (26%)]\tLoss: 1606757.875000\n",
            "Train Epoch: 42 [2080/7471 (28%)]\tLoss: 1548058.125000\n",
            "Train Epoch: 42 [2240/7471 (30%)]\tLoss: 1611313.250000\n",
            "Train Epoch: 42 [2400/7471 (32%)]\tLoss: 1620138.250000\n",
            "Train Epoch: 42 [2560/7471 (34%)]\tLoss: 1610334.000000\n",
            "Train Epoch: 42 [2720/7471 (36%)]\tLoss: 1563979.250000\n",
            "Train Epoch: 42 [2880/7471 (39%)]\tLoss: 1587410.750000\n",
            "Train Epoch: 42 [3040/7471 (41%)]\tLoss: 1619577.375000\n",
            "Train Epoch: 42 [3200/7471 (43%)]\tLoss: 1588123.625000\n",
            "Train Epoch: 42 [3360/7471 (45%)]\tLoss: 1597382.250000\n",
            "Train Epoch: 42 [3520/7471 (47%)]\tLoss: 1612066.250000\n",
            "Train Epoch: 42 [3680/7471 (49%)]\tLoss: 1629859.125000\n",
            "Train Epoch: 42 [3840/7471 (51%)]\tLoss: 1514374.875000\n",
            "Train Epoch: 42 [4000/7471 (54%)]\tLoss: 1629496.375000\n",
            "Train Epoch: 42 [4160/7471 (56%)]\tLoss: 1555819.000000\n",
            "Train Epoch: 42 [4320/7471 (58%)]\tLoss: 1578704.250000\n",
            "Train Epoch: 42 [4480/7471 (60%)]\tLoss: 1521884.750000\n",
            "Train Epoch: 42 [4640/7471 (62%)]\tLoss: 1582593.500000\n",
            "Train Epoch: 42 [4800/7471 (64%)]\tLoss: 1560536.500000\n",
            "Train Epoch: 42 [4960/7471 (66%)]\tLoss: 1621925.125000\n",
            "Train Epoch: 42 [5120/7471 (69%)]\tLoss: 1599476.250000\n",
            "Train Epoch: 42 [5280/7471 (71%)]\tLoss: 1585783.875000\n",
            "Train Epoch: 42 [5440/7471 (73%)]\tLoss: 1619947.250000\n",
            "Train Epoch: 42 [5600/7471 (75%)]\tLoss: 1585415.125000\n",
            "Train Epoch: 42 [5760/7471 (77%)]\tLoss: 1561478.000000\n",
            "Train Epoch: 42 [5920/7471 (79%)]\tLoss: 1592449.500000\n",
            "Train Epoch: 42 [6080/7471 (81%)]\tLoss: 1516540.250000\n",
            "Train Epoch: 42 [6240/7471 (84%)]\tLoss: 1655135.000000\n",
            "Train Epoch: 42 [6400/7471 (86%)]\tLoss: 1559312.250000\n",
            "Train Epoch: 42 [6560/7471 (88%)]\tLoss: 1602361.125000\n",
            "Train Epoch: 42 [6720/7471 (90%)]\tLoss: 1611253.875000\n",
            "Train Epoch: 42 [6880/7471 (92%)]\tLoss: 1582957.250000\n",
            "Train Epoch: 42 [7040/7471 (94%)]\tLoss: 1625370.125000\n",
            "Train Epoch: 42 [7200/7471 (96%)]\tLoss: 1545555.250000\n",
            "Train Epoch: 42 [7360/7471 (99%)]\tLoss: 1493300.500000\n",
            "Epoch 42 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 464997510447131136.0000\n",
            "\n",
            "Train Epoch: 43 [160/7471 (2%)]\tLoss: 1582178.000000\n",
            "Train Epoch: 43 [320/7471 (4%)]\tLoss: 1565687.625000\n",
            "Train Epoch: 43 [480/7471 (6%)]\tLoss: 1547876.875000\n",
            "Train Epoch: 43 [640/7471 (9%)]\tLoss: 1498752.500000\n",
            "Train Epoch: 43 [800/7471 (11%)]\tLoss: 1606051.500000\n",
            "Train Epoch: 43 [960/7471 (13%)]\tLoss: 1608957.500000\n",
            "Train Epoch: 43 [1120/7471 (15%)]\tLoss: 1581211.625000\n",
            "Train Epoch: 43 [1280/7471 (17%)]\tLoss: 1588525.375000\n",
            "Train Epoch: 43 [1440/7471 (19%)]\tLoss: 1631727.375000\n",
            "Train Epoch: 43 [1600/7471 (21%)]\tLoss: 1598245.750000\n",
            "Train Epoch: 43 [1760/7471 (24%)]\tLoss: 1588858.875000\n",
            "Train Epoch: 43 [1920/7471 (26%)]\tLoss: 1608645.625000\n",
            "Train Epoch: 43 [2080/7471 (28%)]\tLoss: 1627062.375000\n",
            "Train Epoch: 43 [2240/7471 (30%)]\tLoss: 1608986.625000\n",
            "Train Epoch: 43 [2400/7471 (32%)]\tLoss: 1585409.125000\n",
            "Train Epoch: 43 [2560/7471 (34%)]\tLoss: 1639731.625000\n",
            "Train Epoch: 43 [2720/7471 (36%)]\tLoss: 1536528.250000\n",
            "Train Epoch: 43 [2880/7471 (39%)]\tLoss: 1481327.000000\n",
            "Train Epoch: 43 [3040/7471 (41%)]\tLoss: 1581743.250000\n",
            "Train Epoch: 43 [3200/7471 (43%)]\tLoss: 1543671.750000\n",
            "Train Epoch: 43 [3360/7471 (45%)]\tLoss: 1565802.500000\n",
            "Train Epoch: 43 [3520/7471 (47%)]\tLoss: 1625828.750000\n",
            "Train Epoch: 43 [3680/7471 (49%)]\tLoss: 1554370.250000\n",
            "Train Epoch: 43 [3840/7471 (51%)]\tLoss: 1584598.250000\n",
            "Train Epoch: 43 [4000/7471 (54%)]\tLoss: 1613277.125000\n",
            "Train Epoch: 43 [4160/7471 (56%)]\tLoss: 1560223.375000\n",
            "Train Epoch: 43 [4320/7471 (58%)]\tLoss: 1630150.625000\n",
            "Train Epoch: 43 [4480/7471 (60%)]\tLoss: 1523328.000000\n",
            "Train Epoch: 43 [4640/7471 (62%)]\tLoss: 1517238.750000\n",
            "Train Epoch: 43 [4800/7471 (64%)]\tLoss: 1543541.000000\n",
            "Train Epoch: 43 [4960/7471 (66%)]\tLoss: 1620248.250000\n",
            "Train Epoch: 43 [5120/7471 (69%)]\tLoss: 1626207.500000\n",
            "Train Epoch: 43 [5280/7471 (71%)]\tLoss: 1519952.750000\n",
            "Train Epoch: 43 [5440/7471 (73%)]\tLoss: 1553866.625000\n",
            "Train Epoch: 43 [5600/7471 (75%)]\tLoss: 1568376.750000\n",
            "Train Epoch: 43 [5760/7471 (77%)]\tLoss: 1629952.625000\n",
            "Train Epoch: 43 [5920/7471 (79%)]\tLoss: 1598584.250000\n",
            "Train Epoch: 43 [6080/7471 (81%)]\tLoss: 1569554.250000\n",
            "Train Epoch: 43 [6240/7471 (84%)]\tLoss: 1476076.625000\n",
            "Train Epoch: 43 [6400/7471 (86%)]\tLoss: 1608547.625000\n",
            "Train Epoch: 43 [6560/7471 (88%)]\tLoss: 1567386.000000\n",
            "Train Epoch: 43 [6720/7471 (90%)]\tLoss: 1593423.625000\n",
            "Train Epoch: 43 [6880/7471 (92%)]\tLoss: 1605983.125000\n",
            "Train Epoch: 43 [7040/7471 (94%)]\tLoss: 1575765.750000\n",
            "Train Epoch: 43 [7200/7471 (96%)]\tLoss: 1552765.750000\n",
            "Train Epoch: 43 [7360/7471 (99%)]\tLoss: 1600835.500000\n",
            "Epoch 43 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98696.1006\n",
            "\n",
            "Train Epoch: 44 [160/7471 (2%)]\tLoss: 1641140.875000\n",
            "Train Epoch: 44 [320/7471 (4%)]\tLoss: 1601313.750000\n",
            "Train Epoch: 44 [480/7471 (6%)]\tLoss: 1580851.375000\n",
            "Train Epoch: 44 [640/7471 (9%)]\tLoss: 1568941.875000\n",
            "Train Epoch: 44 [800/7471 (11%)]\tLoss: 1626766.625000\n",
            "Train Epoch: 44 [960/7471 (13%)]\tLoss: 1599350.750000\n",
            "Train Epoch: 44 [1120/7471 (15%)]\tLoss: 1612609.875000\n",
            "Train Epoch: 44 [1280/7471 (17%)]\tLoss: 1615502.375000\n",
            "Train Epoch: 44 [1440/7471 (19%)]\tLoss: 1633796.750000\n",
            "Train Epoch: 44 [1600/7471 (21%)]\tLoss: 1612037.250000\n",
            "Train Epoch: 44 [1760/7471 (24%)]\tLoss: 1554842.875000\n",
            "Train Epoch: 44 [1920/7471 (26%)]\tLoss: 1600403.250000\n",
            "Train Epoch: 44 [2080/7471 (28%)]\tLoss: 1573406.875000\n",
            "Train Epoch: 44 [2240/7471 (30%)]\tLoss: 1570535.500000\n",
            "Train Epoch: 44 [2400/7471 (32%)]\tLoss: 1580743.625000\n",
            "Train Epoch: 44 [2560/7471 (34%)]\tLoss: 1563004.250000\n",
            "Train Epoch: 44 [2720/7471 (36%)]\tLoss: 1551479.750000\n",
            "Train Epoch: 44 [2880/7471 (39%)]\tLoss: 1528837.125000\n",
            "Train Epoch: 44 [3040/7471 (41%)]\tLoss: 1564517.875000\n",
            "Train Epoch: 44 [3200/7471 (43%)]\tLoss: 1518001.125000\n",
            "Train Epoch: 44 [3360/7471 (45%)]\tLoss: 1559354.875000\n",
            "Train Epoch: 44 [3520/7471 (47%)]\tLoss: 1617936.500000\n",
            "Train Epoch: 44 [3680/7471 (49%)]\tLoss: 1616289.625000\n",
            "Train Epoch: 44 [3840/7471 (51%)]\tLoss: 1554000.375000\n",
            "Train Epoch: 44 [4000/7471 (54%)]\tLoss: 1577969.625000\n",
            "Train Epoch: 44 [4160/7471 (56%)]\tLoss: 1584533.000000\n",
            "Train Epoch: 44 [4320/7471 (58%)]\tLoss: 1616898.250000\n",
            "Train Epoch: 44 [4480/7471 (60%)]\tLoss: 1498230.000000\n",
            "Train Epoch: 44 [4640/7471 (62%)]\tLoss: 1569943.750000\n",
            "Train Epoch: 44 [4800/7471 (64%)]\tLoss: 1583052.875000\n",
            "Train Epoch: 44 [4960/7471 (66%)]\tLoss: 1532858.625000\n",
            "Train Epoch: 44 [5120/7471 (69%)]\tLoss: 1606628.375000\n",
            "Train Epoch: 44 [5280/7471 (71%)]\tLoss: 1611994.000000\n",
            "Train Epoch: 44 [5440/7471 (73%)]\tLoss: 1535562.500000\n",
            "Train Epoch: 44 [5600/7471 (75%)]\tLoss: 1488003.250000\n",
            "Train Epoch: 44 [5760/7471 (77%)]\tLoss: 1586057.375000\n",
            "Train Epoch: 44 [5920/7471 (79%)]\tLoss: 1616736.500000\n",
            "Train Epoch: 44 [6080/7471 (81%)]\tLoss: 1599318.875000\n",
            "Train Epoch: 44 [6240/7471 (84%)]\tLoss: 1577753.375000\n",
            "Train Epoch: 44 [6400/7471 (86%)]\tLoss: 1585333.875000\n",
            "Train Epoch: 44 [6560/7471 (88%)]\tLoss: 1563551.500000\n",
            "Train Epoch: 44 [6720/7471 (90%)]\tLoss: 1621390.500000\n",
            "Train Epoch: 44 [6880/7471 (92%)]\tLoss: 1607486.250000\n",
            "Train Epoch: 44 [7040/7471 (94%)]\tLoss: 1585815.750000\n",
            "Train Epoch: 44 [7200/7471 (96%)]\tLoss: 1593638.375000\n",
            "Train Epoch: 44 [7360/7471 (99%)]\tLoss: 1608059.125000\n",
            "Epoch 44 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98670.2425\n",
            "\n",
            "Train Epoch: 45 [160/7471 (2%)]\tLoss: 1590721.500000\n",
            "Train Epoch: 45 [320/7471 (4%)]\tLoss: 1527302.125000\n",
            "Train Epoch: 45 [480/7471 (6%)]\tLoss: 1617389.375000\n",
            "Train Epoch: 45 [640/7471 (9%)]\tLoss: 1558698.875000\n",
            "Train Epoch: 45 [800/7471 (11%)]\tLoss: 1650449.625000\n",
            "Train Epoch: 45 [960/7471 (13%)]\tLoss: 1555403.500000\n",
            "Train Epoch: 45 [1120/7471 (15%)]\tLoss: 1614083.500000\n",
            "Train Epoch: 45 [1280/7471 (17%)]\tLoss: 1611478.125000\n",
            "Train Epoch: 45 [1440/7471 (19%)]\tLoss: 1506693.875000\n",
            "Train Epoch: 45 [1600/7471 (21%)]\tLoss: 1607278.375000\n",
            "Train Epoch: 45 [1760/7471 (24%)]\tLoss: 1563738.625000\n",
            "Train Epoch: 45 [1920/7471 (26%)]\tLoss: 1607334.250000\n",
            "Train Epoch: 45 [2080/7471 (28%)]\tLoss: 1591564.750000\n",
            "Train Epoch: 45 [2240/7471 (30%)]\tLoss: 1598045.250000\n",
            "Train Epoch: 45 [2400/7471 (32%)]\tLoss: 1595200.500000\n",
            "Train Epoch: 45 [2560/7471 (34%)]\tLoss: 1584240.750000\n",
            "Train Epoch: 45 [2720/7471 (36%)]\tLoss: 1605934.750000\n",
            "Train Epoch: 45 [2880/7471 (39%)]\tLoss: 1550187.000000\n",
            "Train Epoch: 45 [3040/7471 (41%)]\tLoss: 1593446.750000\n",
            "Train Epoch: 45 [3200/7471 (43%)]\tLoss: 1615400.625000\n",
            "Train Epoch: 45 [3360/7471 (45%)]\tLoss: 1572350.375000\n",
            "Train Epoch: 45 [3520/7471 (47%)]\tLoss: 1567616.625000\n",
            "Train Epoch: 45 [3680/7471 (49%)]\tLoss: 1528384.125000\n",
            "Train Epoch: 45 [3840/7471 (51%)]\tLoss: 1575123.625000\n",
            "Train Epoch: 45 [4000/7471 (54%)]\tLoss: 1593890.375000\n",
            "Train Epoch: 45 [4160/7471 (56%)]\tLoss: 1630023.125000\n",
            "Train Epoch: 45 [4320/7471 (58%)]\tLoss: 1475841.750000\n",
            "Train Epoch: 45 [4480/7471 (60%)]\tLoss: 1596393.125000\n",
            "Train Epoch: 45 [4640/7471 (62%)]\tLoss: 1625030.250000\n",
            "Train Epoch: 45 [4800/7471 (64%)]\tLoss: 1550602.875000\n",
            "Train Epoch: 45 [4960/7471 (66%)]\tLoss: 1559076.750000\n",
            "Train Epoch: 45 [5120/7471 (69%)]\tLoss: 1616080.250000\n",
            "Train Epoch: 45 [5280/7471 (71%)]\tLoss: 1569953.000000\n",
            "Train Epoch: 45 [5440/7471 (73%)]\tLoss: 1593911.500000\n",
            "Train Epoch: 45 [5600/7471 (75%)]\tLoss: 1502086.000000\n",
            "Train Epoch: 45 [5760/7471 (77%)]\tLoss: 1603405.375000\n",
            "Train Epoch: 45 [5920/7471 (79%)]\tLoss: 1526464.375000\n",
            "Train Epoch: 45 [6080/7471 (81%)]\tLoss: 1610375.500000\n",
            "Train Epoch: 45 [6240/7471 (84%)]\tLoss: 1613847.625000\n",
            "Train Epoch: 45 [6400/7471 (86%)]\tLoss: 1593108.250000\n",
            "Train Epoch: 45 [6560/7471 (88%)]\tLoss: 1624981.250000\n",
            "Train Epoch: 45 [6720/7471 (90%)]\tLoss: 1556312.625000\n",
            "Train Epoch: 45 [6880/7471 (92%)]\tLoss: 1587251.000000\n",
            "Train Epoch: 45 [7040/7471 (94%)]\tLoss: 1596697.750000\n",
            "Train Epoch: 45 [7200/7471 (96%)]\tLoss: 1592813.250000\n",
            "Train Epoch: 45 [7360/7471 (99%)]\tLoss: 1544927.000000\n",
            "Epoch 45 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98641.9925\n",
            "\n",
            "Train Epoch: 46 [160/7471 (2%)]\tLoss: 1576996.000000\n",
            "Train Epoch: 46 [320/7471 (4%)]\tLoss: 1600741.250000\n",
            "Train Epoch: 46 [480/7471 (6%)]\tLoss: 1648447.625000\n",
            "Train Epoch: 46 [640/7471 (9%)]\tLoss: 1620327.625000\n",
            "Train Epoch: 46 [800/7471 (11%)]\tLoss: 1592771.125000\n",
            "Train Epoch: 46 [960/7471 (13%)]\tLoss: 1541522.250000\n",
            "Train Epoch: 46 [1120/7471 (15%)]\tLoss: 1555896.000000\n",
            "Train Epoch: 46 [1280/7471 (17%)]\tLoss: 1596968.125000\n",
            "Train Epoch: 46 [1440/7471 (19%)]\tLoss: 1604418.375000\n",
            "Train Epoch: 46 [1600/7471 (21%)]\tLoss: 1551365.375000\n",
            "Train Epoch: 46 [1760/7471 (24%)]\tLoss: 1570521.875000\n",
            "Train Epoch: 46 [1920/7471 (26%)]\tLoss: 1567248.375000\n",
            "Train Epoch: 46 [2080/7471 (28%)]\tLoss: 1578973.000000\n",
            "Train Epoch: 46 [2240/7471 (30%)]\tLoss: 1612973.375000\n",
            "Train Epoch: 46 [2400/7471 (32%)]\tLoss: 1587004.375000\n",
            "Train Epoch: 46 [2560/7471 (34%)]\tLoss: 1563945.000000\n",
            "Train Epoch: 46 [2720/7471 (36%)]\tLoss: 1582721.500000\n",
            "Train Epoch: 46 [2880/7471 (39%)]\tLoss: 1587414.625000\n",
            "Train Epoch: 46 [3040/7471 (41%)]\tLoss: 1625098.750000\n",
            "Train Epoch: 46 [3200/7471 (43%)]\tLoss: 1536686.500000\n",
            "Train Epoch: 46 [3360/7471 (45%)]\tLoss: 1629588.875000\n",
            "Train Epoch: 46 [3520/7471 (47%)]\tLoss: 1563563.500000\n",
            "Train Epoch: 46 [3680/7471 (49%)]\tLoss: 1597204.375000\n",
            "Train Epoch: 46 [3840/7471 (51%)]\tLoss: 1490971.625000\n",
            "Train Epoch: 46 [4000/7471 (54%)]\tLoss: 1552796.500000\n",
            "Train Epoch: 46 [4160/7471 (56%)]\tLoss: 1589037.750000\n",
            "Train Epoch: 46 [4320/7471 (58%)]\tLoss: 1605481.000000\n",
            "Train Epoch: 46 [4480/7471 (60%)]\tLoss: 1614937.375000\n",
            "Train Epoch: 46 [4640/7471 (62%)]\tLoss: 1580260.625000\n",
            "Train Epoch: 46 [4800/7471 (64%)]\tLoss: 1615962.125000\n",
            "Train Epoch: 46 [4960/7471 (66%)]\tLoss: 1607879.500000\n",
            "Train Epoch: 46 [5120/7471 (69%)]\tLoss: 1578212.375000\n",
            "Train Epoch: 46 [5280/7471 (71%)]\tLoss: 1622106.125000\n",
            "Train Epoch: 46 [5440/7471 (73%)]\tLoss: 1552165.125000\n",
            "Train Epoch: 46 [5600/7471 (75%)]\tLoss: 1498178.875000\n",
            "Train Epoch: 46 [5760/7471 (77%)]\tLoss: 1560205.500000\n",
            "Train Epoch: 46 [5920/7471 (79%)]\tLoss: 1550162.250000\n",
            "Train Epoch: 46 [6080/7471 (81%)]\tLoss: 1624521.250000\n",
            "Train Epoch: 46 [6240/7471 (84%)]\tLoss: 1637525.250000\n",
            "Train Epoch: 46 [6400/7471 (86%)]\tLoss: 1530318.125000\n",
            "Train Epoch: 46 [6560/7471 (88%)]\tLoss: 1607932.625000\n",
            "Train Epoch: 46 [6720/7471 (90%)]\tLoss: 1587088.375000\n",
            "Train Epoch: 46 [6880/7471 (92%)]\tLoss: 1609609.750000\n",
            "Train Epoch: 46 [7040/7471 (94%)]\tLoss: 1534373.375000\n",
            "Train Epoch: 46 [7200/7471 (96%)]\tLoss: 1578822.375000\n",
            "Train Epoch: 46 [7360/7471 (99%)]\tLoss: 1626248.875000\n",
            "Epoch 46 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98643.0577\n",
            "\n",
            "Train Epoch: 47 [160/7471 (2%)]\tLoss: 1544179.125000\n",
            "Train Epoch: 47 [320/7471 (4%)]\tLoss: 1571171.625000\n",
            "Train Epoch: 47 [480/7471 (6%)]\tLoss: 1487019.875000\n",
            "Train Epoch: 47 [640/7471 (9%)]\tLoss: 1613742.375000\n",
            "Train Epoch: 47 [800/7471 (11%)]\tLoss: 1533853.250000\n",
            "Train Epoch: 47 [960/7471 (13%)]\tLoss: 1545417.500000\n",
            "Train Epoch: 47 [1120/7471 (15%)]\tLoss: 1606167.250000\n",
            "Train Epoch: 47 [1280/7471 (17%)]\tLoss: 1600137.000000\n",
            "Train Epoch: 47 [1440/7471 (19%)]\tLoss: 1608841.625000\n",
            "Train Epoch: 47 [1600/7471 (21%)]\tLoss: 1532955.750000\n",
            "Train Epoch: 47 [1760/7471 (24%)]\tLoss: 1619625.000000\n",
            "Train Epoch: 47 [1920/7471 (26%)]\tLoss: 1608920.000000\n",
            "Train Epoch: 47 [2080/7471 (28%)]\tLoss: 1556452.625000\n",
            "Train Epoch: 47 [2240/7471 (30%)]\tLoss: 1463787.000000\n",
            "Train Epoch: 47 [2400/7471 (32%)]\tLoss: 1605644.000000\n",
            "Train Epoch: 47 [2560/7471 (34%)]\tLoss: 1605019.000000\n",
            "Train Epoch: 47 [2720/7471 (36%)]\tLoss: 1601971.250000\n",
            "Train Epoch: 47 [2880/7471 (39%)]\tLoss: 1563786.750000\n",
            "Train Epoch: 47 [3040/7471 (41%)]\tLoss: 1571373.125000\n",
            "Train Epoch: 47 [3200/7471 (43%)]\tLoss: 1584784.250000\n",
            "Train Epoch: 47 [3360/7471 (45%)]\tLoss: 1605798.125000\n",
            "Train Epoch: 47 [3520/7471 (47%)]\tLoss: 1592580.125000\n",
            "Train Epoch: 47 [3680/7471 (49%)]\tLoss: 1641048.500000\n",
            "Train Epoch: 47 [3840/7471 (51%)]\tLoss: 1607973.750000\n",
            "Train Epoch: 47 [4000/7471 (54%)]\tLoss: 1611942.500000\n",
            "Train Epoch: 47 [4160/7471 (56%)]\tLoss: 1592058.625000\n",
            "Train Epoch: 47 [4320/7471 (58%)]\tLoss: 1544278.500000\n",
            "Train Epoch: 47 [4480/7471 (60%)]\tLoss: 1602897.875000\n",
            "Train Epoch: 47 [4640/7471 (62%)]\tLoss: 1635447.625000\n",
            "Train Epoch: 47 [4800/7471 (64%)]\tLoss: 1602877.375000\n",
            "Train Epoch: 47 [4960/7471 (66%)]\tLoss: 1599890.625000\n",
            "Train Epoch: 47 [5120/7471 (69%)]\tLoss: 1612028.000000\n",
            "Train Epoch: 47 [5280/7471 (71%)]\tLoss: 1525908.000000\n",
            "Train Epoch: 47 [5440/7471 (73%)]\tLoss: 1597098.625000\n",
            "Train Epoch: 47 [5600/7471 (75%)]\tLoss: 1557575.375000\n",
            "Train Epoch: 47 [5760/7471 (77%)]\tLoss: 1542935.375000\n",
            "Train Epoch: 47 [5920/7471 (79%)]\tLoss: 1520707.875000\n",
            "Train Epoch: 47 [6080/7471 (81%)]\tLoss: 1532944.125000\n",
            "Train Epoch: 47 [6240/7471 (84%)]\tLoss: 1604263.125000\n",
            "Train Epoch: 47 [6400/7471 (86%)]\tLoss: 1559815.125000\n",
            "Train Epoch: 47 [6560/7471 (88%)]\tLoss: 1630736.625000\n",
            "Train Epoch: 47 [6720/7471 (90%)]\tLoss: 1643466.875000\n",
            "Train Epoch: 47 [6880/7471 (92%)]\tLoss: 1502669.250000\n",
            "Train Epoch: 47 [7040/7471 (94%)]\tLoss: 1558784.000000\n",
            "Train Epoch: 47 [7200/7471 (96%)]\tLoss: 1511889.625000\n",
            "Train Epoch: 47 [7360/7471 (99%)]\tLoss: 1562543.375000\n",
            "Epoch 47 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98889.9452\n",
            "\n",
            "Train Epoch: 48 [160/7471 (2%)]\tLoss: 1628644.625000\n",
            "Train Epoch: 48 [320/7471 (4%)]\tLoss: 1608522.625000\n",
            "Train Epoch: 48 [480/7471 (6%)]\tLoss: 1547956.000000\n",
            "Train Epoch: 48 [640/7471 (9%)]\tLoss: 1610513.125000\n",
            "Train Epoch: 48 [800/7471 (11%)]\tLoss: 1587166.125000\n",
            "Train Epoch: 48 [960/7471 (13%)]\tLoss: 1538055.625000\n",
            "Train Epoch: 48 [1120/7471 (15%)]\tLoss: 1622694.250000\n",
            "Train Epoch: 48 [1280/7471 (17%)]\tLoss: 1612359.125000\n",
            "Train Epoch: 48 [1440/7471 (19%)]\tLoss: 1570942.125000\n",
            "Train Epoch: 48 [1600/7471 (21%)]\tLoss: 1609106.375000\n",
            "Train Epoch: 48 [1760/7471 (24%)]\tLoss: 1598989.500000\n",
            "Train Epoch: 48 [1920/7471 (26%)]\tLoss: 1570139.125000\n",
            "Train Epoch: 48 [2080/7471 (28%)]\tLoss: 1561135.375000\n",
            "Train Epoch: 48 [2240/7471 (30%)]\tLoss: 1577000.375000\n",
            "Train Epoch: 48 [2400/7471 (32%)]\tLoss: 1573323.750000\n",
            "Train Epoch: 48 [2560/7471 (34%)]\tLoss: 1619975.125000\n",
            "Train Epoch: 48 [2720/7471 (36%)]\tLoss: 1593913.000000\n",
            "Train Epoch: 48 [2880/7471 (39%)]\tLoss: 1557236.750000\n",
            "Train Epoch: 48 [3040/7471 (41%)]\tLoss: 1555309.375000\n",
            "Train Epoch: 48 [3200/7471 (43%)]\tLoss: 1598712.125000\n",
            "Train Epoch: 48 [3360/7471 (45%)]\tLoss: 1581639.875000\n",
            "Train Epoch: 48 [3520/7471 (47%)]\tLoss: 1571445.250000\n",
            "Train Epoch: 48 [3680/7471 (49%)]\tLoss: 1565004.000000\n",
            "Train Epoch: 48 [3840/7471 (51%)]\tLoss: 1601053.250000\n",
            "Train Epoch: 48 [4000/7471 (54%)]\tLoss: 1641309.875000\n",
            "Train Epoch: 48 [4160/7471 (56%)]\tLoss: 1584083.875000\n",
            "Train Epoch: 48 [4320/7471 (58%)]\tLoss: 1554798.875000\n",
            "Train Epoch: 48 [4480/7471 (60%)]\tLoss: 1575482.875000\n",
            "Train Epoch: 48 [4640/7471 (62%)]\tLoss: 1612756.375000\n",
            "Train Epoch: 48 [4800/7471 (64%)]\tLoss: 1572782.250000\n",
            "Train Epoch: 48 [4960/7471 (66%)]\tLoss: 1555854.125000\n",
            "Train Epoch: 48 [5120/7471 (69%)]\tLoss: 1592386.000000\n",
            "Train Epoch: 48 [5280/7471 (71%)]\tLoss: 1576855.000000\n",
            "Train Epoch: 48 [5440/7471 (73%)]\tLoss: 1558962.250000\n",
            "Train Epoch: 48 [5600/7471 (75%)]\tLoss: 1609177.375000\n",
            "Train Epoch: 48 [5760/7471 (77%)]\tLoss: 1601519.750000\n",
            "Train Epoch: 48 [5920/7471 (79%)]\tLoss: 1549710.375000\n",
            "Train Epoch: 48 [6080/7471 (81%)]\tLoss: 1589382.750000\n",
            "Train Epoch: 48 [6240/7471 (84%)]\tLoss: 1597648.000000\n",
            "Train Epoch: 48 [6400/7471 (86%)]\tLoss: 1569824.000000\n",
            "Train Epoch: 48 [6560/7471 (88%)]\tLoss: 1559397.375000\n",
            "Train Epoch: 48 [6720/7471 (90%)]\tLoss: 1599203.500000\n",
            "Train Epoch: 48 [6880/7471 (92%)]\tLoss: 1631887.375000\n",
            "Train Epoch: 48 [7040/7471 (94%)]\tLoss: 1555965.375000\n",
            "Train Epoch: 48 [7200/7471 (96%)]\tLoss: 1639110.500000\n",
            "Train Epoch: 48 [7360/7471 (99%)]\tLoss: 1591172.750000\n",
            "Epoch 48 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 99106.1395\n",
            "\n",
            "Train Epoch: 49 [160/7471 (2%)]\tLoss: 1555179.000000\n",
            "Train Epoch: 49 [320/7471 (4%)]\tLoss: 1608024.125000\n",
            "Train Epoch: 49 [480/7471 (6%)]\tLoss: 1644089.375000\n",
            "Train Epoch: 49 [640/7471 (9%)]\tLoss: 1598810.000000\n",
            "Train Epoch: 49 [800/7471 (11%)]\tLoss: 1627471.625000\n",
            "Train Epoch: 49 [960/7471 (13%)]\tLoss: 1622053.500000\n",
            "Train Epoch: 49 [1120/7471 (15%)]\tLoss: 1568197.125000\n",
            "Train Epoch: 49 [1280/7471 (17%)]\tLoss: 1515400.375000\n",
            "Train Epoch: 49 [1440/7471 (19%)]\tLoss: 1600317.125000\n",
            "Train Epoch: 49 [1600/7471 (21%)]\tLoss: 1625315.500000\n",
            "Train Epoch: 49 [1760/7471 (24%)]\tLoss: 1611097.375000\n",
            "Train Epoch: 49 [1920/7471 (26%)]\tLoss: 1605055.125000\n",
            "Train Epoch: 49 [2080/7471 (28%)]\tLoss: 1485101.000000\n",
            "Train Epoch: 49 [2240/7471 (30%)]\tLoss: 1578825.875000\n",
            "Train Epoch: 49 [2400/7471 (32%)]\tLoss: 1620203.250000\n",
            "Train Epoch: 49 [2560/7471 (34%)]\tLoss: 1607283.000000\n",
            "Train Epoch: 49 [2720/7471 (36%)]\tLoss: 1583672.375000\n",
            "Train Epoch: 49 [2880/7471 (39%)]\tLoss: 1596731.875000\n",
            "Train Epoch: 49 [3040/7471 (41%)]\tLoss: 1593184.125000\n",
            "Train Epoch: 49 [3200/7471 (43%)]\tLoss: 1564504.375000\n",
            "Train Epoch: 49 [3360/7471 (45%)]\tLoss: 1625015.500000\n",
            "Train Epoch: 49 [3520/7471 (47%)]\tLoss: 1601564.750000\n",
            "Train Epoch: 49 [3680/7471 (49%)]\tLoss: 1506765.750000\n",
            "Train Epoch: 49 [3840/7471 (51%)]\tLoss: 1631887.000000\n",
            "Train Epoch: 49 [4000/7471 (54%)]\tLoss: 1607942.000000\n",
            "Train Epoch: 49 [4160/7471 (56%)]\tLoss: 1605026.125000\n",
            "Train Epoch: 49 [4320/7471 (58%)]\tLoss: 1621498.250000\n",
            "Train Epoch: 49 [4480/7471 (60%)]\tLoss: 1599889.375000\n",
            "Train Epoch: 49 [4640/7471 (62%)]\tLoss: 1613995.875000\n",
            "Train Epoch: 49 [4800/7471 (64%)]\tLoss: 1545820.875000\n",
            "Train Epoch: 49 [4960/7471 (66%)]\tLoss: 1544253.750000\n",
            "Train Epoch: 49 [5120/7471 (69%)]\tLoss: 1589514.625000\n",
            "Train Epoch: 49 [5280/7471 (71%)]\tLoss: 1521946.000000\n",
            "Train Epoch: 49 [5440/7471 (73%)]\tLoss: 1554908.500000\n",
            "Train Epoch: 49 [5600/7471 (75%)]\tLoss: 1584115.250000\n",
            "Train Epoch: 49 [5760/7471 (77%)]\tLoss: 1561549.125000\n",
            "Train Epoch: 49 [5920/7471 (79%)]\tLoss: 1553825.125000\n",
            "Train Epoch: 49 [6080/7471 (81%)]\tLoss: 1594818.750000\n",
            "Train Epoch: 49 [6240/7471 (84%)]\tLoss: 1388290.250000\n",
            "Train Epoch: 49 [6400/7471 (86%)]\tLoss: 1631772.625000\n",
            "Train Epoch: 49 [6560/7471 (88%)]\tLoss: 1520496.500000\n",
            "Train Epoch: 49 [6720/7471 (90%)]\tLoss: 1614994.500000\n",
            "Train Epoch: 49 [6880/7471 (92%)]\tLoss: 1617952.750000\n",
            "Train Epoch: 49 [7040/7471 (94%)]\tLoss: 1546907.375000\n",
            "Train Epoch: 49 [7200/7471 (96%)]\tLoss: 1607328.625000\n",
            "Train Epoch: 49 [7360/7471 (99%)]\tLoss: 1620234.125000\n",
            "Epoch 49 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98603.7253\n",
            "\n",
            "Train Epoch: 50 [160/7471 (2%)]\tLoss: 1620111.875000\n",
            "Train Epoch: 50 [320/7471 (4%)]\tLoss: 1640216.000000\n",
            "Train Epoch: 50 [480/7471 (6%)]\tLoss: 1575056.625000\n",
            "Train Epoch: 50 [640/7471 (9%)]\tLoss: 1547548.875000\n",
            "Train Epoch: 50 [800/7471 (11%)]\tLoss: 1579250.875000\n",
            "Train Epoch: 50 [960/7471 (13%)]\tLoss: 1611081.000000\n",
            "Train Epoch: 50 [1120/7471 (15%)]\tLoss: 1533414.375000\n",
            "Train Epoch: 50 [1280/7471 (17%)]\tLoss: 1569378.000000\n",
            "Train Epoch: 50 [1440/7471 (19%)]\tLoss: 1598009.375000\n",
            "Train Epoch: 50 [1600/7471 (21%)]\tLoss: 1545798.625000\n",
            "Train Epoch: 50 [1760/7471 (24%)]\tLoss: 1631457.000000\n",
            "Train Epoch: 50 [1920/7471 (26%)]\tLoss: 1553384.625000\n",
            "Train Epoch: 50 [2080/7471 (28%)]\tLoss: 1543299.625000\n",
            "Train Epoch: 50 [2240/7471 (30%)]\tLoss: 1585402.250000\n",
            "Train Epoch: 50 [2400/7471 (32%)]\tLoss: 1514298.875000\n",
            "Train Epoch: 50 [2560/7471 (34%)]\tLoss: 1620287.750000\n",
            "Train Epoch: 50 [2720/7471 (36%)]\tLoss: 1616872.125000\n",
            "Train Epoch: 50 [2880/7471 (39%)]\tLoss: 1561115.875000\n",
            "Train Epoch: 50 [3040/7471 (41%)]\tLoss: 1557945.500000\n",
            "Train Epoch: 50 [3200/7471 (43%)]\tLoss: 1620918.250000\n",
            "Train Epoch: 50 [3360/7471 (45%)]\tLoss: 1586703.000000\n",
            "Train Epoch: 50 [3520/7471 (47%)]\tLoss: 1585004.125000\n",
            "Train Epoch: 50 [3680/7471 (49%)]\tLoss: 1589503.875000\n",
            "Train Epoch: 50 [3840/7471 (51%)]\tLoss: 1553622.500000\n",
            "Train Epoch: 50 [4000/7471 (54%)]\tLoss: 1569042.125000\n",
            "Train Epoch: 50 [4160/7471 (56%)]\tLoss: 1608705.000000\n",
            "Train Epoch: 50 [4320/7471 (58%)]\tLoss: 1602956.750000\n",
            "Train Epoch: 50 [4480/7471 (60%)]\tLoss: 1525430.875000\n",
            "Train Epoch: 50 [4640/7471 (62%)]\tLoss: 1560402.250000\n",
            "Train Epoch: 50 [4800/7471 (64%)]\tLoss: 1538984.750000\n",
            "Train Epoch: 50 [4960/7471 (66%)]\tLoss: 1537379.750000\n",
            "Train Epoch: 50 [5120/7471 (69%)]\tLoss: 1558039.125000\n",
            "Train Epoch: 50 [5280/7471 (71%)]\tLoss: 1531314.500000\n",
            "Train Epoch: 50 [5440/7471 (73%)]\tLoss: 1611209.250000\n",
            "Train Epoch: 50 [5600/7471 (75%)]\tLoss: 1621486.625000\n",
            "Train Epoch: 50 [5760/7471 (77%)]\tLoss: 1537647.500000\n",
            "Train Epoch: 50 [5920/7471 (79%)]\tLoss: 1611742.375000\n",
            "Train Epoch: 50 [6080/7471 (81%)]\tLoss: 1529586.500000\n",
            "Train Epoch: 50 [6240/7471 (84%)]\tLoss: 1546954.875000\n",
            "Train Epoch: 50 [6400/7471 (86%)]\tLoss: 1579989.625000\n",
            "Train Epoch: 50 [6560/7471 (88%)]\tLoss: 1606646.500000\n",
            "Train Epoch: 50 [6720/7471 (90%)]\tLoss: 1572132.625000\n",
            "Train Epoch: 50 [6880/7471 (92%)]\tLoss: 1563099.250000\n",
            "Train Epoch: 50 [7040/7471 (94%)]\tLoss: 1567068.375000\n",
            "Train Epoch: 50 [7200/7471 (96%)]\tLoss: 1588597.125000\n",
            "Train Epoch: 50 [7360/7471 (99%)]\tLoss: 1639485.750000\n",
            "Epoch 50 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98592.2494\n",
            "\n",
            "Train Epoch: 51 [160/7471 (2%)]\tLoss: 1545507.125000\n",
            "Train Epoch: 51 [320/7471 (4%)]\tLoss: 1598190.000000\n",
            "Train Epoch: 51 [480/7471 (6%)]\tLoss: 1593003.250000\n",
            "Train Epoch: 51 [640/7471 (9%)]\tLoss: 1591991.125000\n",
            "Train Epoch: 51 [800/7471 (11%)]\tLoss: 1544646.750000\n",
            "Train Epoch: 51 [960/7471 (13%)]\tLoss: 1533177.375000\n",
            "Train Epoch: 51 [1120/7471 (15%)]\tLoss: 1605477.500000\n",
            "Train Epoch: 51 [1280/7471 (17%)]\tLoss: 1621611.125000\n",
            "Train Epoch: 51 [1440/7471 (19%)]\tLoss: 1574692.250000\n",
            "Train Epoch: 51 [1600/7471 (21%)]\tLoss: 1622129.250000\n",
            "Train Epoch: 51 [1760/7471 (24%)]\tLoss: 1602614.375000\n",
            "Train Epoch: 51 [1920/7471 (26%)]\tLoss: 1628435.000000\n",
            "Train Epoch: 51 [2080/7471 (28%)]\tLoss: 1617743.750000\n",
            "Train Epoch: 51 [2240/7471 (30%)]\tLoss: 1611372.125000\n",
            "Train Epoch: 51 [2400/7471 (32%)]\tLoss: 1628986.750000\n",
            "Train Epoch: 51 [2560/7471 (34%)]\tLoss: 1488620.125000\n",
            "Train Epoch: 51 [2720/7471 (36%)]\tLoss: 1562236.250000\n",
            "Train Epoch: 51 [2880/7471 (39%)]\tLoss: 1637709.375000\n",
            "Train Epoch: 51 [3040/7471 (41%)]\tLoss: 1563164.250000\n",
            "Train Epoch: 51 [3200/7471 (43%)]\tLoss: 1569640.375000\n",
            "Train Epoch: 51 [3360/7471 (45%)]\tLoss: 1578378.375000\n",
            "Train Epoch: 51 [3520/7471 (47%)]\tLoss: 1596096.875000\n",
            "Train Epoch: 51 [3680/7471 (49%)]\tLoss: 1602500.500000\n",
            "Train Epoch: 51 [3840/7471 (51%)]\tLoss: 1579296.125000\n",
            "Train Epoch: 51 [4000/7471 (54%)]\tLoss: 1627398.750000\n",
            "Train Epoch: 51 [4160/7471 (56%)]\tLoss: 1573194.625000\n",
            "Train Epoch: 51 [4320/7471 (58%)]\tLoss: 1603911.375000\n",
            "Train Epoch: 51 [4480/7471 (60%)]\tLoss: 1612024.375000\n",
            "Train Epoch: 51 [4640/7471 (62%)]\tLoss: 1549814.000000\n",
            "Train Epoch: 51 [4800/7471 (64%)]\tLoss: 1509927.000000\n",
            "Train Epoch: 51 [4960/7471 (66%)]\tLoss: 1646668.125000\n",
            "Train Epoch: 51 [5120/7471 (69%)]\tLoss: 1562836.125000\n",
            "Train Epoch: 51 [5280/7471 (71%)]\tLoss: 1597987.000000\n",
            "Train Epoch: 51 [5440/7471 (73%)]\tLoss: 1609987.625000\n",
            "Train Epoch: 51 [5600/7471 (75%)]\tLoss: 1535576.250000\n",
            "Train Epoch: 51 [5760/7471 (77%)]\tLoss: 1600053.500000\n",
            "Train Epoch: 51 [5920/7471 (79%)]\tLoss: 1544519.125000\n",
            "Train Epoch: 51 [6080/7471 (81%)]\tLoss: 1569017.375000\n",
            "Train Epoch: 51 [6240/7471 (84%)]\tLoss: 1575848.625000\n",
            "Train Epoch: 51 [6400/7471 (86%)]\tLoss: 1584058.125000\n",
            "Train Epoch: 51 [6560/7471 (88%)]\tLoss: 1591562.875000\n",
            "Train Epoch: 51 [6720/7471 (90%)]\tLoss: 1528447.000000\n",
            "Train Epoch: 51 [6880/7471 (92%)]\tLoss: 1611568.625000\n",
            "Train Epoch: 51 [7040/7471 (94%)]\tLoss: 1533924.125000\n",
            "Train Epoch: 51 [7200/7471 (96%)]\tLoss: 1571466.500000\n",
            "Train Epoch: 51 [7360/7471 (99%)]\tLoss: 1566466.750000\n",
            "Epoch 51 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98606.9897\n",
            "\n",
            "Train Epoch: 52 [160/7471 (2%)]\tLoss: 1601572.750000\n",
            "Train Epoch: 52 [320/7471 (4%)]\tLoss: 1614755.000000\n",
            "Train Epoch: 52 [480/7471 (6%)]\tLoss: 1608372.500000\n",
            "Train Epoch: 52 [640/7471 (9%)]\tLoss: 1553574.500000\n",
            "Train Epoch: 52 [800/7471 (11%)]\tLoss: 1637379.625000\n",
            "Train Epoch: 52 [960/7471 (13%)]\tLoss: 1571000.250000\n",
            "Train Epoch: 52 [1120/7471 (15%)]\tLoss: 1551815.625000\n",
            "Train Epoch: 52 [1280/7471 (17%)]\tLoss: 1492663.125000\n",
            "Train Epoch: 52 [1440/7471 (19%)]\tLoss: 1595307.250000\n",
            "Train Epoch: 52 [1600/7471 (21%)]\tLoss: 1603084.500000\n",
            "Train Epoch: 52 [1760/7471 (24%)]\tLoss: 1561270.625000\n",
            "Train Epoch: 52 [1920/7471 (26%)]\tLoss: 1558607.500000\n",
            "Train Epoch: 52 [2080/7471 (28%)]\tLoss: 1571844.375000\n",
            "Train Epoch: 52 [2240/7471 (30%)]\tLoss: 1583203.500000\n",
            "Train Epoch: 52 [2400/7471 (32%)]\tLoss: 1602480.500000\n",
            "Train Epoch: 52 [2560/7471 (34%)]\tLoss: 1542979.750000\n",
            "Train Epoch: 52 [2720/7471 (36%)]\tLoss: 1546553.000000\n",
            "Train Epoch: 52 [2880/7471 (39%)]\tLoss: 1593529.250000\n",
            "Train Epoch: 52 [3040/7471 (41%)]\tLoss: 1460760.625000\n",
            "Train Epoch: 52 [3200/7471 (43%)]\tLoss: 1590982.875000\n",
            "Train Epoch: 52 [3360/7471 (45%)]\tLoss: 1617678.875000\n",
            "Train Epoch: 52 [3520/7471 (47%)]\tLoss: 1589595.625000\n",
            "Train Epoch: 52 [3680/7471 (49%)]\tLoss: 1588098.250000\n",
            "Train Epoch: 52 [3840/7471 (51%)]\tLoss: 1628166.625000\n",
            "Train Epoch: 52 [4000/7471 (54%)]\tLoss: 1623358.375000\n",
            "Train Epoch: 52 [4160/7471 (56%)]\tLoss: 1575955.375000\n",
            "Train Epoch: 52 [4320/7471 (58%)]\tLoss: 1601357.500000\n",
            "Train Epoch: 52 [4480/7471 (60%)]\tLoss: 1514750.750000\n",
            "Train Epoch: 52 [4640/7471 (62%)]\tLoss: 1514072.000000\n",
            "Train Epoch: 52 [4800/7471 (64%)]\tLoss: 1615916.500000\n",
            "Train Epoch: 52 [4960/7471 (66%)]\tLoss: 1549871.375000\n",
            "Train Epoch: 52 [5120/7471 (69%)]\tLoss: 1606356.750000\n",
            "Train Epoch: 52 [5280/7471 (71%)]\tLoss: 1607861.500000\n",
            "Train Epoch: 52 [5440/7471 (73%)]\tLoss: 1568715.750000\n",
            "Train Epoch: 52 [5600/7471 (75%)]\tLoss: 1597772.500000\n",
            "Train Epoch: 52 [5760/7471 (77%)]\tLoss: 1619700.875000\n",
            "Train Epoch: 52 [5920/7471 (79%)]\tLoss: 1653830.875000\n",
            "Train Epoch: 52 [6080/7471 (81%)]\tLoss: 1605100.875000\n",
            "Train Epoch: 52 [6240/7471 (84%)]\tLoss: 1614223.875000\n",
            "Train Epoch: 52 [6400/7471 (86%)]\tLoss: 1593505.250000\n",
            "Train Epoch: 52 [6560/7471 (88%)]\tLoss: 1640365.375000\n",
            "Train Epoch: 52 [6720/7471 (90%)]\tLoss: 1506536.875000\n",
            "Train Epoch: 52 [6880/7471 (92%)]\tLoss: 1607338.500000\n",
            "Train Epoch: 52 [7040/7471 (94%)]\tLoss: 1611552.500000\n",
            "Train Epoch: 52 [7200/7471 (96%)]\tLoss: 1637168.500000\n",
            "Train Epoch: 52 [7360/7471 (99%)]\tLoss: 1539757.875000\n",
            "Epoch 52 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98673.9283\n",
            "\n",
            "Train Epoch: 53 [160/7471 (2%)]\tLoss: 1555717.125000\n",
            "Train Epoch: 53 [320/7471 (4%)]\tLoss: 1551914.000000\n",
            "Train Epoch: 53 [480/7471 (6%)]\tLoss: 1595029.750000\n",
            "Train Epoch: 53 [640/7471 (9%)]\tLoss: 1581352.500000\n",
            "Train Epoch: 53 [800/7471 (11%)]\tLoss: 1541470.500000\n",
            "Train Epoch: 53 [960/7471 (13%)]\tLoss: 1543414.500000\n",
            "Train Epoch: 53 [1120/7471 (15%)]\tLoss: 1639724.500000\n",
            "Train Epoch: 53 [1280/7471 (17%)]\tLoss: 1619119.875000\n",
            "Train Epoch: 53 [1440/7471 (19%)]\tLoss: 1584877.375000\n",
            "Train Epoch: 53 [1600/7471 (21%)]\tLoss: 1584422.000000\n",
            "Train Epoch: 53 [1760/7471 (24%)]\tLoss: 1495611.625000\n",
            "Train Epoch: 53 [1920/7471 (26%)]\tLoss: 1522331.000000\n",
            "Train Epoch: 53 [2080/7471 (28%)]\tLoss: 1586252.875000\n",
            "Train Epoch: 53 [2240/7471 (30%)]\tLoss: 1619389.375000\n",
            "Train Epoch: 53 [2400/7471 (32%)]\tLoss: 1586039.750000\n",
            "Train Epoch: 53 [2560/7471 (34%)]\tLoss: 1538611.250000\n",
            "Train Epoch: 53 [2720/7471 (36%)]\tLoss: 1597077.875000\n",
            "Train Epoch: 53 [2880/7471 (39%)]\tLoss: 1611255.625000\n",
            "Train Epoch: 53 [3040/7471 (41%)]\tLoss: 1577120.500000\n",
            "Train Epoch: 53 [3200/7471 (43%)]\tLoss: 1602650.250000\n",
            "Train Epoch: 53 [3360/7471 (45%)]\tLoss: 1603524.000000\n",
            "Train Epoch: 53 [3520/7471 (47%)]\tLoss: 1570258.000000\n",
            "Train Epoch: 53 [3680/7471 (49%)]\tLoss: 1467973.875000\n",
            "Train Epoch: 53 [3840/7471 (51%)]\tLoss: 1507083.125000\n",
            "Train Epoch: 53 [4000/7471 (54%)]\tLoss: 1625100.250000\n",
            "Train Epoch: 53 [4160/7471 (56%)]\tLoss: 1623047.125000\n",
            "Train Epoch: 53 [4320/7471 (58%)]\tLoss: 1535148.750000\n",
            "Train Epoch: 53 [4480/7471 (60%)]\tLoss: 1621311.625000\n",
            "Train Epoch: 53 [4640/7471 (62%)]\tLoss: 1612079.250000\n",
            "Train Epoch: 53 [4800/7471 (64%)]\tLoss: 1567489.500000\n",
            "Train Epoch: 53 [4960/7471 (66%)]\tLoss: 1611600.375000\n",
            "Train Epoch: 53 [5120/7471 (69%)]\tLoss: 1623826.625000\n",
            "Train Epoch: 53 [5280/7471 (71%)]\tLoss: 1600143.375000\n",
            "Train Epoch: 53 [5440/7471 (73%)]\tLoss: 1568674.000000\n",
            "Train Epoch: 53 [5600/7471 (75%)]\tLoss: 1549429.250000\n",
            "Train Epoch: 53 [5760/7471 (77%)]\tLoss: 1638874.250000\n",
            "Train Epoch: 53 [5920/7471 (79%)]\tLoss: 1577698.875000\n",
            "Train Epoch: 53 [6080/7471 (81%)]\tLoss: 1589683.000000\n",
            "Train Epoch: 53 [6240/7471 (84%)]\tLoss: 1557739.500000\n",
            "Train Epoch: 53 [6400/7471 (86%)]\tLoss: 1531233.875000\n",
            "Train Epoch: 53 [6560/7471 (88%)]\tLoss: 1611268.875000\n",
            "Train Epoch: 53 [6720/7471 (90%)]\tLoss: 1602802.750000\n",
            "Train Epoch: 53 [6880/7471 (92%)]\tLoss: 1551344.750000\n",
            "Train Epoch: 53 [7040/7471 (94%)]\tLoss: 1535859.375000\n",
            "Train Epoch: 53 [7200/7471 (96%)]\tLoss: 1613938.375000\n",
            "Train Epoch: 53 [7360/7471 (99%)]\tLoss: 1599985.625000\n",
            "Epoch 53 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98901.1277\n",
            "\n",
            "Train Epoch: 54 [160/7471 (2%)]\tLoss: 1559581.625000\n",
            "Train Epoch: 54 [320/7471 (4%)]\tLoss: 1587398.750000\n",
            "Train Epoch: 54 [480/7471 (6%)]\tLoss: 1532066.625000\n",
            "Train Epoch: 54 [640/7471 (9%)]\tLoss: 1566675.500000\n",
            "Train Epoch: 54 [800/7471 (11%)]\tLoss: 1588218.500000\n",
            "Train Epoch: 54 [960/7471 (13%)]\tLoss: 1613783.500000\n",
            "Train Epoch: 54 [1120/7471 (15%)]\tLoss: 1528939.875000\n",
            "Train Epoch: 54 [1280/7471 (17%)]\tLoss: 1609115.125000\n",
            "Train Epoch: 54 [1440/7471 (19%)]\tLoss: 1558668.625000\n",
            "Train Epoch: 54 [1600/7471 (21%)]\tLoss: 1612624.250000\n",
            "Train Epoch: 54 [1760/7471 (24%)]\tLoss: 1594462.125000\n",
            "Train Epoch: 54 [1920/7471 (26%)]\tLoss: 1643339.750000\n",
            "Train Epoch: 54 [2080/7471 (28%)]\tLoss: 1573850.375000\n",
            "Train Epoch: 54 [2240/7471 (30%)]\tLoss: 1584530.375000\n",
            "Train Epoch: 54 [2400/7471 (32%)]\tLoss: 1525132.500000\n",
            "Train Epoch: 54 [2560/7471 (34%)]\tLoss: 1589924.625000\n",
            "Train Epoch: 54 [2720/7471 (36%)]\tLoss: 1560872.000000\n",
            "Train Epoch: 54 [2880/7471 (39%)]\tLoss: 1569026.250000\n",
            "Train Epoch: 54 [3040/7471 (41%)]\tLoss: 1574885.750000\n",
            "Train Epoch: 54 [3200/7471 (43%)]\tLoss: 1501230.000000\n",
            "Train Epoch: 54 [3360/7471 (45%)]\tLoss: 1615478.625000\n",
            "Train Epoch: 54 [3520/7471 (47%)]\tLoss: 1526249.125000\n",
            "Train Epoch: 54 [3680/7471 (49%)]\tLoss: 1607618.875000\n",
            "Train Epoch: 54 [3840/7471 (51%)]\tLoss: 1601525.125000\n",
            "Train Epoch: 54 [4000/7471 (54%)]\tLoss: 1619305.250000\n",
            "Train Epoch: 54 [4160/7471 (56%)]\tLoss: 1541421.375000\n",
            "Train Epoch: 54 [4320/7471 (58%)]\tLoss: 1582620.875000\n",
            "Train Epoch: 54 [4480/7471 (60%)]\tLoss: 1545361.750000\n",
            "Train Epoch: 54 [4640/7471 (62%)]\tLoss: 1600800.000000\n",
            "Train Epoch: 54 [4800/7471 (64%)]\tLoss: 1628873.250000\n",
            "Train Epoch: 54 [4960/7471 (66%)]\tLoss: 1555583.375000\n",
            "Train Epoch: 54 [5120/7471 (69%)]\tLoss: 1593052.625000\n",
            "Train Epoch: 54 [5280/7471 (71%)]\tLoss: 1583850.125000\n",
            "Train Epoch: 54 [5440/7471 (73%)]\tLoss: 1554910.125000\n",
            "Train Epoch: 54 [5600/7471 (75%)]\tLoss: 1602736.750000\n",
            "Train Epoch: 54 [5760/7471 (77%)]\tLoss: 1578763.250000\n",
            "Train Epoch: 54 [5920/7471 (79%)]\tLoss: 1558581.625000\n",
            "Train Epoch: 54 [6080/7471 (81%)]\tLoss: 1568688.000000\n",
            "Train Epoch: 54 [6240/7471 (84%)]\tLoss: 1583554.125000\n",
            "Train Epoch: 54 [6400/7471 (86%)]\tLoss: 1609308.375000\n",
            "Train Epoch: 54 [6560/7471 (88%)]\tLoss: 1594125.250000\n",
            "Train Epoch: 54 [6720/7471 (90%)]\tLoss: 1578373.625000\n",
            "Train Epoch: 54 [6880/7471 (92%)]\tLoss: 1570674.375000\n",
            "Train Epoch: 54 [7040/7471 (94%)]\tLoss: 1585669.500000\n",
            "Train Epoch: 54 [7200/7471 (96%)]\tLoss: 1536459.000000\n",
            "Train Epoch: 54 [7360/7471 (99%)]\tLoss: 1589563.625000\n",
            "Epoch 54 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98609.7878\n",
            "\n",
            "Train Epoch: 55 [160/7471 (2%)]\tLoss: 1560911.000000\n",
            "Train Epoch: 55 [320/7471 (4%)]\tLoss: 1551171.625000\n",
            "Train Epoch: 55 [480/7471 (6%)]\tLoss: 1598628.625000\n",
            "Train Epoch: 55 [640/7471 (9%)]\tLoss: 1629318.500000\n",
            "Train Epoch: 55 [800/7471 (11%)]\tLoss: 1585198.250000\n",
            "Train Epoch: 55 [960/7471 (13%)]\tLoss: 1538247.750000\n",
            "Train Epoch: 55 [1120/7471 (15%)]\tLoss: 1595349.750000\n",
            "Train Epoch: 55 [1280/7471 (17%)]\tLoss: 1547313.750000\n",
            "Train Epoch: 55 [1440/7471 (19%)]\tLoss: 1597579.250000\n",
            "Train Epoch: 55 [1600/7471 (21%)]\tLoss: 1522829.125000\n",
            "Train Epoch: 55 [1760/7471 (24%)]\tLoss: 1570212.875000\n",
            "Train Epoch: 55 [1920/7471 (26%)]\tLoss: 1601122.875000\n",
            "Train Epoch: 55 [2080/7471 (28%)]\tLoss: 1617820.500000\n",
            "Train Epoch: 55 [2240/7471 (30%)]\tLoss: 1571184.625000\n",
            "Train Epoch: 55 [2400/7471 (32%)]\tLoss: 1614786.375000\n",
            "Train Epoch: 55 [2560/7471 (34%)]\tLoss: 1444993.000000\n",
            "Train Epoch: 55 [2720/7471 (36%)]\tLoss: 1562986.875000\n",
            "Train Epoch: 55 [2880/7471 (39%)]\tLoss: 1508375.125000\n",
            "Train Epoch: 55 [3040/7471 (41%)]\tLoss: 1639723.000000\n",
            "Train Epoch: 55 [3200/7471 (43%)]\tLoss: 1627053.625000\n",
            "Train Epoch: 55 [3360/7471 (45%)]\tLoss: 1620217.250000\n",
            "Train Epoch: 55 [3520/7471 (47%)]\tLoss: 1573061.000000\n",
            "Train Epoch: 55 [3680/7471 (49%)]\tLoss: 1582025.750000\n",
            "Train Epoch: 55 [3840/7471 (51%)]\tLoss: 1597578.000000\n",
            "Train Epoch: 55 [4000/7471 (54%)]\tLoss: 1608164.000000\n",
            "Train Epoch: 55 [4160/7471 (56%)]\tLoss: 1585720.000000\n",
            "Train Epoch: 55 [4320/7471 (58%)]\tLoss: 1591149.625000\n",
            "Train Epoch: 55 [4480/7471 (60%)]\tLoss: 1574526.750000\n",
            "Train Epoch: 55 [4640/7471 (62%)]\tLoss: 1594393.000000\n",
            "Train Epoch: 55 [4800/7471 (64%)]\tLoss: 1574724.625000\n",
            "Train Epoch: 55 [4960/7471 (66%)]\tLoss: 1567489.500000\n",
            "Train Epoch: 55 [5120/7471 (69%)]\tLoss: 1586632.750000\n",
            "Train Epoch: 55 [5280/7471 (71%)]\tLoss: 1596419.125000\n",
            "Train Epoch: 55 [5440/7471 (73%)]\tLoss: 1556864.250000\n",
            "Train Epoch: 55 [5600/7471 (75%)]\tLoss: 1521355.125000\n",
            "Train Epoch: 55 [5760/7471 (77%)]\tLoss: 1539538.250000\n",
            "Train Epoch: 55 [5920/7471 (79%)]\tLoss: 1627608.375000\n",
            "Train Epoch: 55 [6080/7471 (81%)]\tLoss: 1604466.000000\n",
            "Train Epoch: 55 [6240/7471 (84%)]\tLoss: 1598955.625000\n",
            "Train Epoch: 55 [6400/7471 (86%)]\tLoss: 1573514.875000\n",
            "Train Epoch: 55 [6560/7471 (88%)]\tLoss: 1610992.625000\n",
            "Train Epoch: 55 [6720/7471 (90%)]\tLoss: 1563661.750000\n",
            "Train Epoch: 55 [6880/7471 (92%)]\tLoss: 1560164.250000\n",
            "Train Epoch: 55 [7040/7471 (94%)]\tLoss: 1615914.375000\n",
            "Train Epoch: 55 [7200/7471 (96%)]\tLoss: 1626710.125000\n",
            "Train Epoch: 55 [7360/7471 (99%)]\tLoss: 1626796.750000\n",
            "Epoch 55 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98621.3919\n",
            "\n",
            "Train Epoch: 56 [160/7471 (2%)]\tLoss: 1597075.125000\n",
            "Train Epoch: 56 [320/7471 (4%)]\tLoss: 1657889.125000\n",
            "Train Epoch: 56 [480/7471 (6%)]\tLoss: 1559448.875000\n",
            "Train Epoch: 56 [640/7471 (9%)]\tLoss: 1568495.125000\n",
            "Train Epoch: 56 [800/7471 (11%)]\tLoss: 1606716.000000\n",
            "Train Epoch: 56 [960/7471 (13%)]\tLoss: 1592957.125000\n",
            "Train Epoch: 56 [1120/7471 (15%)]\tLoss: 1519969.250000\n",
            "Train Epoch: 56 [1280/7471 (17%)]\tLoss: 1525032.375000\n",
            "Train Epoch: 56 [1440/7471 (19%)]\tLoss: 1550016.750000\n",
            "Train Epoch: 56 [1600/7471 (21%)]\tLoss: 1610154.125000\n",
            "Train Epoch: 56 [1760/7471 (24%)]\tLoss: 1570467.750000\n",
            "Train Epoch: 56 [1920/7471 (26%)]\tLoss: 1590605.375000\n",
            "Train Epoch: 56 [2080/7471 (28%)]\tLoss: 1476956.125000\n",
            "Train Epoch: 56 [2240/7471 (30%)]\tLoss: 1597669.375000\n",
            "Train Epoch: 56 [2400/7471 (32%)]\tLoss: 1649476.250000\n",
            "Train Epoch: 56 [2560/7471 (34%)]\tLoss: 1567092.750000\n",
            "Train Epoch: 56 [2720/7471 (36%)]\tLoss: 1544365.875000\n",
            "Train Epoch: 56 [2880/7471 (39%)]\tLoss: 1557029.500000\n",
            "Train Epoch: 56 [3040/7471 (41%)]\tLoss: 1614865.875000\n",
            "Train Epoch: 56 [3200/7471 (43%)]\tLoss: 1587127.750000\n",
            "Train Epoch: 56 [3360/7471 (45%)]\tLoss: 1546827.375000\n",
            "Train Epoch: 56 [3520/7471 (47%)]\tLoss: 1593554.375000\n",
            "Train Epoch: 56 [3680/7471 (49%)]\tLoss: 1643182.000000\n",
            "Train Epoch: 56 [3840/7471 (51%)]\tLoss: 1579069.250000\n",
            "Train Epoch: 56 [4000/7471 (54%)]\tLoss: 1602214.875000\n",
            "Train Epoch: 56 [4160/7471 (56%)]\tLoss: 1626907.000000\n",
            "Train Epoch: 56 [4320/7471 (58%)]\tLoss: 1588841.625000\n",
            "Train Epoch: 56 [4480/7471 (60%)]\tLoss: 1620462.625000\n",
            "Train Epoch: 56 [4640/7471 (62%)]\tLoss: 1504633.750000\n",
            "Train Epoch: 56 [4800/7471 (64%)]\tLoss: 1648927.875000\n",
            "Train Epoch: 56 [4960/7471 (66%)]\tLoss: 1599379.125000\n",
            "Train Epoch: 56 [5120/7471 (69%)]\tLoss: 1504390.500000\n",
            "Train Epoch: 56 [5280/7471 (71%)]\tLoss: 1626926.125000\n",
            "Train Epoch: 56 [5440/7471 (73%)]\tLoss: 1525977.250000\n",
            "Train Epoch: 56 [5600/7471 (75%)]\tLoss: 1560920.125000\n",
            "Train Epoch: 56 [5760/7471 (77%)]\tLoss: 1559933.000000\n",
            "Train Epoch: 56 [5920/7471 (79%)]\tLoss: 1571362.375000\n",
            "Train Epoch: 56 [6080/7471 (81%)]\tLoss: 1582832.125000\n",
            "Train Epoch: 56 [6240/7471 (84%)]\tLoss: 1544591.000000\n",
            "Train Epoch: 56 [6400/7471 (86%)]\tLoss: 1520650.875000\n",
            "Train Epoch: 56 [6560/7471 (88%)]\tLoss: 1599550.250000\n",
            "Train Epoch: 56 [6720/7471 (90%)]\tLoss: 1635131.250000\n",
            "Train Epoch: 56 [6880/7471 (92%)]\tLoss: 1526688.125000\n",
            "Train Epoch: 56 [7040/7471 (94%)]\tLoss: 1578709.250000\n",
            "Train Epoch: 56 [7200/7471 (96%)]\tLoss: 1533039.250000\n",
            "Train Epoch: 56 [7360/7471 (99%)]\tLoss: 1618232.500000\n",
            "Epoch 56 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98598.1963\n",
            "\n",
            "Train Epoch: 57 [160/7471 (2%)]\tLoss: 1572212.875000\n",
            "Train Epoch: 57 [320/7471 (4%)]\tLoss: 1555545.500000\n",
            "Train Epoch: 57 [480/7471 (6%)]\tLoss: 1581884.250000\n",
            "Train Epoch: 57 [640/7471 (9%)]\tLoss: 1626617.125000\n",
            "Train Epoch: 57 [800/7471 (11%)]\tLoss: 1584222.625000\n",
            "Train Epoch: 57 [960/7471 (13%)]\tLoss: 1537974.250000\n",
            "Train Epoch: 57 [1120/7471 (15%)]\tLoss: 1494748.000000\n",
            "Train Epoch: 57 [1280/7471 (17%)]\tLoss: 1528745.375000\n",
            "Train Epoch: 57 [1440/7471 (19%)]\tLoss: 1596751.250000\n",
            "Train Epoch: 57 [1600/7471 (21%)]\tLoss: 1640793.375000\n",
            "Train Epoch: 57 [1760/7471 (24%)]\tLoss: 1556347.375000\n",
            "Train Epoch: 57 [1920/7471 (26%)]\tLoss: 1602971.000000\n",
            "Train Epoch: 57 [2080/7471 (28%)]\tLoss: 1609091.125000\n",
            "Train Epoch: 57 [2240/7471 (30%)]\tLoss: 1554674.250000\n",
            "Train Epoch: 57 [2400/7471 (32%)]\tLoss: 1572425.125000\n",
            "Train Epoch: 57 [2560/7471 (34%)]\tLoss: 1552727.250000\n",
            "Train Epoch: 57 [2720/7471 (36%)]\tLoss: 1595777.625000\n",
            "Train Epoch: 57 [2880/7471 (39%)]\tLoss: 1608742.750000\n",
            "Train Epoch: 57 [3040/7471 (41%)]\tLoss: 1562902.375000\n",
            "Train Epoch: 57 [3200/7471 (43%)]\tLoss: 1603074.125000\n",
            "Train Epoch: 57 [3360/7471 (45%)]\tLoss: 1617112.625000\n",
            "Train Epoch: 57 [3520/7471 (47%)]\tLoss: 1620204.500000\n",
            "Train Epoch: 57 [3680/7471 (49%)]\tLoss: 1591914.125000\n",
            "Train Epoch: 57 [3840/7471 (51%)]\tLoss: 1558233.500000\n",
            "Train Epoch: 57 [4000/7471 (54%)]\tLoss: 1563282.875000\n",
            "Train Epoch: 57 [4160/7471 (56%)]\tLoss: 1611685.750000\n",
            "Train Epoch: 57 [4320/7471 (58%)]\tLoss: 1553912.000000\n",
            "Train Epoch: 57 [4480/7471 (60%)]\tLoss: 1515292.125000\n",
            "Train Epoch: 57 [4640/7471 (62%)]\tLoss: 1623388.875000\n",
            "Train Epoch: 57 [4800/7471 (64%)]\tLoss: 1566662.000000\n",
            "Train Epoch: 57 [4960/7471 (66%)]\tLoss: 1524004.000000\n",
            "Train Epoch: 57 [5120/7471 (69%)]\tLoss: 1607987.250000\n",
            "Train Epoch: 57 [5280/7471 (71%)]\tLoss: 1524865.250000\n",
            "Train Epoch: 57 [5440/7471 (73%)]\tLoss: 1626453.875000\n",
            "Train Epoch: 57 [5600/7471 (75%)]\tLoss: 1640496.375000\n",
            "Train Epoch: 57 [5760/7471 (77%)]\tLoss: 1620647.750000\n",
            "Train Epoch: 57 [5920/7471 (79%)]\tLoss: 1520981.375000\n",
            "Train Epoch: 57 [6080/7471 (81%)]\tLoss: 1554905.000000\n",
            "Train Epoch: 57 [6240/7471 (84%)]\tLoss: 1634283.500000\n",
            "Train Epoch: 57 [6400/7471 (86%)]\tLoss: 1564017.250000\n",
            "Train Epoch: 57 [6560/7471 (88%)]\tLoss: 1586993.375000\n",
            "Train Epoch: 57 [6720/7471 (90%)]\tLoss: 1619238.875000\n",
            "Train Epoch: 57 [6880/7471 (92%)]\tLoss: 1580614.375000\n",
            "Train Epoch: 57 [7040/7471 (94%)]\tLoss: 1539775.125000\n",
            "Train Epoch: 57 [7200/7471 (96%)]\tLoss: 1590242.125000\n",
            "Train Epoch: 57 [7360/7471 (99%)]\tLoss: 1541983.125000\n",
            "Epoch 57 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98584.1948\n",
            "\n",
            "Train Epoch: 58 [160/7471 (2%)]\tLoss: 1549913.000000\n",
            "Train Epoch: 58 [320/7471 (4%)]\tLoss: 1545476.875000\n",
            "Train Epoch: 58 [480/7471 (6%)]\tLoss: 1593402.875000\n",
            "Train Epoch: 58 [640/7471 (9%)]\tLoss: 1609287.500000\n",
            "Train Epoch: 58 [800/7471 (11%)]\tLoss: 1600863.500000\n",
            "Train Epoch: 58 [960/7471 (13%)]\tLoss: 1550538.250000\n",
            "Train Epoch: 58 [1120/7471 (15%)]\tLoss: 1566993.250000\n",
            "Train Epoch: 58 [1280/7471 (17%)]\tLoss: 1593992.250000\n",
            "Train Epoch: 58 [1440/7471 (19%)]\tLoss: 1570781.875000\n",
            "Train Epoch: 58 [1600/7471 (21%)]\tLoss: 1624733.000000\n",
            "Train Epoch: 58 [1760/7471 (24%)]\tLoss: 1552496.250000\n",
            "Train Epoch: 58 [1920/7471 (26%)]\tLoss: 1569610.625000\n",
            "Train Epoch: 58 [2080/7471 (28%)]\tLoss: 1546566.500000\n",
            "Train Epoch: 58 [2240/7471 (30%)]\tLoss: 1608491.375000\n",
            "Train Epoch: 58 [2400/7471 (32%)]\tLoss: 1564913.375000\n",
            "Train Epoch: 58 [2560/7471 (34%)]\tLoss: 1630935.250000\n",
            "Train Epoch: 58 [2720/7471 (36%)]\tLoss: 1559623.250000\n",
            "Train Epoch: 58 [2880/7471 (39%)]\tLoss: 1622428.375000\n",
            "Train Epoch: 58 [3040/7471 (41%)]\tLoss: 1601566.125000\n",
            "Train Epoch: 58 [3200/7471 (43%)]\tLoss: 1635933.625000\n",
            "Train Epoch: 58 [3360/7471 (45%)]\tLoss: 1582119.125000\n",
            "Train Epoch: 58 [3520/7471 (47%)]\tLoss: 1586135.125000\n",
            "Train Epoch: 58 [3680/7471 (49%)]\tLoss: 1600863.750000\n",
            "Train Epoch: 58 [3840/7471 (51%)]\tLoss: 1578074.750000\n",
            "Train Epoch: 58 [4000/7471 (54%)]\tLoss: 1524630.250000\n",
            "Train Epoch: 58 [4160/7471 (56%)]\tLoss: 1571538.250000\n",
            "Train Epoch: 58 [4320/7471 (58%)]\tLoss: 1566635.000000\n",
            "Train Epoch: 58 [4480/7471 (60%)]\tLoss: 1592891.500000\n",
            "Train Epoch: 58 [4640/7471 (62%)]\tLoss: 1609792.250000\n",
            "Train Epoch: 58 [4800/7471 (64%)]\tLoss: 1591127.750000\n",
            "Train Epoch: 58 [4960/7471 (66%)]\tLoss: 1546163.500000\n",
            "Train Epoch: 58 [5120/7471 (69%)]\tLoss: 1576816.500000\n",
            "Train Epoch: 58 [5280/7471 (71%)]\tLoss: 1588179.875000\n",
            "Train Epoch: 58 [5440/7471 (73%)]\tLoss: 1579636.500000\n",
            "Train Epoch: 58 [5600/7471 (75%)]\tLoss: 1568389.250000\n",
            "Train Epoch: 58 [5760/7471 (77%)]\tLoss: 1629198.875000\n",
            "Train Epoch: 58 [5920/7471 (79%)]\tLoss: 1563530.000000\n",
            "Train Epoch: 58 [6080/7471 (81%)]\tLoss: 1600997.750000\n",
            "Train Epoch: 58 [6240/7471 (84%)]\tLoss: 1608527.125000\n",
            "Train Epoch: 58 [6400/7471 (86%)]\tLoss: 1566345.000000\n",
            "Train Epoch: 58 [6560/7471 (88%)]\tLoss: 1616374.000000\n",
            "Train Epoch: 58 [6720/7471 (90%)]\tLoss: 1583143.375000\n",
            "Train Epoch: 58 [6880/7471 (92%)]\tLoss: 1553582.250000\n",
            "Train Epoch: 58 [7040/7471 (94%)]\tLoss: 1559102.500000\n",
            "Train Epoch: 58 [7200/7471 (96%)]\tLoss: 1528204.875000\n",
            "Train Epoch: 58 [7360/7471 (99%)]\tLoss: 1579766.750000\n",
            "Epoch 58 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98671.6995\n",
            "\n",
            "Train Epoch: 59 [160/7471 (2%)]\tLoss: 1522576.500000\n",
            "Train Epoch: 59 [320/7471 (4%)]\tLoss: 1595441.500000\n",
            "Train Epoch: 59 [480/7471 (6%)]\tLoss: 1599500.500000\n",
            "Train Epoch: 59 [640/7471 (9%)]\tLoss: 1623048.625000\n",
            "Train Epoch: 59 [800/7471 (11%)]\tLoss: 1537039.625000\n",
            "Train Epoch: 59 [960/7471 (13%)]\tLoss: 1612376.750000\n",
            "Train Epoch: 59 [1120/7471 (15%)]\tLoss: 1515690.250000\n",
            "Train Epoch: 59 [1280/7471 (17%)]\tLoss: 1574136.500000\n",
            "Train Epoch: 59 [1440/7471 (19%)]\tLoss: 1562379.125000\n",
            "Train Epoch: 59 [1600/7471 (21%)]\tLoss: 1562160.000000\n",
            "Train Epoch: 59 [1760/7471 (24%)]\tLoss: 1583966.125000\n",
            "Train Epoch: 59 [1920/7471 (26%)]\tLoss: 1600788.000000\n",
            "Train Epoch: 59 [2080/7471 (28%)]\tLoss: 1501805.375000\n",
            "Train Epoch: 59 [2240/7471 (30%)]\tLoss: 1564758.875000\n",
            "Train Epoch: 59 [2400/7471 (32%)]\tLoss: 1581284.500000\n",
            "Train Epoch: 59 [2560/7471 (34%)]\tLoss: 1542471.750000\n",
            "Train Epoch: 59 [2720/7471 (36%)]\tLoss: 1534741.500000\n",
            "Train Epoch: 59 [2880/7471 (39%)]\tLoss: 1549051.125000\n",
            "Train Epoch: 59 [3040/7471 (41%)]\tLoss: 1560492.125000\n",
            "Train Epoch: 59 [3200/7471 (43%)]\tLoss: 1586109.750000\n",
            "Train Epoch: 59 [3360/7471 (45%)]\tLoss: 1612132.125000\n",
            "Train Epoch: 59 [3520/7471 (47%)]\tLoss: 1511895.875000\n",
            "Train Epoch: 59 [3680/7471 (49%)]\tLoss: 1595003.625000\n",
            "Train Epoch: 59 [3840/7471 (51%)]\tLoss: 1609806.875000\n",
            "Train Epoch: 59 [4000/7471 (54%)]\tLoss: 1527434.875000\n",
            "Train Epoch: 59 [4160/7471 (56%)]\tLoss: 1545098.875000\n",
            "Train Epoch: 59 [4320/7471 (58%)]\tLoss: 1580372.750000\n",
            "Train Epoch: 59 [4480/7471 (60%)]\tLoss: 1542532.500000\n",
            "Train Epoch: 59 [4640/7471 (62%)]\tLoss: 1592521.500000\n",
            "Train Epoch: 59 [4800/7471 (64%)]\tLoss: 1552819.125000\n",
            "Train Epoch: 59 [4960/7471 (66%)]\tLoss: 1563264.375000\n",
            "Train Epoch: 59 [5120/7471 (69%)]\tLoss: 1582020.625000\n",
            "Train Epoch: 59 [5280/7471 (71%)]\tLoss: 1575154.250000\n",
            "Train Epoch: 59 [5440/7471 (73%)]\tLoss: 1566490.375000\n",
            "Train Epoch: 59 [5600/7471 (75%)]\tLoss: 1586694.250000\n",
            "Train Epoch: 59 [5760/7471 (77%)]\tLoss: 1606059.375000\n",
            "Train Epoch: 59 [5920/7471 (79%)]\tLoss: 1625530.250000\n",
            "Train Epoch: 59 [6080/7471 (81%)]\tLoss: 1615997.625000\n",
            "Train Epoch: 59 [6240/7471 (84%)]\tLoss: 1542348.250000\n",
            "Train Epoch: 59 [6400/7471 (86%)]\tLoss: 1629745.000000\n",
            "Train Epoch: 59 [6560/7471 (88%)]\tLoss: 1584383.750000\n",
            "Train Epoch: 59 [6720/7471 (90%)]\tLoss: 1578286.375000\n",
            "Train Epoch: 59 [6880/7471 (92%)]\tLoss: 1538938.750000\n",
            "Train Epoch: 59 [7040/7471 (94%)]\tLoss: 1564959.875000\n",
            "Train Epoch: 59 [7200/7471 (96%)]\tLoss: 1594112.375000\n",
            "Train Epoch: 59 [7360/7471 (99%)]\tLoss: 1547347.500000\n",
            "Epoch 59 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98566.4475\n",
            "\n",
            "Train Epoch: 60 [160/7471 (2%)]\tLoss: 1616310.125000\n",
            "Train Epoch: 60 [320/7471 (4%)]\tLoss: 1599511.750000\n",
            "Train Epoch: 60 [480/7471 (6%)]\tLoss: 1622157.125000\n",
            "Train Epoch: 60 [640/7471 (9%)]\tLoss: 1613219.000000\n",
            "Train Epoch: 60 [800/7471 (11%)]\tLoss: 1566761.875000\n",
            "Train Epoch: 60 [960/7471 (13%)]\tLoss: 1530043.375000\n",
            "Train Epoch: 60 [1120/7471 (15%)]\tLoss: 1588026.375000\n",
            "Train Epoch: 60 [1280/7471 (17%)]\tLoss: 1601484.875000\n",
            "Train Epoch: 60 [1440/7471 (19%)]\tLoss: 1529981.500000\n",
            "Train Epoch: 60 [1600/7471 (21%)]\tLoss: 1553933.250000\n",
            "Train Epoch: 60 [1760/7471 (24%)]\tLoss: 1534061.375000\n",
            "Train Epoch: 60 [1920/7471 (26%)]\tLoss: 1550703.250000\n",
            "Train Epoch: 60 [2080/7471 (28%)]\tLoss: 1562380.250000\n",
            "Train Epoch: 60 [2240/7471 (30%)]\tLoss: 1626125.250000\n",
            "Train Epoch: 60 [2400/7471 (32%)]\tLoss: 1597388.625000\n",
            "Train Epoch: 60 [2560/7471 (34%)]\tLoss: 1498892.750000\n",
            "Train Epoch: 60 [2720/7471 (36%)]\tLoss: 1526304.500000\n",
            "Train Epoch: 60 [2880/7471 (39%)]\tLoss: 1598174.875000\n",
            "Train Epoch: 60 [3040/7471 (41%)]\tLoss: 1561219.875000\n",
            "Train Epoch: 60 [3200/7471 (43%)]\tLoss: 1594421.875000\n",
            "Train Epoch: 60 [3360/7471 (45%)]\tLoss: 1587053.500000\n",
            "Train Epoch: 60 [3520/7471 (47%)]\tLoss: 1532564.750000\n",
            "Train Epoch: 60 [3680/7471 (49%)]\tLoss: 1591417.250000\n",
            "Train Epoch: 60 [3840/7471 (51%)]\tLoss: 1625815.125000\n",
            "Train Epoch: 60 [4000/7471 (54%)]\tLoss: 1508454.500000\n",
            "Train Epoch: 60 [4160/7471 (56%)]\tLoss: 1552652.125000\n",
            "Train Epoch: 60 [4320/7471 (58%)]\tLoss: 1603968.750000\n",
            "Train Epoch: 60 [4480/7471 (60%)]\tLoss: 1571734.625000\n",
            "Train Epoch: 60 [4640/7471 (62%)]\tLoss: 1542038.375000\n",
            "Train Epoch: 60 [4800/7471 (64%)]\tLoss: 1539668.875000\n",
            "Train Epoch: 60 [4960/7471 (66%)]\tLoss: 1600006.875000\n",
            "Train Epoch: 60 [5120/7471 (69%)]\tLoss: 1575169.625000\n",
            "Train Epoch: 60 [5280/7471 (71%)]\tLoss: 1619464.375000\n",
            "Train Epoch: 60 [5440/7471 (73%)]\tLoss: 1522759.250000\n",
            "Train Epoch: 60 [5600/7471 (75%)]\tLoss: 1573575.875000\n",
            "Train Epoch: 60 [5760/7471 (77%)]\tLoss: 1622343.250000\n",
            "Train Epoch: 60 [5920/7471 (79%)]\tLoss: 1611878.000000\n",
            "Train Epoch: 60 [6080/7471 (81%)]\tLoss: 1531475.625000\n",
            "Train Epoch: 60 [6240/7471 (84%)]\tLoss: 1552444.625000\n",
            "Train Epoch: 60 [6400/7471 (86%)]\tLoss: 1543035.500000\n",
            "Train Epoch: 60 [6560/7471 (88%)]\tLoss: 1591894.500000\n",
            "Train Epoch: 60 [6720/7471 (90%)]\tLoss: 1578150.875000\n",
            "Train Epoch: 60 [6880/7471 (92%)]\tLoss: 1564408.875000\n",
            "Train Epoch: 60 [7040/7471 (94%)]\tLoss: 1544952.625000\n",
            "Train Epoch: 60 [7200/7471 (96%)]\tLoss: 1546758.000000\n",
            "Train Epoch: 60 [7360/7471 (99%)]\tLoss: 1610163.500000\n",
            "Epoch 60 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98590.6832\n",
            "\n",
            "Train Epoch: 61 [160/7471 (2%)]\tLoss: 1626120.750000\n",
            "Train Epoch: 61 [320/7471 (4%)]\tLoss: 1595785.375000\n",
            "Train Epoch: 61 [480/7471 (6%)]\tLoss: 1512356.500000\n",
            "Train Epoch: 61 [640/7471 (9%)]\tLoss: 1629493.250000\n",
            "Train Epoch: 61 [800/7471 (11%)]\tLoss: 1566918.125000\n",
            "Train Epoch: 61 [960/7471 (13%)]\tLoss: 1577209.375000\n",
            "Train Epoch: 61 [1120/7471 (15%)]\tLoss: 1538974.500000\n",
            "Train Epoch: 61 [1280/7471 (17%)]\tLoss: 1558791.500000\n",
            "Train Epoch: 61 [1440/7471 (19%)]\tLoss: 1628241.750000\n",
            "Train Epoch: 61 [1600/7471 (21%)]\tLoss: 1618684.250000\n",
            "Train Epoch: 61 [1760/7471 (24%)]\tLoss: 1569906.750000\n",
            "Train Epoch: 61 [1920/7471 (26%)]\tLoss: 1566386.875000\n",
            "Train Epoch: 61 [2080/7471 (28%)]\tLoss: 1581768.500000\n",
            "Train Epoch: 61 [2240/7471 (30%)]\tLoss: 1594148.000000\n",
            "Train Epoch: 61 [2400/7471 (32%)]\tLoss: 1568149.875000\n",
            "Train Epoch: 61 [2560/7471 (34%)]\tLoss: 1597597.125000\n",
            "Train Epoch: 61 [2720/7471 (36%)]\tLoss: 1589714.875000\n",
            "Train Epoch: 61 [2880/7471 (39%)]\tLoss: 1543605.625000\n",
            "Train Epoch: 61 [3040/7471 (41%)]\tLoss: 1616698.500000\n",
            "Train Epoch: 61 [3200/7471 (43%)]\tLoss: 1617438.250000\n",
            "Train Epoch: 61 [3360/7471 (45%)]\tLoss: 1624937.000000\n",
            "Train Epoch: 61 [3520/7471 (47%)]\tLoss: 1571450.375000\n",
            "Train Epoch: 61 [3680/7471 (49%)]\tLoss: 1543937.375000\n",
            "Train Epoch: 61 [3840/7471 (51%)]\tLoss: 1566256.750000\n",
            "Train Epoch: 61 [4000/7471 (54%)]\tLoss: 1600753.125000\n",
            "Train Epoch: 61 [4160/7471 (56%)]\tLoss: 1578124.250000\n",
            "Train Epoch: 61 [4320/7471 (58%)]\tLoss: 1615225.375000\n",
            "Train Epoch: 61 [4480/7471 (60%)]\tLoss: 1609885.375000\n",
            "Train Epoch: 61 [4640/7471 (62%)]\tLoss: 1611924.250000\n",
            "Train Epoch: 61 [4800/7471 (64%)]\tLoss: 1579668.750000\n",
            "Train Epoch: 61 [4960/7471 (66%)]\tLoss: 1602186.625000\n",
            "Train Epoch: 61 [5120/7471 (69%)]\tLoss: 1589091.125000\n",
            "Train Epoch: 61 [5280/7471 (71%)]\tLoss: 1633823.250000\n",
            "Train Epoch: 61 [5440/7471 (73%)]\tLoss: 1520223.500000\n",
            "Train Epoch: 61 [5600/7471 (75%)]\tLoss: 1604707.625000\n",
            "Train Epoch: 61 [5760/7471 (77%)]\tLoss: 1618964.875000\n",
            "Train Epoch: 61 [5920/7471 (79%)]\tLoss: 1608344.750000\n",
            "Train Epoch: 61 [6080/7471 (81%)]\tLoss: 1564239.125000\n",
            "Train Epoch: 61 [6240/7471 (84%)]\tLoss: 1529174.500000\n",
            "Train Epoch: 61 [6400/7471 (86%)]\tLoss: 1608838.125000\n",
            "Train Epoch: 61 [6560/7471 (88%)]\tLoss: 1539675.875000\n",
            "Train Epoch: 61 [6720/7471 (90%)]\tLoss: 1617205.000000\n",
            "Train Epoch: 61 [6880/7471 (92%)]\tLoss: 1627341.000000\n",
            "Train Epoch: 61 [7040/7471 (94%)]\tLoss: 1601694.750000\n",
            "Train Epoch: 61 [7200/7471 (96%)]\tLoss: 1500294.250000\n",
            "Train Epoch: 61 [7360/7471 (99%)]\tLoss: 1604112.125000\n",
            "Epoch 61 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98534.8911\n",
            "\n",
            "Train Epoch: 62 [160/7471 (2%)]\tLoss: 1594415.000000\n",
            "Train Epoch: 62 [320/7471 (4%)]\tLoss: 1528453.250000\n",
            "Train Epoch: 62 [480/7471 (6%)]\tLoss: 1515119.375000\n",
            "Train Epoch: 62 [640/7471 (9%)]\tLoss: 1650063.375000\n",
            "Train Epoch: 62 [800/7471 (11%)]\tLoss: 1534624.625000\n",
            "Train Epoch: 62 [960/7471 (13%)]\tLoss: 1605384.375000\n",
            "Train Epoch: 62 [1120/7471 (15%)]\tLoss: 1499732.500000\n",
            "Train Epoch: 62 [1280/7471 (17%)]\tLoss: 1590701.750000\n",
            "Train Epoch: 62 [1440/7471 (19%)]\tLoss: 1568343.375000\n",
            "Train Epoch: 62 [1600/7471 (21%)]\tLoss: 1581985.625000\n",
            "Train Epoch: 62 [1760/7471 (24%)]\tLoss: 1511473.375000\n",
            "Train Epoch: 62 [1920/7471 (26%)]\tLoss: 1582584.875000\n",
            "Train Epoch: 62 [2080/7471 (28%)]\tLoss: 1528797.000000\n",
            "Train Epoch: 62 [2240/7471 (30%)]\tLoss: 1580281.625000\n",
            "Train Epoch: 62 [2400/7471 (32%)]\tLoss: 1592196.125000\n",
            "Train Epoch: 62 [2560/7471 (34%)]\tLoss: 1597916.000000\n",
            "Train Epoch: 62 [2720/7471 (36%)]\tLoss: 1589399.875000\n",
            "Train Epoch: 62 [2880/7471 (39%)]\tLoss: 1526424.000000\n",
            "Train Epoch: 62 [3040/7471 (41%)]\tLoss: 1604601.750000\n",
            "Train Epoch: 62 [3200/7471 (43%)]\tLoss: 1534370.375000\n",
            "Train Epoch: 62 [3360/7471 (45%)]\tLoss: 1607981.125000\n",
            "Train Epoch: 62 [3520/7471 (47%)]\tLoss: 1557078.125000\n",
            "Train Epoch: 62 [3680/7471 (49%)]\tLoss: 1608506.375000\n",
            "Train Epoch: 62 [3840/7471 (51%)]\tLoss: 1575430.500000\n",
            "Train Epoch: 62 [4000/7471 (54%)]\tLoss: 1626652.000000\n",
            "Train Epoch: 62 [4160/7471 (56%)]\tLoss: 1615517.000000\n",
            "Train Epoch: 62 [4320/7471 (58%)]\tLoss: 1574646.125000\n",
            "Train Epoch: 62 [4480/7471 (60%)]\tLoss: 1595710.750000\n",
            "Train Epoch: 62 [4640/7471 (62%)]\tLoss: 1573339.000000\n",
            "Train Epoch: 62 [4800/7471 (64%)]\tLoss: 1564299.250000\n",
            "Train Epoch: 62 [4960/7471 (66%)]\tLoss: 1618410.125000\n",
            "Train Epoch: 62 [5120/7471 (69%)]\tLoss: 1632161.625000\n",
            "Train Epoch: 62 [5280/7471 (71%)]\tLoss: 1578105.125000\n",
            "Train Epoch: 62 [5440/7471 (73%)]\tLoss: 1503605.500000\n",
            "Train Epoch: 62 [5600/7471 (75%)]\tLoss: 1521217.250000\n",
            "Train Epoch: 62 [5760/7471 (77%)]\tLoss: 1589960.375000\n",
            "Train Epoch: 62 [5920/7471 (79%)]\tLoss: 1555083.875000\n",
            "Train Epoch: 62 [6080/7471 (81%)]\tLoss: 1575207.375000\n",
            "Train Epoch: 62 [6240/7471 (84%)]\tLoss: 1598837.750000\n",
            "Train Epoch: 62 [6400/7471 (86%)]\tLoss: 1543928.250000\n",
            "Train Epoch: 62 [6560/7471 (88%)]\tLoss: 1602317.500000\n",
            "Train Epoch: 62 [6720/7471 (90%)]\tLoss: 1566478.750000\n",
            "Train Epoch: 62 [6880/7471 (92%)]\tLoss: 1624488.250000\n",
            "Train Epoch: 62 [7040/7471 (94%)]\tLoss: 1552079.625000\n",
            "Train Epoch: 62 [7200/7471 (96%)]\tLoss: 1608069.500000\n",
            "Train Epoch: 62 [7360/7471 (99%)]\tLoss: 1575081.625000\n",
            "Epoch 62 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98521.2109\n",
            "\n",
            "Train Epoch: 63 [160/7471 (2%)]\tLoss: 1586357.500000\n",
            "Train Epoch: 63 [320/7471 (4%)]\tLoss: 1549684.750000\n",
            "Train Epoch: 63 [480/7471 (6%)]\tLoss: 1621389.250000\n",
            "Train Epoch: 63 [640/7471 (9%)]\tLoss: 1578824.875000\n",
            "Train Epoch: 63 [800/7471 (11%)]\tLoss: 1626627.750000\n",
            "Train Epoch: 63 [960/7471 (13%)]\tLoss: 1579660.250000\n",
            "Train Epoch: 63 [1120/7471 (15%)]\tLoss: 1566030.375000\n",
            "Train Epoch: 63 [1280/7471 (17%)]\tLoss: 1625935.000000\n",
            "Train Epoch: 63 [1440/7471 (19%)]\tLoss: 1615292.625000\n",
            "Train Epoch: 63 [1600/7471 (21%)]\tLoss: 1560971.750000\n",
            "Train Epoch: 63 [1760/7471 (24%)]\tLoss: 1522508.875000\n",
            "Train Epoch: 63 [1920/7471 (26%)]\tLoss: 1580523.125000\n",
            "Train Epoch: 63 [2080/7471 (28%)]\tLoss: 1588127.000000\n",
            "Train Epoch: 63 [2240/7471 (30%)]\tLoss: 1584694.125000\n",
            "Train Epoch: 63 [2400/7471 (32%)]\tLoss: 1582879.750000\n",
            "Train Epoch: 63 [2560/7471 (34%)]\tLoss: 1584647.750000\n",
            "Train Epoch: 63 [2720/7471 (36%)]\tLoss: 1559974.375000\n",
            "Train Epoch: 63 [2880/7471 (39%)]\tLoss: 1570271.000000\n",
            "Train Epoch: 63 [3040/7471 (41%)]\tLoss: 1495432.500000\n",
            "Train Epoch: 63 [3200/7471 (43%)]\tLoss: 1569905.375000\n",
            "Train Epoch: 63 [3360/7471 (45%)]\tLoss: 1632400.125000\n",
            "Train Epoch: 63 [3520/7471 (47%)]\tLoss: 1627480.625000\n",
            "Train Epoch: 63 [3680/7471 (49%)]\tLoss: 1569944.125000\n",
            "Train Epoch: 63 [3840/7471 (51%)]\tLoss: 1570277.500000\n",
            "Train Epoch: 63 [4000/7471 (54%)]\tLoss: 1556156.125000\n",
            "Train Epoch: 63 [4160/7471 (56%)]\tLoss: 1552488.875000\n",
            "Train Epoch: 63 [4320/7471 (58%)]\tLoss: 1532401.750000\n",
            "Train Epoch: 63 [4480/7471 (60%)]\tLoss: 1592886.875000\n",
            "Train Epoch: 63 [4640/7471 (62%)]\tLoss: 1602952.625000\n",
            "Train Epoch: 63 [4800/7471 (64%)]\tLoss: 1570187.875000\n",
            "Train Epoch: 63 [4960/7471 (66%)]\tLoss: 1632122.125000\n",
            "Train Epoch: 63 [5120/7471 (69%)]\tLoss: 1604307.375000\n",
            "Train Epoch: 63 [5280/7471 (71%)]\tLoss: 1615001.375000\n",
            "Train Epoch: 63 [5440/7471 (73%)]\tLoss: 1597046.250000\n",
            "Train Epoch: 63 [5600/7471 (75%)]\tLoss: 1602162.125000\n",
            "Train Epoch: 63 [5760/7471 (77%)]\tLoss: 1595481.875000\n",
            "Train Epoch: 63 [5920/7471 (79%)]\tLoss: 1637029.375000\n",
            "Train Epoch: 63 [6080/7471 (81%)]\tLoss: 1582139.250000\n",
            "Train Epoch: 63 [6240/7471 (84%)]\tLoss: 1621515.500000\n",
            "Train Epoch: 63 [6400/7471 (86%)]\tLoss: 1538853.625000\n",
            "Train Epoch: 63 [6560/7471 (88%)]\tLoss: 1579242.000000\n",
            "Train Epoch: 63 [6720/7471 (90%)]\tLoss: 1530285.000000\n",
            "Train Epoch: 63 [6880/7471 (92%)]\tLoss: 1567759.375000\n",
            "Train Epoch: 63 [7040/7471 (94%)]\tLoss: 1552351.250000\n",
            "Train Epoch: 63 [7200/7471 (96%)]\tLoss: 1594416.250000\n",
            "Train Epoch: 63 [7360/7471 (99%)]\tLoss: 1595598.250000\n",
            "Epoch 63 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98574.8725\n",
            "\n",
            "Train Epoch: 64 [160/7471 (2%)]\tLoss: 1613497.375000\n",
            "Train Epoch: 64 [320/7471 (4%)]\tLoss: 1583599.375000\n",
            "Train Epoch: 64 [480/7471 (6%)]\tLoss: 1588875.125000\n",
            "Train Epoch: 64 [640/7471 (9%)]\tLoss: 1568137.625000\n",
            "Train Epoch: 64 [800/7471 (11%)]\tLoss: 1516114.500000\n",
            "Train Epoch: 64 [960/7471 (13%)]\tLoss: 1567470.375000\n",
            "Train Epoch: 64 [1120/7471 (15%)]\tLoss: 1598300.375000\n",
            "Train Epoch: 64 [1280/7471 (17%)]\tLoss: 1617963.250000\n",
            "Train Epoch: 64 [1440/7471 (19%)]\tLoss: 1625808.250000\n",
            "Train Epoch: 64 [1600/7471 (21%)]\tLoss: 1591862.500000\n",
            "Train Epoch: 64 [1760/7471 (24%)]\tLoss: 1557563.750000\n",
            "Train Epoch: 64 [1920/7471 (26%)]\tLoss: 1596936.125000\n",
            "Train Epoch: 64 [2080/7471 (28%)]\tLoss: 1583402.500000\n",
            "Train Epoch: 64 [2240/7471 (30%)]\tLoss: 1604676.375000\n",
            "Train Epoch: 64 [2400/7471 (32%)]\tLoss: 1566019.000000\n",
            "Train Epoch: 64 [2560/7471 (34%)]\tLoss: 1579500.250000\n",
            "Train Epoch: 64 [2720/7471 (36%)]\tLoss: 1587231.125000\n",
            "Train Epoch: 64 [2880/7471 (39%)]\tLoss: 1588505.250000\n",
            "Train Epoch: 64 [3040/7471 (41%)]\tLoss: 1560711.000000\n",
            "Train Epoch: 64 [3200/7471 (43%)]\tLoss: 1557356.250000\n",
            "Train Epoch: 64 [3360/7471 (45%)]\tLoss: 1620185.250000\n",
            "Train Epoch: 64 [3520/7471 (47%)]\tLoss: 1608733.125000\n",
            "Train Epoch: 64 [3680/7471 (49%)]\tLoss: 1493484.375000\n",
            "Train Epoch: 64 [3840/7471 (51%)]\tLoss: 1560059.250000\n",
            "Train Epoch: 64 [4000/7471 (54%)]\tLoss: 1564269.125000\n",
            "Train Epoch: 64 [4160/7471 (56%)]\tLoss: 1544529.750000\n",
            "Train Epoch: 64 [4320/7471 (58%)]\tLoss: 1615078.375000\n",
            "Train Epoch: 64 [4480/7471 (60%)]\tLoss: 1581826.250000\n",
            "Train Epoch: 64 [4640/7471 (62%)]\tLoss: 1585245.375000\n",
            "Train Epoch: 64 [4800/7471 (64%)]\tLoss: 1587598.000000\n",
            "Train Epoch: 64 [4960/7471 (66%)]\tLoss: 1608852.875000\n",
            "Train Epoch: 64 [5120/7471 (69%)]\tLoss: 1577378.375000\n",
            "Train Epoch: 64 [5280/7471 (71%)]\tLoss: 1622759.875000\n",
            "Train Epoch: 64 [5440/7471 (73%)]\tLoss: 1561058.000000\n",
            "Train Epoch: 64 [5600/7471 (75%)]\tLoss: 1518686.000000\n",
            "Train Epoch: 64 [5760/7471 (77%)]\tLoss: 1539594.000000\n",
            "Train Epoch: 64 [5920/7471 (79%)]\tLoss: 1622682.500000\n",
            "Train Epoch: 64 [6080/7471 (81%)]\tLoss: 1624458.875000\n",
            "Train Epoch: 64 [6240/7471 (84%)]\tLoss: 1512251.500000\n",
            "Train Epoch: 64 [6400/7471 (86%)]\tLoss: 1570385.375000\n",
            "Train Epoch: 64 [6560/7471 (88%)]\tLoss: 1571234.625000\n",
            "Train Epoch: 64 [6720/7471 (90%)]\tLoss: 1616430.250000\n",
            "Train Epoch: 64 [6880/7471 (92%)]\tLoss: 1610618.500000\n",
            "Train Epoch: 64 [7040/7471 (94%)]\tLoss: 1583030.250000\n",
            "Train Epoch: 64 [7200/7471 (96%)]\tLoss: 1541065.250000\n",
            "Train Epoch: 64 [7360/7471 (99%)]\tLoss: 1590209.125000\n",
            "Epoch 64 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98532.6321\n",
            "\n",
            "Train Epoch: 65 [160/7471 (2%)]\tLoss: 1520193.750000\n",
            "Train Epoch: 65 [320/7471 (4%)]\tLoss: 1617007.250000\n",
            "Train Epoch: 65 [480/7471 (6%)]\tLoss: 1590205.000000\n",
            "Train Epoch: 65 [640/7471 (9%)]\tLoss: 1591745.750000\n",
            "Train Epoch: 65 [800/7471 (11%)]\tLoss: 1582867.625000\n",
            "Train Epoch: 65 [960/7471 (13%)]\tLoss: 1606095.625000\n",
            "Train Epoch: 65 [1120/7471 (15%)]\tLoss: 1551547.875000\n",
            "Train Epoch: 65 [1280/7471 (17%)]\tLoss: 1562165.375000\n",
            "Train Epoch: 65 [1440/7471 (19%)]\tLoss: 1615086.000000\n",
            "Train Epoch: 65 [1600/7471 (21%)]\tLoss: 1597669.625000\n",
            "Train Epoch: 65 [1760/7471 (24%)]\tLoss: 1575215.000000\n",
            "Train Epoch: 65 [1920/7471 (26%)]\tLoss: 1624208.125000\n",
            "Train Epoch: 65 [2080/7471 (28%)]\tLoss: 1509318.875000\n",
            "Train Epoch: 65 [2240/7471 (30%)]\tLoss: 1506027.375000\n",
            "Train Epoch: 65 [2400/7471 (32%)]\tLoss: 1618193.625000\n",
            "Train Epoch: 65 [2560/7471 (34%)]\tLoss: 1611752.375000\n",
            "Train Epoch: 65 [2720/7471 (36%)]\tLoss: 1598119.875000\n",
            "Train Epoch: 65 [2880/7471 (39%)]\tLoss: 1493191.250000\n",
            "Train Epoch: 65 [3040/7471 (41%)]\tLoss: 1547006.125000\n",
            "Train Epoch: 65 [3200/7471 (43%)]\tLoss: 1574465.750000\n",
            "Train Epoch: 65 [3360/7471 (45%)]\tLoss: 1626991.500000\n",
            "Train Epoch: 65 [3520/7471 (47%)]\tLoss: 1606502.000000\n",
            "Train Epoch: 65 [3680/7471 (49%)]\tLoss: 1595504.375000\n",
            "Train Epoch: 65 [3840/7471 (51%)]\tLoss: 1603261.750000\n",
            "Train Epoch: 65 [4000/7471 (54%)]\tLoss: 1510692.000000\n",
            "Train Epoch: 65 [4160/7471 (56%)]\tLoss: 1583486.875000\n",
            "Train Epoch: 65 [4320/7471 (58%)]\tLoss: 1618794.250000\n",
            "Train Epoch: 65 [4480/7471 (60%)]\tLoss: 1538513.625000\n",
            "Train Epoch: 65 [4640/7471 (62%)]\tLoss: 1539436.000000\n",
            "Train Epoch: 65 [4800/7471 (64%)]\tLoss: 1564907.625000\n",
            "Train Epoch: 65 [4960/7471 (66%)]\tLoss: 1592330.875000\n",
            "Train Epoch: 65 [5120/7471 (69%)]\tLoss: 1567379.750000\n",
            "Train Epoch: 65 [5280/7471 (71%)]\tLoss: 1471354.000000\n",
            "Train Epoch: 65 [5440/7471 (73%)]\tLoss: 1542351.375000\n",
            "Train Epoch: 65 [5600/7471 (75%)]\tLoss: 1552682.500000\n",
            "Train Epoch: 65 [5760/7471 (77%)]\tLoss: 1610305.000000\n",
            "Train Epoch: 65 [5920/7471 (79%)]\tLoss: 1600940.250000\n",
            "Train Epoch: 65 [6080/7471 (81%)]\tLoss: 1604177.875000\n",
            "Train Epoch: 65 [6240/7471 (84%)]\tLoss: 1647075.500000\n",
            "Train Epoch: 65 [6400/7471 (86%)]\tLoss: 1634561.125000\n",
            "Train Epoch: 65 [6560/7471 (88%)]\tLoss: 1538909.125000\n",
            "Train Epoch: 65 [6720/7471 (90%)]\tLoss: 1601800.875000\n",
            "Train Epoch: 65 [6880/7471 (92%)]\tLoss: 1600202.625000\n",
            "Train Epoch: 65 [7040/7471 (94%)]\tLoss: 1610417.250000\n",
            "Train Epoch: 65 [7200/7471 (96%)]\tLoss: 1537169.500000\n",
            "Train Epoch: 65 [7360/7471 (99%)]\tLoss: 1629184.875000\n",
            "Epoch 65 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98532.1190\n",
            "\n",
            "Train Epoch: 66 [160/7471 (2%)]\tLoss: 1547710.000000\n",
            "Train Epoch: 66 [320/7471 (4%)]\tLoss: 1589372.375000\n",
            "Train Epoch: 66 [480/7471 (6%)]\tLoss: 1573938.875000\n",
            "Train Epoch: 66 [640/7471 (9%)]\tLoss: 1572166.500000\n",
            "Train Epoch: 66 [800/7471 (11%)]\tLoss: 1566636.000000\n",
            "Train Epoch: 66 [960/7471 (13%)]\tLoss: 1555889.000000\n",
            "Train Epoch: 66 [1120/7471 (15%)]\tLoss: 1566964.750000\n",
            "Train Epoch: 66 [1280/7471 (17%)]\tLoss: 1641898.875000\n",
            "Train Epoch: 66 [1440/7471 (19%)]\tLoss: 1576158.625000\n",
            "Train Epoch: 66 [1600/7471 (21%)]\tLoss: 1589236.500000\n",
            "Train Epoch: 66 [1760/7471 (24%)]\tLoss: 1557789.500000\n",
            "Train Epoch: 66 [1920/7471 (26%)]\tLoss: 1613963.250000\n",
            "Train Epoch: 66 [2080/7471 (28%)]\tLoss: 1582955.875000\n",
            "Train Epoch: 66 [2240/7471 (30%)]\tLoss: 1595179.750000\n",
            "Train Epoch: 66 [2400/7471 (32%)]\tLoss: 1586198.875000\n",
            "Train Epoch: 66 [2560/7471 (34%)]\tLoss: 1526503.375000\n",
            "Train Epoch: 66 [2720/7471 (36%)]\tLoss: 1583947.500000\n",
            "Train Epoch: 66 [2880/7471 (39%)]\tLoss: 1585599.500000\n",
            "Train Epoch: 66 [3040/7471 (41%)]\tLoss: 1570502.250000\n",
            "Train Epoch: 66 [3200/7471 (43%)]\tLoss: 1579343.500000\n",
            "Train Epoch: 66 [3360/7471 (45%)]\tLoss: 1622542.500000\n",
            "Train Epoch: 66 [3520/7471 (47%)]\tLoss: 1548073.125000\n",
            "Train Epoch: 66 [3680/7471 (49%)]\tLoss: 1638262.625000\n",
            "Train Epoch: 66 [3840/7471 (51%)]\tLoss: 1599753.000000\n",
            "Train Epoch: 66 [4000/7471 (54%)]\tLoss: 1558612.625000\n",
            "Train Epoch: 66 [4160/7471 (56%)]\tLoss: 1583866.500000\n",
            "Train Epoch: 66 [4320/7471 (58%)]\tLoss: 1530977.625000\n",
            "Train Epoch: 66 [4480/7471 (60%)]\tLoss: 1539600.500000\n",
            "Train Epoch: 66 [4640/7471 (62%)]\tLoss: 1532538.250000\n",
            "Train Epoch: 66 [4800/7471 (64%)]\tLoss: 1537391.875000\n",
            "Train Epoch: 66 [4960/7471 (66%)]\tLoss: 1592712.375000\n",
            "Train Epoch: 66 [5120/7471 (69%)]\tLoss: 1630930.375000\n",
            "Train Epoch: 66 [5280/7471 (71%)]\tLoss: 1622720.625000\n",
            "Train Epoch: 66 [5440/7471 (73%)]\tLoss: 1554485.375000\n",
            "Train Epoch: 66 [5600/7471 (75%)]\tLoss: 1546092.375000\n",
            "Train Epoch: 66 [5760/7471 (77%)]\tLoss: 1545988.875000\n",
            "Train Epoch: 66 [5920/7471 (79%)]\tLoss: 1556826.250000\n",
            "Train Epoch: 66 [6080/7471 (81%)]\tLoss: 1606325.500000\n",
            "Train Epoch: 66 [6240/7471 (84%)]\tLoss: 1645263.625000\n",
            "Train Epoch: 66 [6400/7471 (86%)]\tLoss: 1509169.750000\n",
            "Train Epoch: 66 [6560/7471 (88%)]\tLoss: 1605368.875000\n",
            "Train Epoch: 66 [6720/7471 (90%)]\tLoss: 1603617.250000\n",
            "Train Epoch: 66 [6880/7471 (92%)]\tLoss: 1623859.500000\n",
            "Train Epoch: 66 [7040/7471 (94%)]\tLoss: 1585406.125000\n",
            "Train Epoch: 66 [7200/7471 (96%)]\tLoss: 1522449.375000\n",
            "Train Epoch: 66 [7360/7471 (99%)]\tLoss: 1612648.875000\n",
            "Epoch 66 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98513.7038\n",
            "\n",
            "Train Epoch: 67 [160/7471 (2%)]\tLoss: 1544207.125000\n",
            "Train Epoch: 67 [320/7471 (4%)]\tLoss: 1627045.125000\n",
            "Train Epoch: 67 [480/7471 (6%)]\tLoss: 1547202.000000\n",
            "Train Epoch: 67 [640/7471 (9%)]\tLoss: 1580680.500000\n",
            "Train Epoch: 67 [800/7471 (11%)]\tLoss: 1613385.625000\n",
            "Train Epoch: 67 [960/7471 (13%)]\tLoss: 1495078.250000\n",
            "Train Epoch: 67 [1120/7471 (15%)]\tLoss: 1556826.125000\n",
            "Train Epoch: 67 [1280/7471 (17%)]\tLoss: 1612847.875000\n",
            "Train Epoch: 67 [1440/7471 (19%)]\tLoss: 1533439.750000\n",
            "Train Epoch: 67 [1600/7471 (21%)]\tLoss: 1653761.750000\n",
            "Train Epoch: 67 [1760/7471 (24%)]\tLoss: 1550309.625000\n",
            "Train Epoch: 67 [1920/7471 (26%)]\tLoss: 1570523.625000\n",
            "Train Epoch: 67 [2080/7471 (28%)]\tLoss: 1566290.750000\n",
            "Train Epoch: 67 [2240/7471 (30%)]\tLoss: 1574813.500000\n",
            "Train Epoch: 67 [2400/7471 (32%)]\tLoss: 1509841.875000\n",
            "Train Epoch: 67 [2560/7471 (34%)]\tLoss: 1568609.875000\n",
            "Train Epoch: 67 [2720/7471 (36%)]\tLoss: 1586413.000000\n",
            "Train Epoch: 67 [2880/7471 (39%)]\tLoss: 1590198.750000\n",
            "Train Epoch: 67 [3040/7471 (41%)]\tLoss: 1532111.250000\n",
            "Train Epoch: 67 [3200/7471 (43%)]\tLoss: 1574761.750000\n",
            "Train Epoch: 67 [3360/7471 (45%)]\tLoss: 1622578.125000\n",
            "Train Epoch: 67 [3520/7471 (47%)]\tLoss: 1596637.500000\n",
            "Train Epoch: 67 [3680/7471 (49%)]\tLoss: 1617943.500000\n",
            "Train Epoch: 67 [3840/7471 (51%)]\tLoss: 1615956.125000\n",
            "Train Epoch: 67 [4000/7471 (54%)]\tLoss: 1567422.375000\n",
            "Train Epoch: 67 [4160/7471 (56%)]\tLoss: 1607013.500000\n",
            "Train Epoch: 67 [4320/7471 (58%)]\tLoss: 1542187.375000\n",
            "Train Epoch: 67 [4480/7471 (60%)]\tLoss: 1620050.750000\n",
            "Train Epoch: 67 [4640/7471 (62%)]\tLoss: 1498992.500000\n",
            "Train Epoch: 67 [4800/7471 (64%)]\tLoss: 1551521.500000\n",
            "Train Epoch: 67 [4960/7471 (66%)]\tLoss: 1506346.875000\n",
            "Train Epoch: 67 [5120/7471 (69%)]\tLoss: 1605809.375000\n",
            "Train Epoch: 67 [5280/7471 (71%)]\tLoss: 1624512.375000\n",
            "Train Epoch: 67 [5440/7471 (73%)]\tLoss: 1529372.250000\n",
            "Train Epoch: 67 [5600/7471 (75%)]\tLoss: 1603167.125000\n",
            "Train Epoch: 67 [5760/7471 (77%)]\tLoss: 1617831.000000\n",
            "Train Epoch: 67 [5920/7471 (79%)]\tLoss: 1573380.750000\n",
            "Train Epoch: 67 [6080/7471 (81%)]\tLoss: 1557391.500000\n",
            "Train Epoch: 67 [6240/7471 (84%)]\tLoss: 1571176.250000\n",
            "Train Epoch: 67 [6400/7471 (86%)]\tLoss: 1538272.625000\n",
            "Train Epoch: 67 [6560/7471 (88%)]\tLoss: 1579974.000000\n",
            "Train Epoch: 67 [6720/7471 (90%)]\tLoss: 1572672.625000\n",
            "Train Epoch: 67 [6880/7471 (92%)]\tLoss: 1576264.250000\n",
            "Train Epoch: 67 [7040/7471 (94%)]\tLoss: 1513821.875000\n",
            "Train Epoch: 67 [7200/7471 (96%)]\tLoss: 1592886.000000\n",
            "Train Epoch: 67 [7360/7471 (99%)]\tLoss: 1619928.000000\n",
            "Epoch 67 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98585.6745\n",
            "\n",
            "Train Epoch: 68 [160/7471 (2%)]\tLoss: 1517024.250000\n",
            "Train Epoch: 68 [320/7471 (4%)]\tLoss: 1586429.250000\n",
            "Train Epoch: 68 [480/7471 (6%)]\tLoss: 1561531.625000\n",
            "Train Epoch: 68 [640/7471 (9%)]\tLoss: 1596098.375000\n",
            "Train Epoch: 68 [800/7471 (11%)]\tLoss: 1540614.000000\n",
            "Train Epoch: 68 [960/7471 (13%)]\tLoss: 1556590.125000\n",
            "Train Epoch: 68 [1120/7471 (15%)]\tLoss: 1526057.250000\n",
            "Train Epoch: 68 [1280/7471 (17%)]\tLoss: 1603173.750000\n",
            "Train Epoch: 68 [1440/7471 (19%)]\tLoss: 1600586.750000\n",
            "Train Epoch: 68 [1600/7471 (21%)]\tLoss: 1597122.750000\n",
            "Train Epoch: 68 [1760/7471 (24%)]\tLoss: 1495667.750000\n",
            "Train Epoch: 68 [1920/7471 (26%)]\tLoss: 1593118.000000\n",
            "Train Epoch: 68 [2080/7471 (28%)]\tLoss: 1517703.875000\n",
            "Train Epoch: 68 [2240/7471 (30%)]\tLoss: 1554085.000000\n",
            "Train Epoch: 68 [2400/7471 (32%)]\tLoss: 1583210.625000\n",
            "Train Epoch: 68 [2560/7471 (34%)]\tLoss: 1542621.250000\n",
            "Train Epoch: 68 [2720/7471 (36%)]\tLoss: 1566199.750000\n",
            "Train Epoch: 68 [2880/7471 (39%)]\tLoss: 1521425.125000\n",
            "Train Epoch: 68 [3040/7471 (41%)]\tLoss: 1610111.250000\n",
            "Train Epoch: 68 [3200/7471 (43%)]\tLoss: 1559262.750000\n",
            "Train Epoch: 68 [3360/7471 (45%)]\tLoss: 1611253.375000\n",
            "Train Epoch: 68 [3520/7471 (47%)]\tLoss: 1555148.375000\n",
            "Train Epoch: 68 [3680/7471 (49%)]\tLoss: 1615423.250000\n",
            "Train Epoch: 68 [3840/7471 (51%)]\tLoss: 1576643.000000\n",
            "Train Epoch: 68 [4000/7471 (54%)]\tLoss: 1536250.375000\n",
            "Train Epoch: 68 [4160/7471 (56%)]\tLoss: 1594509.625000\n",
            "Train Epoch: 68 [4320/7471 (58%)]\tLoss: 1538457.125000\n",
            "Train Epoch: 68 [4480/7471 (60%)]\tLoss: 1558231.625000\n",
            "Train Epoch: 68 [4640/7471 (62%)]\tLoss: 1621311.125000\n",
            "Train Epoch: 68 [4800/7471 (64%)]\tLoss: 1546304.750000\n",
            "Train Epoch: 68 [4960/7471 (66%)]\tLoss: 1634198.500000\n",
            "Train Epoch: 68 [5120/7471 (69%)]\tLoss: 1594655.375000\n",
            "Train Epoch: 68 [5280/7471 (71%)]\tLoss: 1577973.750000\n",
            "Train Epoch: 68 [5440/7471 (73%)]\tLoss: 1610684.875000\n",
            "Train Epoch: 68 [5600/7471 (75%)]\tLoss: 1617628.875000\n",
            "Train Epoch: 68 [5760/7471 (77%)]\tLoss: 1550393.250000\n",
            "Train Epoch: 68 [5920/7471 (79%)]\tLoss: 1540146.125000\n",
            "Train Epoch: 68 [6080/7471 (81%)]\tLoss: 1597158.500000\n",
            "Train Epoch: 68 [6240/7471 (84%)]\tLoss: 1599516.875000\n",
            "Train Epoch: 68 [6400/7471 (86%)]\tLoss: 1585939.875000\n",
            "Train Epoch: 68 [6560/7471 (88%)]\tLoss: 1562108.625000\n",
            "Train Epoch: 68 [6720/7471 (90%)]\tLoss: 1588041.250000\n",
            "Train Epoch: 68 [6880/7471 (92%)]\tLoss: 1599906.875000\n",
            "Train Epoch: 68 [7040/7471 (94%)]\tLoss: 1626754.750000\n",
            "Train Epoch: 68 [7200/7471 (96%)]\tLoss: 1607671.500000\n",
            "Train Epoch: 68 [7360/7471 (99%)]\tLoss: 1523298.000000\n",
            "Epoch 68 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98593.5321\n",
            "\n",
            "Train Epoch: 69 [160/7471 (2%)]\tLoss: 1568612.875000\n",
            "Train Epoch: 69 [320/7471 (4%)]\tLoss: 1560236.125000\n",
            "Train Epoch: 69 [480/7471 (6%)]\tLoss: 1607405.375000\n",
            "Train Epoch: 69 [640/7471 (9%)]\tLoss: 1594780.500000\n",
            "Train Epoch: 69 [800/7471 (11%)]\tLoss: 1575582.375000\n",
            "Train Epoch: 69 [960/7471 (13%)]\tLoss: 1572638.375000\n",
            "Train Epoch: 69 [1120/7471 (15%)]\tLoss: 1564486.375000\n",
            "Train Epoch: 69 [1280/7471 (17%)]\tLoss: 1607511.500000\n",
            "Train Epoch: 69 [1440/7471 (19%)]\tLoss: 1605836.375000\n",
            "Train Epoch: 69 [1600/7471 (21%)]\tLoss: 1556159.375000\n",
            "Train Epoch: 69 [1760/7471 (24%)]\tLoss: 1509847.750000\n",
            "Train Epoch: 69 [1920/7471 (26%)]\tLoss: 1500546.625000\n",
            "Train Epoch: 69 [2080/7471 (28%)]\tLoss: 1560809.125000\n",
            "Train Epoch: 69 [2240/7471 (30%)]\tLoss: 1556483.000000\n",
            "Train Epoch: 69 [2400/7471 (32%)]\tLoss: 1591322.625000\n",
            "Train Epoch: 69 [2560/7471 (34%)]\tLoss: 1585673.250000\n",
            "Train Epoch: 69 [2720/7471 (36%)]\tLoss: 1531209.000000\n",
            "Train Epoch: 69 [2880/7471 (39%)]\tLoss: 1619352.250000\n",
            "Train Epoch: 69 [3040/7471 (41%)]\tLoss: 1598102.125000\n",
            "Train Epoch: 69 [3200/7471 (43%)]\tLoss: 1627748.625000\n",
            "Train Epoch: 69 [3360/7471 (45%)]\tLoss: 1613596.500000\n",
            "Train Epoch: 69 [3520/7471 (47%)]\tLoss: 1603738.750000\n",
            "Train Epoch: 69 [3680/7471 (49%)]\tLoss: 1629453.750000\n",
            "Train Epoch: 69 [3840/7471 (51%)]\tLoss: 1628557.125000\n",
            "Train Epoch: 69 [4000/7471 (54%)]\tLoss: 1574972.625000\n",
            "Train Epoch: 69 [4160/7471 (56%)]\tLoss: 1544134.000000\n",
            "Train Epoch: 69 [4320/7471 (58%)]\tLoss: 1594677.125000\n",
            "Train Epoch: 69 [4480/7471 (60%)]\tLoss: 1625565.000000\n",
            "Train Epoch: 69 [4640/7471 (62%)]\tLoss: 1534249.250000\n",
            "Train Epoch: 69 [4800/7471 (64%)]\tLoss: 1484202.500000\n",
            "Train Epoch: 69 [4960/7471 (66%)]\tLoss: 1612048.250000\n",
            "Train Epoch: 69 [5120/7471 (69%)]\tLoss: 1629820.500000\n",
            "Train Epoch: 69 [5280/7471 (71%)]\tLoss: 1560301.625000\n",
            "Train Epoch: 69 [5440/7471 (73%)]\tLoss: 1537326.625000\n",
            "Train Epoch: 69 [5600/7471 (75%)]\tLoss: 1545804.125000\n",
            "Train Epoch: 69 [5760/7471 (77%)]\tLoss: 1595558.625000\n",
            "Train Epoch: 69 [5920/7471 (79%)]\tLoss: 1569201.500000\n",
            "Train Epoch: 69 [6080/7471 (81%)]\tLoss: 1535587.375000\n",
            "Train Epoch: 69 [6240/7471 (84%)]\tLoss: 1607934.250000\n",
            "Train Epoch: 69 [6400/7471 (86%)]\tLoss: 1545861.500000\n",
            "Train Epoch: 69 [6560/7471 (88%)]\tLoss: 1600782.125000\n",
            "Train Epoch: 69 [6720/7471 (90%)]\tLoss: 1589371.875000\n",
            "Train Epoch: 69 [6880/7471 (92%)]\tLoss: 1562103.875000\n",
            "Train Epoch: 69 [7040/7471 (94%)]\tLoss: 1607187.125000\n",
            "Train Epoch: 69 [7200/7471 (96%)]\tLoss: 1544706.500000\n",
            "Train Epoch: 69 [7360/7471 (99%)]\tLoss: 1573849.875000\n",
            "Epoch 69 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98553.6405\n",
            "\n",
            "Train Epoch: 70 [160/7471 (2%)]\tLoss: 1587213.125000\n",
            "Train Epoch: 70 [320/7471 (4%)]\tLoss: 1571982.375000\n",
            "Train Epoch: 70 [480/7471 (6%)]\tLoss: 1592889.125000\n",
            "Train Epoch: 70 [640/7471 (9%)]\tLoss: 1629779.250000\n",
            "Train Epoch: 70 [800/7471 (11%)]\tLoss: 1611395.125000\n",
            "Train Epoch: 70 [960/7471 (13%)]\tLoss: 1577501.625000\n",
            "Train Epoch: 70 [1120/7471 (15%)]\tLoss: 1546368.500000\n",
            "Train Epoch: 70 [1280/7471 (17%)]\tLoss: 1612689.500000\n",
            "Train Epoch: 70 [1440/7471 (19%)]\tLoss: 1544777.000000\n",
            "Train Epoch: 70 [1600/7471 (21%)]\tLoss: 1547279.250000\n",
            "Train Epoch: 70 [1760/7471 (24%)]\tLoss: 1623437.625000\n",
            "Train Epoch: 70 [1920/7471 (26%)]\tLoss: 1536172.125000\n",
            "Train Epoch: 70 [2080/7471 (28%)]\tLoss: 1586823.500000\n",
            "Train Epoch: 70 [2240/7471 (30%)]\tLoss: 1542635.875000\n",
            "Train Epoch: 70 [2400/7471 (32%)]\tLoss: 1610006.125000\n",
            "Train Epoch: 70 [2560/7471 (34%)]\tLoss: 1585775.625000\n",
            "Train Epoch: 70 [2720/7471 (36%)]\tLoss: 1593253.875000\n",
            "Train Epoch: 70 [2880/7471 (39%)]\tLoss: 1612371.125000\n",
            "Train Epoch: 70 [3040/7471 (41%)]\tLoss: 1548234.750000\n",
            "Train Epoch: 70 [3200/7471 (43%)]\tLoss: 1547650.375000\n",
            "Train Epoch: 70 [3360/7471 (45%)]\tLoss: 1644845.750000\n",
            "Train Epoch: 70 [3520/7471 (47%)]\tLoss: 1572392.750000\n",
            "Train Epoch: 70 [3680/7471 (49%)]\tLoss: 1606200.250000\n",
            "Train Epoch: 70 [3840/7471 (51%)]\tLoss: 1560342.500000\n",
            "Train Epoch: 70 [4000/7471 (54%)]\tLoss: 1617912.500000\n",
            "Train Epoch: 70 [4160/7471 (56%)]\tLoss: 1575742.000000\n",
            "Train Epoch: 70 [4320/7471 (58%)]\tLoss: 1586802.250000\n",
            "Train Epoch: 70 [4480/7471 (60%)]\tLoss: 1579385.250000\n",
            "Train Epoch: 70 [4640/7471 (62%)]\tLoss: 1492386.000000\n",
            "Train Epoch: 70 [4800/7471 (64%)]\tLoss: 1562373.000000\n",
            "Train Epoch: 70 [4960/7471 (66%)]\tLoss: 1536081.125000\n",
            "Train Epoch: 70 [5120/7471 (69%)]\tLoss: 1599380.250000\n",
            "Train Epoch: 70 [5280/7471 (71%)]\tLoss: 1612036.625000\n",
            "Train Epoch: 70 [5440/7471 (73%)]\tLoss: 1577070.000000\n",
            "Train Epoch: 70 [5600/7471 (75%)]\tLoss: 1546230.625000\n",
            "Train Epoch: 70 [5760/7471 (77%)]\tLoss: 1557090.750000\n",
            "Train Epoch: 70 [5920/7471 (79%)]\tLoss: 1530592.250000\n",
            "Train Epoch: 70 [6080/7471 (81%)]\tLoss: 1582414.125000\n",
            "Train Epoch: 70 [6240/7471 (84%)]\tLoss: 1577390.125000\n",
            "Train Epoch: 70 [6400/7471 (86%)]\tLoss: 1617354.250000\n",
            "Train Epoch: 70 [6560/7471 (88%)]\tLoss: 1579825.500000\n",
            "Train Epoch: 70 [6720/7471 (90%)]\tLoss: 1607866.625000\n",
            "Train Epoch: 70 [6880/7471 (92%)]\tLoss: 1554806.250000\n",
            "Train Epoch: 70 [7040/7471 (94%)]\tLoss: 1592966.375000\n",
            "Train Epoch: 70 [7200/7471 (96%)]\tLoss: 1611084.250000\n",
            "Train Epoch: 70 [7360/7471 (99%)]\tLoss: 1556811.125000\n",
            "Epoch 70 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98488.4595\n",
            "\n",
            "Train Epoch: 71 [160/7471 (2%)]\tLoss: 1592646.125000\n",
            "Train Epoch: 71 [320/7471 (4%)]\tLoss: 1588022.375000\n",
            "Train Epoch: 71 [480/7471 (6%)]\tLoss: 1608342.375000\n",
            "Train Epoch: 71 [640/7471 (9%)]\tLoss: 1550004.500000\n",
            "Train Epoch: 71 [800/7471 (11%)]\tLoss: 1583011.000000\n",
            "Train Epoch: 71 [960/7471 (13%)]\tLoss: 1614771.750000\n",
            "Train Epoch: 71 [1120/7471 (15%)]\tLoss: 1624190.875000\n",
            "Train Epoch: 71 [1280/7471 (17%)]\tLoss: 1596235.250000\n",
            "Train Epoch: 71 [1440/7471 (19%)]\tLoss: 1590329.375000\n",
            "Train Epoch: 71 [1600/7471 (21%)]\tLoss: 1581016.500000\n",
            "Train Epoch: 71 [1760/7471 (24%)]\tLoss: 1589321.375000\n",
            "Train Epoch: 71 [1920/7471 (26%)]\tLoss: 1625150.875000\n",
            "Train Epoch: 71 [2080/7471 (28%)]\tLoss: 1587509.625000\n",
            "Train Epoch: 71 [2240/7471 (30%)]\tLoss: 1545725.000000\n",
            "Train Epoch: 71 [2400/7471 (32%)]\tLoss: 1588646.750000\n",
            "Train Epoch: 71 [2560/7471 (34%)]\tLoss: 1556990.250000\n",
            "Train Epoch: 71 [2720/7471 (36%)]\tLoss: 1624873.375000\n",
            "Train Epoch: 71 [2880/7471 (39%)]\tLoss: 1579992.500000\n",
            "Train Epoch: 71 [3040/7471 (41%)]\tLoss: 1559854.625000\n",
            "Train Epoch: 71 [3200/7471 (43%)]\tLoss: 1609634.875000\n",
            "Train Epoch: 71 [3360/7471 (45%)]\tLoss: 1498806.875000\n",
            "Train Epoch: 71 [3520/7471 (47%)]\tLoss: 1641013.125000\n",
            "Train Epoch: 71 [3680/7471 (49%)]\tLoss: 1494185.875000\n",
            "Train Epoch: 71 [3840/7471 (51%)]\tLoss: 1568229.375000\n",
            "Train Epoch: 71 [4000/7471 (54%)]\tLoss: 1526252.375000\n",
            "Train Epoch: 71 [4160/7471 (56%)]\tLoss: 1569650.000000\n",
            "Train Epoch: 71 [4320/7471 (58%)]\tLoss: 1560847.750000\n",
            "Train Epoch: 71 [4480/7471 (60%)]\tLoss: 1547043.875000\n",
            "Train Epoch: 71 [4640/7471 (62%)]\tLoss: 1593931.500000\n",
            "Train Epoch: 71 [4800/7471 (64%)]\tLoss: 1585344.750000\n",
            "Train Epoch: 71 [4960/7471 (66%)]\tLoss: 1571191.375000\n",
            "Train Epoch: 71 [5120/7471 (69%)]\tLoss: 1568386.875000\n",
            "Train Epoch: 71 [5280/7471 (71%)]\tLoss: 1550300.875000\n",
            "Train Epoch: 71 [5440/7471 (73%)]\tLoss: 1559691.750000\n",
            "Train Epoch: 71 [5600/7471 (75%)]\tLoss: 1542002.500000\n",
            "Train Epoch: 71 [5760/7471 (77%)]\tLoss: 1559136.000000\n",
            "Train Epoch: 71 [5920/7471 (79%)]\tLoss: 1610969.625000\n",
            "Train Epoch: 71 [6080/7471 (81%)]\tLoss: 1596700.500000\n",
            "Train Epoch: 71 [6240/7471 (84%)]\tLoss: 1588207.625000\n",
            "Train Epoch: 71 [6400/7471 (86%)]\tLoss: 1556318.875000\n",
            "Train Epoch: 71 [6560/7471 (88%)]\tLoss: 1563113.750000\n",
            "Train Epoch: 71 [6720/7471 (90%)]\tLoss: 1601742.500000\n",
            "Train Epoch: 71 [6880/7471 (92%)]\tLoss: 1613868.000000\n",
            "Train Epoch: 71 [7040/7471 (94%)]\tLoss: 1580105.125000\n",
            "Train Epoch: 71 [7200/7471 (96%)]\tLoss: 1543274.875000\n",
            "Train Epoch: 71 [7360/7471 (99%)]\tLoss: 1587711.375000\n",
            "Epoch 71 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98497.9273\n",
            "\n",
            "Train Epoch: 72 [160/7471 (2%)]\tLoss: 1615221.125000\n",
            "Train Epoch: 72 [320/7471 (4%)]\tLoss: 1556410.125000\n",
            "Train Epoch: 72 [480/7471 (6%)]\tLoss: 1568325.375000\n",
            "Train Epoch: 72 [640/7471 (9%)]\tLoss: 1565840.000000\n",
            "Train Epoch: 72 [800/7471 (11%)]\tLoss: 1618634.750000\n",
            "Train Epoch: 72 [960/7471 (13%)]\tLoss: 1554349.750000\n",
            "Train Epoch: 72 [1120/7471 (15%)]\tLoss: 1631630.875000\n",
            "Train Epoch: 72 [1280/7471 (17%)]\tLoss: 1568440.500000\n",
            "Train Epoch: 72 [1440/7471 (19%)]\tLoss: 1545515.875000\n",
            "Train Epoch: 72 [1600/7471 (21%)]\tLoss: 1484479.125000\n",
            "Train Epoch: 72 [1760/7471 (24%)]\tLoss: 1555950.625000\n",
            "Train Epoch: 72 [1920/7471 (26%)]\tLoss: 1628082.000000\n",
            "Train Epoch: 72 [2080/7471 (28%)]\tLoss: 1557620.625000\n",
            "Train Epoch: 72 [2240/7471 (30%)]\tLoss: 1515346.000000\n",
            "Train Epoch: 72 [2400/7471 (32%)]\tLoss: 1542033.000000\n",
            "Train Epoch: 72 [2560/7471 (34%)]\tLoss: 1529635.000000\n",
            "Train Epoch: 72 [2720/7471 (36%)]\tLoss: 1561431.500000\n",
            "Train Epoch: 72 [2880/7471 (39%)]\tLoss: 1494097.000000\n",
            "Train Epoch: 72 [3040/7471 (41%)]\tLoss: 1588675.750000\n",
            "Train Epoch: 72 [3200/7471 (43%)]\tLoss: 1499401.750000\n",
            "Train Epoch: 72 [3360/7471 (45%)]\tLoss: 1570175.375000\n",
            "Train Epoch: 72 [3520/7471 (47%)]\tLoss: 1533629.875000\n",
            "Train Epoch: 72 [3680/7471 (49%)]\tLoss: 1610478.375000\n",
            "Train Epoch: 72 [3840/7471 (51%)]\tLoss: 1567785.375000\n",
            "Train Epoch: 72 [4000/7471 (54%)]\tLoss: 1558448.250000\n",
            "Train Epoch: 72 [4160/7471 (56%)]\tLoss: 1505112.875000\n",
            "Train Epoch: 72 [4320/7471 (58%)]\tLoss: 1617402.375000\n",
            "Train Epoch: 72 [4480/7471 (60%)]\tLoss: 1566586.875000\n",
            "Train Epoch: 72 [4640/7471 (62%)]\tLoss: 1571993.625000\n",
            "Train Epoch: 72 [4800/7471 (64%)]\tLoss: 1582165.750000\n",
            "Train Epoch: 72 [4960/7471 (66%)]\tLoss: 1596908.500000\n",
            "Train Epoch: 72 [5120/7471 (69%)]\tLoss: 1548479.500000\n",
            "Train Epoch: 72 [5280/7471 (71%)]\tLoss: 1547965.375000\n",
            "Train Epoch: 72 [5440/7471 (73%)]\tLoss: 1530389.125000\n",
            "Train Epoch: 72 [5600/7471 (75%)]\tLoss: 1584295.375000\n",
            "Train Epoch: 72 [5760/7471 (77%)]\tLoss: 1552734.500000\n",
            "Train Epoch: 72 [5920/7471 (79%)]\tLoss: 1597945.000000\n",
            "Train Epoch: 72 [6080/7471 (81%)]\tLoss: 1608739.000000\n",
            "Train Epoch: 72 [6240/7471 (84%)]\tLoss: 1548331.625000\n",
            "Train Epoch: 72 [6400/7471 (86%)]\tLoss: 1527353.875000\n",
            "Train Epoch: 72 [6560/7471 (88%)]\tLoss: 1639134.125000\n",
            "Train Epoch: 72 [6720/7471 (90%)]\tLoss: 1548698.500000\n",
            "Train Epoch: 72 [6880/7471 (92%)]\tLoss: 1577973.125000\n",
            "Train Epoch: 72 [7040/7471 (94%)]\tLoss: 1600128.500000\n",
            "Train Epoch: 72 [7200/7471 (96%)]\tLoss: 1610837.000000\n",
            "Train Epoch: 72 [7360/7471 (99%)]\tLoss: 1494770.375000\n",
            "Epoch 72 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98511.3831\n",
            "\n",
            "Train Epoch: 73 [160/7471 (2%)]\tLoss: 1538096.750000\n",
            "Train Epoch: 73 [320/7471 (4%)]\tLoss: 1501041.125000\n",
            "Train Epoch: 73 [480/7471 (6%)]\tLoss: 1565545.375000\n",
            "Train Epoch: 73 [640/7471 (9%)]\tLoss: 1537300.750000\n",
            "Train Epoch: 73 [800/7471 (11%)]\tLoss: 1561558.000000\n",
            "Train Epoch: 73 [960/7471 (13%)]\tLoss: 1541237.000000\n",
            "Train Epoch: 73 [1120/7471 (15%)]\tLoss: 1527250.000000\n",
            "Train Epoch: 73 [1280/7471 (17%)]\tLoss: 1532577.750000\n",
            "Train Epoch: 73 [1440/7471 (19%)]\tLoss: 1479346.000000\n",
            "Train Epoch: 73 [1600/7471 (21%)]\tLoss: 1607326.750000\n",
            "Train Epoch: 73 [1760/7471 (24%)]\tLoss: 1518652.875000\n",
            "Train Epoch: 73 [1920/7471 (26%)]\tLoss: 1626327.500000\n",
            "Train Epoch: 73 [2080/7471 (28%)]\tLoss: 1531127.000000\n",
            "Train Epoch: 73 [2240/7471 (30%)]\tLoss: 1599618.875000\n",
            "Train Epoch: 73 [2400/7471 (32%)]\tLoss: 1557538.250000\n",
            "Train Epoch: 73 [2560/7471 (34%)]\tLoss: 1606235.625000\n",
            "Train Epoch: 73 [2720/7471 (36%)]\tLoss: 1445092.750000\n",
            "Train Epoch: 73 [2880/7471 (39%)]\tLoss: 1604247.250000\n",
            "Train Epoch: 73 [3040/7471 (41%)]\tLoss: 1614210.000000\n",
            "Train Epoch: 73 [3200/7471 (43%)]\tLoss: 1602448.625000\n",
            "Train Epoch: 73 [3360/7471 (45%)]\tLoss: 1551032.500000\n",
            "Train Epoch: 73 [3520/7471 (47%)]\tLoss: 1584389.125000\n",
            "Train Epoch: 73 [3680/7471 (49%)]\tLoss: 1599425.375000\n",
            "Train Epoch: 73 [3840/7471 (51%)]\tLoss: 1540348.500000\n",
            "Train Epoch: 73 [4000/7471 (54%)]\tLoss: 1503586.750000\n",
            "Train Epoch: 73 [4160/7471 (56%)]\tLoss: 1566590.375000\n",
            "Train Epoch: 73 [4320/7471 (58%)]\tLoss: 1619568.125000\n",
            "Train Epoch: 73 [4480/7471 (60%)]\tLoss: 1602599.000000\n",
            "Train Epoch: 73 [4640/7471 (62%)]\tLoss: 1561937.375000\n",
            "Train Epoch: 73 [4800/7471 (64%)]\tLoss: 1587909.250000\n",
            "Train Epoch: 73 [4960/7471 (66%)]\tLoss: 1509232.375000\n",
            "Train Epoch: 73 [5120/7471 (69%)]\tLoss: 1597098.375000\n",
            "Train Epoch: 73 [5280/7471 (71%)]\tLoss: 1626868.625000\n",
            "Train Epoch: 73 [5440/7471 (73%)]\tLoss: 1579243.000000\n",
            "Train Epoch: 73 [5600/7471 (75%)]\tLoss: 1606973.125000\n",
            "Train Epoch: 73 [5760/7471 (77%)]\tLoss: 1629024.375000\n",
            "Train Epoch: 73 [5920/7471 (79%)]\tLoss: 1623509.000000\n",
            "Train Epoch: 73 [6080/7471 (81%)]\tLoss: 1604848.000000\n",
            "Train Epoch: 73 [6240/7471 (84%)]\tLoss: 1583302.000000\n",
            "Train Epoch: 73 [6400/7471 (86%)]\tLoss: 1545320.750000\n",
            "Train Epoch: 73 [6560/7471 (88%)]\tLoss: 1584724.750000\n",
            "Train Epoch: 73 [6720/7471 (90%)]\tLoss: 1567591.750000\n",
            "Train Epoch: 73 [6880/7471 (92%)]\tLoss: 1544342.625000\n",
            "Train Epoch: 73 [7040/7471 (94%)]\tLoss: 1587386.750000\n",
            "Train Epoch: 73 [7200/7471 (96%)]\tLoss: 1597458.875000\n",
            "Train Epoch: 73 [7360/7471 (99%)]\tLoss: 1514675.375000\n",
            "Epoch 73 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98550.0923\n",
            "\n",
            "Train Epoch: 74 [160/7471 (2%)]\tLoss: 1616927.875000\n",
            "Train Epoch: 74 [320/7471 (4%)]\tLoss: 1584419.375000\n",
            "Train Epoch: 74 [480/7471 (6%)]\tLoss: 1587973.125000\n",
            "Train Epoch: 74 [640/7471 (9%)]\tLoss: 1537636.375000\n",
            "Train Epoch: 74 [800/7471 (11%)]\tLoss: 1586388.375000\n",
            "Train Epoch: 74 [960/7471 (13%)]\tLoss: 1566614.000000\n",
            "Train Epoch: 74 [1120/7471 (15%)]\tLoss: 1576090.375000\n",
            "Train Epoch: 74 [1280/7471 (17%)]\tLoss: 1617365.750000\n",
            "Train Epoch: 74 [1440/7471 (19%)]\tLoss: 1515035.625000\n",
            "Train Epoch: 74 [1600/7471 (21%)]\tLoss: 1509535.875000\n",
            "Train Epoch: 74 [1760/7471 (24%)]\tLoss: 1507078.375000\n",
            "Train Epoch: 74 [1920/7471 (26%)]\tLoss: 1603720.250000\n",
            "Train Epoch: 74 [2080/7471 (28%)]\tLoss: 1499028.000000\n",
            "Train Epoch: 74 [2240/7471 (30%)]\tLoss: 1614207.875000\n",
            "Train Epoch: 74 [2400/7471 (32%)]\tLoss: 1512971.375000\n",
            "Train Epoch: 74 [2560/7471 (34%)]\tLoss: 1570925.500000\n",
            "Train Epoch: 74 [2720/7471 (36%)]\tLoss: 1567491.875000\n",
            "Train Epoch: 74 [2880/7471 (39%)]\tLoss: 1574919.500000\n",
            "Train Epoch: 74 [3040/7471 (41%)]\tLoss: 1608094.250000\n",
            "Train Epoch: 74 [3200/7471 (43%)]\tLoss: 1610733.500000\n",
            "Train Epoch: 74 [3360/7471 (45%)]\tLoss: 1566705.000000\n",
            "Train Epoch: 74 [3520/7471 (47%)]\tLoss: 1464720.500000\n",
            "Train Epoch: 74 [3680/7471 (49%)]\tLoss: 1534691.000000\n",
            "Train Epoch: 74 [3840/7471 (51%)]\tLoss: 1598671.875000\n",
            "Train Epoch: 74 [4000/7471 (54%)]\tLoss: 1599923.000000\n",
            "Train Epoch: 74 [4160/7471 (56%)]\tLoss: 1603666.750000\n",
            "Train Epoch: 74 [4320/7471 (58%)]\tLoss: 1523582.750000\n",
            "Train Epoch: 74 [4480/7471 (60%)]\tLoss: 1533118.250000\n",
            "Train Epoch: 74 [4640/7471 (62%)]\tLoss: 1615185.500000\n",
            "Train Epoch: 74 [4800/7471 (64%)]\tLoss: 1586096.125000\n",
            "Train Epoch: 74 [4960/7471 (66%)]\tLoss: 1611904.875000\n",
            "Train Epoch: 74 [5120/7471 (69%)]\tLoss: 1551448.250000\n",
            "Train Epoch: 74 [5280/7471 (71%)]\tLoss: 1624222.000000\n",
            "Train Epoch: 74 [5440/7471 (73%)]\tLoss: 1567477.625000\n",
            "Train Epoch: 74 [5600/7471 (75%)]\tLoss: 1488205.750000\n",
            "Train Epoch: 74 [5760/7471 (77%)]\tLoss: 1627813.250000\n",
            "Train Epoch: 74 [5920/7471 (79%)]\tLoss: 1585261.250000\n",
            "Train Epoch: 74 [6080/7471 (81%)]\tLoss: 1591836.500000\n",
            "Train Epoch: 74 [6240/7471 (84%)]\tLoss: 1607989.375000\n",
            "Train Epoch: 74 [6400/7471 (86%)]\tLoss: 1497846.875000\n",
            "Train Epoch: 74 [6560/7471 (88%)]\tLoss: 1575699.000000\n",
            "Train Epoch: 74 [6720/7471 (90%)]\tLoss: 1578797.500000\n",
            "Train Epoch: 74 [6880/7471 (92%)]\tLoss: 1627025.875000\n",
            "Train Epoch: 74 [7040/7471 (94%)]\tLoss: 1601494.625000\n",
            "Train Epoch: 74 [7200/7471 (96%)]\tLoss: 1551324.500000\n",
            "Train Epoch: 74 [7360/7471 (99%)]\tLoss: 1616236.000000\n",
            "Epoch 74 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98462.9215\n",
            "\n",
            "Train Epoch: 75 [160/7471 (2%)]\tLoss: 1576053.625000\n",
            "Train Epoch: 75 [320/7471 (4%)]\tLoss: 1567635.125000\n",
            "Train Epoch: 75 [480/7471 (6%)]\tLoss: 1570378.250000\n",
            "Train Epoch: 75 [640/7471 (9%)]\tLoss: 1617431.000000\n",
            "Train Epoch: 75 [800/7471 (11%)]\tLoss: 1581302.125000\n",
            "Train Epoch: 75 [960/7471 (13%)]\tLoss: 1603487.500000\n",
            "Train Epoch: 75 [1120/7471 (15%)]\tLoss: 1609792.375000\n",
            "Train Epoch: 75 [1280/7471 (17%)]\tLoss: 1562960.750000\n",
            "Train Epoch: 75 [1440/7471 (19%)]\tLoss: 1498117.500000\n",
            "Train Epoch: 75 [1600/7471 (21%)]\tLoss: 1509005.625000\n",
            "Train Epoch: 75 [1760/7471 (24%)]\tLoss: 1622654.375000\n",
            "Train Epoch: 75 [1920/7471 (26%)]\tLoss: 1539534.500000\n",
            "Train Epoch: 75 [2080/7471 (28%)]\tLoss: 1533045.125000\n",
            "Train Epoch: 75 [2240/7471 (30%)]\tLoss: 1581028.500000\n",
            "Train Epoch: 75 [2400/7471 (32%)]\tLoss: 1632489.750000\n",
            "Train Epoch: 75 [2560/7471 (34%)]\tLoss: 1565987.375000\n",
            "Train Epoch: 75 [2720/7471 (36%)]\tLoss: 1570346.000000\n",
            "Train Epoch: 75 [2880/7471 (39%)]\tLoss: 1542521.000000\n",
            "Train Epoch: 75 [3040/7471 (41%)]\tLoss: 1609477.625000\n",
            "Train Epoch: 75 [3200/7471 (43%)]\tLoss: 1582618.875000\n",
            "Train Epoch: 75 [3360/7471 (45%)]\tLoss: 1640143.500000\n",
            "Train Epoch: 75 [3520/7471 (47%)]\tLoss: 1552897.625000\n",
            "Train Epoch: 75 [3680/7471 (49%)]\tLoss: 1598907.875000\n",
            "Train Epoch: 75 [3840/7471 (51%)]\tLoss: 1628730.875000\n",
            "Train Epoch: 75 [4000/7471 (54%)]\tLoss: 1548550.625000\n",
            "Train Epoch: 75 [4160/7471 (56%)]\tLoss: 1595901.250000\n",
            "Train Epoch: 75 [4320/7471 (58%)]\tLoss: 1566130.625000\n",
            "Train Epoch: 75 [4480/7471 (60%)]\tLoss: 1577923.125000\n",
            "Train Epoch: 75 [4640/7471 (62%)]\tLoss: 1609705.500000\n",
            "Train Epoch: 75 [4800/7471 (64%)]\tLoss: 1612482.875000\n",
            "Train Epoch: 75 [4960/7471 (66%)]\tLoss: 1587183.875000\n",
            "Train Epoch: 75 [5120/7471 (69%)]\tLoss: 1534464.250000\n",
            "Train Epoch: 75 [5280/7471 (71%)]\tLoss: 1568148.125000\n",
            "Train Epoch: 75 [5440/7471 (73%)]\tLoss: 1505655.375000\n",
            "Train Epoch: 75 [5600/7471 (75%)]\tLoss: 1600038.750000\n",
            "Train Epoch: 75 [5760/7471 (77%)]\tLoss: 1611290.125000\n",
            "Train Epoch: 75 [5920/7471 (79%)]\tLoss: 1589015.500000\n",
            "Train Epoch: 75 [6080/7471 (81%)]\tLoss: 1610555.125000\n",
            "Train Epoch: 75 [6240/7471 (84%)]\tLoss: 1615030.625000\n",
            "Train Epoch: 75 [6400/7471 (86%)]\tLoss: 1555033.500000\n",
            "Train Epoch: 75 [6560/7471 (88%)]\tLoss: 1556508.875000\n",
            "Train Epoch: 75 [6720/7471 (90%)]\tLoss: 1637429.875000\n",
            "Train Epoch: 75 [6880/7471 (92%)]\tLoss: 1521980.625000\n",
            "Train Epoch: 75 [7040/7471 (94%)]\tLoss: 1559904.750000\n",
            "Train Epoch: 75 [7200/7471 (96%)]\tLoss: 1564559.000000\n",
            "Train Epoch: 75 [7360/7471 (99%)]\tLoss: 1561194.375000\n",
            "Epoch 75 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98471.5464\n",
            "\n",
            "Train Epoch: 76 [160/7471 (2%)]\tLoss: 1557706.250000\n",
            "Train Epoch: 76 [320/7471 (4%)]\tLoss: 1611413.875000\n",
            "Train Epoch: 76 [480/7471 (6%)]\tLoss: 1617819.000000\n",
            "Train Epoch: 76 [640/7471 (9%)]\tLoss: 1596911.125000\n",
            "Train Epoch: 76 [800/7471 (11%)]\tLoss: 1601965.250000\n",
            "Train Epoch: 76 [960/7471 (13%)]\tLoss: 1628800.875000\n",
            "Train Epoch: 76 [1120/7471 (15%)]\tLoss: 1629856.375000\n",
            "Train Epoch: 76 [1280/7471 (17%)]\tLoss: 1572419.500000\n",
            "Train Epoch: 76 [1440/7471 (19%)]\tLoss: 1546637.500000\n",
            "Train Epoch: 76 [1600/7471 (21%)]\tLoss: 1593876.375000\n",
            "Train Epoch: 76 [1760/7471 (24%)]\tLoss: 1590647.375000\n",
            "Train Epoch: 76 [1920/7471 (26%)]\tLoss: 1575727.375000\n",
            "Train Epoch: 76 [2080/7471 (28%)]\tLoss: 1556300.375000\n",
            "Train Epoch: 76 [2240/7471 (30%)]\tLoss: 1539090.125000\n",
            "Train Epoch: 76 [2400/7471 (32%)]\tLoss: 1570570.375000\n",
            "Train Epoch: 76 [2560/7471 (34%)]\tLoss: 1561054.500000\n",
            "Train Epoch: 76 [2720/7471 (36%)]\tLoss: 1548865.375000\n",
            "Train Epoch: 76 [2880/7471 (39%)]\tLoss: 1588357.000000\n",
            "Train Epoch: 76 [3040/7471 (41%)]\tLoss: 1587873.750000\n",
            "Train Epoch: 76 [3200/7471 (43%)]\tLoss: 1601214.375000\n",
            "Train Epoch: 76 [3360/7471 (45%)]\tLoss: 1599785.250000\n",
            "Train Epoch: 76 [3520/7471 (47%)]\tLoss: 1586047.000000\n",
            "Train Epoch: 76 [3680/7471 (49%)]\tLoss: 1622086.875000\n",
            "Train Epoch: 76 [3840/7471 (51%)]\tLoss: 1554056.375000\n",
            "Train Epoch: 76 [4000/7471 (54%)]\tLoss: 1616798.750000\n",
            "Train Epoch: 76 [4160/7471 (56%)]\tLoss: 1590372.000000\n",
            "Train Epoch: 76 [4320/7471 (58%)]\tLoss: 1556515.375000\n",
            "Train Epoch: 76 [4480/7471 (60%)]\tLoss: 1519115.125000\n",
            "Train Epoch: 76 [4640/7471 (62%)]\tLoss: 1616728.625000\n",
            "Train Epoch: 76 [4800/7471 (64%)]\tLoss: 1562257.625000\n",
            "Train Epoch: 76 [4960/7471 (66%)]\tLoss: 1525420.250000\n",
            "Train Epoch: 76 [5120/7471 (69%)]\tLoss: 1527426.625000\n",
            "Train Epoch: 76 [5280/7471 (71%)]\tLoss: 1503523.875000\n",
            "Train Epoch: 76 [5440/7471 (73%)]\tLoss: 1563998.875000\n",
            "Train Epoch: 76 [5600/7471 (75%)]\tLoss: 1551152.750000\n",
            "Train Epoch: 76 [5760/7471 (77%)]\tLoss: 1600882.875000\n",
            "Train Epoch: 76 [5920/7471 (79%)]\tLoss: 1623056.375000\n",
            "Train Epoch: 76 [6080/7471 (81%)]\tLoss: 1576111.125000\n",
            "Train Epoch: 76 [6240/7471 (84%)]\tLoss: 1572070.625000\n",
            "Train Epoch: 76 [6400/7471 (86%)]\tLoss: 1605803.625000\n",
            "Train Epoch: 76 [6560/7471 (88%)]\tLoss: 1527801.125000\n",
            "Train Epoch: 76 [6720/7471 (90%)]\tLoss: 1529703.000000\n",
            "Train Epoch: 76 [6880/7471 (92%)]\tLoss: 1607289.875000\n",
            "Train Epoch: 76 [7040/7471 (94%)]\tLoss: 1627141.375000\n",
            "Train Epoch: 76 [7200/7471 (96%)]\tLoss: 1549499.625000\n",
            "Train Epoch: 76 [7360/7471 (99%)]\tLoss: 1610946.000000\n",
            "Epoch 76 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98485.1026\n",
            "\n",
            "Train Epoch: 77 [160/7471 (2%)]\tLoss: 1616493.125000\n",
            "Train Epoch: 77 [320/7471 (4%)]\tLoss: 1531444.625000\n",
            "Train Epoch: 77 [480/7471 (6%)]\tLoss: 1491344.250000\n",
            "Train Epoch: 77 [640/7471 (9%)]\tLoss: 1543605.500000\n",
            "Train Epoch: 77 [800/7471 (11%)]\tLoss: 1623383.500000\n",
            "Train Epoch: 77 [960/7471 (13%)]\tLoss: 1604734.125000\n",
            "Train Epoch: 77 [1120/7471 (15%)]\tLoss: 1477951.875000\n",
            "Train Epoch: 77 [1280/7471 (17%)]\tLoss: 1547071.250000\n",
            "Train Epoch: 77 [1440/7471 (19%)]\tLoss: 1591276.250000\n",
            "Train Epoch: 77 [1600/7471 (21%)]\tLoss: 1534355.250000\n",
            "Train Epoch: 77 [1760/7471 (24%)]\tLoss: 1575552.250000\n",
            "Train Epoch: 77 [1920/7471 (26%)]\tLoss: 1499208.125000\n",
            "Train Epoch: 77 [2080/7471 (28%)]\tLoss: 1610800.375000\n",
            "Train Epoch: 77 [2240/7471 (30%)]\tLoss: 1539927.000000\n",
            "Train Epoch: 77 [2400/7471 (32%)]\tLoss: 1578316.125000\n",
            "Train Epoch: 77 [2560/7471 (34%)]\tLoss: 1579287.125000\n",
            "Train Epoch: 77 [2720/7471 (36%)]\tLoss: 1536202.500000\n",
            "Train Epoch: 77 [2880/7471 (39%)]\tLoss: 1563675.750000\n",
            "Train Epoch: 77 [3040/7471 (41%)]\tLoss: 1565413.875000\n",
            "Train Epoch: 77 [3200/7471 (43%)]\tLoss: 1577777.375000\n",
            "Train Epoch: 77 [3360/7471 (45%)]\tLoss: 1611335.875000\n",
            "Train Epoch: 77 [3520/7471 (47%)]\tLoss: 1610559.625000\n",
            "Train Epoch: 77 [3680/7471 (49%)]\tLoss: 1586114.875000\n",
            "Train Epoch: 77 [3840/7471 (51%)]\tLoss: 1539485.125000\n",
            "Train Epoch: 77 [4000/7471 (54%)]\tLoss: 1574020.875000\n",
            "Train Epoch: 77 [4160/7471 (56%)]\tLoss: 1608858.250000\n",
            "Train Epoch: 77 [4320/7471 (58%)]\tLoss: 1594117.125000\n",
            "Train Epoch: 77 [4480/7471 (60%)]\tLoss: 1588778.375000\n",
            "Train Epoch: 77 [4640/7471 (62%)]\tLoss: 1557717.125000\n",
            "Train Epoch: 77 [4800/7471 (64%)]\tLoss: 1612415.875000\n",
            "Train Epoch: 77 [4960/7471 (66%)]\tLoss: 1516790.500000\n",
            "Train Epoch: 77 [5120/7471 (69%)]\tLoss: 1613671.625000\n",
            "Train Epoch: 77 [5280/7471 (71%)]\tLoss: 1583099.125000\n",
            "Train Epoch: 77 [5440/7471 (73%)]\tLoss: 1592548.500000\n",
            "Train Epoch: 77 [5600/7471 (75%)]\tLoss: 1575346.625000\n",
            "Train Epoch: 77 [5760/7471 (77%)]\tLoss: 1628780.625000\n",
            "Train Epoch: 77 [5920/7471 (79%)]\tLoss: 1622577.000000\n",
            "Train Epoch: 77 [6080/7471 (81%)]\tLoss: 1613501.750000\n",
            "Train Epoch: 77 [6240/7471 (84%)]\tLoss: 1557100.750000\n",
            "Train Epoch: 77 [6400/7471 (86%)]\tLoss: 1594000.125000\n",
            "Train Epoch: 77 [6560/7471 (88%)]\tLoss: 1619838.250000\n",
            "Train Epoch: 77 [6720/7471 (90%)]\tLoss: 1607382.250000\n",
            "Train Epoch: 77 [6880/7471 (92%)]\tLoss: 1528027.625000\n",
            "Train Epoch: 77 [7040/7471 (94%)]\tLoss: 1569533.750000\n",
            "Train Epoch: 77 [7200/7471 (96%)]\tLoss: 1544813.875000\n",
            "Train Epoch: 77 [7360/7471 (99%)]\tLoss: 1570149.125000\n",
            "Epoch 77 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98475.7658\n",
            "\n",
            "Train Epoch: 78 [160/7471 (2%)]\tLoss: 1553348.875000\n",
            "Train Epoch: 78 [320/7471 (4%)]\tLoss: 1582764.500000\n",
            "Train Epoch: 78 [480/7471 (6%)]\tLoss: 1631015.750000\n",
            "Train Epoch: 78 [640/7471 (9%)]\tLoss: 1546909.125000\n",
            "Train Epoch: 78 [800/7471 (11%)]\tLoss: 1633212.250000\n",
            "Train Epoch: 78 [960/7471 (13%)]\tLoss: 1597358.500000\n",
            "Train Epoch: 78 [1120/7471 (15%)]\tLoss: 1514821.625000\n",
            "Train Epoch: 78 [1280/7471 (17%)]\tLoss: 1530423.500000\n",
            "Train Epoch: 78 [1440/7471 (19%)]\tLoss: 1517529.750000\n",
            "Train Epoch: 78 [1600/7471 (21%)]\tLoss: 1590077.250000\n",
            "Train Epoch: 78 [1760/7471 (24%)]\tLoss: 1595486.250000\n",
            "Train Epoch: 78 [1920/7471 (26%)]\tLoss: 1593269.875000\n",
            "Train Epoch: 78 [2080/7471 (28%)]\tLoss: 1543205.375000\n",
            "Train Epoch: 78 [2240/7471 (30%)]\tLoss: 1611863.250000\n",
            "Train Epoch: 78 [2400/7471 (32%)]\tLoss: 1612799.250000\n",
            "Train Epoch: 78 [2560/7471 (34%)]\tLoss: 1530137.875000\n",
            "Train Epoch: 78 [2720/7471 (36%)]\tLoss: 1553341.500000\n",
            "Train Epoch: 78 [2880/7471 (39%)]\tLoss: 1561585.375000\n",
            "Train Epoch: 78 [3040/7471 (41%)]\tLoss: 1579177.875000\n",
            "Train Epoch: 78 [3200/7471 (43%)]\tLoss: 1545685.625000\n",
            "Train Epoch: 78 [3360/7471 (45%)]\tLoss: 1571989.750000\n",
            "Train Epoch: 78 [3520/7471 (47%)]\tLoss: 1600993.625000\n",
            "Train Epoch: 78 [3680/7471 (49%)]\tLoss: 1636234.000000\n",
            "Train Epoch: 78 [3840/7471 (51%)]\tLoss: 1604608.875000\n",
            "Train Epoch: 78 [4000/7471 (54%)]\tLoss: 1545391.000000\n",
            "Train Epoch: 78 [4160/7471 (56%)]\tLoss: 1567288.500000\n",
            "Train Epoch: 78 [4320/7471 (58%)]\tLoss: 1530688.375000\n",
            "Train Epoch: 78 [4480/7471 (60%)]\tLoss: 1594742.625000\n",
            "Train Epoch: 78 [4640/7471 (62%)]\tLoss: 1585699.375000\n",
            "Train Epoch: 78 [4800/7471 (64%)]\tLoss: 1589350.250000\n",
            "Train Epoch: 78 [4960/7471 (66%)]\tLoss: 1565256.375000\n",
            "Train Epoch: 78 [5120/7471 (69%)]\tLoss: 1583525.375000\n",
            "Train Epoch: 78 [5280/7471 (71%)]\tLoss: 1514799.250000\n",
            "Train Epoch: 78 [5440/7471 (73%)]\tLoss: 1591569.500000\n",
            "Train Epoch: 78 [5600/7471 (75%)]\tLoss: 1596273.625000\n",
            "Train Epoch: 78 [5760/7471 (77%)]\tLoss: 1512898.375000\n",
            "Train Epoch: 78 [5920/7471 (79%)]\tLoss: 1521757.000000\n",
            "Train Epoch: 78 [6080/7471 (81%)]\tLoss: 1565531.375000\n",
            "Train Epoch: 78 [6240/7471 (84%)]\tLoss: 1524885.500000\n",
            "Train Epoch: 78 [6400/7471 (86%)]\tLoss: 1584368.000000\n",
            "Train Epoch: 78 [6560/7471 (88%)]\tLoss: 1595796.750000\n",
            "Train Epoch: 78 [6720/7471 (90%)]\tLoss: 1552813.500000\n",
            "Train Epoch: 78 [6880/7471 (92%)]\tLoss: 1530221.375000\n",
            "Train Epoch: 78 [7040/7471 (94%)]\tLoss: 1586029.875000\n",
            "Train Epoch: 78 [7200/7471 (96%)]\tLoss: 1622835.625000\n",
            "Train Epoch: 78 [7360/7471 (99%)]\tLoss: 1555076.500000\n",
            "Epoch 78 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98506.3151\n",
            "\n",
            "Train Epoch: 79 [160/7471 (2%)]\tLoss: 1612423.750000\n",
            "Train Epoch: 79 [320/7471 (4%)]\tLoss: 1584480.750000\n",
            "Train Epoch: 79 [480/7471 (6%)]\tLoss: 1577067.375000\n",
            "Train Epoch: 79 [640/7471 (9%)]\tLoss: 1573894.750000\n",
            "Train Epoch: 79 [800/7471 (11%)]\tLoss: 1529640.000000\n",
            "Train Epoch: 79 [960/7471 (13%)]\tLoss: 1583491.750000\n",
            "Train Epoch: 79 [1120/7471 (15%)]\tLoss: 1609954.375000\n",
            "Train Epoch: 79 [1280/7471 (17%)]\tLoss: 1520107.750000\n",
            "Train Epoch: 79 [1440/7471 (19%)]\tLoss: 1573206.125000\n",
            "Train Epoch: 79 [1600/7471 (21%)]\tLoss: 1572334.500000\n",
            "Train Epoch: 79 [1760/7471 (24%)]\tLoss: 1567634.375000\n",
            "Train Epoch: 79 [1920/7471 (26%)]\tLoss: 1606181.625000\n",
            "Train Epoch: 79 [2080/7471 (28%)]\tLoss: 1561636.500000\n",
            "Train Epoch: 79 [2240/7471 (30%)]\tLoss: 1596780.750000\n",
            "Train Epoch: 79 [2400/7471 (32%)]\tLoss: 1634803.625000\n",
            "Train Epoch: 79 [2560/7471 (34%)]\tLoss: 1627043.250000\n",
            "Train Epoch: 79 [2720/7471 (36%)]\tLoss: 1568064.375000\n",
            "Train Epoch: 79 [2880/7471 (39%)]\tLoss: 1533839.750000\n",
            "Train Epoch: 79 [3040/7471 (41%)]\tLoss: 1596802.875000\n",
            "Train Epoch: 79 [3200/7471 (43%)]\tLoss: 1584244.500000\n",
            "Train Epoch: 79 [3360/7471 (45%)]\tLoss: 1587462.500000\n",
            "Train Epoch: 79 [3520/7471 (47%)]\tLoss: 1613699.000000\n",
            "Train Epoch: 79 [3680/7471 (49%)]\tLoss: 1527730.375000\n",
            "Train Epoch: 79 [3840/7471 (51%)]\tLoss: 1610748.500000\n",
            "Train Epoch: 79 [4000/7471 (54%)]\tLoss: 1545823.125000\n",
            "Train Epoch: 79 [4160/7471 (56%)]\tLoss: 1564022.000000\n",
            "Train Epoch: 79 [4320/7471 (58%)]\tLoss: 1547582.750000\n",
            "Train Epoch: 79 [4480/7471 (60%)]\tLoss: 1503474.625000\n",
            "Train Epoch: 79 [4640/7471 (62%)]\tLoss: 1520722.000000\n",
            "Train Epoch: 79 [4800/7471 (64%)]\tLoss: 1539781.000000\n",
            "Train Epoch: 79 [4960/7471 (66%)]\tLoss: 1584487.250000\n",
            "Train Epoch: 79 [5120/7471 (69%)]\tLoss: 1489424.750000\n",
            "Train Epoch: 79 [5280/7471 (71%)]\tLoss: 1584143.000000\n",
            "Train Epoch: 79 [5440/7471 (73%)]\tLoss: 1598693.625000\n",
            "Train Epoch: 79 [5600/7471 (75%)]\tLoss: 1596102.375000\n",
            "Train Epoch: 79 [5760/7471 (77%)]\tLoss: 1617715.875000\n",
            "Train Epoch: 79 [5920/7471 (79%)]\tLoss: 1519298.375000\n",
            "Train Epoch: 79 [6080/7471 (81%)]\tLoss: 1577686.875000\n",
            "Train Epoch: 79 [6240/7471 (84%)]\tLoss: 1555738.875000\n",
            "Train Epoch: 79 [6400/7471 (86%)]\tLoss: 1590445.375000\n",
            "Train Epoch: 79 [6560/7471 (88%)]\tLoss: 1566718.000000\n",
            "Train Epoch: 79 [6720/7471 (90%)]\tLoss: 1555021.125000\n",
            "Train Epoch: 79 [6880/7471 (92%)]\tLoss: 1583214.875000\n",
            "Train Epoch: 79 [7040/7471 (94%)]\tLoss: 1583644.875000\n",
            "Train Epoch: 79 [7200/7471 (96%)]\tLoss: 1608733.375000\n",
            "Train Epoch: 79 [7360/7471 (99%)]\tLoss: 1585396.125000\n",
            "Epoch 79 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98482.8543\n",
            "\n",
            "Train Epoch: 80 [160/7471 (2%)]\tLoss: 1583927.750000\n",
            "Train Epoch: 80 [320/7471 (4%)]\tLoss: 1615514.500000\n",
            "Train Epoch: 80 [480/7471 (6%)]\tLoss: 1616017.125000\n",
            "Train Epoch: 80 [640/7471 (9%)]\tLoss: 1600304.875000\n",
            "Train Epoch: 80 [800/7471 (11%)]\tLoss: 1520297.750000\n",
            "Train Epoch: 80 [960/7471 (13%)]\tLoss: 1547245.000000\n",
            "Train Epoch: 80 [1120/7471 (15%)]\tLoss: 1565920.125000\n",
            "Train Epoch: 80 [1280/7471 (17%)]\tLoss: 1502564.000000\n",
            "Train Epoch: 80 [1440/7471 (19%)]\tLoss: 1515926.000000\n",
            "Train Epoch: 80 [1600/7471 (21%)]\tLoss: 1573698.000000\n",
            "Train Epoch: 80 [1760/7471 (24%)]\tLoss: 1587537.125000\n",
            "Train Epoch: 80 [1920/7471 (26%)]\tLoss: 1601540.875000\n",
            "Train Epoch: 80 [2080/7471 (28%)]\tLoss: 1596580.125000\n",
            "Train Epoch: 80 [2240/7471 (30%)]\tLoss: 1612447.625000\n",
            "Train Epoch: 80 [2400/7471 (32%)]\tLoss: 1572698.250000\n",
            "Train Epoch: 80 [2560/7471 (34%)]\tLoss: 1470451.500000\n",
            "Train Epoch: 80 [2720/7471 (36%)]\tLoss: 1558505.875000\n",
            "Train Epoch: 80 [2880/7471 (39%)]\tLoss: 1629816.250000\n",
            "Train Epoch: 80 [3040/7471 (41%)]\tLoss: 1580590.375000\n",
            "Train Epoch: 80 [3200/7471 (43%)]\tLoss: 1520088.000000\n",
            "Train Epoch: 80 [3360/7471 (45%)]\tLoss: 1555471.500000\n",
            "Train Epoch: 80 [3520/7471 (47%)]\tLoss: 1595465.375000\n",
            "Train Epoch: 80 [3680/7471 (49%)]\tLoss: 1615760.500000\n",
            "Train Epoch: 80 [3840/7471 (51%)]\tLoss: 1633082.625000\n",
            "Train Epoch: 80 [4000/7471 (54%)]\tLoss: 1587166.625000\n",
            "Train Epoch: 80 [4160/7471 (56%)]\tLoss: 1503607.375000\n",
            "Train Epoch: 80 [4320/7471 (58%)]\tLoss: 1502401.750000\n",
            "Train Epoch: 80 [4480/7471 (60%)]\tLoss: 1565214.625000\n",
            "Train Epoch: 80 [4640/7471 (62%)]\tLoss: 1601330.000000\n",
            "Train Epoch: 80 [4800/7471 (64%)]\tLoss: 1613935.250000\n",
            "Train Epoch: 80 [4960/7471 (66%)]\tLoss: 1496106.875000\n",
            "Train Epoch: 80 [5120/7471 (69%)]\tLoss: 1574309.750000\n",
            "Train Epoch: 80 [5280/7471 (71%)]\tLoss: 1591109.500000\n",
            "Train Epoch: 80 [5440/7471 (73%)]\tLoss: 1566015.750000\n",
            "Train Epoch: 80 [5600/7471 (75%)]\tLoss: 1582940.500000\n",
            "Train Epoch: 80 [5760/7471 (77%)]\tLoss: 1601049.625000\n",
            "Train Epoch: 80 [5920/7471 (79%)]\tLoss: 1549037.500000\n",
            "Train Epoch: 80 [6080/7471 (81%)]\tLoss: 1593685.750000\n",
            "Train Epoch: 80 [6240/7471 (84%)]\tLoss: 1616389.125000\n",
            "Train Epoch: 80 [6400/7471 (86%)]\tLoss: 1581626.625000\n",
            "Train Epoch: 80 [6560/7471 (88%)]\tLoss: 1594715.000000\n",
            "Train Epoch: 80 [6720/7471 (90%)]\tLoss: 1607388.375000\n",
            "Train Epoch: 80 [6880/7471 (92%)]\tLoss: 1603074.250000\n",
            "Train Epoch: 80 [7040/7471 (94%)]\tLoss: 1626044.625000\n",
            "Train Epoch: 80 [7200/7471 (96%)]\tLoss: 1571719.125000\n",
            "Train Epoch: 80 [7360/7471 (99%)]\tLoss: 1596523.750000\n",
            "Epoch 80 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98524.2495\n",
            "\n",
            "Train Epoch: 81 [160/7471 (2%)]\tLoss: 1577399.250000\n",
            "Train Epoch: 81 [320/7471 (4%)]\tLoss: 1546306.875000\n",
            "Train Epoch: 81 [480/7471 (6%)]\tLoss: 1560568.125000\n",
            "Train Epoch: 81 [640/7471 (9%)]\tLoss: 1550509.000000\n",
            "Train Epoch: 81 [800/7471 (11%)]\tLoss: 1615546.750000\n",
            "Train Epoch: 81 [960/7471 (13%)]\tLoss: 1574862.375000\n",
            "Train Epoch: 81 [1120/7471 (15%)]\tLoss: 1562901.625000\n",
            "Train Epoch: 81 [1280/7471 (17%)]\tLoss: 1610620.500000\n",
            "Train Epoch: 81 [1440/7471 (19%)]\tLoss: 1543880.250000\n",
            "Train Epoch: 81 [1600/7471 (21%)]\tLoss: 1601976.250000\n",
            "Train Epoch: 81 [1760/7471 (24%)]\tLoss: 1556120.000000\n",
            "Train Epoch: 81 [1920/7471 (26%)]\tLoss: 1610946.750000\n",
            "Train Epoch: 81 [2080/7471 (28%)]\tLoss: 1588512.875000\n",
            "Train Epoch: 81 [2240/7471 (30%)]\tLoss: 1614995.500000\n",
            "Train Epoch: 81 [2400/7471 (32%)]\tLoss: 1629048.000000\n",
            "Train Epoch: 81 [2560/7471 (34%)]\tLoss: 1561204.000000\n",
            "Train Epoch: 81 [2720/7471 (36%)]\tLoss: 1580699.625000\n",
            "Train Epoch: 81 [2880/7471 (39%)]\tLoss: 1527643.500000\n",
            "Train Epoch: 81 [3040/7471 (41%)]\tLoss: 1586189.625000\n",
            "Train Epoch: 81 [3200/7471 (43%)]\tLoss: 1607546.375000\n",
            "Train Epoch: 81 [3360/7471 (45%)]\tLoss: 1589011.000000\n",
            "Train Epoch: 81 [3520/7471 (47%)]\tLoss: 1529034.875000\n",
            "Train Epoch: 81 [3680/7471 (49%)]\tLoss: 1561313.750000\n",
            "Train Epoch: 81 [3840/7471 (51%)]\tLoss: 1625114.625000\n",
            "Train Epoch: 81 [4000/7471 (54%)]\tLoss: 1609508.375000\n",
            "Train Epoch: 81 [4160/7471 (56%)]\tLoss: 1554033.000000\n",
            "Train Epoch: 81 [4320/7471 (58%)]\tLoss: 1621369.875000\n",
            "Train Epoch: 81 [4480/7471 (60%)]\tLoss: 1616121.750000\n",
            "Train Epoch: 81 [4640/7471 (62%)]\tLoss: 1591220.625000\n",
            "Train Epoch: 81 [4800/7471 (64%)]\tLoss: 1585743.125000\n",
            "Train Epoch: 81 [4960/7471 (66%)]\tLoss: 1536815.375000\n",
            "Train Epoch: 81 [5120/7471 (69%)]\tLoss: 1542932.625000\n",
            "Train Epoch: 81 [5280/7471 (71%)]\tLoss: 1596147.250000\n",
            "Train Epoch: 81 [5440/7471 (73%)]\tLoss: 1570459.125000\n",
            "Train Epoch: 81 [5600/7471 (75%)]\tLoss: 1556943.875000\n",
            "Train Epoch: 81 [5760/7471 (77%)]\tLoss: 1554301.375000\n",
            "Train Epoch: 81 [5920/7471 (79%)]\tLoss: 1607869.875000\n",
            "Train Epoch: 81 [6080/7471 (81%)]\tLoss: 1631752.125000\n",
            "Train Epoch: 81 [6240/7471 (84%)]\tLoss: 1539074.750000\n",
            "Train Epoch: 81 [6400/7471 (86%)]\tLoss: 1548135.500000\n",
            "Train Epoch: 81 [6560/7471 (88%)]\tLoss: 1548362.750000\n",
            "Train Epoch: 81 [6720/7471 (90%)]\tLoss: 1543696.000000\n",
            "Train Epoch: 81 [6880/7471 (92%)]\tLoss: 1457218.875000\n",
            "Train Epoch: 81 [7040/7471 (94%)]\tLoss: 1638122.625000\n",
            "Train Epoch: 81 [7200/7471 (96%)]\tLoss: 1564676.500000\n",
            "Train Epoch: 81 [7360/7471 (99%)]\tLoss: 1596966.125000\n",
            "Epoch 81 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98480.2659\n",
            "\n",
            "Train Epoch: 82 [160/7471 (2%)]\tLoss: 1584362.250000\n",
            "Train Epoch: 82 [320/7471 (4%)]\tLoss: 1523651.500000\n",
            "Train Epoch: 82 [480/7471 (6%)]\tLoss: 1593926.875000\n",
            "Train Epoch: 82 [640/7471 (9%)]\tLoss: 1581987.750000\n",
            "Train Epoch: 82 [800/7471 (11%)]\tLoss: 1609361.125000\n",
            "Train Epoch: 82 [960/7471 (13%)]\tLoss: 1550637.500000\n",
            "Train Epoch: 82 [1120/7471 (15%)]\tLoss: 1592675.125000\n",
            "Train Epoch: 82 [1280/7471 (17%)]\tLoss: 1536761.000000\n",
            "Train Epoch: 82 [1440/7471 (19%)]\tLoss: 1600872.875000\n",
            "Train Epoch: 82 [1600/7471 (21%)]\tLoss: 1628331.625000\n",
            "Train Epoch: 82 [1760/7471 (24%)]\tLoss: 1578002.000000\n",
            "Train Epoch: 82 [1920/7471 (26%)]\tLoss: 1606298.375000\n",
            "Train Epoch: 82 [2080/7471 (28%)]\tLoss: 1538032.500000\n",
            "Train Epoch: 82 [2240/7471 (30%)]\tLoss: 1600554.000000\n",
            "Train Epoch: 82 [2400/7471 (32%)]\tLoss: 1595684.625000\n",
            "Train Epoch: 82 [2560/7471 (34%)]\tLoss: 1623559.250000\n",
            "Train Epoch: 82 [2720/7471 (36%)]\tLoss: 1560412.375000\n",
            "Train Epoch: 82 [2880/7471 (39%)]\tLoss: 1601478.250000\n",
            "Train Epoch: 82 [3040/7471 (41%)]\tLoss: 1579391.375000\n",
            "Train Epoch: 82 [3200/7471 (43%)]\tLoss: 1499405.375000\n",
            "Train Epoch: 82 [3360/7471 (45%)]\tLoss: 1544220.625000\n",
            "Train Epoch: 82 [3520/7471 (47%)]\tLoss: 1644165.250000\n",
            "Train Epoch: 82 [3680/7471 (49%)]\tLoss: 1592986.250000\n",
            "Train Epoch: 82 [3840/7471 (51%)]\tLoss: 1563309.375000\n",
            "Train Epoch: 82 [4000/7471 (54%)]\tLoss: 1547019.500000\n",
            "Train Epoch: 82 [4160/7471 (56%)]\tLoss: 1553375.625000\n",
            "Train Epoch: 82 [4320/7471 (58%)]\tLoss: 1594421.750000\n",
            "Train Epoch: 82 [4480/7471 (60%)]\tLoss: 1613992.875000\n",
            "Train Epoch: 82 [4640/7471 (62%)]\tLoss: 1585421.375000\n",
            "Train Epoch: 82 [4800/7471 (64%)]\tLoss: 1606914.000000\n",
            "Train Epoch: 82 [4960/7471 (66%)]\tLoss: 1489671.625000\n",
            "Train Epoch: 82 [5120/7471 (69%)]\tLoss: 1582491.750000\n",
            "Train Epoch: 82 [5280/7471 (71%)]\tLoss: 1528520.375000\n",
            "Train Epoch: 82 [5440/7471 (73%)]\tLoss: 1566139.125000\n",
            "Train Epoch: 82 [5600/7471 (75%)]\tLoss: 1512980.500000\n",
            "Train Epoch: 82 [5760/7471 (77%)]\tLoss: 1555964.375000\n",
            "Train Epoch: 82 [5920/7471 (79%)]\tLoss: 1584941.000000\n",
            "Train Epoch: 82 [6080/7471 (81%)]\tLoss: 1594666.000000\n",
            "Train Epoch: 82 [6240/7471 (84%)]\tLoss: 1553238.250000\n",
            "Train Epoch: 82 [6400/7471 (86%)]\tLoss: 1562228.625000\n",
            "Train Epoch: 82 [6560/7471 (88%)]\tLoss: 1557898.500000\n",
            "Train Epoch: 82 [6720/7471 (90%)]\tLoss: 1506736.375000\n",
            "Train Epoch: 82 [6880/7471 (92%)]\tLoss: 1574904.375000\n",
            "Train Epoch: 82 [7040/7471 (94%)]\tLoss: 1581203.875000\n",
            "Train Epoch: 82 [7200/7471 (96%)]\tLoss: 1495748.750000\n",
            "Train Epoch: 82 [7360/7471 (99%)]\tLoss: 1566338.375000\n",
            "Epoch 82 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98549.2924\n",
            "\n",
            "Train Epoch: 83 [160/7471 (2%)]\tLoss: 1623741.125000\n",
            "Train Epoch: 83 [320/7471 (4%)]\tLoss: 1579273.750000\n",
            "Train Epoch: 83 [480/7471 (6%)]\tLoss: 1570093.750000\n",
            "Train Epoch: 83 [640/7471 (9%)]\tLoss: 1569408.250000\n",
            "Train Epoch: 83 [800/7471 (11%)]\tLoss: 1610023.625000\n",
            "Train Epoch: 83 [960/7471 (13%)]\tLoss: 1571353.875000\n",
            "Train Epoch: 83 [1120/7471 (15%)]\tLoss: 1587601.750000\n",
            "Train Epoch: 83 [1280/7471 (17%)]\tLoss: 1552954.625000\n",
            "Train Epoch: 83 [1440/7471 (19%)]\tLoss: 1614767.500000\n",
            "Train Epoch: 83 [1600/7471 (21%)]\tLoss: 1518389.500000\n",
            "Train Epoch: 83 [1760/7471 (24%)]\tLoss: 1515214.875000\n",
            "Train Epoch: 83 [1920/7471 (26%)]\tLoss: 1610385.750000\n",
            "Train Epoch: 83 [2080/7471 (28%)]\tLoss: 1582681.000000\n",
            "Train Epoch: 83 [2240/7471 (30%)]\tLoss: 1568577.500000\n",
            "Train Epoch: 83 [2400/7471 (32%)]\tLoss: 1587180.125000\n",
            "Train Epoch: 83 [2560/7471 (34%)]\tLoss: 1546247.750000\n",
            "Train Epoch: 83 [2720/7471 (36%)]\tLoss: 1545640.125000\n",
            "Train Epoch: 83 [2880/7471 (39%)]\tLoss: 1623004.750000\n",
            "Train Epoch: 83 [3040/7471 (41%)]\tLoss: 1627225.125000\n",
            "Train Epoch: 83 [3200/7471 (43%)]\tLoss: 1606322.625000\n",
            "Train Epoch: 83 [3360/7471 (45%)]\tLoss: 1567418.250000\n",
            "Train Epoch: 83 [3520/7471 (47%)]\tLoss: 1599969.750000\n",
            "Train Epoch: 83 [3680/7471 (49%)]\tLoss: 1555796.875000\n",
            "Train Epoch: 83 [3840/7471 (51%)]\tLoss: 1569963.250000\n",
            "Train Epoch: 83 [4000/7471 (54%)]\tLoss: 1623122.375000\n",
            "Train Epoch: 83 [4160/7471 (56%)]\tLoss: 1592151.750000\n",
            "Train Epoch: 83 [4320/7471 (58%)]\tLoss: 1556882.750000\n",
            "Train Epoch: 83 [4480/7471 (60%)]\tLoss: 1576774.000000\n",
            "Train Epoch: 83 [4640/7471 (62%)]\tLoss: 1520948.750000\n",
            "Train Epoch: 83 [4800/7471 (64%)]\tLoss: 1580715.625000\n",
            "Train Epoch: 83 [4960/7471 (66%)]\tLoss: 1575210.875000\n",
            "Train Epoch: 83 [5120/7471 (69%)]\tLoss: 1533005.875000\n",
            "Train Epoch: 83 [5280/7471 (71%)]\tLoss: 1607015.750000\n",
            "Train Epoch: 83 [5440/7471 (73%)]\tLoss: 1619940.625000\n",
            "Train Epoch: 83 [5600/7471 (75%)]\tLoss: 1609851.500000\n",
            "Train Epoch: 83 [5760/7471 (77%)]\tLoss: 1558092.250000\n",
            "Train Epoch: 83 [5920/7471 (79%)]\tLoss: 1539472.875000\n",
            "Train Epoch: 83 [6080/7471 (81%)]\tLoss: 1611940.250000\n",
            "Train Epoch: 83 [6240/7471 (84%)]\tLoss: 1577003.125000\n",
            "Train Epoch: 83 [6400/7471 (86%)]\tLoss: 1607269.125000\n",
            "Train Epoch: 83 [6560/7471 (88%)]\tLoss: 1528324.250000\n",
            "Train Epoch: 83 [6720/7471 (90%)]\tLoss: 1574347.125000\n",
            "Train Epoch: 83 [6880/7471 (92%)]\tLoss: 1555097.750000\n",
            "Train Epoch: 83 [7040/7471 (94%)]\tLoss: 1590334.375000\n",
            "Train Epoch: 83 [7200/7471 (96%)]\tLoss: 1550014.125000\n",
            "Train Epoch: 83 [7360/7471 (99%)]\tLoss: 1571051.875000\n",
            "Epoch 83 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98505.6253\n",
            "\n",
            "Train Epoch: 84 [160/7471 (2%)]\tLoss: 1547541.000000\n",
            "Train Epoch: 84 [320/7471 (4%)]\tLoss: 1611054.750000\n",
            "Train Epoch: 84 [480/7471 (6%)]\tLoss: 1591324.750000\n",
            "Train Epoch: 84 [640/7471 (9%)]\tLoss: 1584342.000000\n",
            "Train Epoch: 84 [800/7471 (11%)]\tLoss: 1577366.500000\n",
            "Train Epoch: 84 [960/7471 (13%)]\tLoss: 1611276.125000\n",
            "Train Epoch: 84 [1120/7471 (15%)]\tLoss: 1580636.875000\n",
            "Train Epoch: 84 [1280/7471 (17%)]\tLoss: 1583390.500000\n",
            "Train Epoch: 84 [1440/7471 (19%)]\tLoss: 1617385.375000\n",
            "Train Epoch: 84 [1600/7471 (21%)]\tLoss: 1569882.750000\n",
            "Train Epoch: 84 [1760/7471 (24%)]\tLoss: 1588981.250000\n",
            "Train Epoch: 84 [1920/7471 (26%)]\tLoss: 1568565.625000\n",
            "Train Epoch: 84 [2080/7471 (28%)]\tLoss: 1638637.000000\n",
            "Train Epoch: 84 [2240/7471 (30%)]\tLoss: 1574648.750000\n",
            "Train Epoch: 84 [2400/7471 (32%)]\tLoss: 1619118.500000\n",
            "Train Epoch: 84 [2560/7471 (34%)]\tLoss: 1572107.250000\n",
            "Train Epoch: 84 [2720/7471 (36%)]\tLoss: 1496760.875000\n",
            "Train Epoch: 84 [2880/7471 (39%)]\tLoss: 1611861.500000\n",
            "Train Epoch: 84 [3040/7471 (41%)]\tLoss: 1582562.750000\n",
            "Train Epoch: 84 [3200/7471 (43%)]\tLoss: 1546631.875000\n",
            "Train Epoch: 84 [3360/7471 (45%)]\tLoss: 1575118.500000\n",
            "Train Epoch: 84 [3520/7471 (47%)]\tLoss: 1607516.000000\n",
            "Train Epoch: 84 [3680/7471 (49%)]\tLoss: 1612561.500000\n",
            "Train Epoch: 84 [3840/7471 (51%)]\tLoss: 1574698.500000\n",
            "Train Epoch: 84 [4000/7471 (54%)]\tLoss: 1590083.000000\n",
            "Train Epoch: 84 [4160/7471 (56%)]\tLoss: 1501902.125000\n",
            "Train Epoch: 84 [4320/7471 (58%)]\tLoss: 1586143.750000\n",
            "Train Epoch: 84 [4480/7471 (60%)]\tLoss: 1608838.500000\n",
            "Train Epoch: 84 [4640/7471 (62%)]\tLoss: 1577081.000000\n",
            "Train Epoch: 84 [4800/7471 (64%)]\tLoss: 1622873.625000\n",
            "Train Epoch: 84 [4960/7471 (66%)]\tLoss: 1599261.250000\n",
            "Train Epoch: 84 [5120/7471 (69%)]\tLoss: 1622628.500000\n",
            "Train Epoch: 84 [5280/7471 (71%)]\tLoss: 1559650.750000\n",
            "Train Epoch: 84 [5440/7471 (73%)]\tLoss: 1626918.875000\n",
            "Train Epoch: 84 [5600/7471 (75%)]\tLoss: 1581701.250000\n",
            "Train Epoch: 84 [5760/7471 (77%)]\tLoss: 1557813.000000\n",
            "Train Epoch: 84 [5920/7471 (79%)]\tLoss: 1621357.125000\n",
            "Train Epoch: 84 [6080/7471 (81%)]\tLoss: 1592972.000000\n",
            "Train Epoch: 84 [6240/7471 (84%)]\tLoss: 1597546.250000\n",
            "Train Epoch: 84 [6400/7471 (86%)]\tLoss: 1516983.125000\n",
            "Train Epoch: 84 [6560/7471 (88%)]\tLoss: 1472791.125000\n",
            "Train Epoch: 84 [6720/7471 (90%)]\tLoss: 1540932.750000\n",
            "Train Epoch: 84 [6880/7471 (92%)]\tLoss: 1577836.125000\n",
            "Train Epoch: 84 [7040/7471 (94%)]\tLoss: 1537255.500000\n",
            "Train Epoch: 84 [7200/7471 (96%)]\tLoss: 1578442.500000\n",
            "Train Epoch: 84 [7360/7471 (99%)]\tLoss: 1574974.250000\n",
            "Epoch 84 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98450.1005\n",
            "\n",
            "Train Epoch: 85 [160/7471 (2%)]\tLoss: 1518179.750000\n",
            "Train Epoch: 85 [320/7471 (4%)]\tLoss: 1599835.375000\n",
            "Train Epoch: 85 [480/7471 (6%)]\tLoss: 1598115.250000\n",
            "Train Epoch: 85 [640/7471 (9%)]\tLoss: 1626772.750000\n",
            "Train Epoch: 85 [800/7471 (11%)]\tLoss: 1630179.250000\n",
            "Train Epoch: 85 [960/7471 (13%)]\tLoss: 1603124.750000\n",
            "Train Epoch: 85 [1120/7471 (15%)]\tLoss: 1559357.875000\n",
            "Train Epoch: 85 [1280/7471 (17%)]\tLoss: 1550649.375000\n",
            "Train Epoch: 85 [1440/7471 (19%)]\tLoss: 1585918.875000\n",
            "Train Epoch: 85 [1600/7471 (21%)]\tLoss: 1574477.125000\n",
            "Train Epoch: 85 [1760/7471 (24%)]\tLoss: 1604829.375000\n",
            "Train Epoch: 85 [1920/7471 (26%)]\tLoss: 1563927.625000\n",
            "Train Epoch: 85 [2080/7471 (28%)]\tLoss: 1619480.000000\n",
            "Train Epoch: 85 [2240/7471 (30%)]\tLoss: 1529499.375000\n",
            "Train Epoch: 85 [2400/7471 (32%)]\tLoss: 1581851.500000\n",
            "Train Epoch: 85 [2560/7471 (34%)]\tLoss: 1562012.250000\n",
            "Train Epoch: 85 [2720/7471 (36%)]\tLoss: 1621358.875000\n",
            "Train Epoch: 85 [2880/7471 (39%)]\tLoss: 1600879.125000\n",
            "Train Epoch: 85 [3040/7471 (41%)]\tLoss: 1583291.875000\n",
            "Train Epoch: 85 [3200/7471 (43%)]\tLoss: 1598146.500000\n",
            "Train Epoch: 85 [3360/7471 (45%)]\tLoss: 1522839.875000\n",
            "Train Epoch: 85 [3520/7471 (47%)]\tLoss: 1590861.250000\n",
            "Train Epoch: 85 [3680/7471 (49%)]\tLoss: 1569245.125000\n",
            "Train Epoch: 85 [3840/7471 (51%)]\tLoss: 1493879.625000\n",
            "Train Epoch: 85 [4000/7471 (54%)]\tLoss: 1634240.500000\n",
            "Train Epoch: 85 [4160/7471 (56%)]\tLoss: 1537521.125000\n",
            "Train Epoch: 85 [4320/7471 (58%)]\tLoss: 1535391.750000\n",
            "Train Epoch: 85 [4480/7471 (60%)]\tLoss: 1618039.500000\n",
            "Train Epoch: 85 [4640/7471 (62%)]\tLoss: 1574309.250000\n",
            "Train Epoch: 85 [4800/7471 (64%)]\tLoss: 1580296.875000\n",
            "Train Epoch: 85 [4960/7471 (66%)]\tLoss: 1546345.125000\n",
            "Train Epoch: 85 [5120/7471 (69%)]\tLoss: 1468635.000000\n",
            "Train Epoch: 85 [5280/7471 (71%)]\tLoss: 1539149.250000\n",
            "Train Epoch: 85 [5440/7471 (73%)]\tLoss: 1589696.625000\n",
            "Train Epoch: 85 [5600/7471 (75%)]\tLoss: 1538575.250000\n",
            "Train Epoch: 85 [5760/7471 (77%)]\tLoss: 1608118.250000\n",
            "Train Epoch: 85 [5920/7471 (79%)]\tLoss: 1624446.125000\n",
            "Train Epoch: 85 [6080/7471 (81%)]\tLoss: 1602742.250000\n",
            "Train Epoch: 85 [6240/7471 (84%)]\tLoss: 1578147.500000\n",
            "Train Epoch: 85 [6400/7471 (86%)]\tLoss: 1633280.875000\n",
            "Train Epoch: 85 [6560/7471 (88%)]\tLoss: 1570277.500000\n",
            "Train Epoch: 85 [6720/7471 (90%)]\tLoss: 1612424.125000\n",
            "Train Epoch: 85 [6880/7471 (92%)]\tLoss: 1613671.000000\n",
            "Train Epoch: 85 [7040/7471 (94%)]\tLoss: 1560547.250000\n",
            "Train Epoch: 85 [7200/7471 (96%)]\tLoss: 1555033.500000\n",
            "Train Epoch: 85 [7360/7471 (99%)]\tLoss: 1484191.500000\n",
            "Epoch 85 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98507.3227\n",
            "\n",
            "Train Epoch: 86 [160/7471 (2%)]\tLoss: 1597028.875000\n",
            "Train Epoch: 86 [320/7471 (4%)]\tLoss: 1615188.750000\n",
            "Train Epoch: 86 [480/7471 (6%)]\tLoss: 1615842.625000\n",
            "Train Epoch: 86 [640/7471 (9%)]\tLoss: 1553689.875000\n",
            "Train Epoch: 86 [800/7471 (11%)]\tLoss: 1593413.250000\n",
            "Train Epoch: 86 [960/7471 (13%)]\tLoss: 1523021.625000\n",
            "Train Epoch: 86 [1120/7471 (15%)]\tLoss: 1534948.375000\n",
            "Train Epoch: 86 [1280/7471 (17%)]\tLoss: 1539848.125000\n",
            "Train Epoch: 86 [1440/7471 (19%)]\tLoss: 1513952.250000\n",
            "Train Epoch: 86 [1600/7471 (21%)]\tLoss: 1587914.250000\n",
            "Train Epoch: 86 [1760/7471 (24%)]\tLoss: 1512523.625000\n",
            "Train Epoch: 86 [1920/7471 (26%)]\tLoss: 1570445.375000\n",
            "Train Epoch: 86 [2080/7471 (28%)]\tLoss: 1539501.500000\n",
            "Train Epoch: 86 [2240/7471 (30%)]\tLoss: 1618300.500000\n",
            "Train Epoch: 86 [2400/7471 (32%)]\tLoss: 1548213.500000\n",
            "Train Epoch: 86 [2560/7471 (34%)]\tLoss: 1599806.000000\n",
            "Train Epoch: 86 [2720/7471 (36%)]\tLoss: 1587456.750000\n",
            "Train Epoch: 86 [2880/7471 (39%)]\tLoss: 1580894.500000\n",
            "Train Epoch: 86 [3040/7471 (41%)]\tLoss: 1591703.625000\n",
            "Train Epoch: 86 [3200/7471 (43%)]\tLoss: 1530023.375000\n",
            "Train Epoch: 86 [3360/7471 (45%)]\tLoss: 1562955.250000\n",
            "Train Epoch: 86 [3520/7471 (47%)]\tLoss: 1616975.125000\n",
            "Train Epoch: 86 [3680/7471 (49%)]\tLoss: 1598393.875000\n",
            "Train Epoch: 86 [3840/7471 (51%)]\tLoss: 1623487.250000\n",
            "Train Epoch: 86 [4000/7471 (54%)]\tLoss: 1550512.375000\n",
            "Train Epoch: 86 [4160/7471 (56%)]\tLoss: 1467673.625000\n",
            "Train Epoch: 86 [4320/7471 (58%)]\tLoss: 1597624.125000\n",
            "Train Epoch: 86 [4480/7471 (60%)]\tLoss: 1503784.000000\n",
            "Train Epoch: 86 [4640/7471 (62%)]\tLoss: 1622902.625000\n",
            "Train Epoch: 86 [4800/7471 (64%)]\tLoss: 1593547.875000\n",
            "Train Epoch: 86 [4960/7471 (66%)]\tLoss: 1558259.250000\n",
            "Train Epoch: 86 [5120/7471 (69%)]\tLoss: 1591593.500000\n",
            "Train Epoch: 86 [5280/7471 (71%)]\tLoss: 1452949.500000\n",
            "Train Epoch: 86 [5440/7471 (73%)]\tLoss: 1590785.500000\n",
            "Train Epoch: 86 [5600/7471 (75%)]\tLoss: 1538380.375000\n",
            "Train Epoch: 86 [5760/7471 (77%)]\tLoss: 1624021.125000\n",
            "Train Epoch: 86 [5920/7471 (79%)]\tLoss: 1617932.875000\n",
            "Train Epoch: 86 [6080/7471 (81%)]\tLoss: 1634083.125000\n",
            "Train Epoch: 86 [6240/7471 (84%)]\tLoss: 1565198.375000\n",
            "Train Epoch: 86 [6400/7471 (86%)]\tLoss: 1616291.875000\n",
            "Train Epoch: 86 [6560/7471 (88%)]\tLoss: 1613145.875000\n",
            "Train Epoch: 86 [6720/7471 (90%)]\tLoss: 1587139.500000\n",
            "Train Epoch: 86 [6880/7471 (92%)]\tLoss: 1598133.375000\n",
            "Train Epoch: 86 [7040/7471 (94%)]\tLoss: 1575372.000000\n",
            "Train Epoch: 86 [7200/7471 (96%)]\tLoss: 1587852.875000\n",
            "Train Epoch: 86 [7360/7471 (99%)]\tLoss: 1594381.000000\n",
            "Epoch 86 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98466.0906\n",
            "\n",
            "Train Epoch: 87 [160/7471 (2%)]\tLoss: 1546360.625000\n",
            "Train Epoch: 87 [320/7471 (4%)]\tLoss: 1608239.125000\n",
            "Train Epoch: 87 [480/7471 (6%)]\tLoss: 1594885.625000\n",
            "Train Epoch: 87 [640/7471 (9%)]\tLoss: 1567896.125000\n",
            "Train Epoch: 87 [800/7471 (11%)]\tLoss: 1611130.625000\n",
            "Train Epoch: 87 [960/7471 (13%)]\tLoss: 1590829.375000\n",
            "Train Epoch: 87 [1120/7471 (15%)]\tLoss: 1524003.000000\n",
            "Train Epoch: 87 [1280/7471 (17%)]\tLoss: 1613007.625000\n",
            "Train Epoch: 87 [1440/7471 (19%)]\tLoss: 1557791.875000\n",
            "Train Epoch: 87 [1600/7471 (21%)]\tLoss: 1619387.250000\n",
            "Train Epoch: 87 [1760/7471 (24%)]\tLoss: 1560739.250000\n",
            "Train Epoch: 87 [1920/7471 (26%)]\tLoss: 1492247.375000\n",
            "Train Epoch: 87 [2080/7471 (28%)]\tLoss: 1581337.250000\n",
            "Train Epoch: 87 [2240/7471 (30%)]\tLoss: 1602032.750000\n",
            "Train Epoch: 87 [2400/7471 (32%)]\tLoss: 1606378.500000\n",
            "Train Epoch: 87 [2560/7471 (34%)]\tLoss: 1626186.875000\n",
            "Train Epoch: 87 [2720/7471 (36%)]\tLoss: 1574828.750000\n",
            "Train Epoch: 87 [2880/7471 (39%)]\tLoss: 1603496.000000\n",
            "Train Epoch: 87 [3040/7471 (41%)]\tLoss: 1614505.625000\n",
            "Train Epoch: 87 [3200/7471 (43%)]\tLoss: 1598230.250000\n",
            "Train Epoch: 87 [3360/7471 (45%)]\tLoss: 1572187.750000\n",
            "Train Epoch: 87 [3520/7471 (47%)]\tLoss: 1575185.500000\n",
            "Train Epoch: 87 [3680/7471 (49%)]\tLoss: 1556133.125000\n",
            "Train Epoch: 87 [3840/7471 (51%)]\tLoss: 1593483.625000\n",
            "Train Epoch: 87 [4000/7471 (54%)]\tLoss: 1601395.000000\n",
            "Train Epoch: 87 [4160/7471 (56%)]\tLoss: 1588065.125000\n",
            "Train Epoch: 87 [4320/7471 (58%)]\tLoss: 1603421.500000\n",
            "Train Epoch: 87 [4480/7471 (60%)]\tLoss: 1559590.000000\n",
            "Train Epoch: 87 [4640/7471 (62%)]\tLoss: 1628795.875000\n",
            "Train Epoch: 87 [4800/7471 (64%)]\tLoss: 1589360.000000\n",
            "Train Epoch: 87 [4960/7471 (66%)]\tLoss: 1584171.375000\n",
            "Train Epoch: 87 [5120/7471 (69%)]\tLoss: 1524179.750000\n",
            "Train Epoch: 87 [5280/7471 (71%)]\tLoss: 1608439.625000\n",
            "Train Epoch: 87 [5440/7471 (73%)]\tLoss: 1580338.500000\n",
            "Train Epoch: 87 [5600/7471 (75%)]\tLoss: 1575006.875000\n",
            "Train Epoch: 87 [5760/7471 (77%)]\tLoss: 1618752.375000\n",
            "Train Epoch: 87 [5920/7471 (79%)]\tLoss: 1614086.750000\n",
            "Train Epoch: 87 [6080/7471 (81%)]\tLoss: 1618862.625000\n",
            "Train Epoch: 87 [6240/7471 (84%)]\tLoss: 1532108.000000\n",
            "Train Epoch: 87 [6400/7471 (86%)]\tLoss: 1510169.750000\n",
            "Train Epoch: 87 [6560/7471 (88%)]\tLoss: 1554415.125000\n",
            "Train Epoch: 87 [6720/7471 (90%)]\tLoss: 1540541.875000\n",
            "Train Epoch: 87 [6880/7471 (92%)]\tLoss: 1556799.125000\n",
            "Train Epoch: 87 [7040/7471 (94%)]\tLoss: 1606871.250000\n",
            "Train Epoch: 87 [7200/7471 (96%)]\tLoss: 1552578.625000\n",
            "Train Epoch: 87 [7360/7471 (99%)]\tLoss: 1609180.125000\n",
            "Epoch 87 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98476.5147\n",
            "\n",
            "Train Epoch: 88 [160/7471 (2%)]\tLoss: 1599397.750000\n",
            "Train Epoch: 88 [320/7471 (4%)]\tLoss: 1592910.500000\n",
            "Train Epoch: 88 [480/7471 (6%)]\tLoss: 1479291.500000\n",
            "Train Epoch: 88 [640/7471 (9%)]\tLoss: 1628376.625000\n",
            "Train Epoch: 88 [800/7471 (11%)]\tLoss: 1597183.750000\n",
            "Train Epoch: 88 [960/7471 (13%)]\tLoss: 1559517.625000\n",
            "Train Epoch: 88 [1120/7471 (15%)]\tLoss: 1632966.875000\n",
            "Train Epoch: 88 [1280/7471 (17%)]\tLoss: 1607754.250000\n",
            "Train Epoch: 88 [1440/7471 (19%)]\tLoss: 1562722.250000\n",
            "Train Epoch: 88 [1600/7471 (21%)]\tLoss: 1587572.625000\n",
            "Train Epoch: 88 [1760/7471 (24%)]\tLoss: 1622936.000000\n",
            "Train Epoch: 88 [1920/7471 (26%)]\tLoss: 1560106.250000\n",
            "Train Epoch: 88 [2080/7471 (28%)]\tLoss: 1568418.375000\n",
            "Train Epoch: 88 [2240/7471 (30%)]\tLoss: 1498868.125000\n",
            "Train Epoch: 88 [2400/7471 (32%)]\tLoss: 1558863.125000\n",
            "Train Epoch: 88 [2560/7471 (34%)]\tLoss: 1635278.625000\n",
            "Train Epoch: 88 [2720/7471 (36%)]\tLoss: 1539454.000000\n",
            "Train Epoch: 88 [2880/7471 (39%)]\tLoss: 1584402.375000\n",
            "Train Epoch: 88 [3040/7471 (41%)]\tLoss: 1496897.500000\n",
            "Train Epoch: 88 [3200/7471 (43%)]\tLoss: 1570439.875000\n",
            "Train Epoch: 88 [3360/7471 (45%)]\tLoss: 1589065.625000\n",
            "Train Epoch: 88 [3520/7471 (47%)]\tLoss: 1598213.625000\n",
            "Train Epoch: 88 [3680/7471 (49%)]\tLoss: 1517627.625000\n",
            "Train Epoch: 88 [3840/7471 (51%)]\tLoss: 1611328.875000\n",
            "Train Epoch: 88 [4000/7471 (54%)]\tLoss: 1519245.500000\n",
            "Train Epoch: 88 [4160/7471 (56%)]\tLoss: 1597887.750000\n",
            "Train Epoch: 88 [4320/7471 (58%)]\tLoss: 1628403.500000\n",
            "Train Epoch: 88 [4480/7471 (60%)]\tLoss: 1540903.375000\n",
            "Train Epoch: 88 [4640/7471 (62%)]\tLoss: 1482134.500000\n",
            "Train Epoch: 88 [4800/7471 (64%)]\tLoss: 1548918.000000\n",
            "Train Epoch: 88 [4960/7471 (66%)]\tLoss: 1417743.000000\n",
            "Train Epoch: 88 [5120/7471 (69%)]\tLoss: 1572141.500000\n",
            "Train Epoch: 88 [5280/7471 (71%)]\tLoss: 1573680.000000\n",
            "Train Epoch: 88 [5440/7471 (73%)]\tLoss: 1541394.250000\n",
            "Train Epoch: 88 [5600/7471 (75%)]\tLoss: 1615438.875000\n",
            "Train Epoch: 88 [5760/7471 (77%)]\tLoss: 1581971.375000\n",
            "Train Epoch: 88 [5920/7471 (79%)]\tLoss: 1578495.250000\n",
            "Train Epoch: 88 [6080/7471 (81%)]\tLoss: 1549288.625000\n",
            "Train Epoch: 88 [6240/7471 (84%)]\tLoss: 1584275.000000\n",
            "Train Epoch: 88 [6400/7471 (86%)]\tLoss: 1529769.750000\n",
            "Train Epoch: 88 [6560/7471 (88%)]\tLoss: 1598800.750000\n",
            "Train Epoch: 88 [6720/7471 (90%)]\tLoss: 1554227.500000\n",
            "Train Epoch: 88 [6880/7471 (92%)]\tLoss: 1605949.875000\n",
            "Train Epoch: 88 [7040/7471 (94%)]\tLoss: 1532701.250000\n",
            "Train Epoch: 88 [7200/7471 (96%)]\tLoss: 1512796.000000\n",
            "Train Epoch: 88 [7360/7471 (99%)]\tLoss: 1612351.125000\n",
            "Epoch 88 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98433.7928\n",
            "\n",
            "Train Epoch: 89 [160/7471 (2%)]\tLoss: 1543269.125000\n",
            "Train Epoch: 89 [320/7471 (4%)]\tLoss: 1609692.750000\n",
            "Train Epoch: 89 [480/7471 (6%)]\tLoss: 1604511.000000\n",
            "Train Epoch: 89 [640/7471 (9%)]\tLoss: 1554401.250000\n",
            "Train Epoch: 89 [800/7471 (11%)]\tLoss: 1470431.000000\n",
            "Train Epoch: 89 [960/7471 (13%)]\tLoss: 1609683.625000\n",
            "Train Epoch: 89 [1120/7471 (15%)]\tLoss: 1633988.125000\n",
            "Train Epoch: 89 [1280/7471 (17%)]\tLoss: 1527409.500000\n",
            "Train Epoch: 89 [1440/7471 (19%)]\tLoss: 1571382.125000\n",
            "Train Epoch: 89 [1600/7471 (21%)]\tLoss: 1619583.250000\n",
            "Train Epoch: 89 [1760/7471 (24%)]\tLoss: 1584955.875000\n",
            "Train Epoch: 89 [1920/7471 (26%)]\tLoss: 1618465.875000\n",
            "Train Epoch: 89 [2080/7471 (28%)]\tLoss: 1582989.125000\n",
            "Train Epoch: 89 [2240/7471 (30%)]\tLoss: 1562199.250000\n",
            "Train Epoch: 89 [2400/7471 (32%)]\tLoss: 1627650.250000\n",
            "Train Epoch: 89 [2560/7471 (34%)]\tLoss: 1582782.625000\n",
            "Train Epoch: 89 [2720/7471 (36%)]\tLoss: 1612421.625000\n",
            "Train Epoch: 89 [2880/7471 (39%)]\tLoss: 1626647.250000\n",
            "Train Epoch: 89 [3040/7471 (41%)]\tLoss: 1591710.750000\n",
            "Train Epoch: 89 [3200/7471 (43%)]\tLoss: 1545631.625000\n",
            "Train Epoch: 89 [3360/7471 (45%)]\tLoss: 1605580.000000\n",
            "Train Epoch: 89 [3520/7471 (47%)]\tLoss: 1600222.000000\n",
            "Train Epoch: 89 [3680/7471 (49%)]\tLoss: 1587566.375000\n",
            "Train Epoch: 89 [3840/7471 (51%)]\tLoss: 1544179.625000\n",
            "Train Epoch: 89 [4000/7471 (54%)]\tLoss: 1492735.250000\n",
            "Train Epoch: 89 [4160/7471 (56%)]\tLoss: 1593933.875000\n",
            "Train Epoch: 89 [4320/7471 (58%)]\tLoss: 1570397.125000\n",
            "Train Epoch: 89 [4480/7471 (60%)]\tLoss: 1617943.875000\n",
            "Train Epoch: 89 [4640/7471 (62%)]\tLoss: 1503263.750000\n",
            "Train Epoch: 89 [4800/7471 (64%)]\tLoss: 1573418.500000\n",
            "Train Epoch: 89 [4960/7471 (66%)]\tLoss: 1592846.000000\n",
            "Train Epoch: 89 [5120/7471 (69%)]\tLoss: 1563313.500000\n",
            "Train Epoch: 89 [5280/7471 (71%)]\tLoss: 1534630.125000\n",
            "Train Epoch: 89 [5440/7471 (73%)]\tLoss: 1577041.000000\n",
            "Train Epoch: 89 [5600/7471 (75%)]\tLoss: 1578226.000000\n",
            "Train Epoch: 89 [5760/7471 (77%)]\tLoss: 1560525.125000\n",
            "Train Epoch: 89 [5920/7471 (79%)]\tLoss: 1600989.000000\n",
            "Train Epoch: 89 [6080/7471 (81%)]\tLoss: 1516444.875000\n",
            "Train Epoch: 89 [6240/7471 (84%)]\tLoss: 1594647.375000\n",
            "Train Epoch: 89 [6400/7471 (86%)]\tLoss: 1547038.625000\n",
            "Train Epoch: 89 [6560/7471 (88%)]\tLoss: 1599565.000000\n",
            "Train Epoch: 89 [6720/7471 (90%)]\tLoss: 1596027.625000\n",
            "Train Epoch: 89 [6880/7471 (92%)]\tLoss: 1545694.125000\n",
            "Train Epoch: 89 [7040/7471 (94%)]\tLoss: 1578138.625000\n",
            "Train Epoch: 89 [7200/7471 (96%)]\tLoss: 1583309.375000\n",
            "Train Epoch: 89 [7360/7471 (99%)]\tLoss: 1589730.250000\n",
            "Epoch 89 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98442.6701\n",
            "\n",
            "Train Epoch: 90 [160/7471 (2%)]\tLoss: 1582542.125000\n",
            "Train Epoch: 90 [320/7471 (4%)]\tLoss: 1545481.750000\n",
            "Train Epoch: 90 [480/7471 (6%)]\tLoss: 1577856.375000\n",
            "Train Epoch: 90 [640/7471 (9%)]\tLoss: 1564905.250000\n",
            "Train Epoch: 90 [800/7471 (11%)]\tLoss: 1606089.750000\n",
            "Train Epoch: 90 [960/7471 (13%)]\tLoss: 1622695.750000\n",
            "Train Epoch: 90 [1120/7471 (15%)]\tLoss: 1582087.875000\n",
            "Train Epoch: 90 [1280/7471 (17%)]\tLoss: 1524199.125000\n",
            "Train Epoch: 90 [1440/7471 (19%)]\tLoss: 1545055.500000\n",
            "Train Epoch: 90 [1600/7471 (21%)]\tLoss: 1562655.875000\n",
            "Train Epoch: 90 [1760/7471 (24%)]\tLoss: 1610046.625000\n",
            "Train Epoch: 90 [1920/7471 (26%)]\tLoss: 1508068.750000\n",
            "Train Epoch: 90 [2080/7471 (28%)]\tLoss: 1556983.250000\n",
            "Train Epoch: 90 [2240/7471 (30%)]\tLoss: 1632632.125000\n",
            "Train Epoch: 90 [2400/7471 (32%)]\tLoss: 1568108.750000\n",
            "Train Epoch: 90 [2560/7471 (34%)]\tLoss: 1577853.250000\n",
            "Train Epoch: 90 [2720/7471 (36%)]\tLoss: 1561643.125000\n",
            "Train Epoch: 90 [2880/7471 (39%)]\tLoss: 1535627.875000\n",
            "Train Epoch: 90 [3040/7471 (41%)]\tLoss: 1593610.875000\n",
            "Train Epoch: 90 [3200/7471 (43%)]\tLoss: 1579314.375000\n",
            "Train Epoch: 90 [3360/7471 (45%)]\tLoss: 1571885.375000\n",
            "Train Epoch: 90 [3520/7471 (47%)]\tLoss: 1585744.250000\n",
            "Train Epoch: 90 [3680/7471 (49%)]\tLoss: 1572111.250000\n",
            "Train Epoch: 90 [3840/7471 (51%)]\tLoss: 1565837.375000\n",
            "Train Epoch: 90 [4000/7471 (54%)]\tLoss: 1617641.875000\n",
            "Train Epoch: 90 [4160/7471 (56%)]\tLoss: 1474609.000000\n",
            "Train Epoch: 90 [4320/7471 (58%)]\tLoss: 1559529.625000\n",
            "Train Epoch: 90 [4480/7471 (60%)]\tLoss: 1588875.000000\n",
            "Train Epoch: 90 [4640/7471 (62%)]\tLoss: 1552453.875000\n",
            "Train Epoch: 90 [4800/7471 (64%)]\tLoss: 1582668.125000\n",
            "Train Epoch: 90 [4960/7471 (66%)]\tLoss: 1563310.250000\n",
            "Train Epoch: 90 [5120/7471 (69%)]\tLoss: 1593605.500000\n",
            "Train Epoch: 90 [5280/7471 (71%)]\tLoss: 1570361.125000\n",
            "Train Epoch: 90 [5440/7471 (73%)]\tLoss: 1571690.500000\n",
            "Train Epoch: 90 [5600/7471 (75%)]\tLoss: 1615115.250000\n",
            "Train Epoch: 90 [5760/7471 (77%)]\tLoss: 1630172.625000\n",
            "Train Epoch: 90 [5920/7471 (79%)]\tLoss: 1613107.125000\n",
            "Train Epoch: 90 [6080/7471 (81%)]\tLoss: 1569674.000000\n",
            "Train Epoch: 90 [6240/7471 (84%)]\tLoss: 1599150.750000\n",
            "Train Epoch: 90 [6400/7471 (86%)]\tLoss: 1549097.375000\n",
            "Train Epoch: 90 [6560/7471 (88%)]\tLoss: 1505037.875000\n",
            "Train Epoch: 90 [6720/7471 (90%)]\tLoss: 1559921.250000\n",
            "Train Epoch: 90 [6880/7471 (92%)]\tLoss: 1583059.625000\n",
            "Train Epoch: 90 [7040/7471 (94%)]\tLoss: 1563462.375000\n",
            "Train Epoch: 90 [7200/7471 (96%)]\tLoss: 1625937.750000\n",
            "Train Epoch: 90 [7360/7471 (99%)]\tLoss: 1572733.750000\n",
            "Epoch 90 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98463.4076\n",
            "\n",
            "Train Epoch: 91 [160/7471 (2%)]\tLoss: 1607634.125000\n",
            "Train Epoch: 91 [320/7471 (4%)]\tLoss: 1565086.375000\n",
            "Train Epoch: 91 [480/7471 (6%)]\tLoss: 1603336.375000\n",
            "Train Epoch: 91 [640/7471 (9%)]\tLoss: 1484555.125000\n",
            "Train Epoch: 91 [800/7471 (11%)]\tLoss: 1617050.750000\n",
            "Train Epoch: 91 [960/7471 (13%)]\tLoss: 1581393.625000\n",
            "Train Epoch: 91 [1120/7471 (15%)]\tLoss: 1550789.625000\n",
            "Train Epoch: 91 [1280/7471 (17%)]\tLoss: 1556392.625000\n",
            "Train Epoch: 91 [1440/7471 (19%)]\tLoss: 1615352.125000\n",
            "Train Epoch: 91 [1600/7471 (21%)]\tLoss: 1619191.625000\n",
            "Train Epoch: 91 [1760/7471 (24%)]\tLoss: 1551182.000000\n",
            "Train Epoch: 91 [1920/7471 (26%)]\tLoss: 1566169.000000\n",
            "Train Epoch: 91 [2080/7471 (28%)]\tLoss: 1561045.000000\n",
            "Train Epoch: 91 [2240/7471 (30%)]\tLoss: 1545766.000000\n",
            "Train Epoch: 91 [2400/7471 (32%)]\tLoss: 1573765.750000\n",
            "Train Epoch: 91 [2560/7471 (34%)]\tLoss: 1621935.625000\n",
            "Train Epoch: 91 [2720/7471 (36%)]\tLoss: 1491550.250000\n",
            "Train Epoch: 91 [2880/7471 (39%)]\tLoss: 1578940.000000\n",
            "Train Epoch: 91 [3040/7471 (41%)]\tLoss: 1588658.125000\n",
            "Train Epoch: 91 [3200/7471 (43%)]\tLoss: 1563838.750000\n",
            "Train Epoch: 91 [3360/7471 (45%)]\tLoss: 1545138.375000\n",
            "Train Epoch: 91 [3520/7471 (47%)]\tLoss: 1519119.875000\n",
            "Train Epoch: 91 [3680/7471 (49%)]\tLoss: 1517001.250000\n",
            "Train Epoch: 91 [3840/7471 (51%)]\tLoss: 1638128.375000\n",
            "Train Epoch: 91 [4000/7471 (54%)]\tLoss: 1544661.500000\n",
            "Train Epoch: 91 [4160/7471 (56%)]\tLoss: 1627224.750000\n",
            "Train Epoch: 91 [4320/7471 (58%)]\tLoss: 1580023.625000\n",
            "Train Epoch: 91 [4480/7471 (60%)]\tLoss: 1629746.000000\n",
            "Train Epoch: 91 [4640/7471 (62%)]\tLoss: 1554256.250000\n",
            "Train Epoch: 91 [4800/7471 (64%)]\tLoss: 1607939.625000\n",
            "Train Epoch: 91 [4960/7471 (66%)]\tLoss: 1575751.875000\n",
            "Train Epoch: 91 [5120/7471 (69%)]\tLoss: 1546332.250000\n",
            "Train Epoch: 91 [5280/7471 (71%)]\tLoss: 1617320.375000\n",
            "Train Epoch: 91 [5440/7471 (73%)]\tLoss: 1599555.500000\n",
            "Train Epoch: 91 [5600/7471 (75%)]\tLoss: 1526741.875000\n",
            "Train Epoch: 91 [5760/7471 (77%)]\tLoss: 1544829.375000\n",
            "Train Epoch: 91 [5920/7471 (79%)]\tLoss: 1619966.750000\n",
            "Train Epoch: 91 [6080/7471 (81%)]\tLoss: 1507392.250000\n",
            "Train Epoch: 91 [6240/7471 (84%)]\tLoss: 1580333.750000\n",
            "Train Epoch: 91 [6400/7471 (86%)]\tLoss: 1526373.250000\n",
            "Train Epoch: 91 [6560/7471 (88%)]\tLoss: 1587410.375000\n",
            "Train Epoch: 91 [6720/7471 (90%)]\tLoss: 1627550.000000\n",
            "Train Epoch: 91 [6880/7471 (92%)]\tLoss: 1619702.250000\n",
            "Train Epoch: 91 [7040/7471 (94%)]\tLoss: 1552357.250000\n",
            "Train Epoch: 91 [7200/7471 (96%)]\tLoss: 1584636.375000\n",
            "Train Epoch: 91 [7360/7471 (99%)]\tLoss: 1608614.750000\n",
            "Epoch 91 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98473.8756\n",
            "\n",
            "Train Epoch: 92 [160/7471 (2%)]\tLoss: 1564537.125000\n",
            "Train Epoch: 92 [320/7471 (4%)]\tLoss: 1614207.125000\n",
            "Train Epoch: 92 [480/7471 (6%)]\tLoss: 1578372.625000\n",
            "Train Epoch: 92 [640/7471 (9%)]\tLoss: 1611137.625000\n",
            "Train Epoch: 92 [800/7471 (11%)]\tLoss: 1558831.125000\n",
            "Train Epoch: 92 [960/7471 (13%)]\tLoss: 1564077.625000\n",
            "Train Epoch: 92 [1120/7471 (15%)]\tLoss: 1516363.750000\n",
            "Train Epoch: 92 [1280/7471 (17%)]\tLoss: 1619175.250000\n",
            "Train Epoch: 92 [1440/7471 (19%)]\tLoss: 1595575.375000\n",
            "Train Epoch: 92 [1600/7471 (21%)]\tLoss: 1548647.750000\n",
            "Train Epoch: 92 [1760/7471 (24%)]\tLoss: 1603641.375000\n",
            "Train Epoch: 92 [1920/7471 (26%)]\tLoss: 1610762.625000\n",
            "Train Epoch: 92 [2080/7471 (28%)]\tLoss: 1510929.125000\n",
            "Train Epoch: 92 [2240/7471 (30%)]\tLoss: 1582603.000000\n",
            "Train Epoch: 92 [2400/7471 (32%)]\tLoss: 1593866.500000\n",
            "Train Epoch: 92 [2560/7471 (34%)]\tLoss: 1532696.875000\n",
            "Train Epoch: 92 [2720/7471 (36%)]\tLoss: 1607348.750000\n",
            "Train Epoch: 92 [2880/7471 (39%)]\tLoss: 1564938.250000\n",
            "Train Epoch: 92 [3040/7471 (41%)]\tLoss: 1590881.625000\n",
            "Train Epoch: 92 [3200/7471 (43%)]\tLoss: 1609967.125000\n",
            "Train Epoch: 92 [3360/7471 (45%)]\tLoss: 1620000.625000\n",
            "Train Epoch: 92 [3520/7471 (47%)]\tLoss: 1583417.375000\n",
            "Train Epoch: 92 [3680/7471 (49%)]\tLoss: 1547950.375000\n",
            "Train Epoch: 92 [3840/7471 (51%)]\tLoss: 1580545.875000\n",
            "Train Epoch: 92 [4000/7471 (54%)]\tLoss: 1494219.500000\n",
            "Train Epoch: 92 [4160/7471 (56%)]\tLoss: 1589668.625000\n",
            "Train Epoch: 92 [4320/7471 (58%)]\tLoss: 1592585.000000\n",
            "Train Epoch: 92 [4480/7471 (60%)]\tLoss: 1598769.750000\n",
            "Train Epoch: 92 [4640/7471 (62%)]\tLoss: 1619406.250000\n",
            "Train Epoch: 92 [4800/7471 (64%)]\tLoss: 1575601.750000\n",
            "Train Epoch: 92 [4960/7471 (66%)]\tLoss: 1548147.375000\n",
            "Train Epoch: 92 [5120/7471 (69%)]\tLoss: 1628643.125000\n",
            "Train Epoch: 92 [5280/7471 (71%)]\tLoss: 1585569.250000\n",
            "Train Epoch: 92 [5440/7471 (73%)]\tLoss: 1600776.500000\n",
            "Train Epoch: 92 [5600/7471 (75%)]\tLoss: 1616187.750000\n",
            "Train Epoch: 92 [5760/7471 (77%)]\tLoss: 1594676.250000\n",
            "Train Epoch: 92 [5920/7471 (79%)]\tLoss: 1603516.875000\n",
            "Train Epoch: 92 [6080/7471 (81%)]\tLoss: 1595249.375000\n",
            "Train Epoch: 92 [6240/7471 (84%)]\tLoss: 1565824.125000\n",
            "Train Epoch: 92 [6400/7471 (86%)]\tLoss: 1544123.000000\n",
            "Train Epoch: 92 [6560/7471 (88%)]\tLoss: 1614792.625000\n",
            "Train Epoch: 92 [6720/7471 (90%)]\tLoss: 1574481.375000\n",
            "Train Epoch: 92 [6880/7471 (92%)]\tLoss: 1594961.125000\n",
            "Train Epoch: 92 [7040/7471 (94%)]\tLoss: 1572018.375000\n",
            "Train Epoch: 92 [7200/7471 (96%)]\tLoss: 1600984.875000\n",
            "Train Epoch: 92 [7360/7471 (99%)]\tLoss: 1530855.500000\n",
            "Epoch 92 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98465.5270\n",
            "\n",
            "Train Epoch: 93 [160/7471 (2%)]\tLoss: 1635728.250000\n",
            "Train Epoch: 93 [320/7471 (4%)]\tLoss: 1517170.750000\n",
            "Train Epoch: 93 [480/7471 (6%)]\tLoss: 1564072.375000\n",
            "Train Epoch: 93 [640/7471 (9%)]\tLoss: 1578786.750000\n",
            "Train Epoch: 93 [800/7471 (11%)]\tLoss: 1560762.500000\n",
            "Train Epoch: 93 [960/7471 (13%)]\tLoss: 1487543.750000\n",
            "Train Epoch: 93 [1120/7471 (15%)]\tLoss: 1540907.000000\n",
            "Train Epoch: 93 [1280/7471 (17%)]\tLoss: 1599896.250000\n",
            "Train Epoch: 93 [1440/7471 (19%)]\tLoss: 1602244.000000\n",
            "Train Epoch: 93 [1600/7471 (21%)]\tLoss: 1620847.375000\n",
            "Train Epoch: 93 [1760/7471 (24%)]\tLoss: 1537863.375000\n",
            "Train Epoch: 93 [1920/7471 (26%)]\tLoss: 1591346.375000\n",
            "Train Epoch: 93 [2080/7471 (28%)]\tLoss: 1556067.875000\n",
            "Train Epoch: 93 [2240/7471 (30%)]\tLoss: 1597585.375000\n",
            "Train Epoch: 93 [2400/7471 (32%)]\tLoss: 1529660.375000\n",
            "Train Epoch: 93 [2560/7471 (34%)]\tLoss: 1589395.875000\n",
            "Train Epoch: 93 [2720/7471 (36%)]\tLoss: 1631595.750000\n",
            "Train Epoch: 93 [2880/7471 (39%)]\tLoss: 1601191.125000\n",
            "Train Epoch: 93 [3040/7471 (41%)]\tLoss: 1554682.750000\n",
            "Train Epoch: 93 [3200/7471 (43%)]\tLoss: 1575053.375000\n",
            "Train Epoch: 93 [3360/7471 (45%)]\tLoss: 1531543.375000\n",
            "Train Epoch: 93 [3520/7471 (47%)]\tLoss: 1534141.750000\n",
            "Train Epoch: 93 [3680/7471 (49%)]\tLoss: 1641479.125000\n",
            "Train Epoch: 93 [3840/7471 (51%)]\tLoss: 1600119.375000\n",
            "Train Epoch: 93 [4000/7471 (54%)]\tLoss: 1613320.125000\n",
            "Train Epoch: 93 [4160/7471 (56%)]\tLoss: 1543837.875000\n",
            "Train Epoch: 93 [4320/7471 (58%)]\tLoss: 1604082.250000\n",
            "Train Epoch: 93 [4480/7471 (60%)]\tLoss: 1572296.625000\n",
            "Train Epoch: 93 [4640/7471 (62%)]\tLoss: 1594246.250000\n",
            "Train Epoch: 93 [4800/7471 (64%)]\tLoss: 1526482.875000\n",
            "Train Epoch: 93 [4960/7471 (66%)]\tLoss: 1562529.000000\n",
            "Train Epoch: 93 [5120/7471 (69%)]\tLoss: 1567051.250000\n",
            "Train Epoch: 93 [5280/7471 (71%)]\tLoss: 1542222.875000\n",
            "Train Epoch: 93 [5440/7471 (73%)]\tLoss: 1614915.875000\n",
            "Train Epoch: 93 [5600/7471 (75%)]\tLoss: 1573429.000000\n",
            "Train Epoch: 93 [5760/7471 (77%)]\tLoss: 1538793.000000\n",
            "Train Epoch: 93 [5920/7471 (79%)]\tLoss: 1579806.875000\n",
            "Train Epoch: 93 [6080/7471 (81%)]\tLoss: 1621271.375000\n",
            "Train Epoch: 93 [6240/7471 (84%)]\tLoss: 1561193.750000\n",
            "Train Epoch: 93 [6400/7471 (86%)]\tLoss: 1601599.875000\n",
            "Train Epoch: 93 [6560/7471 (88%)]\tLoss: 1627093.625000\n",
            "Train Epoch: 93 [6720/7471 (90%)]\tLoss: 1589359.625000\n",
            "Train Epoch: 93 [6880/7471 (92%)]\tLoss: 1622143.750000\n",
            "Train Epoch: 93 [7040/7471 (94%)]\tLoss: 1575909.750000\n",
            "Train Epoch: 93 [7200/7471 (96%)]\tLoss: 1551324.500000\n",
            "Train Epoch: 93 [7360/7471 (99%)]\tLoss: 1559093.625000\n",
            "Epoch 93 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98444.7675\n",
            "\n",
            "Train Epoch: 94 [160/7471 (2%)]\tLoss: 1528345.750000\n",
            "Train Epoch: 94 [320/7471 (4%)]\tLoss: 1512573.750000\n",
            "Train Epoch: 94 [480/7471 (6%)]\tLoss: 1613015.625000\n",
            "Train Epoch: 94 [640/7471 (9%)]\tLoss: 1559995.250000\n",
            "Train Epoch: 94 [800/7471 (11%)]\tLoss: 1572908.750000\n",
            "Train Epoch: 94 [960/7471 (13%)]\tLoss: 1615278.250000\n",
            "Train Epoch: 94 [1120/7471 (15%)]\tLoss: 1511891.625000\n",
            "Train Epoch: 94 [1280/7471 (17%)]\tLoss: 1569411.875000\n",
            "Train Epoch: 94 [1440/7471 (19%)]\tLoss: 1576463.000000\n",
            "Train Epoch: 94 [1600/7471 (21%)]\tLoss: 1567419.625000\n",
            "Train Epoch: 94 [1760/7471 (24%)]\tLoss: 1569806.375000\n",
            "Train Epoch: 94 [1920/7471 (26%)]\tLoss: 1569929.250000\n",
            "Train Epoch: 94 [2080/7471 (28%)]\tLoss: 1597311.125000\n",
            "Train Epoch: 94 [2240/7471 (30%)]\tLoss: 1576433.125000\n",
            "Train Epoch: 94 [2400/7471 (32%)]\tLoss: 1563867.500000\n",
            "Train Epoch: 94 [2560/7471 (34%)]\tLoss: 1439818.125000\n",
            "Train Epoch: 94 [2720/7471 (36%)]\tLoss: 1562870.625000\n",
            "Train Epoch: 94 [2880/7471 (39%)]\tLoss: 1525697.625000\n",
            "Train Epoch: 94 [3040/7471 (41%)]\tLoss: 1532642.375000\n",
            "Train Epoch: 94 [3200/7471 (43%)]\tLoss: 1560955.000000\n",
            "Train Epoch: 94 [3360/7471 (45%)]\tLoss: 1510391.500000\n",
            "Train Epoch: 94 [3520/7471 (47%)]\tLoss: 1491343.000000\n",
            "Train Epoch: 94 [3680/7471 (49%)]\tLoss: 1530540.250000\n",
            "Train Epoch: 94 [3840/7471 (51%)]\tLoss: 1560963.125000\n",
            "Train Epoch: 94 [4000/7471 (54%)]\tLoss: 1619856.500000\n",
            "Train Epoch: 94 [4160/7471 (56%)]\tLoss: 1571285.375000\n",
            "Train Epoch: 94 [4320/7471 (58%)]\tLoss: 1570078.250000\n",
            "Train Epoch: 94 [4480/7471 (60%)]\tLoss: 1482132.625000\n",
            "Train Epoch: 94 [4640/7471 (62%)]\tLoss: 1572701.750000\n",
            "Train Epoch: 94 [4800/7471 (64%)]\tLoss: 1634081.000000\n",
            "Train Epoch: 94 [4960/7471 (66%)]\tLoss: 1512120.125000\n",
            "Train Epoch: 94 [5120/7471 (69%)]\tLoss: 1626701.000000\n",
            "Train Epoch: 94 [5280/7471 (71%)]\tLoss: 1590867.375000\n",
            "Train Epoch: 94 [5440/7471 (73%)]\tLoss: 1598024.375000\n",
            "Train Epoch: 94 [5600/7471 (75%)]\tLoss: 1549037.500000\n",
            "Train Epoch: 94 [5760/7471 (77%)]\tLoss: 1590105.500000\n",
            "Train Epoch: 94 [5920/7471 (79%)]\tLoss: 1545227.125000\n",
            "Train Epoch: 94 [6080/7471 (81%)]\tLoss: 1583747.375000\n",
            "Train Epoch: 94 [6240/7471 (84%)]\tLoss: 1622312.750000\n",
            "Train Epoch: 94 [6400/7471 (86%)]\tLoss: 1562346.750000\n",
            "Train Epoch: 94 [6560/7471 (88%)]\tLoss: 1629913.375000\n",
            "Train Epoch: 94 [6720/7471 (90%)]\tLoss: 1576526.000000\n",
            "Train Epoch: 94 [6880/7471 (92%)]\tLoss: 1511802.500000\n",
            "Train Epoch: 94 [7040/7471 (94%)]\tLoss: 1592893.375000\n",
            "Train Epoch: 94 [7200/7471 (96%)]\tLoss: 1577985.250000\n",
            "Train Epoch: 94 [7360/7471 (99%)]\tLoss: 1591320.625000\n",
            "Epoch 94 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98434.8281\n",
            "\n",
            "Train Epoch: 95 [160/7471 (2%)]\tLoss: 1597101.000000\n",
            "Train Epoch: 95 [320/7471 (4%)]\tLoss: 1557706.375000\n",
            "Train Epoch: 95 [480/7471 (6%)]\tLoss: 1613305.000000\n",
            "Train Epoch: 95 [640/7471 (9%)]\tLoss: 1607950.500000\n",
            "Train Epoch: 95 [800/7471 (11%)]\tLoss: 1625206.125000\n",
            "Train Epoch: 95 [960/7471 (13%)]\tLoss: 1539572.625000\n",
            "Train Epoch: 95 [1120/7471 (15%)]\tLoss: 1474848.000000\n",
            "Train Epoch: 95 [1280/7471 (17%)]\tLoss: 1582932.625000\n",
            "Train Epoch: 95 [1440/7471 (19%)]\tLoss: 1600367.875000\n",
            "Train Epoch: 95 [1600/7471 (21%)]\tLoss: 1603795.375000\n",
            "Train Epoch: 95 [1760/7471 (24%)]\tLoss: 1590701.375000\n",
            "Train Epoch: 95 [1920/7471 (26%)]\tLoss: 1579713.375000\n",
            "Train Epoch: 95 [2080/7471 (28%)]\tLoss: 1628841.500000\n",
            "Train Epoch: 95 [2240/7471 (30%)]\tLoss: 1541296.625000\n",
            "Train Epoch: 95 [2400/7471 (32%)]\tLoss: 1574358.125000\n",
            "Train Epoch: 95 [2560/7471 (34%)]\tLoss: 1524317.750000\n",
            "Train Epoch: 95 [2720/7471 (36%)]\tLoss: 1560030.750000\n",
            "Train Epoch: 95 [2880/7471 (39%)]\tLoss: 1521047.500000\n",
            "Train Epoch: 95 [3040/7471 (41%)]\tLoss: 1524025.875000\n",
            "Train Epoch: 95 [3200/7471 (43%)]\tLoss: 1550870.750000\n",
            "Train Epoch: 95 [3360/7471 (45%)]\tLoss: 1531009.125000\n",
            "Train Epoch: 95 [3520/7471 (47%)]\tLoss: 1572848.500000\n",
            "Train Epoch: 95 [3680/7471 (49%)]\tLoss: 1552562.375000\n",
            "Train Epoch: 95 [3840/7471 (51%)]\tLoss: 1553109.000000\n",
            "Train Epoch: 95 [4000/7471 (54%)]\tLoss: 1573882.875000\n",
            "Train Epoch: 95 [4160/7471 (56%)]\tLoss: 1613093.375000\n",
            "Train Epoch: 95 [4320/7471 (58%)]\tLoss: 1521323.625000\n",
            "Train Epoch: 95 [4480/7471 (60%)]\tLoss: 1608323.625000\n",
            "Train Epoch: 95 [4640/7471 (62%)]\tLoss: 1617182.500000\n",
            "Train Epoch: 95 [4800/7471 (64%)]\tLoss: 1612065.250000\n",
            "Train Epoch: 95 [4960/7471 (66%)]\tLoss: 1637708.000000\n",
            "Train Epoch: 95 [5120/7471 (69%)]\tLoss: 1563992.875000\n",
            "Train Epoch: 95 [5280/7471 (71%)]\tLoss: 1621675.625000\n",
            "Train Epoch: 95 [5440/7471 (73%)]\tLoss: 1564084.000000\n",
            "Train Epoch: 95 [5600/7471 (75%)]\tLoss: 1582732.250000\n",
            "Train Epoch: 95 [5760/7471 (77%)]\tLoss: 1633128.000000\n",
            "Train Epoch: 95 [5920/7471 (79%)]\tLoss: 1565364.750000\n",
            "Train Epoch: 95 [6080/7471 (81%)]\tLoss: 1603101.750000\n",
            "Train Epoch: 95 [6240/7471 (84%)]\tLoss: 1504314.875000\n",
            "Train Epoch: 95 [6400/7471 (86%)]\tLoss: 1488778.500000\n",
            "Train Epoch: 95 [6560/7471 (88%)]\tLoss: 1602722.500000\n",
            "Train Epoch: 95 [6720/7471 (90%)]\tLoss: 1589268.000000\n",
            "Train Epoch: 95 [6880/7471 (92%)]\tLoss: 1591887.250000\n",
            "Train Epoch: 95 [7040/7471 (94%)]\tLoss: 1585944.375000\n",
            "Train Epoch: 95 [7200/7471 (96%)]\tLoss: 1557664.750000\n",
            "Train Epoch: 95 [7360/7471 (99%)]\tLoss: 1572737.375000\n",
            "Epoch 95 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98435.7628\n",
            "\n",
            "Train Epoch: 96 [160/7471 (2%)]\tLoss: 1529906.375000\n",
            "Train Epoch: 96 [320/7471 (4%)]\tLoss: 1547458.625000\n",
            "Train Epoch: 96 [480/7471 (6%)]\tLoss: 1572138.875000\n",
            "Train Epoch: 96 [640/7471 (9%)]\tLoss: 1572203.625000\n",
            "Train Epoch: 96 [800/7471 (11%)]\tLoss: 1542554.375000\n",
            "Train Epoch: 96 [960/7471 (13%)]\tLoss: 1556196.625000\n",
            "Train Epoch: 96 [1120/7471 (15%)]\tLoss: 1628000.250000\n",
            "Train Epoch: 96 [1280/7471 (17%)]\tLoss: 1556829.250000\n",
            "Train Epoch: 96 [1440/7471 (19%)]\tLoss: 1585594.500000\n",
            "Train Epoch: 96 [1600/7471 (21%)]\tLoss: 1579845.500000\n",
            "Train Epoch: 96 [1760/7471 (24%)]\tLoss: 1493801.750000\n",
            "Train Epoch: 96 [1920/7471 (26%)]\tLoss: 1564001.500000\n",
            "Train Epoch: 96 [2080/7471 (28%)]\tLoss: 1592259.125000\n",
            "Train Epoch: 96 [2240/7471 (30%)]\tLoss: 1542479.000000\n",
            "Train Epoch: 96 [2400/7471 (32%)]\tLoss: 1537819.000000\n",
            "Train Epoch: 96 [2560/7471 (34%)]\tLoss: 1549188.500000\n",
            "Train Epoch: 96 [2720/7471 (36%)]\tLoss: 1520639.250000\n",
            "Train Epoch: 96 [2880/7471 (39%)]\tLoss: 1556920.375000\n",
            "Train Epoch: 96 [3040/7471 (41%)]\tLoss: 1573162.250000\n",
            "Train Epoch: 96 [3200/7471 (43%)]\tLoss: 1564828.125000\n",
            "Train Epoch: 96 [3360/7471 (45%)]\tLoss: 1523182.375000\n",
            "Train Epoch: 96 [3520/7471 (47%)]\tLoss: 1630648.000000\n",
            "Train Epoch: 96 [3680/7471 (49%)]\tLoss: 1588499.250000\n",
            "Train Epoch: 96 [3840/7471 (51%)]\tLoss: 1594118.125000\n",
            "Train Epoch: 96 [4000/7471 (54%)]\tLoss: 1605000.750000\n",
            "Train Epoch: 96 [4160/7471 (56%)]\tLoss: 1554552.000000\n",
            "Train Epoch: 96 [4320/7471 (58%)]\tLoss: 1588620.875000\n",
            "Train Epoch: 96 [4480/7471 (60%)]\tLoss: 1630138.000000\n",
            "Train Epoch: 96 [4640/7471 (62%)]\tLoss: 1511392.375000\n",
            "Train Epoch: 96 [4800/7471 (64%)]\tLoss: 1618069.375000\n",
            "Train Epoch: 96 [4960/7471 (66%)]\tLoss: 1549049.375000\n",
            "Train Epoch: 96 [5120/7471 (69%)]\tLoss: 1631757.250000\n",
            "Train Epoch: 96 [5280/7471 (71%)]\tLoss: 1609245.000000\n",
            "Train Epoch: 96 [5440/7471 (73%)]\tLoss: 1544146.500000\n",
            "Train Epoch: 96 [5600/7471 (75%)]\tLoss: 1629383.625000\n",
            "Train Epoch: 96 [5760/7471 (77%)]\tLoss: 1604044.250000\n",
            "Train Epoch: 96 [5920/7471 (79%)]\tLoss: 1620026.000000\n",
            "Train Epoch: 96 [6080/7471 (81%)]\tLoss: 1608797.875000\n",
            "Train Epoch: 96 [6240/7471 (84%)]\tLoss: 1544305.750000\n",
            "Train Epoch: 96 [6400/7471 (86%)]\tLoss: 1599473.500000\n",
            "Train Epoch: 96 [6560/7471 (88%)]\tLoss: 1539686.125000\n",
            "Train Epoch: 96 [6720/7471 (90%)]\tLoss: 1558159.125000\n",
            "Train Epoch: 96 [6880/7471 (92%)]\tLoss: 1577005.125000\n",
            "Train Epoch: 96 [7040/7471 (94%)]\tLoss: 1585502.625000\n",
            "Train Epoch: 96 [7200/7471 (96%)]\tLoss: 1610673.625000\n",
            "Train Epoch: 96 [7360/7471 (99%)]\tLoss: 1591725.625000\n",
            "Epoch 96 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98468.6730\n",
            "\n",
            "Train Epoch: 97 [160/7471 (2%)]\tLoss: 1626186.375000\n",
            "Train Epoch: 97 [320/7471 (4%)]\tLoss: 1617719.625000\n",
            "Train Epoch: 97 [480/7471 (6%)]\tLoss: 1549722.000000\n",
            "Train Epoch: 97 [640/7471 (9%)]\tLoss: 1617562.750000\n",
            "Train Epoch: 97 [800/7471 (11%)]\tLoss: 1592881.875000\n",
            "Train Epoch: 97 [960/7471 (13%)]\tLoss: 1456895.500000\n",
            "Train Epoch: 97 [1120/7471 (15%)]\tLoss: 1524315.250000\n",
            "Train Epoch: 97 [1280/7471 (17%)]\tLoss: 1578864.125000\n",
            "Train Epoch: 97 [1440/7471 (19%)]\tLoss: 1516618.000000\n",
            "Train Epoch: 97 [1600/7471 (21%)]\tLoss: 1626210.750000\n",
            "Train Epoch: 97 [1760/7471 (24%)]\tLoss: 1625874.375000\n",
            "Train Epoch: 97 [1920/7471 (26%)]\tLoss: 1547586.500000\n",
            "Train Epoch: 97 [2080/7471 (28%)]\tLoss: 1602562.500000\n",
            "Train Epoch: 97 [2240/7471 (30%)]\tLoss: 1562835.000000\n",
            "Train Epoch: 97 [2400/7471 (32%)]\tLoss: 1603000.250000\n",
            "Train Epoch: 97 [2560/7471 (34%)]\tLoss: 1553661.750000\n",
            "Train Epoch: 97 [2720/7471 (36%)]\tLoss: 1596159.875000\n",
            "Train Epoch: 97 [2880/7471 (39%)]\tLoss: 1550200.000000\n",
            "Train Epoch: 97 [3040/7471 (41%)]\tLoss: 1622686.875000\n",
            "Train Epoch: 97 [3200/7471 (43%)]\tLoss: 1563672.750000\n",
            "Train Epoch: 97 [3360/7471 (45%)]\tLoss: 1549867.875000\n",
            "Train Epoch: 97 [3520/7471 (47%)]\tLoss: 1573066.125000\n",
            "Train Epoch: 97 [3680/7471 (49%)]\tLoss: 1577150.625000\n",
            "Train Epoch: 97 [3840/7471 (51%)]\tLoss: 1583861.625000\n",
            "Train Epoch: 97 [4000/7471 (54%)]\tLoss: 1585603.250000\n",
            "Train Epoch: 97 [4160/7471 (56%)]\tLoss: 1551846.125000\n",
            "Train Epoch: 97 [4320/7471 (58%)]\tLoss: 1476634.625000\n",
            "Train Epoch: 97 [4480/7471 (60%)]\tLoss: 1601529.750000\n",
            "Train Epoch: 97 [4640/7471 (62%)]\tLoss: 1490386.375000\n",
            "Train Epoch: 97 [4800/7471 (64%)]\tLoss: 1572249.750000\n",
            "Train Epoch: 97 [4960/7471 (66%)]\tLoss: 1550135.875000\n",
            "Train Epoch: 97 [5120/7471 (69%)]\tLoss: 1514838.000000\n",
            "Train Epoch: 97 [5280/7471 (71%)]\tLoss: 1573461.000000\n",
            "Train Epoch: 97 [5440/7471 (73%)]\tLoss: 1550615.875000\n",
            "Train Epoch: 97 [5600/7471 (75%)]\tLoss: 1584710.000000\n",
            "Train Epoch: 97 [5760/7471 (77%)]\tLoss: 1513307.375000\n",
            "Train Epoch: 97 [5920/7471 (79%)]\tLoss: 1588630.875000\n",
            "Train Epoch: 97 [6080/7471 (81%)]\tLoss: 1576717.625000\n",
            "Train Epoch: 97 [6240/7471 (84%)]\tLoss: 1570419.625000\n",
            "Train Epoch: 97 [6400/7471 (86%)]\tLoss: 1592976.125000\n",
            "Train Epoch: 97 [6560/7471 (88%)]\tLoss: 1544147.875000\n",
            "Train Epoch: 97 [6720/7471 (90%)]\tLoss: 1585486.875000\n",
            "Train Epoch: 97 [6880/7471 (92%)]\tLoss: 1583296.875000\n",
            "Train Epoch: 97 [7040/7471 (94%)]\tLoss: 1461829.125000\n",
            "Train Epoch: 97 [7200/7471 (96%)]\tLoss: 1507475.000000\n",
            "Train Epoch: 97 [7360/7471 (99%)]\tLoss: 1594706.375000\n",
            "Epoch 97 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98525.7165\n",
            "\n",
            "Train Epoch: 98 [160/7471 (2%)]\tLoss: 1574534.250000\n",
            "Train Epoch: 98 [320/7471 (4%)]\tLoss: 1510457.250000\n",
            "Train Epoch: 98 [480/7471 (6%)]\tLoss: 1623423.750000\n",
            "Train Epoch: 98 [640/7471 (9%)]\tLoss: 1587943.875000\n",
            "Train Epoch: 98 [800/7471 (11%)]\tLoss: 1599030.875000\n",
            "Train Epoch: 98 [960/7471 (13%)]\tLoss: 1549109.250000\n",
            "Train Epoch: 98 [1120/7471 (15%)]\tLoss: 1630971.250000\n",
            "Train Epoch: 98 [1280/7471 (17%)]\tLoss: 1528924.625000\n",
            "Train Epoch: 98 [1440/7471 (19%)]\tLoss: 1601676.375000\n",
            "Train Epoch: 98 [1600/7471 (21%)]\tLoss: 1589057.500000\n",
            "Train Epoch: 98 [1760/7471 (24%)]\tLoss: 1566117.625000\n",
            "Train Epoch: 98 [1920/7471 (26%)]\tLoss: 1614176.625000\n",
            "Train Epoch: 98 [2080/7471 (28%)]\tLoss: 1588644.875000\n",
            "Train Epoch: 98 [2240/7471 (30%)]\tLoss: 1551461.750000\n",
            "Train Epoch: 98 [2400/7471 (32%)]\tLoss: 1584499.000000\n",
            "Train Epoch: 98 [2560/7471 (34%)]\tLoss: 1585488.500000\n",
            "Train Epoch: 98 [2720/7471 (36%)]\tLoss: 1609589.625000\n",
            "Train Epoch: 98 [2880/7471 (39%)]\tLoss: 1561305.125000\n",
            "Train Epoch: 98 [3040/7471 (41%)]\tLoss: 1613804.125000\n",
            "Train Epoch: 98 [3200/7471 (43%)]\tLoss: 1561054.000000\n",
            "Train Epoch: 98 [3360/7471 (45%)]\tLoss: 1503094.750000\n",
            "Train Epoch: 98 [3520/7471 (47%)]\tLoss: 1552439.500000\n",
            "Train Epoch: 98 [3680/7471 (49%)]\tLoss: 1594809.250000\n",
            "Train Epoch: 98 [3840/7471 (51%)]\tLoss: 1541831.750000\n",
            "Train Epoch: 98 [4000/7471 (54%)]\tLoss: 1593633.500000\n",
            "Train Epoch: 98 [4160/7471 (56%)]\tLoss: 1553759.875000\n",
            "Train Epoch: 98 [4320/7471 (58%)]\tLoss: 1593042.250000\n",
            "Train Epoch: 98 [4480/7471 (60%)]\tLoss: 1571399.250000\n",
            "Train Epoch: 98 [4640/7471 (62%)]\tLoss: 1628800.500000\n",
            "Train Epoch: 98 [4800/7471 (64%)]\tLoss: 1531672.500000\n",
            "Train Epoch: 98 [4960/7471 (66%)]\tLoss: 1501724.625000\n",
            "Train Epoch: 98 [5120/7471 (69%)]\tLoss: 1558545.125000\n",
            "Train Epoch: 98 [5280/7471 (71%)]\tLoss: 1557868.750000\n",
            "Train Epoch: 98 [5440/7471 (73%)]\tLoss: 1577409.250000\n",
            "Train Epoch: 98 [5600/7471 (75%)]\tLoss: 1561408.875000\n",
            "Train Epoch: 98 [5760/7471 (77%)]\tLoss: 1609849.875000\n",
            "Train Epoch: 98 [5920/7471 (79%)]\tLoss: 1599615.250000\n",
            "Train Epoch: 98 [6080/7471 (81%)]\tLoss: 1573246.500000\n",
            "Train Epoch: 98 [6240/7471 (84%)]\tLoss: 1524756.000000\n",
            "Train Epoch: 98 [6400/7471 (86%)]\tLoss: 1569628.750000\n",
            "Train Epoch: 98 [6560/7471 (88%)]\tLoss: 1586800.000000\n",
            "Train Epoch: 98 [6720/7471 (90%)]\tLoss: 1550272.250000\n",
            "Train Epoch: 98 [6880/7471 (92%)]\tLoss: 1552778.000000\n",
            "Train Epoch: 98 [7040/7471 (94%)]\tLoss: 1561837.000000\n",
            "Train Epoch: 98 [7200/7471 (96%)]\tLoss: 1584244.000000\n",
            "Train Epoch: 98 [7360/7471 (99%)]\tLoss: 1577333.625000\n",
            "Epoch 98 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98406.9307\n",
            "\n",
            "Train Epoch: 99 [160/7471 (2%)]\tLoss: 1586241.250000\n",
            "Train Epoch: 99 [320/7471 (4%)]\tLoss: 1544545.375000\n",
            "Train Epoch: 99 [480/7471 (6%)]\tLoss: 1615604.250000\n",
            "Train Epoch: 99 [640/7471 (9%)]\tLoss: 1619727.750000\n",
            "Train Epoch: 99 [800/7471 (11%)]\tLoss: 1577767.875000\n",
            "Train Epoch: 99 [960/7471 (13%)]\tLoss: 1560543.125000\n",
            "Train Epoch: 99 [1120/7471 (15%)]\tLoss: 1579481.500000\n",
            "Train Epoch: 99 [1280/7471 (17%)]\tLoss: 1612835.500000\n",
            "Train Epoch: 99 [1440/7471 (19%)]\tLoss: 1617744.000000\n",
            "Train Epoch: 99 [1600/7471 (21%)]\tLoss: 1624702.125000\n",
            "Train Epoch: 99 [1760/7471 (24%)]\tLoss: 1582522.125000\n",
            "Train Epoch: 99 [1920/7471 (26%)]\tLoss: 1619865.875000\n",
            "Train Epoch: 99 [2080/7471 (28%)]\tLoss: 1589063.375000\n",
            "Train Epoch: 99 [2240/7471 (30%)]\tLoss: 1505249.750000\n",
            "Train Epoch: 99 [2400/7471 (32%)]\tLoss: 1577477.125000\n",
            "Train Epoch: 99 [2560/7471 (34%)]\tLoss: 1505537.750000\n",
            "Train Epoch: 99 [2720/7471 (36%)]\tLoss: 1549556.000000\n",
            "Train Epoch: 99 [2880/7471 (39%)]\tLoss: 1611346.500000\n",
            "Train Epoch: 99 [3040/7471 (41%)]\tLoss: 1492504.375000\n",
            "Train Epoch: 99 [3200/7471 (43%)]\tLoss: 1624043.750000\n",
            "Train Epoch: 99 [3360/7471 (45%)]\tLoss: 1597789.625000\n",
            "Train Epoch: 99 [3520/7471 (47%)]\tLoss: 1559112.875000\n",
            "Train Epoch: 99 [3680/7471 (49%)]\tLoss: 1597139.625000\n",
            "Train Epoch: 99 [3840/7471 (51%)]\tLoss: 1596796.750000\n",
            "Train Epoch: 99 [4000/7471 (54%)]\tLoss: 1621146.500000\n",
            "Train Epoch: 99 [4160/7471 (56%)]\tLoss: 1573081.125000\n",
            "Train Epoch: 99 [4320/7471 (58%)]\tLoss: 1558127.375000\n",
            "Train Epoch: 99 [4480/7471 (60%)]\tLoss: 1529818.375000\n",
            "Train Epoch: 99 [4640/7471 (62%)]\tLoss: 1623619.125000\n",
            "Train Epoch: 99 [4800/7471 (64%)]\tLoss: 1532652.875000\n",
            "Train Epoch: 99 [4960/7471 (66%)]\tLoss: 1575077.625000\n",
            "Train Epoch: 99 [5120/7471 (69%)]\tLoss: 1541625.750000\n",
            "Train Epoch: 99 [5280/7471 (71%)]\tLoss: 1552932.750000\n",
            "Train Epoch: 99 [5440/7471 (73%)]\tLoss: 1585141.875000\n",
            "Train Epoch: 99 [5600/7471 (75%)]\tLoss: 1613520.375000\n",
            "Train Epoch: 99 [5760/7471 (77%)]\tLoss: 1497589.000000\n",
            "Train Epoch: 99 [5920/7471 (79%)]\tLoss: 1626544.500000\n",
            "Train Epoch: 99 [6080/7471 (81%)]\tLoss: 1622971.500000\n",
            "Train Epoch: 99 [6240/7471 (84%)]\tLoss: 1601050.375000\n",
            "Train Epoch: 99 [6400/7471 (86%)]\tLoss: 1554395.500000\n",
            "Train Epoch: 99 [6560/7471 (88%)]\tLoss: 1588460.375000\n",
            "Train Epoch: 99 [6720/7471 (90%)]\tLoss: 1639108.000000\n",
            "Train Epoch: 99 [6880/7471 (92%)]\tLoss: 1599797.875000\n",
            "Train Epoch: 99 [7040/7471 (94%)]\tLoss: 1553048.875000\n",
            "Train Epoch: 99 [7200/7471 (96%)]\tLoss: 1589609.125000\n",
            "Train Epoch: 99 [7360/7471 (99%)]\tLoss: 1607403.250000\n",
            "Epoch 99 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 99005.6195\n",
            "\n",
            "Train Epoch: 100 [160/7471 (2%)]\tLoss: 1557447.750000\n",
            "Train Epoch: 100 [320/7471 (4%)]\tLoss: 1613803.000000\n",
            "Train Epoch: 100 [480/7471 (6%)]\tLoss: 1589912.250000\n",
            "Train Epoch: 100 [640/7471 (9%)]\tLoss: 1553577.000000\n",
            "Train Epoch: 100 [800/7471 (11%)]\tLoss: 1618574.250000\n",
            "Train Epoch: 100 [960/7471 (13%)]\tLoss: 1591011.875000\n",
            "Train Epoch: 100 [1120/7471 (15%)]\tLoss: 1559544.375000\n",
            "Train Epoch: 100 [1280/7471 (17%)]\tLoss: 1513707.750000\n",
            "Train Epoch: 100 [1440/7471 (19%)]\tLoss: 1632811.875000\n",
            "Train Epoch: 100 [1600/7471 (21%)]\tLoss: 1592571.125000\n",
            "Train Epoch: 100 [1760/7471 (24%)]\tLoss: 1549451.500000\n",
            "Train Epoch: 100 [1920/7471 (26%)]\tLoss: 1607970.375000\n",
            "Train Epoch: 100 [2080/7471 (28%)]\tLoss: 1610298.875000\n",
            "Train Epoch: 100 [2240/7471 (30%)]\tLoss: 1549423.250000\n",
            "Train Epoch: 100 [2400/7471 (32%)]\tLoss: 1545236.875000\n",
            "Train Epoch: 100 [2560/7471 (34%)]\tLoss: 1524233.875000\n",
            "Train Epoch: 100 [2720/7471 (36%)]\tLoss: 1617395.125000\n",
            "Train Epoch: 100 [2880/7471 (39%)]\tLoss: 1538326.125000\n",
            "Train Epoch: 100 [3040/7471 (41%)]\tLoss: 1597264.500000\n",
            "Train Epoch: 100 [3200/7471 (43%)]\tLoss: 1574156.750000\n",
            "Train Epoch: 100 [3360/7471 (45%)]\tLoss: 1591088.125000\n",
            "Train Epoch: 100 [3520/7471 (47%)]\tLoss: 1579153.000000\n",
            "Train Epoch: 100 [3680/7471 (49%)]\tLoss: 1610317.625000\n",
            "Train Epoch: 100 [3840/7471 (51%)]\tLoss: 1555971.625000\n",
            "Train Epoch: 100 [4000/7471 (54%)]\tLoss: 1622688.250000\n",
            "Train Epoch: 100 [4160/7471 (56%)]\tLoss: 1527957.375000\n",
            "Train Epoch: 100 [4320/7471 (58%)]\tLoss: 1598225.500000\n",
            "Train Epoch: 100 [4480/7471 (60%)]\tLoss: 1575167.250000\n",
            "Train Epoch: 100 [4640/7471 (62%)]\tLoss: 1560734.375000\n",
            "Train Epoch: 100 [4800/7471 (64%)]\tLoss: 1615322.375000\n",
            "Train Epoch: 100 [4960/7471 (66%)]\tLoss: 1610757.000000\n",
            "Train Epoch: 100 [5120/7471 (69%)]\tLoss: 1592696.125000\n",
            "Train Epoch: 100 [5280/7471 (71%)]\tLoss: 1551536.875000\n",
            "Train Epoch: 100 [5440/7471 (73%)]\tLoss: 1584720.000000\n",
            "Train Epoch: 100 [5600/7471 (75%)]\tLoss: 1620469.625000\n",
            "Train Epoch: 100 [5760/7471 (77%)]\tLoss: 1526462.375000\n",
            "Train Epoch: 100 [5920/7471 (79%)]\tLoss: 1518774.875000\n",
            "Train Epoch: 100 [6080/7471 (81%)]\tLoss: 1547443.375000\n",
            "Train Epoch: 100 [6240/7471 (84%)]\tLoss: 1631281.750000\n",
            "Train Epoch: 100 [6400/7471 (86%)]\tLoss: 1593176.875000\n",
            "Train Epoch: 100 [6560/7471 (88%)]\tLoss: 1580194.375000\n",
            "Train Epoch: 100 [6720/7471 (90%)]\tLoss: 1565154.500000\n",
            "Train Epoch: 100 [6880/7471 (92%)]\tLoss: 1555931.500000\n",
            "Train Epoch: 100 [7040/7471 (94%)]\tLoss: 1542754.000000\n",
            "Train Epoch: 100 [7200/7471 (96%)]\tLoss: 1603993.875000\n",
            "Train Epoch: 100 [7360/7471 (99%)]\tLoss: 1589589.375000\n",
            "Epoch 100 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98457.4625\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZauWbfkuy9i",
        "colab_type": "text"
      },
      "source": [
        "* make Exp Module for <code>Hyperparameter opitmization</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsNLKuYziD8b",
        "colab_type": "text"
      },
      "source": [
        "Add-plot<br>\n",
        "<code>train_val plot</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7EcU6z-nVLA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c4e597c1-9359-4cc9-d336-2ff92837c263"
      },
      "source": [
        "epoch_train_losses = np.load('./results_ResNet-VAE_Exp01/ResNet_VAE_training_loss.npy')\n",
        "epoch_test_losses = np.load('./results_ResNet-VAE_Exp01/ResNet_VAE_test_loss.npy')\n",
        "\n",
        "print(epoch_train_losses.shape, epoch_test_losses.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 467) (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo9-u_XXtUJv",
        "colab_type": "text"
      },
      "source": [
        "<code>batch_size</code>를 argparser에서는 50으로 정해줬는데 결과가 16이라 의아했는데 원인을 찾았다.<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "Train_Loader에서 batch_size를 16으로 정해줬었어...!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPd3EkJSsCet",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "676093f2-27b6-400c-8103-c9604774ef08"
      },
      "source": [
        "print(epoch_train_losses.shape, epoch_test_losses.shape)\n",
        "print(epoch_train_losses[0].shape)\n",
        "\n",
        "print(len(train_loader.dataset))\n",
        "print(len(valid_loader.dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 467) (100,)\n",
            "(467,)\n",
            "7471\n",
            "1868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_t2oJR7_zBO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b7e39b6e-a679-4c65-fdac-85210090b034"
      },
      "source": [
        "epoch_train_loss = np.array(np.sum(epoch_train_losses, axis=1)/len(train_loader))\n",
        "print(type(epoch_train_loss), epoch_train_loss.shape)\n",
        "print(type(epoch_test_losses), epoch_test_losses.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (100,)\n",
            "<class 'numpy.ndarray'> (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk4l5aRhFnTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "f72bc469-297a-469e-bafc-23ec98535006"
      },
      "source": [
        "# print(epoch_train_loss)\n",
        "print(epoch_train_loss)\n",
        "print(epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1638542.16300857 1620145.3011242  1613838.34635974 1611309.79309422\n",
            " 1609294.58110278 1604131.50910064 1605361.17665953 1603169.00722698\n",
            " 1597750.57842612 1594670.69566381 1596908.4638651  1594820.83110278\n",
            " 1591787.57441113 1589724.83458244 1589253.08003212 1590662.44619914\n",
            " 1587715.87794433 1586307.64052463 1585468.14641328 1586667.18843683\n",
            " 1585635.30995717 1584972.41595289 1584345.36911135 1585177.11482869\n",
            " 1584231.52382227 1584188.90042827 1582912.37044968 1583511.24116702\n",
            " 1584135.60626338 1582163.17773019 1582088.10438972 1581855.36911135\n",
            " 1581567.88436831 1581862.14025696 1581171.07441113 1580937.06343683\n",
            " 1580699.83752677 1579828.38356531 1580349.66675589 1580600.1761242\n",
            " 1580038.31664882 1579667.83244111 1579829.20262313 1579679.86589936\n",
            " 1579711.6988758  1578713.22135974 1578928.89400428 1578330.98420771\n",
            " 1578694.30968951 1578644.82601713 1578315.04255889 1577831.36884368\n",
            " 1577992.97698073 1577751.36643469 1577560.05888651 1577430.78988223\n",
            " 1577293.02248394 1577069.23688437 1577013.47082441 1576958.40845824\n",
            " 1576406.12366167 1576562.86509636 1576422.33029979 1576229.95583512\n",
            " 1575853.71493576 1575683.0872591  1575741.07307281 1575780.82360814\n",
            " 1575882.30861884 1574954.02168094 1575597.125      1575033.44405782\n",
            " 1575318.14373662 1575119.48126338 1574604.7240364  1574612.86991435\n",
            " 1574526.29255889 1574299.13463597 1574445.46279443 1574408.07494647\n",
            " 1574237.87847966 1574076.05246253 1573984.6367773  1574016.43120985\n",
            " 1573851.62285867 1573942.85706638 1573719.92264454 1574223.66514989\n",
            " 1573480.73875803 1573358.25802998 1573214.28479657 1573489.65149893\n",
            " 1573379.83672377 1573342.13222698 1573365.32414347 1573396.05808351\n",
            " 1573183.27328694 1573067.21279443 1573034.38195931 1573201.46707709]\n",
            "[           inf            inf            inf            inf\n",
            " 2.04339794e+15            inf 1.01770823e+05 1.00493531e+05\n",
            "            inf            inf 9.95151711e+04            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            " 9.91129865e+04            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf 4.64997510e+17 9.86961006e+04 9.86702425e+04\n",
            " 9.86419925e+04 9.86430577e+04 9.88899452e+04 9.91061395e+04\n",
            " 9.86037253e+04 9.85922494e+04 9.86069897e+04 9.86739283e+04\n",
            " 9.89011277e+04 9.86097878e+04 9.86213919e+04 9.85981963e+04\n",
            " 9.85841948e+04 9.86716995e+04 9.85664475e+04 9.85906832e+04\n",
            " 9.85348911e+04 9.85212109e+04 9.85748725e+04 9.85326321e+04\n",
            " 9.85321190e+04 9.85137038e+04 9.85856745e+04 9.85935321e+04\n",
            " 9.85536405e+04 9.84884595e+04 9.84979273e+04 9.85113831e+04\n",
            " 9.85500923e+04 9.84629215e+04 9.84715464e+04 9.84851026e+04\n",
            " 9.84757658e+04 9.85063151e+04 9.84828543e+04 9.85242495e+04\n",
            " 9.84802659e+04 9.85492924e+04 9.85056253e+04 9.84501005e+04\n",
            " 9.85073227e+04 9.84660906e+04 9.84765147e+04 9.84337928e+04\n",
            " 9.84426701e+04 9.84634076e+04 9.84738756e+04 9.84655270e+04\n",
            " 9.84447675e+04 9.84348281e+04 9.84357628e+04 9.84686730e+04\n",
            " 9.85257165e+04 9.84069307e+04 9.90056195e+04 9.84574625e+04]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLuB6-kLMyum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "10387f42-425b-4b9b-8330-e7305ec70226"
      },
      "source": [
        "print(min(epoch_train_loss))\n",
        "print(max(epoch_train_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1573034.3819593147\n",
            "1638542.1630085653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VciLlUVrAy-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "0a045914-f4d1-404d-ee6f-d66fa8963778"
      },
      "source": [
        "list_epoch = np.array(range(100))\n",
        "list_epoch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
              "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6YdKnjxiI2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "1e180af9-142e-4be8-a85f-17523c4a01a7"
      },
      "source": [
        "# Reference: Stand-DL Assignment #1\n",
        "\n",
        "# ======== Loss Fluctuation ======== #\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax1.plot(list_epoch, epoch_train_loss, label='train_loss')\n",
        "ax1.plot(list_epoch, epoch_test_losses, '--', label='test_loss')\n",
        "ax1.set_xlabel('epoch')\n",
        "ax1.set_ylabel('loss')\n",
        "ax1.grid()\n",
        "ax1.legend()\n",
        "ax1.set_title('epoch vs loss')\n",
        "\n",
        "# ======== save plot ======== #\n",
        "plt.savefig('./train_val_plot.png', dpi=300)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3yJz_I9BCAP",
        "colab_type": "text"
      },
      "source": [
        "### Epoch별로 loss graph를 그려볼까??"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkKHFr1sB1lO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Backup Code\n",
        "\n",
        "backup_epoch_train_loss = epoch_train_loss\n",
        "backup_epoch_test_loss = epoch_test_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqOb7PKzNLN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "##### Back Up #####\n",
        "# Reference: Stand-DL Assignment #1\n",
        "\n",
        "# ======== Loss Fluctuation ======== #\n",
        "def plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses):\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    # set range\n",
        "    # rng = np.arange(min(epoch_train_losses), max(epoch_train_losses))\n",
        "\n",
        "    ax1.plot(list_epoch, epoch_train_losses, label='train_loss')\n",
        "    ax1.plot(list_epoch, epoch_test_losses, '--', label='test_loss')\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.set_ylabel('loss')\n",
        "    ax1.grid()\n",
        "    ax1.legend()\n",
        "    ax1.set_title('epoch vs loss')\n",
        "\n",
        "    # ======== save plot ======== #\n",
        "    plt.savefig('./results_ResNet-VAE_Exp01/loss_plot/{}train_val_plot.png'.format(list_epoch[-1]+1), dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcUoN8m7CsxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reference: Stand-DL Assignment #1\n",
        "\n",
        "# ======== Loss Fluctuation ======== #\n",
        "def plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses):\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    # set range\n",
        "    # rng = np.arange(np.amin(epoch_train_losses), np.amin(epoch_train_losses), 5000)\n",
        "\n",
        "    ax1.plot(list_epoch, epoch_train_losses, label='train_loss')\n",
        "    ax1.plot(list_epoch, epoch_test_losses+1460000, '--', label='test_loss')\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.set_ylabel('loss')\n",
        "    ax1.grid()\n",
        "    ax1.legend()\n",
        "    ax1.set_title('epoch vs loss')\n",
        "    # set limits\n",
        "    # ax1.autoscale(enable=True, axis='y')\n",
        "    \n",
        "    # Reference\n",
        "    # ax1.set_ylim(np.amin(epoch_train_losses), np.max(epoch_train_losses))\n",
        "    ax1.set_ylim(np.amin(epoch_test_losses+1460000), np.max(epoch_train_losses))\n",
        "\n",
        "    # ======== save plot ======== #\n",
        "    plt.savefig('./results_ResNet-VAE_Exp01/loss_plot/{}train_val_plot__for-test__.png'.format(list_epoch[-1]+1), dpi=300)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPf8LjDtTeAm",
        "colab_type": "text"
      },
      "source": [
        "* Min: test_losses, Max: test_losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPG-VaKgTia0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reference: Stand-DL Assignment #1\n",
        "\n",
        "# ======== Loss Fluctuation ======== #\n",
        "def plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses):\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    # set range\n",
        "    # rng = np.arange(np.amin(epoch_train_losses), np.amin(epoch_train_losses), 5000)\n",
        "\n",
        "    ax1.plot(list_epoch, epoch_train_losses, label='train_loss')\n",
        "    ax1.plot(list_epoch, epoch_test_losses, '--', label='test_loss')\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.set_ylabel('loss')\n",
        "    ax1.grid()\n",
        "    ax1.legend()\n",
        "    ax1.set_title('epoch vs loss')\n",
        "    # set limits\n",
        "    # ax1.autoscale(enable=True, axis='y')\n",
        "    \n",
        "    # Reference\n",
        "    ax1.set_ylim(np.amin(epoch_train_losses), np.amin(epoch_train_losses))\n",
        "    # ======== save plot ======== #\n",
        "    # plt.savefig('./results_ResNet-VAE_Exp01/loss_plot/{}train_val_plot.png'.format(list_epoch[-1]+1), dpi=300)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr5a66JVsuAd",
        "colab_type": "text"
      },
      "source": [
        "Test-plot<br>\n",
        "Try as many plot as possible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PISUwR3ns1Di",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "d906f0e6-6189-4853-df29-45648bac0223"
      },
      "source": [
        "# 01. 0 Epoch ~ 8 Epoch\n",
        "list_epoch = np.array(range(100))\n",
        "epoch_train_losses = backup_epoch_train_loss[:]\n",
        "epoch_test_losses = backup_epoch_test_loss[:]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvYGfea6E2v1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "108ca272-e50d-49bf-bf9f-62cd7b9b963b"
      },
      "source": [
        "# 01. 0 Epoch ~ 8 Epoch\n",
        "list_epoch = np.array(range(100))\n",
        "epoch_train_losses = backup_epoch_train_loss[:]\n",
        "epoch_test_losses = backup_epoch_test_loss[:]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Attempting to set identical bottom == top == 1573034.3819593147 results in singular transformations; automatically expanding.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoFG20bkTne1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "761fe8b1-467a-45b1-9f44-af3d2c789ac8"
      },
      "source": [
        "# 01. 0 Epoch ~ 8 Epoch\n",
        "list_epoch = np.array(range(10))\n",
        "epoch_train_losses = backup_epoch_train_loss[:]\n",
        "epoch_test_losses = backup_epoch_test_loss[:]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Attempting to set identical bottom == top == 98406.9306745182 results in singular transformations; automatically expanding.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hQJ5yAisBIp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "024b1f36-7548-4a50-9d7b-4a7a12351bd8"
      },
      "source": [
        "# 01. 0 Epoch ~ 8 Epoch\n",
        "list_epoch = np.array(range(100))\n",
        "epoch_train_losses = backup_epoch_train_loss[:]\n",
        "epoch_test_losses = backup_epoch_test_loss[:]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Attempting to set identical bottom == top == 98406.9306745182 results in singular transformations; automatically expanding.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "045VPwO7UP5X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "c336fd0b-1e17-4888-922c-a135e7318c11"
      },
      "source": [
        "# 03. 0 Epoch ~ 100 Epoch\n",
        "list_epoch = np.array(range(100))\n",
        "epoch_train_losses = backup_epoch_train_loss[:]\n",
        "epoch_test_losses = backup_epoch_test_loss[:]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmUH4stqR8pr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "fca93ba5-cb6b-4d76-be38-f337f277be70"
      },
      "source": [
        "# 01. 0 Epoch ~ 8 Epoch\n",
        "list_epoch = np.array(range(50))\n",
        "epoch_train_losses = backup_epoch_train_loss[:50]\n",
        "epoch_test_losses = backup_epoch_test_loss[:50]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Attempting to set identical bottom == top == 1578330.9842077089 results in singular transformations; automatically expanding.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4kpKVxCEpBs",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 10::</code>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6tXLL2rErou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "f0ad1420-d017-471d-c2f7-c28a4d8cf79f"
      },
      "source": [
        "# 01. 0 Epoch ~ 10 Epoch\n",
        "list_epoch = np.array(range(45))\n",
        "epoch_train_losses = backup_epoch_train_loss[:45]\n",
        "epoch_test_losses = backup_epoch_test_loss[:45]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Attempting to set identical bottom == top == 1579667.8324411134 results in singular transformations; automatically expanding.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WkyuNtrCLJJ",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 20::</code>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXv9m-SxBIfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "d3ecba5d-f14c-4807-cf02-b9c9cdfed1ef"
      },
      "source": [
        "# 01. 0 Epoch ~ 20 Epoch\n",
        "list_epoch = np.array(range(20))\n",
        "epoch_train_losses = backup_epoch_train_loss[:20]\n",
        "epoch_test_losses = backup_epoch_test_loss[:20]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Ukvua8BBkb",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 40::</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQGuW3TtC_5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "2d0f989b-8e7a-4563-a772-cf73e0c4d890"
      },
      "source": [
        "# 02. 0 Epoch ~ 40 Epoch\n",
        "list_epoch = np.array(range(45))\n",
        "epoch_train_losses = backup_epoch_train_loss[:45]\n",
        "epoch_test_losses = backup_epoch_test_loss[:45]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXa8FCwPQpzM",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 60::</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C7EuJKTDNiA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "4fa53f7a-23ff-4665-fec6-348b9b7b5cca"
      },
      "source": [
        "# 03. 0 Epoch ~ 60 Epoch\n",
        "list_epoch = np.array(range(60))\n",
        "epoch_train_losses = backup_epoch_train_loss[:60]\n",
        "epoch_test_losses = backup_epoch_test_loss[:60]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiFH0axWQtXo",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 80::</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yehSO166D7Ih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "7518f4bf-9b1a-4c95-9c19-1396e87984e1"
      },
      "source": [
        "# 03. 0 Epoch ~ 80 Epoch\n",
        "list_epoch = np.array(range(80))\n",
        "epoch_train_losses = backup_epoch_train_loss[:80]\n",
        "epoch_test_losses = backup_epoch_test_loss[:80]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KmmwyW4QwdC",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 100::</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f152o_NwD-5z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "08b95dd7-6a35-45bd-957f-968670560e5d"
      },
      "source": [
        "# 03. 0 Epoch ~ 100 Epoch\n",
        "list_epoch = np.array(range(100))\n",
        "epoch_train_losses = backup_epoch_train_loss[:]\n",
        "epoch_test_losses = backup_epoch_test_loss[:]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Attempting to set identical bottom == top == 1573034.3819593147 results in singular transformations; automatically expanding.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cofyXuQJuI3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2d2f3a21-36cf-4d02-8a18-3b1c68a39ed8"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL8ixXdrdLpi",
        "colab_type": "text"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uKB5_XmdMzz",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation\n",
        "* train, val, test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0uHo2uDdUYr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "ff953b65-6912-4613-f006-1a562a58e87c"
      },
      "source": [
        "transforms = transforms.Compose([\n",
        "                                transforms.Resize((224, 224)),                # Change size of Image to (224, 224)\n",
        "                                transforms.Grayscale(num_output_channels=1),  # Makes it 1-dimension channel\n",
        "                                transforms.ToTensor(),                        # Convert a PIL Image or numpy.ndarray to tensor.\n",
        "                                                                              # Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8\n",
        "                                                                              # In the other cases, tensors are returned without scaling.\n",
        "                                # transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "                                \n",
        "                                ])\n",
        "\n",
        "# make custom dataset \n",
        "# use torchvision.datasets.ImageFolder()\n",
        "trainset = torchvision.datasets.ImageFolder(root='../../../../InformationSecurity_Summer/malimg',\n",
        "                                            transform=transforms)  # make custom dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-11eb43b621a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transforms = transforms.Compose([\n\u001b[0m\u001b[1;32m      2\u001b[0m                                 \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0;31m# Change size of Image to (224, 224)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGrayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_output_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Makes it 1-dimension channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0;31m# Convert a PIL Image or numpy.ndarray to tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                               \u001b[0;31m# Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Compose' object has no attribute 'Compose'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5wfy8WBdWch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_dataset = trainset\n",
        "\n",
        "# maek train, val, test: 8:1:1\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = int(0.1 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "print(train_size, val_size, test_size)\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBJn4oeOdaBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make custom data_loader\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                         batch_size=16,\n",
        "                         shuffle=True,\n",
        "                         pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=16,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True)\n",
        " \n",
        "test_loader = DataLoader(test_dataset,\n",
        "                        batch_size=16,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True)  # Instead, we recommend using automatic memory pinning (i.e., setting pin_memory=True)\n",
        "                                          #  which enables fast data transfer to CUDA-enabled GPUs\n",
        "\n",
        "# First, insert all test dataset\n",
        "# z_loader: for latent vector extraction\n",
        "z_loader = DataLoader(full_dataset,\n",
        "                        batch_size=9339,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}