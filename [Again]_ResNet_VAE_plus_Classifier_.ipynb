{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Again] ResNet-VAE_plus_Classifier .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNKjTHDf+MwitfNpcfCjx6E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "892ef94e32504b9fa82c808f45dd7c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_da93b1b9e0b64ea399c5e14d2994c297",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f3ca641cddb94bcb800a4362e95a8f60",
              "IPY_MODEL_e98da672ff8d4e5097315ed5ee2a434e"
            ]
          }
        },
        "da93b1b9e0b64ea399c5e14d2994c297": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f3ca641cddb94bcb800a4362e95a8f60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7e099b1d051041aab953f50343e86c4a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fa13ad8fb8e6409cbf96390caff9ce2d"
          }
        },
        "e98da672ff8d4e5097315ed5ee2a434e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_18fbe0541c204c54a187f9bbf1d12f40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [00:01&lt;00:00, 190MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_608e694744b24e229103ed845dae7cb9"
          }
        },
        "7e099b1d051041aab953f50343e86c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fa13ad8fb8e6409cbf96390caff9ce2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18fbe0541c204c54a187f9bbf1d12f40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "608e694744b24e229103ed845dae7cb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Steve-YJ/Colab_Exercise/blob/master/%5BAgain%5D_ResNet_VAE_plus_Classifier_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyu1S97RFUTe",
        "colab_type": "text"
      },
      "source": [
        "# README.ME\n",
        "* From <code>[Again] Re-Start_Training_ResNet-VAE .ipynb</code>\n",
        "* Add CNN-based Clasifier...!!\n",
        "* LEGO~\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHP0jjvZFhfQ",
        "colab_type": "text"
      },
      "source": [
        "✅ Check Point<br>\n",
        "> 1. Add CNN Classifier\n",
        "> 2. Classify Model\n",
        "> 3. 📌📌📌 Save Model's Parameter...! (with Optimizer's Parameters...!)\n",
        "\n",
        "<br>\n",
        "<code>::start:: 20.09.15.Tue pm4:10 ~ </code><br>\n",
        "<code>::Continue:: ~</code><br>\n",
        "<code>::Add:</code><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC9TFykmnfUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "913b7888-3308-4d57-e74a-20e38c536cc6"
      },
      "source": [
        "\"\"\"\n",
        "Can Do so many things... like,\n",
        "1) Just Train CNN to our 3-channel Data\n",
        "2) User pre-trained ResNet-VAE Encoder to Classifiy Malware Families\n",
        "3) Extend Our Research to more...!\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nCan Do so many things... like,\\n1) Just Train CNN to our 3-channel Data\\n2) User pre-trained ResNet-VAE Encoder to Classifiy Malware Families\\n3) Extend Our Research to more...!\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7_BuKPEYXfH",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "< Questions ><br>\n",
        "Q1.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vigexrljzsdN",
        "colab_type": "text"
      },
      "source": [
        "*  Reference: https://www.programcreek.com/python/example/108010/torchvision.models.resnet152"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5L94y80VeP1",
        "colab_type": "text"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGODey7EFOAI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d46b657-b61d-4aaa-cc02-ff79df456a0d"
      },
      "source": [
        "# Mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZttVYB7WlCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuAGvGTZWwIK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "outputId": "0c0d5262-4404-405c-d181-61553c19f056"
      },
      "source": [
        "%cd drive/My\\ Drive/Post_InfoSec_Exps/ResNet-VAE/ResNetVAE-master.zip (Unzipped Files)/ResNetVAE-master\n",
        "! ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Post_InfoSec_Exps/ResNet-VAE/ResNetVAE-master.zip (Unzipped Files)/ResNetVAE-master\n",
            "'01.Tutorial-ResNet-VAE.ipynb의 사본'\n",
            " 01.Tutorial-ResNet-VAE-Recon.ipynb\n",
            " 02.Tutorial-ResNet-VAE-Tunning.ipynb\n",
            " 03.Tutorial-ResNet-VAE-Tunning.ipynb\n",
            "'03.Tutorial-ResNet-VAE-Tunning.ipynb의 사본'\n",
            "'04.Post-01.Tutorial-ResNet-VAE.ipynb사본의 사본'\n",
            "'05.Post-01.Tutorial-ResNet-VAE_Train_Again.ipynb의 사본'\n",
            " 19train_val_plot.png\n",
            " 39train_val_plot.png\n",
            " 59train_val_plot.png\n",
            " Again_ResNet-VAE_Exp01\n",
            "'[Again] ResNet-VAE_plus_Classifier .ipynb'\n",
            "'[Again] ResNet-VAE_plus_Classifier .ipynb의 사본'\n",
            "'[Again] Re-Start_Training_ResNet-VAE .ipynb'\n",
            " fig\n",
            " generated_Malimg.png\n",
            " modules.py\n",
            " plot_latent.ipynb\n",
            " plot_latent_vector\n",
            " plot_train_test_loss\n",
            "'[Post_Exp]04-2.ResNet-VAE_Train_Again.ipynb'\n",
            "'[Post_Exp]04-2.ResNet-VAE_Train_Again.ipynb의 사본'\n",
            "'[Post_Exp]04-3.ResNet-VAE_Train_Again.ipynb'\n",
            "'[Post_Exp]04-3.ResNet-VAE_Train_Again.ipynb의 사본'\n",
            "'[Post_Exp]05-2.ResNet-VAE_Reduce_lr.ipynb'\n",
            " __pycache__\n",
            " README.md\n",
            " recon_sampling\n",
            " reconstruction_Malimg.png\n",
            " ResNetVAE_cifar10.py\n",
            " ResNetVAE_FACE.py\n",
            " ResNetVAE_MNIST.py\n",
            " ResNetVAE_reconstruction.ipynb\n",
            " results_Malimg\n",
            " results_Malimg_Exp4\n",
            " results_Malimg_Exp4_3\n",
            " results_ResNet-VAE_Exp01\n",
            " results_ResNet-VAE_Exp01-Classification_Test\n",
            " train_test_loss_plot.png\n",
            " train_val_plot.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YmzTtwbW7nl",
        "colab_type": "text"
      },
      "source": [
        " our working directory results should be saved in 'Again_ResNet-VAE_Exp01'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObxrqpqMVg-x",
        "colab_type": "text"
      },
      "source": [
        "## 01. Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LO1BCfgVh9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "91aecdc1-7b8f-4793-c545-917cc9b205e3"
      },
      "source": [
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# save single numpy array\n",
        "from tempfile import TemporaryFile\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import torch\n",
        "import torch.utils.data  # torch.utils.data\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torchvision \n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# load modules\n",
        "from torchvision import models\n",
        "from modules import *"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG2NjnRqVi7s",
        "colab_type": "text"
      },
      "source": [
        "## 02. Data Preparation\n",
        "\n",
        "* Make Custom Dataset\n",
        "* Make Custom DataLoader\n",
        "* 📌 edit Train_Val_Test Split\n",
        "    * From <code>Train_Test Split</code> => <code>Train_Val_Test Split</code> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtOGbYZ9ZcaG",
        "colab_type": "text"
      },
      "source": [
        "### 📍 < Notice ><br>\n",
        "* ResNet-VAE 모델을 훈련시킬 때는 데이터셋을 train, test만 준비한다.(Validation set 없이!)<br>\n",
        "이는 일반적인 VAE Model Tutorial의 학습방식을 따른다. <code>-20.09.08.Tue. pm3:00-</code>\n",
        "---\n",
        "* ResNet-VAE Model 이후 Classifier 학습\n",
        "    * Malware Family Claassification을 위해 Dataset을 다시 구성한다.\n",
        "    * Train, Test Set이외에도 Validation Set을 구성할 수 있도록 하자...!\n",
        "    * Reference: Stand-DL  <code>-20.09.15.Tue-</code>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAIvvzgqVlnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor()])  # Composes several transforms together.\n",
        "\n",
        "# make custom dataset\n",
        "trainset = torchvision.datasets.ImageFolder(root='../../../../InformationSecurity_Summer/malimg',\n",
        "                                            transform=transforms)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O6Dfsmfbeoa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "3065c607-5d2f-4dce-af9a-768e0fc463b1"
      },
      "source": [
        "classes = trainset.classes\n",
        "classes"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Adialer.C',\n",
              " 'Agent.FYI',\n",
              " 'Allaple.A',\n",
              " 'Allaple.L',\n",
              " 'Alueron.gen!J',\n",
              " 'Autorun.K',\n",
              " 'C2LOP.P',\n",
              " 'C2LOP.gen!g',\n",
              " 'Dialplatform.B',\n",
              " 'Dontovo.A',\n",
              " 'Fakerean',\n",
              " 'Instantaccess',\n",
              " 'Lolyda.AA1',\n",
              " 'Lolyda.AA2',\n",
              " 'Lolyda.AA3',\n",
              " 'Lolyda.AT',\n",
              " 'Malex.gen!J',\n",
              " 'Obfuscator.AD',\n",
              " 'Rbot!gen',\n",
              " 'Skintrim.N',\n",
              " 'Swizzor.gen!E',\n",
              " 'Swizzor.gen!I',\n",
              " 'VB.AT',\n",
              " 'Wintrim.BX',\n",
              " 'Yuner.A']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPMemVTjbkPX",
        "colab_type": "text"
      },
      "source": [
        "Split Train Data to Train, Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z84JNpgWboc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1d74acdb-8abd-4407-9634-e97553538c28"
      },
      "source": [
        "full_dataset = trainset\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = int(0.1 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "print('print train_size, val_size, test_size: ', train_size, val_size, test_size)\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])  # Randomly split a dataset into non-overlapping new datasets of given lengths. Optionally fix the generator for reproducible results, e.g.:\n",
        "print('print train_dataset, val_dataset, test_dataset: ', len(train_dataset), len(val_dataset), len(test_dataset))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "print train_size, val_size, test_size:  7471 933 935\n",
            "print train_dataset, val_dataset, test_dataset:  7471 933 935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bmFNAWlbjpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=16,  # 16 to args.batch_size\n",
        "                                           shuffle=True,\n",
        "                                           pin_memory=True)\n",
        "valid_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                           batch_size=16,\n",
        "                                           shuffle=True,\n",
        "                                           pin_memory=True)  # (i.e., setting pin_memory=True)\n",
        "                                                             #  which enables fast data transfer to CUDA-enabled GPUs\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                           batch_size=16,\n",
        "                                           shuffle=True,\n",
        "                                           pin_memory=True) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9eC3rSSdeu_",
        "colab_type": "text"
      },
      "source": [
        "3-channel Image 출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjS2jwTLdgds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    np_img = img.numpy()\n",
        "\n",
        "    plt.imshow(np.transpose(np_img, (1, 2, 0)))  # Convert (C, W, H) to (W, H, C)\n",
        "\n",
        "    print(np_img.shape)  # np_img shape\n",
        "    print((np.transpose(np_img, (1, 2, 0))).shape)  # transposed shape "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh144ur6dx9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "251bf25b-626d-4e51-a03b-ee6838ed5234"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "print(labels)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 2,  2,  9,  2,  4,  3,  2, 24,  2,  3,  3,  2,  4,  2, 21, 11])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-McJWBIRdyO8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "996c0008-1084-4862-b874-933732b94fc4"
      },
      "source": [
        "print(images.shape)\n",
        "imshow(torchvision.utils.make_grid(images, nrow=4))\n",
        "print(images.shape)\n",
        "print((torchvision.utils.make_grid(images)).shape)\n",
        "print(\"\".join(\"%5s \"%classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 3, 224, 224])\n",
            "(3, 906, 906)\n",
            "(906, 906, 3)\n",
            "torch.Size([16, 3, 224, 224])\n",
            "torch.Size([3, 454, 1810])\n",
            "Allaple.A Allaple.A Dontovo.A Allaple.A \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGrfMu6cx0Hw",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVH33sgwxyxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test ResNet Classifier to Malimg 3-Channel Data"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pOQCqNosbon",
        "colab_type": "text"
      },
      "source": [
        "## 03. Model Architecture\n",
        "* <code>pre-trained ResNet-152</code>\n",
        "* <code>Transfer Learning</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6IF7Bdyx7HU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reference: https://www.programcreek.com/python/example/108010/torchvision.models.resnet152\n",
        "\n",
        "def get_pretrained_resnet(new_fc_dim=None):\n",
        "    \"\"\"\n",
        "    Fetches a pretrained resnet model (downloading if necessary) and chops off the top linear\n",
        "    layer. If new_fc_dim isn't None, then a new linear layer is added.\n",
        "    :param new_fc_dim: \n",
        "    :return: \n",
        "    \"\"\"\n",
        "\n",
        "    # resnet152 = models.resnet152(pretrained=True, progress=True)\n",
        "    resnet152 = models.resnet152(pretrained=True)\n",
        "    # del resnet152.fc\n",
        "\n",
        "    # num_ftrs\n",
        "    # Reference: https://tutorials.pytorch.kr/beginner/transfer_learning_tutorial.html\n",
        "    num_ftrs = resnet152.fc.in_features\n",
        "\n",
        "    if new_fc_dim is not None:\n",
        "        # resnet152.fc = nn.Linear(ENCODING_SIZE, new_fc_dim)\n",
        "        \n",
        "        resnet152.fc = nn.Linear(num_ftrs, new_fc_dim)\n",
        "\n",
        "        # _init_fc(resnet152.fc)\n",
        "    else:\n",
        "        \n",
        "        resnet152.fc = lambda x:x\n",
        "\n",
        "    return resnet152"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id_BKtZuyj3W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "892ef94e32504b9fa82c808f45dd7c34",
            "da93b1b9e0b64ea399c5e14d2994c297",
            "f3ca641cddb94bcb800a4362e95a8f60",
            "e98da672ff8d4e5097315ed5ee2a434e",
            "7e099b1d051041aab953f50343e86c4a",
            "fa13ad8fb8e6409cbf96390caff9ce2d",
            "18fbe0541c204c54a187f9bbf1d12f40",
            "608e694744b24e229103ed845dae7cb9"
          ]
        },
        "outputId": "7f1d3230-96db-4d95-ff47-dca2445db9fe"
      },
      "source": [
        "resnet152 = get_pretrained_resnet(25)  # load pre-trained ResNet152\n",
        "resnet152"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "892ef94e32504b9fa82c808f45dd7c34",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (6): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (7): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (6): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (7): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (8): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (9): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (10): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (11): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (12): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (13): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (14): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (15): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (16): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (17): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (18): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (19): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (20): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (21): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (22): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (23): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (24): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (25): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (26): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (27): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (28): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (29): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (30): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (31): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (32): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (33): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (34): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (35): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=25, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh0dA2eMBT-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet152 = resnet152.to(device = ('cuda:0' if torch.cuda.is_available() else 'cpu'))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(resnet152.parameters(), lr=0.001)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81_9gs2rs28K",
        "colab_type": "text"
      },
      "source": [
        "* Change\n",
        "    * 기존 함수 코드에서 실행 코드로 변경해보자...!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVy4vK1HtSAW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc60a2d3-e049-4503-dccc-dbe3abac0f29"
      },
      "source": [
        "import argparse\n",
        "\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "args.net = resnet152\n",
        "args.criterion = criterion\n",
        "args.optim = optimizer\n",
        "\n",
        "args.train_loader = train_loader\n",
        "args.val_loader = valid_loader\n",
        "args.test_loader = test_loader\n",
        "\n",
        "# args.n_layer = 5\n",
        "# args.in_dim = 3072\n",
        "# args.out_dim = 10\n",
        "# args.hid_dim = 100\n",
        "# args.act = 'relu'\n",
        "\n",
        "args.lr = 0.001\n",
        "args.mm = 0.9\n",
        "args.epoch = 100\n",
        "\n",
        "\n",
        "print(args)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(criterion=CrossEntropyLoss(), epoch=100, lr=0.001, mm=0.9, net=ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (7): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (7): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (12): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (13): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (14): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (15): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (16): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (17): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (18): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (19): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (20): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (21): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (22): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (23): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (24): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (25): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (26): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (27): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (28): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (29): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (30): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (31): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (32): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (33): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (34): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (35): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=25, bias=True)\n",
            "), optim=Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            "), test_loader=<torch.utils.data.dataloader.DataLoader object at 0x7f128e9ac6d8>, train_loader=<torch.utils.data.dataloader.DataLoader object at 0x7f128e9ac550>, val_loader=<torch.utils.data.dataloader.DataLoader object at 0x7f128e9acc88>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_uteUw8YDPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_model_path = './results_ResNet-VAE_Exp01-Classification_Test'  "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNKlBGN6_jhY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d8a4332-cb75-4630-a67d-fa10a10030bf"
      },
      "source": [
        "# Reference: https://github.com/Steve-YJ/Exp-Standalone-DeepLearning/blob/master/%5BPractice%5D_Cifar10.ipynb\n",
        "\n",
        "net = args.net\n",
        "# criterion = criterion\n",
        "# optimizer = optimizer\n",
        "\n",
        "list_epoch = []\n",
        "list_train_loss = []\n",
        "list_val_loss = []\n",
        "list_test_acc = []\n",
        "list_acc_epoch = []\n",
        "\n",
        "for epoch in range(args.epoch):  # loop over the dataset multiple itmes\n",
        "\n",
        "    # ===== Train ===== #\n",
        "    net.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(args.train_loader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)  # input 값의 shape이 맞는지 확인을 했는가? -20.09.16.Wed- am11:10...\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        train_loss += loss.item()\n",
        "        if i % 100 == 99:  # print every 2000 mini-batches => print every 2 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch+1, i+1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "    # append list_values\n",
        "    list_epoch.append(epoch)\n",
        "    list_train_loss.append(train_loss)\n",
        "    \n",
        "    # save Pytorch models of best record\n",
        "    torch.save(net.state_dict(), os.path.join(save_model_path, 'model_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
        "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
        "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
        "    # ===== Validation ===== #\n",
        "    net.eval()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in args.val_loader:\n",
        "            images, labels = data\n",
        "            # images = images.view(-1, 3072)\n",
        "\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            '''\n",
        "            What this code mean?\n",
        "            '''\n",
        "            _, predicted = torch.max(outputs.data, 1)  # return: values, indices\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(args.val_loader)\n",
        "        val_acc = 100 * correct / total\n",
        "    list_val_loss.append(val_loss)\n",
        "    print('Epoch {}, Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(epoch, train_loss, val_loss, val_acc))\n",
        "    \n",
        "    # ===== Evaluation ===== #\n",
        "    net.eval()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in args.test_loader:\n",
        "            images, labels = data\n",
        "            # images = images.view(-1, 3072)\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "        print('Epoch {}, Test Acc: {}'.format(epoch, test_acc))\n",
        "    list_test_acc.append(test_acc)\n",
        "    list_acc_epoch.append(epoch)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 3.754\n",
            "[1,   200] loss: 4.391\n",
            "[1,   300] loss: 2.293\n",
            "[1,   400] loss: 2.132\n",
            "Epoch 1 model saved!\n",
            "Epoch 0, Train Loss: 1382.0751917362213, Val Loss: 365.4843326604973, Val Acc: 32.154340836012864\n",
            "Epoch 0, Test Acc: 30.37433155080214\n",
            "[2,   100] loss: 2.042\n",
            "[2,   200] loss: 2.011\n",
            "[2,   300] loss: 1.747\n",
            "[2,   400] loss: 1.568\n",
            "Epoch 2 model saved!\n",
            "Epoch 1, Train Loss: 835.6926978230476, Val Loss: 2.0464935252221963, Val Acc: 66.34512325830654\n",
            "Epoch 1, Test Acc: 64.06417112299465\n",
            "[3,   100] loss: 1.478\n",
            "[3,   200] loss: 1.639\n",
            "[3,   300] loss: 1.509\n",
            "[3,   400] loss: 1.453\n",
            "Epoch 3 model saved!\n",
            "Epoch 2, Train Loss: 700.8405755162239, Val Loss: 1.7328343876337602, Val Acc: 47.37406216505895\n",
            "Epoch 2, Test Acc: 50.160427807486634\n",
            "[4,   100] loss: 1.303\n",
            "[4,   200] loss: 1.079\n",
            "[4,   300] loss: 0.971\n",
            "[4,   400] loss: 0.870\n",
            "Epoch 4 model saved!\n",
            "Epoch 3, Train Loss: 476.15960267186165, Val Loss: 0.6133948183160717, Val Acc: 81.56484458735262\n",
            "Epoch 3, Test Acc: 81.71122994652407\n",
            "[5,   100] loss: 0.721\n",
            "[5,   200] loss: 0.633\n",
            "[5,   300] loss: 0.735\n",
            "[5,   400] loss: 0.733\n",
            "Epoch 5 model saved!\n",
            "Epoch 4, Train Loss: 334.0526153370738, Val Loss: 1.1611892904265453, Val Acc: 64.63022508038586\n",
            "Epoch 4, Test Acc: 64.91978609625669\n",
            "[6,   100] loss: 0.609\n",
            "[6,   200] loss: 0.591\n",
            "[6,   300] loss: 0.626\n",
            "[6,   400] loss: 0.766\n",
            "Epoch 6 model saved!\n",
            "Epoch 5, Train Loss: 309.89582883939147, Val Loss: 0.7141890297249213, Val Acc: 83.92282958199357\n",
            "Epoch 5, Test Acc: 83.9572192513369\n",
            "[7,   100] loss: 0.520\n",
            "[7,   200] loss: 0.607\n",
            "[7,   300] loss: 0.503\n",
            "[7,   400] loss: 0.523\n",
            "Epoch 7 model saved!\n",
            "Epoch 6, Train Loss: 247.22110400907695, Val Loss: 0.6941603419639296, Val Acc: 82.7438370846731\n",
            "Epoch 6, Test Acc: 84.59893048128342\n",
            "[8,   100] loss: 0.461\n",
            "[8,   200] loss: 0.466\n",
            "[8,   300] loss: 0.449\n",
            "[8,   400] loss: 0.435\n",
            "Epoch 8 model saved!\n",
            "Epoch 7, Train Loss: 209.62262956611812, Val Loss: 0.4605996013319088, Val Acc: 88.21007502679528\n",
            "Epoch 7, Test Acc: 89.0909090909091\n",
            "[9,   100] loss: 0.334\n",
            "[9,   200] loss: 0.301\n",
            "[9,   300] loss: 0.381\n",
            "[9,   400] loss: 0.348\n",
            "Epoch 9 model saved!\n",
            "Epoch 8, Train Loss: 155.4629283491522, Val Loss: 0.4134611624087823, Val Acc: 86.38799571275456\n",
            "Epoch 8, Test Acc: 88.23529411764706\n",
            "[10,   100] loss: 0.271\n",
            "[10,   200] loss: 0.288\n",
            "[10,   300] loss: 0.286\n",
            "[10,   400] loss: 0.333\n",
            "Epoch 10 model saved!\n",
            "Epoch 9, Train Loss: 139.32107853051275, Val Loss: 0.26781141716923756, Val Acc: 92.92604501607717\n",
            "Epoch 9, Test Acc: 92.29946524064171\n",
            "[11,   100] loss: 0.256\n",
            "[11,   200] loss: 0.236\n",
            "[11,   300] loss: 0.299\n",
            "[11,   400] loss: 0.280\n",
            "Epoch 11 model saved!\n",
            "Epoch 10, Train Loss: 126.56886480608955, Val Loss: 0.2517533464578249, Val Acc: 93.14040728831726\n",
            "Epoch 10, Test Acc: 92.94117647058823\n",
            "[12,   100] loss: 0.249\n",
            "[12,   200] loss: 0.349\n",
            "[12,   300] loss: 0.326\n",
            "[12,   400] loss: 0.422\n",
            "Epoch 12 model saved!\n",
            "Epoch 11, Train Loss: 165.81542020989582, Val Loss: 0.5053414713780758, Val Acc: 87.35262593783494\n",
            "Epoch 11, Test Acc: 88.66310160427807\n",
            "[13,   100] loss: 0.355\n",
            "[13,   200] loss: 0.452\n",
            "[13,   300] loss: 0.353\n",
            "[13,   400] loss: 0.447\n",
            "Epoch 13 model saved!\n",
            "Epoch 12, Train Loss: 186.7882877368247, Val Loss: 1.1349617936095948, Val Acc: 88.53161843515541\n",
            "Epoch 12, Test Acc: 87.80748663101605\n",
            "[14,   100] loss: 0.260\n",
            "[14,   200] loss: 0.219\n",
            "[14,   300] loss: 0.211\n",
            "[14,   400] loss: 0.243\n",
            "Epoch 14 model saved!\n",
            "Epoch 13, Train Loss: 103.56468104198575, Val Loss: 0.19799218818513772, Val Acc: 93.14040728831726\n",
            "Epoch 13, Test Acc: 93.2620320855615\n",
            "[15,   100] loss: 0.162\n",
            "[15,   200] loss: 0.213\n",
            "[15,   300] loss: 0.255\n",
            "[15,   400] loss: 0.226\n",
            "Epoch 15 model saved!\n",
            "Epoch 14, Train Loss: 98.98416827665642, Val Loss: 0.15210437835430948, Val Acc: 94.64094319399786\n",
            "Epoch 14, Test Acc: 95.18716577540107\n",
            "[16,   100] loss: 0.284\n",
            "[16,   200] loss: 0.292\n",
            "[16,   300] loss: 0.307\n",
            "[16,   400] loss: 0.413\n",
            "Epoch 16 model saved!\n",
            "Epoch 15, Train Loss: 150.11663045268506, Val Loss: 2.6823700051754713, Val Acc: 83.06538049303323\n",
            "Epoch 15, Test Acc: 82.67379679144385\n",
            "[17,   100] loss: 0.362\n",
            "[17,   200] loss: 0.268\n",
            "[17,   300] loss: 0.210\n",
            "[17,   400] loss: 0.208\n",
            "Epoch 17 model saved!\n",
            "Epoch 16, Train Loss: 117.42892044084147, Val Loss: 0.1300604017767108, Val Acc: 95.39121114683816\n",
            "Epoch 16, Test Acc: 94.6524064171123\n",
            "[18,   100] loss: 0.190\n",
            "[18,   200] loss: 0.189\n",
            "[18,   300] loss: 0.245\n",
            "[18,   400] loss: 0.172\n",
            "Epoch 18 model saved!\n",
            "Epoch 17, Train Loss: 93.67019312200136, Val Loss: 0.19301904548527832, Val Acc: 93.14040728831726\n",
            "Epoch 17, Test Acc: 92.72727272727273\n",
            "[19,   100] loss: 0.216\n",
            "[19,   200] loss: 0.159\n",
            "[19,   300] loss: 0.146\n",
            "[19,   400] loss: 0.181\n",
            "Epoch 19 model saved!\n",
            "Epoch 18, Train Loss: 83.80687524657696, Val Loss: 0.13198720958573207, Val Acc: 96.14147909967846\n",
            "Epoch 18, Test Acc: 95.50802139037434\n",
            "[20,   100] loss: 0.134\n",
            "[20,   200] loss: 0.124\n",
            "[20,   300] loss: 0.141\n",
            "[20,   400] loss: 0.175\n",
            "Epoch 20 model saved!\n",
            "Epoch 19, Train Loss: 68.24021366424859, Val Loss: 0.17654747184823757, Val Acc: 94.31939978563773\n",
            "Epoch 19, Test Acc: 94.11764705882354\n",
            "[21,   100] loss: 0.129\n",
            "[21,   200] loss: 0.142\n",
            "[21,   300] loss: 0.154\n",
            "[21,   400] loss: 0.146\n",
            "Epoch 21 model saved!\n",
            "Epoch 20, Train Loss: 70.85911151702749, Val Loss: 0.3563491234839973, Val Acc: 87.45980707395498\n",
            "Epoch 20, Test Acc: 88.55614973262033\n",
            "[22,   100] loss: 0.127\n",
            "[22,   200] loss: 0.163\n",
            "[22,   300] loss: 0.238\n",
            "[22,   400] loss: 0.266\n",
            "Epoch 22 model saved!\n",
            "Epoch 21, Train Loss: 102.5314184386807, Val Loss: 0.66468954736651, Val Acc: 89.17470525187566\n",
            "Epoch 21, Test Acc: 87.9144385026738\n",
            "[23,   100] loss: 0.319\n",
            "[23,   200] loss: 0.599\n",
            "[23,   300] loss: 0.531\n",
            "[23,   400] loss: 0.469\n",
            "Epoch 23 model saved!\n",
            "Epoch 22, Train Loss: 208.88491539470851, Val Loss: 2.517184071100743, Val Acc: 87.78135048231512\n",
            "Epoch 22, Test Acc: 87.27272727272727\n",
            "[24,   100] loss: 0.189\n",
            "[24,   200] loss: 0.217\n",
            "[24,   300] loss: 0.201\n",
            "[24,   400] loss: 0.224\n",
            "Epoch 24 model saved!\n",
            "Epoch 23, Train Loss: 97.96738019725308, Val Loss: 0.11702344529093954, Val Acc: 96.57020364415862\n",
            "Epoch 23, Test Acc: 95.50802139037434\n",
            "[25,   100] loss: 0.148\n",
            "[25,   200] loss: 0.130\n",
            "[25,   300] loss: 0.140\n",
            "[25,   400] loss: 0.157\n",
            "Epoch 25 model saved!\n",
            "Epoch 24, Train Loss: 67.0833411788335, Val Loss: 0.11099210427087432, Val Acc: 96.46302250803859\n",
            "Epoch 24, Test Acc: 96.0427807486631\n",
            "[26,   100] loss: 0.117\n",
            "[26,   200] loss: 0.154\n",
            "[26,   300] loss: 0.155\n",
            "[26,   400] loss: 0.178\n",
            "Epoch 26 model saved!\n",
            "Epoch 25, Train Loss: 74.16659027842252, Val Loss: 0.16380640450653905, Val Acc: 94.85530546623794\n",
            "Epoch 25, Test Acc: 93.68983957219251\n",
            "[27,   100] loss: 0.154\n",
            "[27,   200] loss: 0.113\n",
            "[27,   300] loss: 0.142\n",
            "[27,   400] loss: 0.177\n",
            "Epoch 27 model saved!\n",
            "Epoch 26, Train Loss: 69.8249946058495, Val Loss: 0.14779737735092166, Val Acc: 96.2486602357985\n",
            "Epoch 26, Test Acc: 94.6524064171123\n",
            "[28,   100] loss: 0.149\n",
            "[28,   200] loss: 0.145\n",
            "[28,   300] loss: 0.171\n",
            "[28,   400] loss: 0.173\n",
            "Epoch 28 model saved!\n",
            "Epoch 27, Train Loss: 75.42082768314867, Val Loss: 0.18067928283648976, Val Acc: 94.53376205787781\n",
            "Epoch 27, Test Acc: 93.36898395721926\n",
            "[29,   100] loss: 0.114\n",
            "[29,   200] loss: 0.139\n",
            "[29,   300] loss: 0.128\n",
            "[29,   400] loss: 0.139\n",
            "Epoch 29 model saved!\n",
            "Epoch 28, Train Loss: 58.96026975318091, Val Loss: 0.09131759582003539, Val Acc: 96.78456591639872\n",
            "Epoch 28, Test Acc: 96.47058823529412\n",
            "[30,   100] loss: 0.102\n",
            "[30,   200] loss: 0.102\n",
            "[30,   300] loss: 0.098\n",
            "[30,   400] loss: 0.133\n",
            "Epoch 30 model saved!\n",
            "Epoch 29, Train Loss: 52.661485013217316, Val Loss: 0.11460188930751598, Val Acc: 95.92711682743837\n",
            "Epoch 29, Test Acc: 95.93582887700535\n",
            "[31,   100] loss: 0.091\n",
            "[31,   200] loss: 0.127\n",
            "[31,   300] loss: 0.131\n",
            "[31,   400] loss: 0.166\n",
            "Epoch 31 model saved!\n",
            "Epoch 30, Train Loss: 61.95811633484118, Val Loss: 0.1450857725665289, Val Acc: 96.03429796355842\n",
            "Epoch 30, Test Acc: 94.43850267379679\n",
            "[32,   100] loss: 0.121\n",
            "[32,   200] loss: 0.118\n",
            "[32,   300] loss: 0.123\n",
            "[32,   400] loss: 0.125\n",
            "Epoch 32 model saved!\n",
            "Epoch 31, Train Loss: 56.480734586832114, Val Loss: 0.15280978850291754, Val Acc: 95.92711682743837\n",
            "Epoch 31, Test Acc: 94.97326203208556\n",
            "[33,   100] loss: 0.113\n",
            "[33,   200] loss: 0.146\n",
            "[33,   300] loss: 0.151\n",
            "[33,   400] loss: 0.125\n",
            "Epoch 33 model saved!\n",
            "Epoch 32, Train Loss: 59.28405973521876, Val Loss: 0.10507518607112816, Val Acc: 96.2486602357985\n",
            "Epoch 32, Test Acc: 94.54545454545455\n",
            "[34,   100] loss: 0.088\n",
            "[34,   200] loss: 0.078\n",
            "[34,   300] loss: 0.127\n",
            "[34,   400] loss: 0.135\n",
            "Epoch 34 model saved!\n",
            "Epoch 33, Train Loss: 49.18983589517302, Val Loss: 0.2100576893189701, Val Acc: 91.63987138263666\n",
            "Epoch 33, Test Acc: 89.62566844919786\n",
            "[35,   100] loss: 0.133\n",
            "[35,   200] loss: 0.204\n",
            "[35,   300] loss: 0.198\n",
            "[35,   400] loss: 0.221\n",
            "Epoch 35 model saved!\n",
            "Epoch 34, Train Loss: 85.7074898097344, Val Loss: 0.2139760713108768, Val Acc: 95.60557341907824\n",
            "Epoch 34, Test Acc: 94.75935828877006\n",
            "[36,   100] loss: 0.132\n",
            "[36,   200] loss: 0.161\n",
            "[36,   300] loss: 0.152\n",
            "[36,   400] loss: 0.201\n",
            "Epoch 36 model saved!\n",
            "Epoch 35, Train Loss: 74.58465370786143, Val Loss: 0.14772894037680273, Val Acc: 96.89174705251875\n",
            "Epoch 35, Test Acc: 95.29411764705883\n",
            "[37,   100] loss: 0.131\n",
            "[37,   200] loss: 0.172\n",
            "[37,   300] loss: 0.167\n",
            "[37,   400] loss: 0.125\n",
            "Epoch 37 model saved!\n",
            "Epoch 36, Train Loss: 71.43462489458034, Val Loss: 0.1227626160025928, Val Acc: 96.46302250803859\n",
            "Epoch 36, Test Acc: 95.08021390374331\n",
            "[38,   100] loss: 0.139\n",
            "[38,   200] loss: 0.078\n",
            "[38,   300] loss: 0.121\n",
            "[38,   400] loss: 0.110\n",
            "Epoch 38 model saved!\n",
            "Epoch 37, Train Loss: 52.26281636798467, Val Loss: 0.10458274415750411, Val Acc: 96.67738478027867\n",
            "Epoch 37, Test Acc: 95.61497326203208\n",
            "[39,   100] loss: 0.096\n",
            "[39,   200] loss: 0.145\n",
            "[39,   300] loss: 0.090\n",
            "[39,   400] loss: 0.108\n",
            "Epoch 39 model saved!\n",
            "Epoch 38, Train Loss: 50.455027180185425, Val Loss: 0.0940369830638501, Val Acc: 97.32047159699893\n",
            "Epoch 38, Test Acc: 96.36363636363636\n",
            "[40,   100] loss: 0.081\n",
            "[40,   200] loss: 0.090\n",
            "[40,   300] loss: 0.126\n",
            "[40,   400] loss: 0.120\n",
            "Epoch 40 model saved!\n",
            "Epoch 39, Train Loss: 48.45271329692332, Val Loss: 0.10292804021471164, Val Acc: 96.89174705251875\n",
            "Epoch 39, Test Acc: 95.93582887700535\n",
            "[41,   100] loss: 0.095\n",
            "[41,   200] loss: 0.070\n",
            "[41,   300] loss: 0.083\n",
            "[41,   400] loss: 0.110\n",
            "Epoch 41 model saved!\n",
            "Epoch 40, Train Loss: 46.12094787483511, Val Loss: 0.10551191850252015, Val Acc: 96.9989281886388\n",
            "Epoch 40, Test Acc: 96.14973262032086\n",
            "[42,   100] loss: 0.077\n",
            "[42,   200] loss: 0.083\n",
            "[42,   300] loss: 0.090\n",
            "[42,   400] loss: 0.073\n",
            "Epoch 42 model saved!\n",
            "Epoch 41, Train Loss: 38.912788555149746, Val Loss: 0.10598638876843448, Val Acc: 96.46302250803859\n",
            "Epoch 41, Test Acc: 95.93582887700535\n",
            "[43,   100] loss: 0.114\n",
            "[43,   200] loss: 0.110\n",
            "[43,   300] loss: 0.138\n",
            "[43,   400] loss: 0.134\n",
            "Epoch 43 model saved!\n",
            "Epoch 42, Train Loss: 58.979404165103915, Val Loss: 0.18564235723687936, Val Acc: 94.42658092175778\n",
            "Epoch 42, Test Acc: 91.97860962566845\n",
            "[44,   100] loss: 0.144\n",
            "[44,   200] loss: 0.137\n",
            "[44,   300] loss: 0.145\n",
            "[44,   400] loss: 0.144\n",
            "Epoch 44 model saved!\n",
            "Epoch 43, Train Loss: 67.16145050685736, Val Loss: 0.13341774333044454, Val Acc: 95.92711682743837\n",
            "Epoch 43, Test Acc: 95.72192513368984\n",
            "[45,   100] loss: 0.129\n",
            "[45,   200] loss: 0.115\n",
            "[45,   300] loss: 0.106\n",
            "[45,   400] loss: 0.116\n",
            "Epoch 45 model saved!\n",
            "Epoch 44, Train Loss: 52.98142469445884, Val Loss: 0.10640747798751655, Val Acc: 97.10610932475885\n",
            "Epoch 44, Test Acc: 96.47058823529412\n",
            "[46,   100] loss: 0.099\n",
            "[46,   200] loss: 0.095\n",
            "[46,   300] loss: 0.102\n",
            "[46,   400] loss: 0.090\n",
            "Epoch 46 model saved!\n",
            "Epoch 45, Train Loss: 46.08630061720032, Val Loss: 0.11803232145609661, Val Acc: 96.57020364415862\n",
            "Epoch 45, Test Acc: 96.14973262032086\n",
            "[47,   100] loss: 0.082\n",
            "[47,   200] loss: 0.073\n",
            "[47,   300] loss: 0.069\n",
            "[47,   400] loss: 0.102\n",
            "Epoch 47 model saved!\n",
            "Epoch 46, Train Loss: 42.12879311137658, Val Loss: 0.09227973733874732, Val Acc: 97.96355841371918\n",
            "Epoch 46, Test Acc: 96.68449197860963\n",
            "[48,   100] loss: 0.078\n",
            "[48,   200] loss: 0.074\n",
            "[48,   300] loss: 0.092\n",
            "[48,   400] loss: 0.086\n",
            "Epoch 48 model saved!\n",
            "Epoch 47, Train Loss: 39.87390816980951, Val Loss: 0.09515140386441598, Val Acc: 97.7491961414791\n",
            "Epoch 47, Test Acc: 97.00534759358288\n",
            "[49,   100] loss: 0.074\n",
            "[49,   200] loss: 0.082\n",
            "[49,   300] loss: 0.113\n",
            "[49,   400] loss: 0.095\n",
            "Epoch 49 model saved!\n",
            "Epoch 48, Train Loss: 43.65262496530522, Val Loss: 0.13572420418914605, Val Acc: 96.46302250803859\n",
            "Epoch 48, Test Acc: 95.61497326203208\n",
            "[50,   100] loss: 0.118\n",
            "[50,   200] loss: 0.091\n",
            "[50,   300] loss: 0.094\n",
            "[50,   400] loss: 0.106\n",
            "Epoch 50 model saved!\n",
            "Epoch 49, Train Loss: 46.356438197259195, Val Loss: 0.08705892693510346, Val Acc: 97.42765273311898\n",
            "Epoch 49, Test Acc: 96.2566844919786\n",
            "[51,   100] loss: 0.092\n",
            "[51,   200] loss: 0.089\n",
            "[51,   300] loss: 0.085\n",
            "[51,   400] loss: 0.076\n",
            "Epoch 51 model saved!\n",
            "Epoch 50, Train Loss: 39.57405831613141, Val Loss: 0.0878038369626018, Val Acc: 96.9989281886388\n",
            "Epoch 50, Test Acc: 96.14973262032086\n",
            "[52,   100] loss: 0.071\n",
            "[52,   200] loss: 0.065\n",
            "[52,   300] loss: 0.071\n",
            "[52,   400] loss: 0.096\n",
            "Epoch 52 model saved!\n",
            "Epoch 51, Train Loss: 38.15800277919152, Val Loss: 0.07615348413205307, Val Acc: 97.53483386923901\n",
            "Epoch 51, Test Acc: 96.36363636363636\n",
            "[53,   100] loss: 0.070\n",
            "[53,   200] loss: 0.059\n",
            "[53,   300] loss: 0.082\n",
            "[53,   400] loss: 0.062\n",
            "Epoch 53 model saved!\n",
            "Epoch 52, Train Loss: 31.537439714393486, Val Loss: 0.08923608237240441, Val Acc: 97.21329046087888\n",
            "Epoch 52, Test Acc: 96.68449197860963\n",
            "[54,   100] loss: 0.063\n",
            "[54,   200] loss: 0.077\n",
            "[54,   300] loss: 0.071\n",
            "[54,   400] loss: 0.069\n",
            "Epoch 54 model saved!\n",
            "Epoch 53, Train Loss: 35.89344831328981, Val Loss: 0.06998334817442317, Val Acc: 97.53483386923901\n",
            "Epoch 53, Test Acc: 96.47058823529412\n",
            "[55,   100] loss: 0.080\n",
            "[55,   200] loss: 0.078\n",
            "[55,   300] loss: 0.075\n",
            "[55,   400] loss: 0.070\n",
            "Epoch 55 model saved!\n",
            "Epoch 54, Train Loss: 35.59756925515467, Val Loss: 0.0782337358125961, Val Acc: 97.64201500535906\n",
            "Epoch 54, Test Acc: 96.47058823529412\n",
            "[56,   100] loss: 0.075\n",
            "[56,   200] loss: 0.083\n",
            "[56,   300] loss: 0.109\n",
            "[56,   400] loss: 0.100\n",
            "Epoch 56 model saved!\n",
            "Epoch 55, Train Loss: 43.67700692134031, Val Loss: 0.09553835783114309, Val Acc: 97.21329046087888\n",
            "Epoch 55, Test Acc: 96.57754010695187\n",
            "[57,   100] loss: 0.077\n",
            "[57,   200] loss: 0.068\n",
            "[57,   300] loss: 0.088\n",
            "[57,   400] loss: 0.075\n",
            "Epoch 57 model saved!\n",
            "Epoch 56, Train Loss: 39.503718987274624, Val Loss: 0.09157857517826506, Val Acc: 97.32047159699893\n",
            "Epoch 56, Test Acc: 96.89839572192513\n",
            "[58,   100] loss: 0.090\n",
            "[58,   200] loss: 0.061\n",
            "[58,   300] loss: 0.095\n",
            "[58,   400] loss: 0.094\n",
            "Epoch 58 model saved!\n",
            "Epoch 57, Train Loss: 43.318530419737726, Val Loss: 0.1410919575778357, Val Acc: 95.92711682743837\n",
            "Epoch 57, Test Acc: 95.72192513368984\n",
            "[59,   100] loss: 0.090\n",
            "[59,   200] loss: 0.079\n",
            "[59,   300] loss: 0.112\n",
            "[59,   400] loss: 0.123\n",
            "Epoch 59 model saved!\n",
            "Epoch 58, Train Loss: 45.5241375011513, Val Loss: 0.0887017150289158, Val Acc: 96.78456591639872\n",
            "Epoch 58, Test Acc: 96.36363636363636\n",
            "[60,   100] loss: 0.068\n",
            "[60,   200] loss: 0.053\n",
            "[60,   300] loss: 0.077\n",
            "[60,   400] loss: 0.082\n",
            "Epoch 60 model saved!\n",
            "Epoch 59, Train Loss: 34.12786899099166, Val Loss: 0.09216033790298907, Val Acc: 97.42765273311898\n",
            "Epoch 59, Test Acc: 97.2192513368984\n",
            "[61,   100] loss: 0.072\n",
            "[61,   200] loss: 0.071\n",
            "[61,   300] loss: 0.064\n",
            "[61,   400] loss: 0.114\n",
            "Epoch 61 model saved!\n",
            "Epoch 60, Train Loss: 37.52166199975204, Val Loss: 0.08623354553128593, Val Acc: 97.7491961414791\n",
            "Epoch 60, Test Acc: 96.2566844919786\n",
            "[62,   100] loss: 0.081\n",
            "[62,   200] loss: 0.066\n",
            "[62,   300] loss: 0.075\n",
            "[62,   400] loss: 0.082\n",
            "Epoch 62 model saved!\n",
            "Epoch 61, Train Loss: 34.25030682193665, Val Loss: 0.05738522993602847, Val Acc: 98.17792068595926\n",
            "Epoch 61, Test Acc: 96.2566844919786\n",
            "[63,   100] loss: 0.051\n",
            "[63,   200] loss: 0.070\n",
            "[63,   300] loss: 0.069\n",
            "[63,   400] loss: 0.071\n",
            "Epoch 63 model saved!\n",
            "Epoch 62, Train Loss: 31.658584557462746, Val Loss: 0.1377097951680644, Val Acc: 96.78456591639872\n",
            "Epoch 62, Test Acc: 96.14973262032086\n",
            "[64,   100] loss: 0.075\n",
            "[64,   200] loss: 0.085\n",
            "[64,   300] loss: 0.094\n",
            "[64,   400] loss: 0.077\n",
            "Epoch 64 model saved!\n",
            "Epoch 63, Train Loss: 40.782882389057704, Val Loss: 0.060168681137453196, Val Acc: 98.07073954983923\n",
            "Epoch 63, Test Acc: 96.79144385026738\n",
            "[65,   100] loss: 0.075\n",
            "[65,   200] loss: 0.075\n",
            "[65,   300] loss: 0.063\n",
            "[65,   400] loss: 0.101\n",
            "Epoch 65 model saved!\n",
            "Epoch 64, Train Loss: 34.77717106993964, Val Loss: 0.07703778094073102, Val Acc: 97.64201500535906\n",
            "Epoch 64, Test Acc: 97.2192513368984\n",
            "[66,   100] loss: 0.063\n",
            "[66,   200] loss: 0.060\n",
            "[66,   300] loss: 0.068\n",
            "[66,   400] loss: 0.074\n",
            "Epoch 66 model saved!\n",
            "Epoch 65, Train Loss: 31.634263942152074, Val Loss: 0.08763987680009389, Val Acc: 97.21329046087888\n",
            "Epoch 65, Test Acc: 96.79144385026738\n",
            "[67,   100] loss: 0.063\n",
            "[67,   200] loss: 0.060\n",
            "[67,   300] loss: 0.055\n",
            "[67,   400] loss: 0.066\n",
            "Epoch 67 model saved!\n",
            "Epoch 66, Train Loss: 28.912838640462894, Val Loss: 0.07150950083936458, Val Acc: 97.85637727759914\n",
            "Epoch 66, Test Acc: 96.0427807486631\n",
            "[68,   100] loss: 0.062\n",
            "[68,   200] loss: 0.061\n",
            "[68,   300] loss: 0.071\n",
            "[68,   400] loss: 0.088\n",
            "Epoch 68 model saved!\n",
            "Epoch 67, Train Loss: 34.087707912355654, Val Loss: 0.0972444818258675, Val Acc: 97.10610932475885\n",
            "Epoch 67, Test Acc: 96.79144385026738\n",
            "[69,   100] loss: 0.087\n",
            "[69,   200] loss: 0.072\n",
            "[69,   300] loss: 0.057\n",
            "[69,   400] loss: 0.126\n",
            "Epoch 69 model saved!\n",
            "Epoch 68, Train Loss: 41.64548893247775, Val Loss: 0.10125399793598601, Val Acc: 97.21329046087888\n",
            "Epoch 68, Test Acc: 97.00534759358288\n",
            "[70,   100] loss: 0.069\n",
            "[70,   200] loss: 0.079\n",
            "[70,   300] loss: 0.070\n",
            "[70,   400] loss: 0.069\n",
            "Epoch 70 model saved!\n",
            "Epoch 69, Train Loss: 35.03029885109845, Val Loss: 0.1728041184221455, Val Acc: 95.06966773847803\n",
            "Epoch 69, Test Acc: 95.29411764705883\n",
            "[71,   100] loss: 0.071\n",
            "[71,   200] loss: 0.092\n",
            "[71,   300] loss: 0.080\n",
            "[71,   400] loss: 0.101\n",
            "Epoch 71 model saved!\n",
            "Epoch 70, Train Loss: 41.48132853370387, Val Loss: 0.09654556585305765, Val Acc: 96.9989281886388\n",
            "Epoch 70, Test Acc: 96.47058823529412\n",
            "[72,   100] loss: 0.085\n",
            "[72,   200] loss: 0.092\n",
            "[72,   300] loss: 0.104\n",
            "[72,   400] loss: 0.096\n",
            "Epoch 72 model saved!\n",
            "Epoch 71, Train Loss: 44.43222881461577, Val Loss: 0.10668535701296282, Val Acc: 96.67738478027867\n",
            "Epoch 71, Test Acc: 96.2566844919786\n",
            "[73,   100] loss: 0.070\n",
            "[73,   200] loss: 0.059\n",
            "[73,   300] loss: 0.073\n",
            "[73,   400] loss: 0.067\n",
            "Epoch 73 model saved!\n",
            "Epoch 72, Train Loss: 32.89538280571992, Val Loss: 0.07757420535176607, Val Acc: 97.64201500535906\n",
            "Epoch 72, Test Acc: 97.00534759358288\n",
            "[74,   100] loss: 0.066\n",
            "[74,   200] loss: 0.079\n",
            "[74,   300] loss: 0.081\n",
            "[74,   400] loss: 0.065\n",
            "Epoch 74 model saved!\n",
            "Epoch 73, Train Loss: 33.35777409621937, Val Loss: 0.0650887772591609, Val Acc: 97.53483386923901\n",
            "Epoch 73, Test Acc: 96.79144385026738\n",
            "[75,   100] loss: 0.058\n",
            "[75,   200] loss: 0.045\n",
            "[75,   300] loss: 0.064\n",
            "[75,   400] loss: 0.059\n",
            "Epoch 75 model saved!\n",
            "Epoch 74, Train Loss: 28.593016980603352, Val Loss: 0.10429611521495814, Val Acc: 96.67738478027867\n",
            "Epoch 74, Test Acc: 97.00534759358288\n",
            "[76,   100] loss: 0.079\n",
            "[76,   200] loss: 0.068\n",
            "[76,   300] loss: 0.073\n",
            "[76,   400] loss: 0.092\n",
            "Epoch 76 model saved!\n",
            "Epoch 75, Train Loss: 38.60121573022442, Val Loss: 0.09056957830813898, Val Acc: 97.85637727759914\n",
            "Epoch 75, Test Acc: 96.47058823529412\n",
            "[77,   100] loss: 0.078\n",
            "[77,   200] loss: 0.054\n",
            "[77,   300] loss: 0.065\n",
            "[77,   400] loss: 0.075\n",
            "Epoch 77 model saved!\n",
            "Epoch 76, Train Loss: 31.502722724054934, Val Loss: 0.13121738578869768, Val Acc: 97.10610932475885\n",
            "Epoch 76, Test Acc: 96.47058823529412\n",
            "[78,   100] loss: 0.100\n",
            "[78,   200] loss: 0.042\n",
            "[78,   300] loss: 0.070\n",
            "[78,   400] loss: 0.092\n",
            "Epoch 78 model saved!\n",
            "Epoch 77, Train Loss: 34.70871270590487, Val Loss: 0.07763289334671769, Val Acc: 97.42765273311898\n",
            "Epoch 77, Test Acc: 96.68449197860963\n",
            "[79,   100] loss: 0.061\n",
            "[79,   200] loss: 0.063\n",
            "[79,   300] loss: 0.055\n",
            "[79,   400] loss: 0.055\n",
            "Epoch 79 model saved!\n",
            "Epoch 78, Train Loss: 27.559065306447337, Val Loss: 0.06718944209769226, Val Acc: 97.64201500535906\n",
            "Epoch 78, Test Acc: 97.2192513368984\n",
            "[80,   100] loss: 0.051\n",
            "[80,   200] loss: 0.052\n",
            "[80,   300] loss: 0.096\n",
            "[80,   400] loss: 0.076\n",
            "Epoch 80 model saved!\n",
            "Epoch 79, Train Loss: 32.47816031138541, Val Loss: 0.10494998794520259, Val Acc: 96.89174705251875\n",
            "Epoch 79, Test Acc: 96.47058823529412\n",
            "[81,   100] loss: 0.069\n",
            "[81,   200] loss: 0.109\n",
            "[81,   300] loss: 0.057\n",
            "[81,   400] loss: 0.128\n",
            "Epoch 81 model saved!\n",
            "Epoch 80, Train Loss: 42.963696124838464, Val Loss: 0.10708161994625698, Val Acc: 97.42765273311898\n",
            "Epoch 80, Test Acc: 96.57754010695187\n",
            "[82,   100] loss: 0.076\n",
            "[82,   200] loss: 0.065\n",
            "[82,   300] loss: 0.070\n",
            "[82,   400] loss: 0.103\n",
            "Epoch 82 model saved!\n",
            "Epoch 81, Train Loss: 38.68674564274227, Val Loss: 0.11150494870596928, Val Acc: 97.53483386923901\n",
            "Epoch 81, Test Acc: 96.89839572192513\n",
            "[83,   100] loss: 0.078\n",
            "[83,   200] loss: 0.089\n",
            "[83,   300] loss: 0.076\n",
            "[83,   400] loss: 0.110\n",
            "Epoch 83 model saved!\n",
            "Epoch 82, Train Loss: 42.17667471036748, Val Loss: 0.0886047442834239, Val Acc: 97.21329046087888\n",
            "Epoch 82, Test Acc: 96.14973262032086\n",
            "[84,   100] loss: 0.062\n",
            "[84,   200] loss: 0.069\n",
            "[84,   300] loss: 0.069\n",
            "[84,   400] loss: 0.166\n",
            "Epoch 84 model saved!\n",
            "Epoch 83, Train Loss: 47.776381616410326, Val Loss: 0.07338759629681714, Val Acc: 98.07073954983923\n",
            "Epoch 83, Test Acc: 96.68449197860963\n",
            "[85,   100] loss: 0.084\n",
            "[85,   200] loss: 0.049\n",
            "[85,   300] loss: 0.065\n",
            "[85,   400] loss: 0.051\n",
            "Epoch 85 model saved!\n",
            "Epoch 84, Train Loss: 28.746295061499154, Val Loss: 0.08287189781060822, Val Acc: 97.7491961414791\n",
            "Epoch 84, Test Acc: 96.57754010695187\n",
            "[86,   100] loss: 0.083\n",
            "[86,   200] loss: 0.065\n",
            "[86,   300] loss: 0.069\n",
            "[86,   400] loss: 0.073\n",
            "Epoch 86 model saved!\n",
            "Epoch 85, Train Loss: 34.0854236440173, Val Loss: 0.1390925801600131, Val Acc: 97.10610932475885\n",
            "Epoch 85, Test Acc: 96.68449197860963\n",
            "[87,   100] loss: 0.079\n",
            "[87,   200] loss: 0.066\n",
            "[87,   300] loss: 0.050\n",
            "[87,   400] loss: 0.096\n",
            "Epoch 87 model saved!\n",
            "Epoch 86, Train Loss: 33.42843239450549, Val Loss: 0.12374355760962548, Val Acc: 97.21329046087888\n",
            "Epoch 86, Test Acc: 96.36363636363636\n",
            "[88,   100] loss: 0.058\n",
            "[88,   200] loss: 0.080\n",
            "[88,   300] loss: 0.073\n",
            "[88,   400] loss: 0.085\n",
            "Epoch 88 model saved!\n",
            "Epoch 87, Train Loss: 36.873127295846935, Val Loss: 0.11183920745840346, Val Acc: 96.9989281886388\n",
            "Epoch 87, Test Acc: 96.36363636363636\n",
            "[89,   100] loss: 0.088\n",
            "[89,   200] loss: 0.061\n",
            "[89,   300] loss: 0.078\n",
            "[89,   400] loss: 0.058\n",
            "Epoch 89 model saved!\n",
            "Epoch 88, Train Loss: 33.3675623821956, Val Loss: 0.08661394665831514, Val Acc: 97.96355841371918\n",
            "Epoch 88, Test Acc: 97.54010695187166\n",
            "[90,   100] loss: 0.068\n",
            "[90,   200] loss: 0.061\n",
            "[90,   300] loss: 0.060\n",
            "[90,   400] loss: 0.066\n",
            "Epoch 90 model saved!\n",
            "Epoch 89, Train Loss: 30.694928091965636, Val Loss: 0.061504698481070076, Val Acc: 98.07073954983923\n",
            "Epoch 89, Test Acc: 97.11229946524064\n",
            "[91,   100] loss: 0.054\n",
            "[91,   200] loss: 0.052\n",
            "[91,   300] loss: 0.049\n",
            "[91,   400] loss: 0.045\n",
            "Epoch 91 model saved!\n",
            "Epoch 90, Train Loss: 24.29538564859703, Val Loss: 0.07411313087194665, Val Acc: 97.53483386923901\n",
            "Epoch 90, Test Acc: 97.2192513368984\n",
            "[92,   100] loss: 0.057\n",
            "[92,   200] loss: 0.047\n",
            "[92,   300] loss: 0.050\n",
            "[92,   400] loss: 0.074\n",
            "Epoch 92 model saved!\n",
            "Epoch 91, Train Loss: 27.767197272909904, Val Loss: 0.08354620884075682, Val Acc: 97.85637727759914\n",
            "Epoch 91, Test Acc: 96.57754010695187\n",
            "[93,   100] loss: 0.059\n",
            "[93,   200] loss: 0.073\n",
            "[93,   300] loss: 0.075\n",
            "[93,   400] loss: 0.091\n",
            "Epoch 93 model saved!\n",
            "Epoch 92, Train Loss: 34.79937864748865, Val Loss: 0.0731671434507134, Val Acc: 97.53483386923901\n",
            "Epoch 92, Test Acc: 97.6470588235294\n",
            "[94,   100] loss: 0.058\n",
            "[94,   200] loss: 0.057\n",
            "[94,   300] loss: 0.059\n",
            "[94,   400] loss: 0.066\n",
            "Epoch 94 model saved!\n",
            "Epoch 93, Train Loss: 27.986639449728045, Val Loss: 0.08526998818926733, Val Acc: 97.64201500535906\n",
            "Epoch 93, Test Acc: 96.57754010695187\n",
            "[95,   100] loss: 0.069\n",
            "[95,   200] loss: 0.066\n",
            "[95,   300] loss: 0.067\n",
            "[95,   400] loss: 0.072\n",
            "Epoch 95 model saved!\n",
            "Epoch 94, Train Loss: 33.90802492738612, Val Loss: 0.0908859253341704, Val Acc: 97.64201500535906\n",
            "Epoch 94, Test Acc: 96.36363636363636\n",
            "[96,   100] loss: 0.062\n",
            "[96,   200] loss: 0.068\n",
            "[96,   300] loss: 0.059\n",
            "[96,   400] loss: 0.061\n",
            "Epoch 96 model saved!\n",
            "Epoch 95, Train Loss: 28.80824212901402, Val Loss: 0.15607069805036328, Val Acc: 96.89174705251875\n",
            "Epoch 95, Test Acc: 95.72192513368984\n",
            "[97,   100] loss: 0.114\n",
            "[97,   200] loss: 0.114\n",
            "[97,   300] loss: 0.095\n",
            "[97,   400] loss: 0.137\n",
            "Epoch 97 model saved!\n",
            "Epoch 96, Train Loss: 51.972361376152776, Val Loss: 0.1076499039348981, Val Acc: 97.21329046087888\n",
            "Epoch 96, Test Acc: 96.47058823529412\n",
            "[98,   100] loss: 0.082\n",
            "[98,   200] loss: 0.078\n",
            "[98,   300] loss: 0.084\n",
            "[98,   400] loss: 0.069\n",
            "Epoch 98 model saved!\n",
            "Epoch 97, Train Loss: 35.38811163893297, Val Loss: 0.123996481949789, Val Acc: 96.67738478027867\n",
            "Epoch 97, Test Acc: 96.57754010695187\n",
            "[99,   100] loss: 0.083\n",
            "[99,   200] loss: 0.055\n",
            "[99,   300] loss: 0.057\n",
            "[99,   400] loss: 0.070\n",
            "Epoch 99 model saved!\n",
            "Epoch 98, Train Loss: 30.788928298408134, Val Loss: 0.20794512928861594, Val Acc: 96.67738478027867\n",
            "Epoch 98, Test Acc: 96.57754010695187\n",
            "[100,   100] loss: 0.060\n",
            "[100,   200] loss: 0.069\n",
            "[100,   300] loss: 0.071\n",
            "[100,   400] loss: 0.074\n",
            "Epoch 100 model saved!\n",
            "Epoch 99, Train Loss: 31.41529819153436, Val Loss: 0.09059534793317305, Val Acc: 97.32047159699893\n",
            "Epoch 99, Test Acc: 96.79144385026738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tx87lo0AV3r",
        "colab_type": "text"
      },
      "source": [
        "### Add\n",
        "* 1. epoch, loss, test_acc 등 값 저장 \n",
        "* 2. Graph 출력 및 저장\n",
        "* <code>-20.09.16.Wed. pm12:--</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI7ZylP_A5DO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "bf7d77f5-26c0-4bfb-ec35-ee7798c70c8e"
      },
      "source": [
        "# plot\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "# ===== Loss Function ====== #\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax1.plot(list_epoch, list_train_loss, label='train_loss')\n",
        "ax1.plot(list_epoch, list_val_loss, '--', label='val_loss')\n",
        "ax1.set_xlabel('epoch')\n",
        "ax1.set_ylabel('loss')\n",
        "ax1.grid()\n",
        "ax1.legend()\n",
        "ax1.set_title('epoch vs loss')\n",
        "\n",
        "# ====== Metric Fluctuation ===== #\n",
        "ax2 = fig.add_subplot(1, 2, 2)\n",
        "ax2.plot(list_acc_epoch, list_test_acc, marker='x', label='Accuracy metric')\n",
        "ax2.set_xlabel('epoch')\n",
        "ax2.set_ylabel('Acc')\n",
        "ax2.grid()\n",
        "ax2.legend()\n",
        "ax2.set_title('epoch vs Accuracy')\n",
        "\n",
        "plt.savefig('save_loss_graph_accuracy.png')\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1Swk5O6ljuw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a69ef678-d6f0-41ea-926d-22852bdeed84"
      },
      "source": [
        "max(list_test_acc)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97.6470588235294"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdlv-NRjlrrO",
        "colab_type": "text"
      },
      "source": [
        "* 100 Epoch까지 학습했을 떄, Accuracy 최대 값은 97.647...!\n",
        "* 92Epoch에서 최대값...!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPMyqPj4mMD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "358e2323-2364-4c6f-cafd-7d98c4878c28"
      },
      "source": [
        "list_test_acc[92]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97.6470588235294"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuXogTLCCu9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save loss values\n",
        "# save all train, test results\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# save model\n",
        "save_model_path = './results_ResNet-VAE_Exp01-Classification_Test'  \n",
        "\n",
        "\n",
        "train_losses = list_train_loss\n",
        "val_losses = list_val_loss\n",
        "test_acc = list_test_acc\n",
        "\n",
        "np.save(os.path.join(save_model_path, 'ResNet-152_Training_loss.npy'), train_losses)\n",
        "np.save(os.path.join(save_model_path, 'ResNet-152_Test_loss.npy'), val_losses)\n",
        "np.save(os.path.join(save_model_path, 'ResNet-152_Test_Acc.npy'), test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thfttTjF8-Wm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5dac9343-c821-4dc2-b23b-749c3d6008c6"
      },
      "source": [
        "import argparse\n",
        "\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "args.net = resnet152\n",
        "args.criterion = criterion\n",
        "args.optim = optimizer\n",
        "\n",
        "args.train_loader = train_loader\n",
        "args.val_loader = valid_loader\n",
        "args.test_loader = test_loader\n",
        "\n",
        "# args.n_layer = 5\n",
        "# args.in_dim = 3072\n",
        "# args.out_dim = 10\n",
        "# args.hid_dim = 100\n",
        "# args.act = 'relu'\n",
        "\n",
        "args.lr = 0.001\n",
        "args.mm = 0.9\n",
        "args.epoch = 100\n",
        "\n",
        "print(args.epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB51INQhAxTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7c22bfb7-df40-4f11-da1a-e98adec0038e"
      },
      "source": [
        "experiment(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Train Loss: 985.2862548828125, Val Loss: 2091.101662643885, Val Acc: 34.833869239013936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(985.2862548828125, 2091.101662643885, 34.833869239013936, 31.764705882352942)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHhsevzzx1VG",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXtaJBsMVnki",
        "colab_type": "text"
      },
      "source": [
        "## 03. Model Architecture\n",
        "\n",
        "* argparser\n",
        "* save model parameters\n",
        "* loss function\n",
        "* define hyperthesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gupryt18d2Pk",
        "colab_type": "text"
      },
      "source": [
        "✅ Have to-do <br>\n",
        "> 1. Train ResNet-VAE\n",
        "> 2. 📌 Hyperparameter Optimization >>> argparser...!!\n",
        "> 3. 📌 Save Model's Parameters\n",
        "> 4. 📌 Plot & Save values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmCALY3ogDOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save model\n",
        "save_model_path = './results_ResNet-VAE_Exp01'  # save_model parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57uBwQXKgUsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_mkdir(dir_name):\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.mkdir(dir_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzUcKJYGgivD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    # MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    MSE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return MSE + KLD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzJXvtlnmvUb",
        "colab_type": "text"
      },
      "source": [
        "model from this repository: https://github.com/hsinyilin19/ResNetVAE/blob/master/modules.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diJLO0clmrL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet_VAE(nn.Module):\n",
        "    def __init__(self, fc_hidden1=1024, fc_hidden2=768, drop_p=0.3, CNN_embed_dim=256):\n",
        "        super(ResNet_VAE, self).__init__()\n",
        "\n",
        "        self.fc_hidden1, self.fc_hidden2, self.CNN_embed_dim = fc_hidden1, fc_hidden2, CNN_embed_dim\n",
        "\n",
        "        # CNN architechtures\n",
        "        self.ch1, self.ch2, self.ch3, self.ch4 = 16, 32, 64, 128\n",
        "        self.k1, self.k2, self.k3, self.k4 = (5, 5), (3, 3), (3, 3), (3, 3)      # 2d kernal size\n",
        "        self.s1, self.s2, self.s3, self.s4 = (2, 2), (2, 2), (2, 2), (2, 2)      # 2d strides\n",
        "        self.pd1, self.pd2, self.pd3, self.pd4 = (0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\n",
        "\n",
        "        # encoding components\n",
        "        resnet = models.resnet152(pretrained=True)  # ResNet-VAE -20.09.15.Tue- pm5:00\n",
        "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.fc1 = nn.Linear(resnet.fc.in_features, self.fc_hidden1)\n",
        "        self.bn1 = nn.BatchNorm1d(self.fc_hidden1, momentum=0.01)\n",
        "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
        "        self.bn2 = nn.BatchNorm1d(self.fc_hidden2, momentum=0.01)\n",
        "        # Latent vectors mu and sigma\n",
        "        self.fc3_mu = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)      # output = CNN embedding latent variables\n",
        "        self.fc3_logvar = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)  # output = CNN embedding latent variables\n",
        "\n",
        "        # Sampling vector\n",
        "        self.fc4 = nn.Linear(self.CNN_embed_dim, self.fc_hidden2)\n",
        "        self.fc_bn4 = nn.BatchNorm1d(self.fc_hidden2)\n",
        "        self.fc5 = nn.Linear(self.fc_hidden2, 64 * 4 * 4)\n",
        "        self.fc_bn5 = nn.BatchNorm1d(64 * 4 * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Decoder\n",
        "        self.convTrans6 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=self.k4, stride=self.s4,\n",
        "                               padding=self.pd4),\n",
        "            nn.BatchNorm2d(32, momentum=0.01),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.convTrans7 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=32, out_channels=8, kernel_size=self.k3, stride=self.s3,\n",
        "                               padding=self.pd3),\n",
        "            nn.BatchNorm2d(8, momentum=0.01),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.convTrans8 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=self.k2, stride=self.s2,\n",
        "                               padding=self.pd2),\n",
        "            nn.BatchNorm2d(3, momentum=0.01),\n",
        "            nn.Sigmoid()    # y = (y1, y2, y3) \\in [0 ,1]^3\n",
        "        )\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.resnet(x)  # ResNet\n",
        "        x = x.view(x.size(0), -1)  # flatten output of conv\n",
        "\n",
        "        # FC layers\n",
        "        x = self.bn1(self.fc1(x))\n",
        "        x = self.relu(x)\n",
        "        x = self.bn2(self.fc2(x))\n",
        "        x = self.relu(x)\n",
        "        # x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "        mu, logvar = self.fc3_mu(x), self.fc3_logvar(x)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = Variable(std.data.new(std.size()).normal_())\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def decode(self, z):\n",
        "        x = self.relu(self.fc_bn4(self.fc4(z)))\n",
        "        x = self.relu(self.fc_bn5(self.fc5(x))).view(-1, 64, 4, 4)\n",
        "        x = self.convTrans6(x)\n",
        "        x = self.convTrans7(x)\n",
        "        x = self.convTrans8(x)\n",
        "        x = F.interpolate(x, size=(224, 224), mode='bilinear')\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_reconst = self.decode(z)\n",
        "\n",
        "        return x_reconst, z, mu, logvar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuGbzAY_Vp7g",
        "colab_type": "text"
      },
      "source": [
        "## 04. Define Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dfNGY24giRq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a86a3158-9cd0-4bec-fb56-866b28a1f153"
      },
      "source": [
        "# Detect devices\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mDXrJ4bgfpM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ecf199e1-f75e-42b8-e7fc-1743f3ea8d2c"
      },
      "source": [
        "'''\n",
        "### If you want to use pre-trained model ####\n",
        "pre_saved_model_path = './results_Malimg'\n",
        "epoch=20\n",
        "'''\n",
        "\n",
        "# Create model\n",
        "resnet_vae = ResNet_VAE(fc_hidden1=args.CNN_fc_hidden1, fc_hidden2=args.CNN_fc_hidden2, drop_p=args.dropout_p, CNN_embed_dim=args.CNN_embed_dim).to(device)\n",
        "'''\n",
        "### If you want to use pre-trained model ####\n",
        "resnet_vae.load_state_dict(torch.load(os.path.join(pre_saved_model_path, 'model_epoch{}.pth'.format(epoch))))\n",
        "'''\n",
        "print(resnet_vae)\n",
        "print('Number of {} parameters'.format(sum(p.numel() for p in resnet_vae.parameters() if p.requires_grad)))\n",
        "print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n",
        "model_params = list(resnet_vae.parameters())\n",
        "optimizer = torch.optim.Adam(model_params, lr=args.learning_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet_VAE(\n",
            "  (resnet): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (6): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (7): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (6): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (7): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (8): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (9): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (10): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (11): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (12): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (13): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (14): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (15): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (16): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (17): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (18): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (19): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (20): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (21): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (22): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (23): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (24): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (25): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (26): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (27): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (28): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (29): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (30): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (31): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (32): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (33): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (34): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (35): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
            "  (fc3_mu): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (fc3_logvar): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (fc4): Linear(in_features=256, out_features=1024, bias=True)\n",
            "  (fc_bn4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (fc_bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (convTrans6): Sequential(\n",
            "    (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (convTrans7): Sequential(\n",
            "    (0): ConvTranspose2d(32, 8, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (convTrans8): Sequential(\n",
            "    (0): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
            "    (2): Sigmoid()\n",
            "  )\n",
            ")\n",
            "Number of 63158425 parameters\n",
            "Using 1 GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh1Ve1PdgjAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
        "    # set model as training mode\n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "    all_y, all_z, all_mu, all_logvar = [], [], [], []\n",
        "    N_count = 0   # counting total trained sample in one epoch\n",
        "    for batch_idx, (X, y) in enumerate(train_loader):\n",
        "        # distribute data to device\n",
        "        X, y = X.to(device), y.to(device).view(-1, )\n",
        "        N_count += X.size(0)  # count batch_size sample\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        X_reconst, z, mu, logvar = model(X)  # VAE\n",
        "        loss = loss_function(X_reconst, X, mu, logvar)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        all_y.extend(y.data.cpu().numpy())\n",
        "        all_z.extend(z.data.cpu().numpy())\n",
        "        all_mu.extend(mu.data.cpu().numpy())\n",
        "        all_logvar.extend(logvar.data.cpu().numpy())\n",
        "\n",
        "        # show information\n",
        "        if (batch_idx + 1) % log_interval == 0:  # if batch_size = 16 => 160\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "    # calculate train_loss\n",
        "    losses /= len(train_loader.dataset)\n",
        "    all_y = np.stack(all_y, axis=0)\n",
        "    all_z = np.stack(all_z, axis=0)\n",
        "    all_mu = np.stack(all_mu, axis=0)\n",
        "    all_logvar = np.stack(all_logvar, axis=0)\n",
        "\n",
        "    # save Pytorch models of best record\n",
        "    torch.save(model.state_dict(), os.path.join(save_model_path, 'model_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
        "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
        "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
        "\n",
        "    return X.data.cpu().numpy(), all_y, all_z, all_mu, all_logvar, losses\n",
        "\n",
        "\n",
        "def validation(model, device, optimizer, test_loader):\n",
        "    # set model as testing mode\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    all_y, all_z, all_mu, all_logvar = [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y) in enumerate(test_loader):\n",
        "            # distribute data to device\n",
        "            X, y = X.to(device), y.to(device).view(-1, )\n",
        "            X_reconst, z, mu, logvar = model(X)\n",
        "\n",
        "            loss = loss_function(X_reconst, X, mu, logvar)\n",
        "            test_loss += loss.item()  # sum up batch loss\n",
        "\n",
        "            all_y.extend(y.data.cpu().numpy())\n",
        "            all_z.extend(z.data.cpu().numpy())\n",
        "            all_mu.extend(mu.data.cpu().numpy())\n",
        "            all_logvar.extend(logvar.data.cpu().numpy())\n",
        "            \n",
        "        ## Save_Recon_Malimg\n",
        "            if i == 0:\n",
        "                n = min(X.size(0), 8)\n",
        "                comparison = torch.cat([X[:n],\n",
        "                                    X_reconst.view(16, 3, 224, 224)[:n]])  # Recon 4 Images\n",
        "                save_image(comparison.cpu(),\n",
        "                        './results_ResNet-VAE_Exp01/recon_sampling/reconstruction_' + str(epoch + 1) + '.png', nrow=n)\n",
        "                # save_image(comparison.cpu(),\n",
        "                #         os.path.join(save_model_path, '/recon_sampling/reconstruction{}.png'.format(epoch + 1)))\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    all_y = np.stack(all_y, axis=0)\n",
        "    all_z = np.stack(all_z, axis=0)\n",
        "    all_mu = np.stack(all_mu, axis=0)\n",
        "    all_logvar = np.stack(all_logvar, axis=0)\n",
        "\n",
        "    # show information\n",
        "    print('\\nTest set ({:d} samples): Average loss: {:.4f}\\n'.format(len(test_loader.dataset), test_loss))\n",
        "    return X.data.cpu().numpy(), all_y, all_z, all_mu, all_logvar, test_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWrOAYCHw-Dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(resnet_vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGZkel39Vo0L",
        "colab_type": "text"
      },
      "source": [
        "### Argparse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o89ggU79exTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "972732d1-92fa-40f7-a9c0-e9ed3ea90d5f"
      },
      "source": [
        "import argparse\n",
        "\n",
        "seed = 123\n",
        "np.random.seed(seed)  # Reseed a legacy MT19937 BitGenerator\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "args.CNN_fc_hidden1 = 1024\n",
        "args.CNN_fc_hidden2 = 1024\n",
        "args.CNN_embed_dim = 256  # latent dim extracted by 2D CNN\n",
        "args.res_size = 224       # ResNet Image size\n",
        "args.dropout_p = 0.2           # dropout probability\n",
        "\n",
        "# training parameters\n",
        "args.epochs = 20\n",
        "args.batch_size = 50\n",
        "args.learning_rate = 1e-3\n",
        "args.log_interval = 10  # interval for displaying training info\n",
        "\n",
        "print(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(CNN_embed_dim=256, CNN_fc_hidden1=1024, CNN_fc_hidden2=1024, batch_size=50, dropout_p=0.2, epochs=20, learning_rate=0.001, log_interval=10, res_size=224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFuyo3QTrfJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "10bbf9e7-0578-4e2c-f8a8-5276ef6e8c57"
      },
      "source": [
        "print(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(CNN_embed_dim=256, CNN_fc_hidden1=1024, CNN_fc_hidden2=1024, batch_size=50, dropout_p=0.2, epochs=20, learning_rate=0.001, log_interval=10, res_size=224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMvQL9RPtkPM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "4430fbad-b7b1-4aa6-f7c6-a4b4319b7742"
      },
      "source": [
        "args.epochs = 100\n",
        "print(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(CNN_embed_dim=256, CNN_fc_hidden1=1024, CNN_fc_hidden2=1024, batch_size=50, dropout_p=0.2, epochs=100, learning_rate=0.001, log_interval=10, res_size=224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2-9ZnrDVuoI",
        "colab_type": "text"
      },
      "source": [
        "## 05. Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdrpoBUQVwz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dad3eb7f-992f-4b19-988e-162f9ab52081"
      },
      "source": [
        "# start training\n",
        "\n",
        "list_epoch=[]\n",
        "epoch_train_losses = []\n",
        "epoch_test_losses = []\n",
        "list_acc = []\n",
        "list_acc_epoch =[]\n",
        "check_mkdir(save_model_path)\n",
        "\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "\n",
        "    # train, test model\n",
        "    X_train, y_train, z_train, mu_train, logvar_train, epoch_train_loss = train(args.log_interval, resnet_vae, device, train_loader, optimizer, epoch)\n",
        "    X_test, y_test, z_test, mu_test, logvar_test, epoch_test_loss = validation(resnet_vae, device, optimizer, valid_loader)\n",
        "\n",
        "    # save results\n",
        "    list_epoch.append(epoch)\n",
        "    epoch_train_losses.append(epoch_train_loss)\n",
        "    epoch_test_losses.append(epoch_test_loss)\n",
        "\n",
        "    \n",
        "    # save all train test results\n",
        "    # A = np.array(all_train_losses)\n",
        "    B = np.array(epoch_train_losses)\n",
        "    C = np.array(epoch_test_losses)\n",
        "    \n",
        "    # np.save(os.path.join(save_model_path, 'ResNet_VAE_training_loss_all.npy'), A)\n",
        "    np.save(os.path.join(save_model_path, 'ResNet_VAE_training_loss.npy'), B)\n",
        "    np.save(os.path.join(save_model_path, 'ResNet_VAE_test_loss.npy'), C)\n",
        "\n",
        "    np.save(os.path.join(save_model_path, 'X_Malimg_train_epoch{}.npy'.format(epoch + 1)), X_train) #save last batch\n",
        "    np.save(os.path.join(save_model_path, 'y_Malimg_train_epoch{}.npy'.format(epoch + 1)), y_train)\n",
        "    np.save(os.path.join(save_model_path, 'z_Malimg_train_epoch{}.npy'.format(epoch + 1)), z_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Train Epoch: 1 [320/7471 (4%)]\tLoss: 1663872.000000\n",
            "Train Epoch: 1 [480/7471 (6%)]\tLoss: 1673245.250000\n",
            "Train Epoch: 1 [640/7471 (9%)]\tLoss: 1666329.125000\n",
            "Train Epoch: 1 [800/7471 (11%)]\tLoss: 1655970.125000\n",
            "Train Epoch: 1 [960/7471 (13%)]\tLoss: 1652011.250000\n",
            "Train Epoch: 1 [1120/7471 (15%)]\tLoss: 1653168.625000\n",
            "Train Epoch: 1 [1280/7471 (17%)]\tLoss: 1664516.125000\n",
            "Train Epoch: 1 [1440/7471 (19%)]\tLoss: 1650637.125000\n",
            "Train Epoch: 1 [1600/7471 (21%)]\tLoss: 1638597.250000\n",
            "Train Epoch: 1 [1760/7471 (24%)]\tLoss: 1656080.250000\n",
            "Train Epoch: 1 [1920/7471 (26%)]\tLoss: 1664741.750000\n",
            "Train Epoch: 1 [2080/7471 (28%)]\tLoss: 1627625.750000\n",
            "Train Epoch: 1 [2240/7471 (30%)]\tLoss: 1633431.375000\n",
            "Train Epoch: 1 [2400/7471 (32%)]\tLoss: 1643345.000000\n",
            "Train Epoch: 1 [2560/7471 (34%)]\tLoss: 1627399.750000\n",
            "Train Epoch: 1 [2720/7471 (36%)]\tLoss: 1617293.500000\n",
            "Train Epoch: 1 [2880/7471 (39%)]\tLoss: 1625719.125000\n",
            "Train Epoch: 1 [3040/7471 (41%)]\tLoss: 1655760.625000\n",
            "Train Epoch: 1 [3200/7471 (43%)]\tLoss: 1654996.000000\n",
            "Train Epoch: 1 [3360/7471 (45%)]\tLoss: 1645986.000000\n",
            "Train Epoch: 1 [3520/7471 (47%)]\tLoss: 1671683.250000\n",
            "Train Epoch: 1 [3680/7471 (49%)]\tLoss: 1647754.625000\n",
            "Train Epoch: 1 [3840/7471 (51%)]\tLoss: 1618821.125000\n",
            "Train Epoch: 1 [4000/7471 (54%)]\tLoss: 1630413.125000\n",
            "Train Epoch: 1 [4160/7471 (56%)]\tLoss: 1627947.875000\n",
            "Train Epoch: 1 [4320/7471 (58%)]\tLoss: 1643958.500000\n",
            "Train Epoch: 1 [4480/7471 (60%)]\tLoss: 1651149.375000\n",
            "Train Epoch: 1 [4640/7471 (62%)]\tLoss: 1650797.375000\n",
            "Train Epoch: 1 [4800/7471 (64%)]\tLoss: 1614956.750000\n",
            "Train Epoch: 1 [4960/7471 (66%)]\tLoss: 1621760.125000\n",
            "Train Epoch: 1 [5120/7471 (69%)]\tLoss: 1614780.500000\n",
            "Train Epoch: 1 [5280/7471 (71%)]\tLoss: 1595116.250000\n",
            "Train Epoch: 1 [5440/7471 (73%)]\tLoss: 1627965.375000\n",
            "Train Epoch: 1 [5600/7471 (75%)]\tLoss: 1627943.625000\n",
            "Train Epoch: 1 [5760/7471 (77%)]\tLoss: 1606273.250000\n",
            "Train Epoch: 1 [5920/7471 (79%)]\tLoss: 1610396.875000\n",
            "Train Epoch: 1 [6080/7471 (81%)]\tLoss: 1643688.000000\n",
            "Train Epoch: 1 [6240/7471 (84%)]\tLoss: 1628070.000000\n",
            "Train Epoch: 1 [6400/7471 (86%)]\tLoss: 1648356.250000\n",
            "Train Epoch: 1 [6560/7471 (88%)]\tLoss: 1565806.625000\n",
            "Train Epoch: 1 [6720/7471 (90%)]\tLoss: 1600846.125000\n",
            "Train Epoch: 1 [6880/7471 (92%)]\tLoss: 1642413.625000\n",
            "Train Epoch: 1 [7040/7471 (94%)]\tLoss: 1615052.500000\n",
            "Train Epoch: 1 [7200/7471 (96%)]\tLoss: 1624421.500000\n",
            "Train Epoch: 1 [7360/7471 (99%)]\tLoss: 1595021.000000\n",
            "Epoch 1 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 2 [160/7471 (2%)]\tLoss: 1638124.750000\n",
            "Train Epoch: 2 [320/7471 (4%)]\tLoss: 1630460.625000\n",
            "Train Epoch: 2 [480/7471 (6%)]\tLoss: 1615888.125000\n",
            "Train Epoch: 2 [640/7471 (9%)]\tLoss: 1600745.625000\n",
            "Train Epoch: 2 [800/7471 (11%)]\tLoss: 1608715.375000\n",
            "Train Epoch: 2 [960/7471 (13%)]\tLoss: 1652586.500000\n",
            "Train Epoch: 2 [1120/7471 (15%)]\tLoss: 1606889.500000\n",
            "Train Epoch: 2 [1280/7471 (17%)]\tLoss: 1625643.000000\n",
            "Train Epoch: 2 [1440/7471 (19%)]\tLoss: 1599501.875000\n",
            "Train Epoch: 2 [1600/7471 (21%)]\tLoss: 1660855.125000\n",
            "Train Epoch: 2 [1760/7471 (24%)]\tLoss: 1617220.000000\n",
            "Train Epoch: 2 [1920/7471 (26%)]\tLoss: 1623305.125000\n",
            "Train Epoch: 2 [2080/7471 (28%)]\tLoss: 1650226.750000\n",
            "Train Epoch: 2 [2240/7471 (30%)]\tLoss: 1642594.375000\n",
            "Train Epoch: 2 [2400/7471 (32%)]\tLoss: 1618370.375000\n",
            "Train Epoch: 2 [2560/7471 (34%)]\tLoss: 1617079.500000\n",
            "Train Epoch: 2 [2720/7471 (36%)]\tLoss: 1633970.875000\n",
            "Train Epoch: 2 [2880/7471 (39%)]\tLoss: 1638984.375000\n",
            "Train Epoch: 2 [3040/7471 (41%)]\tLoss: 1640539.375000\n",
            "Train Epoch: 2 [3200/7471 (43%)]\tLoss: 1595387.500000\n",
            "Train Epoch: 2 [3360/7471 (45%)]\tLoss: 1612598.250000\n",
            "Train Epoch: 2 [3520/7471 (47%)]\tLoss: 1587785.125000\n",
            "Train Epoch: 2 [3680/7471 (49%)]\tLoss: 1618284.250000\n",
            "Train Epoch: 2 [3840/7471 (51%)]\tLoss: 1670260.125000\n",
            "Train Epoch: 2 [4000/7471 (54%)]\tLoss: 1639710.750000\n",
            "Train Epoch: 2 [4160/7471 (56%)]\tLoss: 1585709.875000\n",
            "Train Epoch: 2 [4320/7471 (58%)]\tLoss: 1619067.625000\n",
            "Train Epoch: 2 [4480/7471 (60%)]\tLoss: 1600471.875000\n",
            "Train Epoch: 2 [4640/7471 (62%)]\tLoss: 1581970.000000\n",
            "Train Epoch: 2 [4800/7471 (64%)]\tLoss: 1568222.500000\n",
            "Train Epoch: 2 [4960/7471 (66%)]\tLoss: 1625650.250000\n",
            "Train Epoch: 2 [5120/7471 (69%)]\tLoss: 1637322.250000\n",
            "Train Epoch: 2 [5280/7471 (71%)]\tLoss: 1579114.375000\n",
            "Train Epoch: 2 [5440/7471 (73%)]\tLoss: 1614714.875000\n",
            "Train Epoch: 2 [5600/7471 (75%)]\tLoss: 1600310.375000\n",
            "Train Epoch: 2 [5760/7471 (77%)]\tLoss: 1613841.375000\n",
            "Train Epoch: 2 [5920/7471 (79%)]\tLoss: 1589390.250000\n",
            "Train Epoch: 2 [6080/7471 (81%)]\tLoss: 1622623.750000\n",
            "Train Epoch: 2 [6240/7471 (84%)]\tLoss: 1533361.375000\n",
            "Train Epoch: 2 [6400/7471 (86%)]\tLoss: 1606695.125000\n",
            "Train Epoch: 2 [6560/7471 (88%)]\tLoss: 1623617.625000\n",
            "Train Epoch: 2 [6720/7471 (90%)]\tLoss: 1636675.000000\n",
            "Train Epoch: 2 [6880/7471 (92%)]\tLoss: 1632004.625000\n",
            "Train Epoch: 2 [7040/7471 (94%)]\tLoss: 1614112.875000\n",
            "Train Epoch: 2 [7200/7471 (96%)]\tLoss: 1614774.625000\n",
            "Train Epoch: 2 [7360/7471 (99%)]\tLoss: 1649852.000000\n",
            "Epoch 2 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 3 [160/7471 (2%)]\tLoss: 1646686.750000\n",
            "Train Epoch: 3 [320/7471 (4%)]\tLoss: 1577615.375000\n",
            "Train Epoch: 3 [480/7471 (6%)]\tLoss: 1621973.125000\n",
            "Train Epoch: 3 [640/7471 (9%)]\tLoss: 1622402.375000\n",
            "Train Epoch: 3 [800/7471 (11%)]\tLoss: 1615995.000000\n",
            "Train Epoch: 3 [960/7471 (13%)]\tLoss: 1646218.750000\n",
            "Train Epoch: 3 [1120/7471 (15%)]\tLoss: 1610477.250000\n",
            "Train Epoch: 3 [1280/7471 (17%)]\tLoss: 1621476.875000\n",
            "Train Epoch: 3 [1440/7471 (19%)]\tLoss: 1632998.500000\n",
            "Train Epoch: 3 [1600/7471 (21%)]\tLoss: 1621830.625000\n",
            "Train Epoch: 3 [1760/7471 (24%)]\tLoss: 1574590.375000\n",
            "Train Epoch: 3 [1920/7471 (26%)]\tLoss: 1622255.000000\n",
            "Train Epoch: 3 [2080/7471 (28%)]\tLoss: 1627478.750000\n",
            "Train Epoch: 3 [2240/7471 (30%)]\tLoss: 1588174.750000\n",
            "Train Epoch: 3 [2400/7471 (32%)]\tLoss: 1647936.500000\n",
            "Train Epoch: 3 [2560/7471 (34%)]\tLoss: 1600886.875000\n",
            "Train Epoch: 3 [2720/7471 (36%)]\tLoss: 1608709.000000\n",
            "Train Epoch: 3 [2880/7471 (39%)]\tLoss: 1609930.250000\n",
            "Train Epoch: 3 [3040/7471 (41%)]\tLoss: 1589301.250000\n",
            "Train Epoch: 3 [3200/7471 (43%)]\tLoss: 1601460.375000\n",
            "Train Epoch: 3 [3360/7471 (45%)]\tLoss: 1615997.375000\n",
            "Train Epoch: 3 [3520/7471 (47%)]\tLoss: 1613724.750000\n",
            "Train Epoch: 3 [3680/7471 (49%)]\tLoss: 1559322.000000\n",
            "Train Epoch: 3 [3840/7471 (51%)]\tLoss: 1565618.250000\n",
            "Train Epoch: 3 [4000/7471 (54%)]\tLoss: 1623344.750000\n",
            "Train Epoch: 3 [4160/7471 (56%)]\tLoss: 1550898.500000\n",
            "Train Epoch: 3 [4320/7471 (58%)]\tLoss: 1611786.000000\n",
            "Train Epoch: 3 [4480/7471 (60%)]\tLoss: 1613765.250000\n",
            "Train Epoch: 3 [4640/7471 (62%)]\tLoss: 1593745.875000\n",
            "Train Epoch: 3 [4800/7471 (64%)]\tLoss: 1566163.625000\n",
            "Train Epoch: 3 [4960/7471 (66%)]\tLoss: 1576930.750000\n",
            "Train Epoch: 3 [5120/7471 (69%)]\tLoss: 1643413.000000\n",
            "Train Epoch: 3 [5280/7471 (71%)]\tLoss: 1674588.750000\n",
            "Train Epoch: 3 [5440/7471 (73%)]\tLoss: 1603321.500000\n",
            "Train Epoch: 3 [5600/7471 (75%)]\tLoss: 1615882.875000\n",
            "Train Epoch: 3 [5760/7471 (77%)]\tLoss: 1574501.250000\n",
            "Train Epoch: 3 [5920/7471 (79%)]\tLoss: 1629088.375000\n",
            "Train Epoch: 3 [6080/7471 (81%)]\tLoss: 1629943.125000\n",
            "Train Epoch: 3 [6240/7471 (84%)]\tLoss: 1585108.125000\n",
            "Train Epoch: 3 [6400/7471 (86%)]\tLoss: 1592669.875000\n",
            "Train Epoch: 3 [6560/7471 (88%)]\tLoss: 1621371.875000\n",
            "Train Epoch: 3 [6720/7471 (90%)]\tLoss: 1655093.875000\n",
            "Train Epoch: 3 [6880/7471 (92%)]\tLoss: 1583736.375000\n",
            "Train Epoch: 3 [7040/7471 (94%)]\tLoss: 1615828.625000\n",
            "Train Epoch: 3 [7200/7471 (96%)]\tLoss: 1631256.625000\n",
            "Train Epoch: 3 [7360/7471 (99%)]\tLoss: 1578683.250000\n",
            "Epoch 3 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 4 [160/7471 (2%)]\tLoss: 1577907.625000\n",
            "Train Epoch: 4 [320/7471 (4%)]\tLoss: 1617395.625000\n",
            "Train Epoch: 4 [480/7471 (6%)]\tLoss: 1601049.125000\n",
            "Train Epoch: 4 [640/7471 (9%)]\tLoss: 1608837.000000\n",
            "Train Epoch: 4 [800/7471 (11%)]\tLoss: 1601950.125000\n",
            "Train Epoch: 4 [960/7471 (13%)]\tLoss: 1585947.375000\n",
            "Train Epoch: 4 [1120/7471 (15%)]\tLoss: 1618168.250000\n",
            "Train Epoch: 4 [1280/7471 (17%)]\tLoss: 1652667.625000\n",
            "Train Epoch: 4 [1440/7471 (19%)]\tLoss: 1620717.875000\n",
            "Train Epoch: 4 [1600/7471 (21%)]\tLoss: 1611458.250000\n",
            "Train Epoch: 4 [1760/7471 (24%)]\tLoss: 1633973.500000\n",
            "Train Epoch: 4 [1920/7471 (26%)]\tLoss: 1623466.375000\n",
            "Train Epoch: 4 [2080/7471 (28%)]\tLoss: 1628139.625000\n",
            "Train Epoch: 4 [2240/7471 (30%)]\tLoss: 1622696.375000\n",
            "Train Epoch: 4 [2400/7471 (32%)]\tLoss: 1609970.375000\n",
            "Train Epoch: 4 [2560/7471 (34%)]\tLoss: 1602367.875000\n",
            "Train Epoch: 4 [2720/7471 (36%)]\tLoss: 1623455.500000\n",
            "Train Epoch: 4 [2880/7471 (39%)]\tLoss: 1652228.000000\n",
            "Train Epoch: 4 [3040/7471 (41%)]\tLoss: 1612147.125000\n",
            "Train Epoch: 4 [3200/7471 (43%)]\tLoss: 1620009.375000\n",
            "Train Epoch: 4 [3360/7471 (45%)]\tLoss: 1582172.250000\n",
            "Train Epoch: 4 [3520/7471 (47%)]\tLoss: 1576030.125000\n",
            "Train Epoch: 4 [3680/7471 (49%)]\tLoss: 1594783.500000\n",
            "Train Epoch: 4 [3840/7471 (51%)]\tLoss: 1520140.500000\n",
            "Train Epoch: 4 [4000/7471 (54%)]\tLoss: 1591723.375000\n",
            "Train Epoch: 4 [4160/7471 (56%)]\tLoss: 1590871.250000\n",
            "Train Epoch: 4 [4320/7471 (58%)]\tLoss: 1648433.875000\n",
            "Train Epoch: 4 [4480/7471 (60%)]\tLoss: 1643392.000000\n",
            "Train Epoch: 4 [4640/7471 (62%)]\tLoss: 1545399.250000\n",
            "Train Epoch: 4 [4800/7471 (64%)]\tLoss: 1607695.000000\n",
            "Train Epoch: 4 [4960/7471 (66%)]\tLoss: 1596125.625000\n",
            "Train Epoch: 4 [5120/7471 (69%)]\tLoss: 1630229.500000\n",
            "Train Epoch: 4 [5280/7471 (71%)]\tLoss: 1618706.625000\n",
            "Train Epoch: 4 [5440/7471 (73%)]\tLoss: 1677364.125000\n",
            "Train Epoch: 4 [5600/7471 (75%)]\tLoss: 1647619.000000\n",
            "Train Epoch: 4 [5760/7471 (77%)]\tLoss: 1609320.500000\n",
            "Train Epoch: 4 [5920/7471 (79%)]\tLoss: 1620627.750000\n",
            "Train Epoch: 4 [6080/7471 (81%)]\tLoss: 1642723.625000\n",
            "Train Epoch: 4 [6240/7471 (84%)]\tLoss: 1590775.500000\n",
            "Train Epoch: 4 [6400/7471 (86%)]\tLoss: 1604084.750000\n",
            "Train Epoch: 4 [6560/7471 (88%)]\tLoss: 1620020.375000\n",
            "Train Epoch: 4 [6720/7471 (90%)]\tLoss: 1617686.000000\n",
            "Train Epoch: 4 [6880/7471 (92%)]\tLoss: 1579645.125000\n",
            "Train Epoch: 4 [7040/7471 (94%)]\tLoss: 1627866.875000\n",
            "Train Epoch: 4 [7200/7471 (96%)]\tLoss: 1575168.625000\n",
            "Train Epoch: 4 [7360/7471 (99%)]\tLoss: 1554660.875000\n",
            "Epoch 4 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 5 [160/7471 (2%)]\tLoss: 1636329.875000\n",
            "Train Epoch: 5 [320/7471 (4%)]\tLoss: 1620082.625000\n",
            "Train Epoch: 5 [480/7471 (6%)]\tLoss: 1629063.375000\n",
            "Train Epoch: 5 [640/7471 (9%)]\tLoss: 1657729.000000\n",
            "Train Epoch: 5 [800/7471 (11%)]\tLoss: 1624493.500000\n",
            "Train Epoch: 5 [960/7471 (13%)]\tLoss: 1631253.125000\n",
            "Train Epoch: 5 [1120/7471 (15%)]\tLoss: 1599759.875000\n",
            "Train Epoch: 5 [1280/7471 (17%)]\tLoss: 1583858.625000\n",
            "Train Epoch: 5 [1440/7471 (19%)]\tLoss: 1564886.250000\n",
            "Train Epoch: 5 [1600/7471 (21%)]\tLoss: 1639682.125000\n",
            "Train Epoch: 5 [1760/7471 (24%)]\tLoss: 1600521.875000\n",
            "Train Epoch: 5 [1920/7471 (26%)]\tLoss: 1596378.750000\n",
            "Train Epoch: 5 [2080/7471 (28%)]\tLoss: 1595632.125000\n",
            "Train Epoch: 5 [2240/7471 (30%)]\tLoss: 1627861.375000\n",
            "Train Epoch: 5 [2400/7471 (32%)]\tLoss: 1610138.500000\n",
            "Train Epoch: 5 [2560/7471 (34%)]\tLoss: 1633927.250000\n",
            "Train Epoch: 5 [2720/7471 (36%)]\tLoss: 1587774.000000\n",
            "Train Epoch: 5 [2880/7471 (39%)]\tLoss: 1564040.875000\n",
            "Train Epoch: 5 [3040/7471 (41%)]\tLoss: 1603307.750000\n",
            "Train Epoch: 5 [3200/7471 (43%)]\tLoss: 1569016.750000\n",
            "Train Epoch: 5 [3360/7471 (45%)]\tLoss: 1614844.875000\n",
            "Train Epoch: 5 [3520/7471 (47%)]\tLoss: 1607187.250000\n",
            "Train Epoch: 5 [3680/7471 (49%)]\tLoss: 1566551.375000\n",
            "Train Epoch: 5 [3840/7471 (51%)]\tLoss: 1618628.125000\n",
            "Train Epoch: 5 [4000/7471 (54%)]\tLoss: 1602401.125000\n",
            "Train Epoch: 5 [4160/7471 (56%)]\tLoss: 1624442.625000\n",
            "Train Epoch: 5 [4320/7471 (58%)]\tLoss: 1612315.875000\n",
            "Train Epoch: 5 [4480/7471 (60%)]\tLoss: 1563085.500000\n",
            "Train Epoch: 5 [4640/7471 (62%)]\tLoss: 1599928.375000\n",
            "Train Epoch: 5 [4800/7471 (64%)]\tLoss: 1636901.875000\n",
            "Train Epoch: 5 [4960/7471 (66%)]\tLoss: 1653554.375000\n",
            "Train Epoch: 5 [5120/7471 (69%)]\tLoss: 1592159.375000\n",
            "Train Epoch: 5 [5280/7471 (71%)]\tLoss: 1601088.375000\n",
            "Train Epoch: 5 [5440/7471 (73%)]\tLoss: 1664219.375000\n",
            "Train Epoch: 5 [5600/7471 (75%)]\tLoss: 1658508.625000\n",
            "Train Epoch: 5 [5760/7471 (77%)]\tLoss: 1620997.000000\n",
            "Train Epoch: 5 [5920/7471 (79%)]\tLoss: 1627283.750000\n",
            "Train Epoch: 5 [6080/7471 (81%)]\tLoss: 1582777.500000\n",
            "Train Epoch: 5 [6240/7471 (84%)]\tLoss: 1603502.625000\n",
            "Train Epoch: 5 [6400/7471 (86%)]\tLoss: 1601128.625000\n",
            "Train Epoch: 5 [6560/7471 (88%)]\tLoss: 1590542.500000\n",
            "Train Epoch: 5 [6720/7471 (90%)]\tLoss: 1649821.625000\n",
            "Train Epoch: 5 [6880/7471 (92%)]\tLoss: 1533051.500000\n",
            "Train Epoch: 5 [7040/7471 (94%)]\tLoss: 1621246.875000\n",
            "Train Epoch: 5 [7200/7471 (96%)]\tLoss: 1565614.500000\n",
            "Train Epoch: 5 [7360/7471 (99%)]\tLoss: 1586562.375000\n",
            "Epoch 5 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 2043397937569089.7500\n",
            "\n",
            "Train Epoch: 6 [160/7471 (2%)]\tLoss: 1646461.750000\n",
            "Train Epoch: 6 [320/7471 (4%)]\tLoss: 1590788.625000\n",
            "Train Epoch: 6 [480/7471 (6%)]\tLoss: 1610837.750000\n",
            "Train Epoch: 6 [640/7471 (9%)]\tLoss: 1647338.500000\n",
            "Train Epoch: 6 [800/7471 (11%)]\tLoss: 1610491.500000\n",
            "Train Epoch: 6 [960/7471 (13%)]\tLoss: 1555603.375000\n",
            "Train Epoch: 6 [1120/7471 (15%)]\tLoss: 1588647.500000\n",
            "Train Epoch: 6 [1280/7471 (17%)]\tLoss: 1630242.000000\n",
            "Train Epoch: 6 [1440/7471 (19%)]\tLoss: 1598875.250000\n",
            "Train Epoch: 6 [1600/7471 (21%)]\tLoss: 1641983.000000\n",
            "Train Epoch: 6 [1760/7471 (24%)]\tLoss: 1622674.750000\n",
            "Train Epoch: 6 [1920/7471 (26%)]\tLoss: 1584188.375000\n",
            "Train Epoch: 6 [2080/7471 (28%)]\tLoss: 1562322.500000\n",
            "Train Epoch: 6 [2240/7471 (30%)]\tLoss: 1632849.250000\n",
            "Train Epoch: 6 [2400/7471 (32%)]\tLoss: 1611517.500000\n",
            "Train Epoch: 6 [2560/7471 (34%)]\tLoss: 1632644.000000\n",
            "Train Epoch: 6 [2720/7471 (36%)]\tLoss: 1620246.250000\n",
            "Train Epoch: 6 [2880/7471 (39%)]\tLoss: 1567492.000000\n",
            "Train Epoch: 6 [3040/7471 (41%)]\tLoss: 1627843.375000\n",
            "Train Epoch: 6 [3200/7471 (43%)]\tLoss: 1569358.750000\n",
            "Train Epoch: 6 [3360/7471 (45%)]\tLoss: 1623988.125000\n",
            "Train Epoch: 6 [3520/7471 (47%)]\tLoss: 1543995.750000\n",
            "Train Epoch: 6 [3680/7471 (49%)]\tLoss: 1589607.625000\n",
            "Train Epoch: 6 [3840/7471 (51%)]\tLoss: 1608273.625000\n",
            "Train Epoch: 6 [4000/7471 (54%)]\tLoss: 1624285.250000\n",
            "Train Epoch: 6 [4160/7471 (56%)]\tLoss: 1596753.750000\n",
            "Train Epoch: 6 [4320/7471 (58%)]\tLoss: 1627423.375000\n",
            "Train Epoch: 6 [4480/7471 (60%)]\tLoss: 1585216.375000\n",
            "Train Epoch: 6 [4640/7471 (62%)]\tLoss: 1590045.250000\n",
            "Train Epoch: 6 [4800/7471 (64%)]\tLoss: 1632040.125000\n",
            "Train Epoch: 6 [4960/7471 (66%)]\tLoss: 1627175.500000\n",
            "Train Epoch: 6 [5120/7471 (69%)]\tLoss: 1581858.250000\n",
            "Train Epoch: 6 [5280/7471 (71%)]\tLoss: 1615847.625000\n",
            "Train Epoch: 6 [5440/7471 (73%)]\tLoss: 1664251.500000\n",
            "Train Epoch: 6 [5600/7471 (75%)]\tLoss: 1655505.375000\n",
            "Train Epoch: 6 [5760/7471 (77%)]\tLoss: 1579515.625000\n",
            "Train Epoch: 6 [5920/7471 (79%)]\tLoss: 1585560.875000\n",
            "Train Epoch: 6 [6080/7471 (81%)]\tLoss: 1573832.500000\n",
            "Train Epoch: 6 [6240/7471 (84%)]\tLoss: 1645162.750000\n",
            "Train Epoch: 6 [6400/7471 (86%)]\tLoss: 1597319.500000\n",
            "Train Epoch: 6 [6560/7471 (88%)]\tLoss: 1577627.500000\n",
            "Train Epoch: 6 [6720/7471 (90%)]\tLoss: 1579047.500000\n",
            "Train Epoch: 6 [6880/7471 (92%)]\tLoss: 1626357.750000\n",
            "Train Epoch: 6 [7040/7471 (94%)]\tLoss: 1644365.500000\n",
            "Train Epoch: 6 [7200/7471 (96%)]\tLoss: 1656482.375000\n",
            "Train Epoch: 6 [7360/7471 (99%)]\tLoss: 1639596.375000\n",
            "Epoch 6 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 7 [160/7471 (2%)]\tLoss: 1617760.125000\n",
            "Train Epoch: 7 [320/7471 (4%)]\tLoss: 1580578.625000\n",
            "Train Epoch: 7 [480/7471 (6%)]\tLoss: 1636048.125000\n",
            "Train Epoch: 7 [640/7471 (9%)]\tLoss: 1605419.250000\n",
            "Train Epoch: 7 [800/7471 (11%)]\tLoss: 1633288.000000\n",
            "Train Epoch: 7 [960/7471 (13%)]\tLoss: 1581317.000000\n",
            "Train Epoch: 7 [1120/7471 (15%)]\tLoss: 1637222.875000\n",
            "Train Epoch: 7 [1280/7471 (17%)]\tLoss: 1605729.500000\n",
            "Train Epoch: 7 [1440/7471 (19%)]\tLoss: 1617294.375000\n",
            "Train Epoch: 7 [1600/7471 (21%)]\tLoss: 1563806.250000\n",
            "Train Epoch: 7 [1760/7471 (24%)]\tLoss: 1598177.750000\n",
            "Train Epoch: 7 [1920/7471 (26%)]\tLoss: 1582031.750000\n",
            "Train Epoch: 7 [2080/7471 (28%)]\tLoss: 1604806.000000\n",
            "Train Epoch: 7 [2240/7471 (30%)]\tLoss: 1610064.375000\n",
            "Train Epoch: 7 [2400/7471 (32%)]\tLoss: 1578898.125000\n",
            "Train Epoch: 7 [2560/7471 (34%)]\tLoss: 1616955.875000\n",
            "Train Epoch: 7 [2720/7471 (36%)]\tLoss: 1524218.250000\n",
            "Train Epoch: 7 [2880/7471 (39%)]\tLoss: 1632576.750000\n",
            "Train Epoch: 7 [3040/7471 (41%)]\tLoss: 1637747.500000\n",
            "Train Epoch: 7 [3200/7471 (43%)]\tLoss: 1589013.875000\n",
            "Train Epoch: 7 [3360/7471 (45%)]\tLoss: 1596517.250000\n",
            "Train Epoch: 7 [3520/7471 (47%)]\tLoss: 1629036.125000\n",
            "Train Epoch: 7 [3680/7471 (49%)]\tLoss: 1621452.125000\n",
            "Train Epoch: 7 [3840/7471 (51%)]\tLoss: 1611401.125000\n",
            "Train Epoch: 7 [4000/7471 (54%)]\tLoss: 1612265.000000\n",
            "Train Epoch: 7 [4160/7471 (56%)]\tLoss: 1611193.625000\n",
            "Train Epoch: 7 [4320/7471 (58%)]\tLoss: 1663525.625000\n",
            "Train Epoch: 7 [4480/7471 (60%)]\tLoss: 1537043.500000\n",
            "Train Epoch: 7 [4640/7471 (62%)]\tLoss: 1615439.375000\n",
            "Train Epoch: 7 [4800/7471 (64%)]\tLoss: 1634931.875000\n",
            "Train Epoch: 7 [4960/7471 (66%)]\tLoss: 1634702.625000\n",
            "Train Epoch: 7 [5120/7471 (69%)]\tLoss: 1639354.000000\n",
            "Train Epoch: 7 [5280/7471 (71%)]\tLoss: 1613216.875000\n",
            "Train Epoch: 7 [5440/7471 (73%)]\tLoss: 1623273.375000\n",
            "Train Epoch: 7 [5600/7471 (75%)]\tLoss: 1568277.500000\n",
            "Train Epoch: 7 [5760/7471 (77%)]\tLoss: 1575440.125000\n",
            "Train Epoch: 7 [5920/7471 (79%)]\tLoss: 1632654.625000\n",
            "Train Epoch: 7 [6080/7471 (81%)]\tLoss: 1637149.250000\n",
            "Train Epoch: 7 [6240/7471 (84%)]\tLoss: 1645262.000000\n",
            "Train Epoch: 7 [6400/7471 (86%)]\tLoss: 1601657.125000\n",
            "Train Epoch: 7 [6560/7471 (88%)]\tLoss: 1619254.750000\n",
            "Train Epoch: 7 [6720/7471 (90%)]\tLoss: 1572113.250000\n",
            "Train Epoch: 7 [6880/7471 (92%)]\tLoss: 1614074.000000\n",
            "Train Epoch: 7 [7040/7471 (94%)]\tLoss: 1620417.750000\n",
            "Train Epoch: 7 [7200/7471 (96%)]\tLoss: 1560626.750000\n",
            "Train Epoch: 7 [7360/7471 (99%)]\tLoss: 1593482.000000\n",
            "Epoch 7 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 101770.8230\n",
            "\n",
            "Train Epoch: 8 [160/7471 (2%)]\tLoss: 1627113.625000\n",
            "Train Epoch: 8 [320/7471 (4%)]\tLoss: 1606368.875000\n",
            "Train Epoch: 8 [480/7471 (6%)]\tLoss: 1578488.500000\n",
            "Train Epoch: 8 [640/7471 (9%)]\tLoss: 1596773.875000\n",
            "Train Epoch: 8 [800/7471 (11%)]\tLoss: 1564150.750000\n",
            "Train Epoch: 8 [960/7471 (13%)]\tLoss: 1607177.125000\n",
            "Train Epoch: 8 [1120/7471 (15%)]\tLoss: 1634424.750000\n",
            "Train Epoch: 8 [1280/7471 (17%)]\tLoss: 1593597.250000\n",
            "Train Epoch: 8 [1440/7471 (19%)]\tLoss: 1564774.000000\n",
            "Train Epoch: 8 [1600/7471 (21%)]\tLoss: 1609698.750000\n",
            "Train Epoch: 8 [1760/7471 (24%)]\tLoss: 1606271.875000\n",
            "Train Epoch: 8 [1920/7471 (26%)]\tLoss: 1627314.375000\n",
            "Train Epoch: 8 [2080/7471 (28%)]\tLoss: 1591004.375000\n",
            "Train Epoch: 8 [2240/7471 (30%)]\tLoss: 1668009.250000\n",
            "Train Epoch: 8 [2400/7471 (32%)]\tLoss: 1597317.875000\n",
            "Train Epoch: 8 [2560/7471 (34%)]\tLoss: 1603816.750000\n",
            "Train Epoch: 8 [2720/7471 (36%)]\tLoss: 1616842.125000\n",
            "Train Epoch: 8 [2880/7471 (39%)]\tLoss: 1597892.875000\n",
            "Train Epoch: 8 [3040/7471 (41%)]\tLoss: 1553766.125000\n",
            "Train Epoch: 8 [3200/7471 (43%)]\tLoss: 1601841.125000\n",
            "Train Epoch: 8 [3360/7471 (45%)]\tLoss: 1636914.000000\n",
            "Train Epoch: 8 [3520/7471 (47%)]\tLoss: 1633996.000000\n",
            "Train Epoch: 8 [3680/7471 (49%)]\tLoss: 1579913.875000\n",
            "Train Epoch: 8 [3840/7471 (51%)]\tLoss: 1625016.625000\n",
            "Train Epoch: 8 [4000/7471 (54%)]\tLoss: 1637213.250000\n",
            "Train Epoch: 8 [4160/7471 (56%)]\tLoss: 1589665.125000\n",
            "Train Epoch: 8 [4320/7471 (58%)]\tLoss: 1598753.250000\n",
            "Train Epoch: 8 [4480/7471 (60%)]\tLoss: 1563494.625000\n",
            "Train Epoch: 8 [4640/7471 (62%)]\tLoss: 1649750.250000\n",
            "Train Epoch: 8 [4800/7471 (64%)]\tLoss: 1597230.875000\n",
            "Train Epoch: 8 [4960/7471 (66%)]\tLoss: 1578616.875000\n",
            "Train Epoch: 8 [5120/7471 (69%)]\tLoss: 1552191.250000\n",
            "Train Epoch: 8 [5280/7471 (71%)]\tLoss: 1627956.250000\n",
            "Train Epoch: 8 [5440/7471 (73%)]\tLoss: 1595706.250000\n",
            "Train Epoch: 8 [5600/7471 (75%)]\tLoss: 1608619.875000\n",
            "Train Epoch: 8 [5760/7471 (77%)]\tLoss: 1627565.250000\n",
            "Train Epoch: 8 [5920/7471 (79%)]\tLoss: 1644788.875000\n",
            "Train Epoch: 8 [6080/7471 (81%)]\tLoss: 1646253.000000\n",
            "Train Epoch: 8 [6240/7471 (84%)]\tLoss: 1636913.375000\n",
            "Train Epoch: 8 [6400/7471 (86%)]\tLoss: 1604534.000000\n",
            "Train Epoch: 8 [6560/7471 (88%)]\tLoss: 1535052.250000\n",
            "Train Epoch: 8 [6720/7471 (90%)]\tLoss: 1583749.500000\n",
            "Train Epoch: 8 [6880/7471 (92%)]\tLoss: 1496856.125000\n",
            "Train Epoch: 8 [7040/7471 (94%)]\tLoss: 1599821.125000\n",
            "Train Epoch: 8 [7200/7471 (96%)]\tLoss: 1612866.750000\n",
            "Train Epoch: 8 [7360/7471 (99%)]\tLoss: 1643911.000000\n",
            "Epoch 8 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 100493.5306\n",
            "\n",
            "Train Epoch: 9 [160/7471 (2%)]\tLoss: 1591276.750000\n",
            "Train Epoch: 9 [320/7471 (4%)]\tLoss: 1574161.125000\n",
            "Train Epoch: 9 [480/7471 (6%)]\tLoss: 1587974.250000\n",
            "Train Epoch: 9 [640/7471 (9%)]\tLoss: 1571128.000000\n",
            "Train Epoch: 9 [800/7471 (11%)]\tLoss: 1579048.000000\n",
            "Train Epoch: 9 [960/7471 (13%)]\tLoss: 1597728.875000\n",
            "Train Epoch: 9 [1120/7471 (15%)]\tLoss: 1583069.250000\n",
            "Train Epoch: 9 [1280/7471 (17%)]\tLoss: 1572368.875000\n",
            "Train Epoch: 9 [1440/7471 (19%)]\tLoss: 1638794.500000\n",
            "Train Epoch: 9 [1600/7471 (21%)]\tLoss: 1625061.125000\n",
            "Train Epoch: 9 [1760/7471 (24%)]\tLoss: 1611392.500000\n",
            "Train Epoch: 9 [1920/7471 (26%)]\tLoss: 1633471.625000\n",
            "Train Epoch: 9 [2080/7471 (28%)]\tLoss: 1583158.875000\n",
            "Train Epoch: 9 [2240/7471 (30%)]\tLoss: 1628403.125000\n",
            "Train Epoch: 9 [2400/7471 (32%)]\tLoss: 1611244.250000\n",
            "Train Epoch: 9 [2560/7471 (34%)]\tLoss: 1616632.375000\n",
            "Train Epoch: 9 [2720/7471 (36%)]\tLoss: 1583725.250000\n",
            "Train Epoch: 9 [2880/7471 (39%)]\tLoss: 1590799.625000\n",
            "Train Epoch: 9 [3040/7471 (41%)]\tLoss: 1616859.125000\n",
            "Train Epoch: 9 [3200/7471 (43%)]\tLoss: 1623124.750000\n",
            "Train Epoch: 9 [3360/7471 (45%)]\tLoss: 1525932.375000\n",
            "Train Epoch: 9 [3520/7471 (47%)]\tLoss: 1590273.875000\n",
            "Train Epoch: 9 [3680/7471 (49%)]\tLoss: 1591966.000000\n",
            "Train Epoch: 9 [3840/7471 (51%)]\tLoss: 1584940.125000\n",
            "Train Epoch: 9 [4000/7471 (54%)]\tLoss: 1569780.750000\n",
            "Train Epoch: 9 [4160/7471 (56%)]\tLoss: 1547131.625000\n",
            "Train Epoch: 9 [4320/7471 (58%)]\tLoss: 1588752.250000\n",
            "Train Epoch: 9 [4480/7471 (60%)]\tLoss: 1560256.000000\n",
            "Train Epoch: 9 [4640/7471 (62%)]\tLoss: 1535226.625000\n",
            "Train Epoch: 9 [4800/7471 (64%)]\tLoss: 1617287.625000\n",
            "Train Epoch: 9 [4960/7471 (66%)]\tLoss: 1579919.750000\n",
            "Train Epoch: 9 [5120/7471 (69%)]\tLoss: 1598268.375000\n",
            "Train Epoch: 9 [5280/7471 (71%)]\tLoss: 1576143.875000\n",
            "Train Epoch: 9 [5440/7471 (73%)]\tLoss: 1568704.875000\n",
            "Train Epoch: 9 [5600/7471 (75%)]\tLoss: 1490757.250000\n",
            "Train Epoch: 9 [5760/7471 (77%)]\tLoss: 1635608.125000\n",
            "Train Epoch: 9 [5920/7471 (79%)]\tLoss: 1547085.875000\n",
            "Train Epoch: 9 [6080/7471 (81%)]\tLoss: 1547617.125000\n",
            "Train Epoch: 9 [6240/7471 (84%)]\tLoss: 1621658.875000\n",
            "Train Epoch: 9 [6400/7471 (86%)]\tLoss: 1620033.625000\n",
            "Train Epoch: 9 [6560/7471 (88%)]\tLoss: 1599629.500000\n",
            "Train Epoch: 9 [6720/7471 (90%)]\tLoss: 1629686.000000\n",
            "Train Epoch: 9 [6880/7471 (92%)]\tLoss: 1628076.125000\n",
            "Train Epoch: 9 [7040/7471 (94%)]\tLoss: 1627341.875000\n",
            "Train Epoch: 9 [7200/7471 (96%)]\tLoss: 1611895.625000\n",
            "Train Epoch: 9 [7360/7471 (99%)]\tLoss: 1581065.250000\n",
            "Epoch 9 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 10 [160/7471 (2%)]\tLoss: 1617140.625000\n",
            "Train Epoch: 10 [320/7471 (4%)]\tLoss: 1550745.875000\n",
            "Train Epoch: 10 [480/7471 (6%)]\tLoss: 1566244.375000\n",
            "Train Epoch: 10 [640/7471 (9%)]\tLoss: 1597871.375000\n",
            "Train Epoch: 10 [800/7471 (11%)]\tLoss: 1525755.625000\n",
            "Train Epoch: 10 [960/7471 (13%)]\tLoss: 1621986.875000\n",
            "Train Epoch: 10 [1120/7471 (15%)]\tLoss: 1594872.000000\n",
            "Train Epoch: 10 [1280/7471 (17%)]\tLoss: 1624306.500000\n",
            "Train Epoch: 10 [1440/7471 (19%)]\tLoss: 1580050.250000\n",
            "Train Epoch: 10 [1600/7471 (21%)]\tLoss: 1617848.125000\n",
            "Train Epoch: 10 [1760/7471 (24%)]\tLoss: 1557918.625000\n",
            "Train Epoch: 10 [1920/7471 (26%)]\tLoss: 1640531.500000\n",
            "Train Epoch: 10 [2080/7471 (28%)]\tLoss: 1570439.875000\n",
            "Train Epoch: 10 [2240/7471 (30%)]\tLoss: 1660403.875000\n",
            "Train Epoch: 10 [2400/7471 (32%)]\tLoss: 1562102.375000\n",
            "Train Epoch: 10 [2560/7471 (34%)]\tLoss: 1607533.875000\n",
            "Train Epoch: 10 [2720/7471 (36%)]\tLoss: 1543178.125000\n",
            "Train Epoch: 10 [2880/7471 (39%)]\tLoss: 1551943.875000\n",
            "Train Epoch: 10 [3040/7471 (41%)]\tLoss: 1636552.375000\n",
            "Train Epoch: 10 [3200/7471 (43%)]\tLoss: 1632708.625000\n",
            "Train Epoch: 10 [3360/7471 (45%)]\tLoss: 1622049.000000\n",
            "Train Epoch: 10 [3520/7471 (47%)]\tLoss: 1563728.500000\n",
            "Train Epoch: 10 [3680/7471 (49%)]\tLoss: 1587369.250000\n",
            "Train Epoch: 10 [3840/7471 (51%)]\tLoss: 1591968.250000\n",
            "Train Epoch: 10 [4000/7471 (54%)]\tLoss: 1606621.000000\n",
            "Train Epoch: 10 [4160/7471 (56%)]\tLoss: 1619672.125000\n",
            "Train Epoch: 10 [4320/7471 (58%)]\tLoss: 1621953.875000\n",
            "Train Epoch: 10 [4480/7471 (60%)]\tLoss: 1557957.000000\n",
            "Train Epoch: 10 [4640/7471 (62%)]\tLoss: 1568729.750000\n",
            "Train Epoch: 10 [4800/7471 (64%)]\tLoss: 1484590.375000\n",
            "Train Epoch: 10 [4960/7471 (66%)]\tLoss: 1605514.375000\n",
            "Train Epoch: 10 [5120/7471 (69%)]\tLoss: 1576060.000000\n",
            "Train Epoch: 10 [5280/7471 (71%)]\tLoss: 1648159.625000\n",
            "Train Epoch: 10 [5440/7471 (73%)]\tLoss: 1574701.625000\n",
            "Train Epoch: 10 [5600/7471 (75%)]\tLoss: 1587709.375000\n",
            "Train Epoch: 10 [5760/7471 (77%)]\tLoss: 1637279.375000\n",
            "Train Epoch: 10 [5920/7471 (79%)]\tLoss: 1613054.000000\n",
            "Train Epoch: 10 [6080/7471 (81%)]\tLoss: 1601194.250000\n",
            "Train Epoch: 10 [6240/7471 (84%)]\tLoss: 1581760.500000\n",
            "Train Epoch: 10 [6400/7471 (86%)]\tLoss: 1575330.000000\n",
            "Train Epoch: 10 [6560/7471 (88%)]\tLoss: 1570170.625000\n",
            "Train Epoch: 10 [6720/7471 (90%)]\tLoss: 1621502.500000\n",
            "Train Epoch: 10 [6880/7471 (92%)]\tLoss: 1585563.875000\n",
            "Train Epoch: 10 [7040/7471 (94%)]\tLoss: 1601013.500000\n",
            "Train Epoch: 10 [7200/7471 (96%)]\tLoss: 1599401.875000\n",
            "Train Epoch: 10 [7360/7471 (99%)]\tLoss: 1652462.500000\n",
            "Epoch 10 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 11 [160/7471 (2%)]\tLoss: 1526891.250000\n",
            "Train Epoch: 11 [320/7471 (4%)]\tLoss: 1598266.000000\n",
            "Train Epoch: 11 [480/7471 (6%)]\tLoss: 1581917.000000\n",
            "Train Epoch: 11 [640/7471 (9%)]\tLoss: 1644553.250000\n",
            "Train Epoch: 11 [800/7471 (11%)]\tLoss: 1603105.250000\n",
            "Train Epoch: 11 [960/7471 (13%)]\tLoss: 1620001.875000\n",
            "Train Epoch: 11 [1120/7471 (15%)]\tLoss: 1608315.500000\n",
            "Train Epoch: 11 [1280/7471 (17%)]\tLoss: 1625671.875000\n",
            "Train Epoch: 11 [1440/7471 (19%)]\tLoss: 1632931.375000\n",
            "Train Epoch: 11 [1600/7471 (21%)]\tLoss: 1577659.250000\n",
            "Train Epoch: 11 [1760/7471 (24%)]\tLoss: 1606280.250000\n",
            "Train Epoch: 11 [1920/7471 (26%)]\tLoss: 1600284.750000\n",
            "Train Epoch: 11 [2080/7471 (28%)]\tLoss: 1577747.250000\n",
            "Train Epoch: 11 [2240/7471 (30%)]\tLoss: 1593128.500000\n",
            "Train Epoch: 11 [2400/7471 (32%)]\tLoss: 1609523.000000\n",
            "Train Epoch: 11 [2560/7471 (34%)]\tLoss: 1623756.875000\n",
            "Train Epoch: 11 [2720/7471 (36%)]\tLoss: 1591125.000000\n",
            "Train Epoch: 11 [2880/7471 (39%)]\tLoss: 1609310.125000\n",
            "Train Epoch: 11 [3040/7471 (41%)]\tLoss: 1622996.125000\n",
            "Train Epoch: 11 [3200/7471 (43%)]\tLoss: 1633673.250000\n",
            "Train Epoch: 11 [3360/7471 (45%)]\tLoss: 1623186.000000\n",
            "Train Epoch: 11 [3520/7471 (47%)]\tLoss: 1603090.500000\n",
            "Train Epoch: 11 [3680/7471 (49%)]\tLoss: 1619301.500000\n",
            "Train Epoch: 11 [3840/7471 (51%)]\tLoss: 1639240.500000\n",
            "Train Epoch: 11 [4000/7471 (54%)]\tLoss: 1561738.875000\n",
            "Train Epoch: 11 [4160/7471 (56%)]\tLoss: 1585055.500000\n",
            "Train Epoch: 11 [4320/7471 (58%)]\tLoss: 1567187.625000\n",
            "Train Epoch: 11 [4480/7471 (60%)]\tLoss: 1571996.125000\n",
            "Train Epoch: 11 [4640/7471 (62%)]\tLoss: 1612093.625000\n",
            "Train Epoch: 11 [4800/7471 (64%)]\tLoss: 1572187.375000\n",
            "Train Epoch: 11 [4960/7471 (66%)]\tLoss: 1570021.125000\n",
            "Train Epoch: 11 [5120/7471 (69%)]\tLoss: 1618451.500000\n",
            "Train Epoch: 11 [5280/7471 (71%)]\tLoss: 1539592.750000\n",
            "Train Epoch: 11 [5440/7471 (73%)]\tLoss: 1547196.625000\n",
            "Train Epoch: 11 [5600/7471 (75%)]\tLoss: 1560873.500000\n",
            "Train Epoch: 11 [5760/7471 (77%)]\tLoss: 1595088.375000\n",
            "Train Epoch: 11 [5920/7471 (79%)]\tLoss: 1636530.125000\n",
            "Train Epoch: 11 [6080/7471 (81%)]\tLoss: 1604775.875000\n",
            "Train Epoch: 11 [6240/7471 (84%)]\tLoss: 1573878.375000\n",
            "Train Epoch: 11 [6400/7471 (86%)]\tLoss: 1614799.750000\n",
            "Train Epoch: 11 [6560/7471 (88%)]\tLoss: 1567784.625000\n",
            "Train Epoch: 11 [6720/7471 (90%)]\tLoss: 1610238.375000\n",
            "Train Epoch: 11 [6880/7471 (92%)]\tLoss: 1575593.125000\n",
            "Train Epoch: 11 [7040/7471 (94%)]\tLoss: 1623169.625000\n",
            "Train Epoch: 11 [7200/7471 (96%)]\tLoss: 1655590.625000\n",
            "Train Epoch: 11 [7360/7471 (99%)]\tLoss: 1627673.250000\n",
            "Epoch 11 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 99515.1711\n",
            "\n",
            "Train Epoch: 12 [160/7471 (2%)]\tLoss: 1591741.250000\n",
            "Train Epoch: 12 [320/7471 (4%)]\tLoss: 1623391.625000\n",
            "Train Epoch: 12 [480/7471 (6%)]\tLoss: 1558865.375000\n",
            "Train Epoch: 12 [640/7471 (9%)]\tLoss: 1584075.375000\n",
            "Train Epoch: 12 [800/7471 (11%)]\tLoss: 1597124.625000\n",
            "Train Epoch: 12 [960/7471 (13%)]\tLoss: 1622183.250000\n",
            "Train Epoch: 12 [1120/7471 (15%)]\tLoss: 1602825.750000\n",
            "Train Epoch: 12 [1280/7471 (17%)]\tLoss: 1539896.250000\n",
            "Train Epoch: 12 [1440/7471 (19%)]\tLoss: 1565424.375000\n",
            "Train Epoch: 12 [1600/7471 (21%)]\tLoss: 1620013.625000\n",
            "Train Epoch: 12 [1760/7471 (24%)]\tLoss: 1566347.125000\n",
            "Train Epoch: 12 [1920/7471 (26%)]\tLoss: 1616167.750000\n",
            "Train Epoch: 12 [2080/7471 (28%)]\tLoss: 1562404.000000\n",
            "Train Epoch: 12 [2240/7471 (30%)]\tLoss: 1634954.375000\n",
            "Train Epoch: 12 [2400/7471 (32%)]\tLoss: 1619961.125000\n",
            "Train Epoch: 12 [2560/7471 (34%)]\tLoss: 1608059.750000\n",
            "Train Epoch: 12 [2720/7471 (36%)]\tLoss: 1530705.875000\n",
            "Train Epoch: 12 [2880/7471 (39%)]\tLoss: 1600595.500000\n",
            "Train Epoch: 12 [3040/7471 (41%)]\tLoss: 1584016.875000\n",
            "Train Epoch: 12 [3200/7471 (43%)]\tLoss: 1586017.750000\n",
            "Train Epoch: 12 [3360/7471 (45%)]\tLoss: 1604691.500000\n",
            "Train Epoch: 12 [3520/7471 (47%)]\tLoss: 1613466.125000\n",
            "Train Epoch: 12 [3680/7471 (49%)]\tLoss: 1602365.000000\n",
            "Train Epoch: 12 [3840/7471 (51%)]\tLoss: 1600627.000000\n",
            "Train Epoch: 12 [4000/7471 (54%)]\tLoss: 1621291.625000\n",
            "Train Epoch: 12 [4160/7471 (56%)]\tLoss: 1576439.000000\n",
            "Train Epoch: 12 [4320/7471 (58%)]\tLoss: 1583792.500000\n",
            "Train Epoch: 12 [4480/7471 (60%)]\tLoss: 1539381.625000\n",
            "Train Epoch: 12 [4640/7471 (62%)]\tLoss: 1622748.125000\n",
            "Train Epoch: 12 [4800/7471 (64%)]\tLoss: 1589426.500000\n",
            "Train Epoch: 12 [4960/7471 (66%)]\tLoss: 1585651.625000\n",
            "Train Epoch: 12 [5120/7471 (69%)]\tLoss: 1577569.500000\n",
            "Train Epoch: 12 [5280/7471 (71%)]\tLoss: 1616269.250000\n",
            "Train Epoch: 12 [5440/7471 (73%)]\tLoss: 1597055.250000\n",
            "Train Epoch: 12 [5600/7471 (75%)]\tLoss: 1609540.125000\n",
            "Train Epoch: 12 [5760/7471 (77%)]\tLoss: 1589482.000000\n",
            "Train Epoch: 12 [5920/7471 (79%)]\tLoss: 1584022.875000\n",
            "Train Epoch: 12 [6080/7471 (81%)]\tLoss: 1584818.750000\n",
            "Train Epoch: 12 [6240/7471 (84%)]\tLoss: 1652080.500000\n",
            "Train Epoch: 12 [6400/7471 (86%)]\tLoss: 1593448.500000\n",
            "Train Epoch: 12 [6560/7471 (88%)]\tLoss: 1618563.375000\n",
            "Train Epoch: 12 [6720/7471 (90%)]\tLoss: 1625835.625000\n",
            "Train Epoch: 12 [6880/7471 (92%)]\tLoss: 1663843.375000\n",
            "Train Epoch: 12 [7040/7471 (94%)]\tLoss: 1582280.875000\n",
            "Train Epoch: 12 [7200/7471 (96%)]\tLoss: 1565758.125000\n",
            "Train Epoch: 12 [7360/7471 (99%)]\tLoss: 1627608.500000\n",
            "Epoch 12 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 13 [160/7471 (2%)]\tLoss: 1544594.250000\n",
            "Train Epoch: 13 [320/7471 (4%)]\tLoss: 1620878.000000\n",
            "Train Epoch: 13 [480/7471 (6%)]\tLoss: 1581865.500000\n",
            "Train Epoch: 13 [640/7471 (9%)]\tLoss: 1578402.000000\n",
            "Train Epoch: 13 [800/7471 (11%)]\tLoss: 1603594.375000\n",
            "Train Epoch: 13 [960/7471 (13%)]\tLoss: 1596538.375000\n",
            "Train Epoch: 13 [1120/7471 (15%)]\tLoss: 1615045.000000\n",
            "Train Epoch: 13 [1280/7471 (17%)]\tLoss: 1644850.125000\n",
            "Train Epoch: 13 [1440/7471 (19%)]\tLoss: 1621869.875000\n",
            "Train Epoch: 13 [1600/7471 (21%)]\tLoss: 1581102.625000\n",
            "Train Epoch: 13 [1760/7471 (24%)]\tLoss: 1632906.250000\n",
            "Train Epoch: 13 [1920/7471 (26%)]\tLoss: 1564247.625000\n",
            "Train Epoch: 13 [2080/7471 (28%)]\tLoss: 1612766.250000\n",
            "Train Epoch: 13 [2240/7471 (30%)]\tLoss: 1586361.500000\n",
            "Train Epoch: 13 [2400/7471 (32%)]\tLoss: 1572988.750000\n",
            "Train Epoch: 13 [2560/7471 (34%)]\tLoss: 1599654.625000\n",
            "Train Epoch: 13 [2720/7471 (36%)]\tLoss: 1632457.000000\n",
            "Train Epoch: 13 [2880/7471 (39%)]\tLoss: 1608829.125000\n",
            "Train Epoch: 13 [3040/7471 (41%)]\tLoss: 1646266.625000\n",
            "Train Epoch: 13 [3200/7471 (43%)]\tLoss: 1637689.375000\n",
            "Train Epoch: 13 [3360/7471 (45%)]\tLoss: 1612534.875000\n",
            "Train Epoch: 13 [3520/7471 (47%)]\tLoss: 1640371.250000\n",
            "Train Epoch: 13 [3680/7471 (49%)]\tLoss: 1586847.375000\n",
            "Train Epoch: 13 [3840/7471 (51%)]\tLoss: 1601380.500000\n",
            "Train Epoch: 13 [4000/7471 (54%)]\tLoss: 1620791.125000\n",
            "Train Epoch: 13 [4160/7471 (56%)]\tLoss: 1607049.750000\n",
            "Train Epoch: 13 [4320/7471 (58%)]\tLoss: 1620527.625000\n",
            "Train Epoch: 13 [4480/7471 (60%)]\tLoss: 1632870.250000\n",
            "Train Epoch: 13 [4640/7471 (62%)]\tLoss: 1626277.875000\n",
            "Train Epoch: 13 [4800/7471 (64%)]\tLoss: 1631748.500000\n",
            "Train Epoch: 13 [4960/7471 (66%)]\tLoss: 1577885.375000\n",
            "Train Epoch: 13 [5120/7471 (69%)]\tLoss: 1626241.625000\n",
            "Train Epoch: 13 [5280/7471 (71%)]\tLoss: 1573533.250000\n",
            "Train Epoch: 13 [5440/7471 (73%)]\tLoss: 1583372.375000\n",
            "Train Epoch: 13 [5600/7471 (75%)]\tLoss: 1616025.750000\n",
            "Train Epoch: 13 [5760/7471 (77%)]\tLoss: 1590710.500000\n",
            "Train Epoch: 13 [5920/7471 (79%)]\tLoss: 1597758.125000\n",
            "Train Epoch: 13 [6080/7471 (81%)]\tLoss: 1548348.125000\n",
            "Train Epoch: 13 [6240/7471 (84%)]\tLoss: 1561913.500000\n",
            "Train Epoch: 13 [6400/7471 (86%)]\tLoss: 1607364.625000\n",
            "Train Epoch: 13 [6560/7471 (88%)]\tLoss: 1598832.750000\n",
            "Train Epoch: 13 [6720/7471 (90%)]\tLoss: 1599590.250000\n",
            "Train Epoch: 13 [6880/7471 (92%)]\tLoss: 1542201.875000\n",
            "Train Epoch: 13 [7040/7471 (94%)]\tLoss: 1627410.375000\n",
            "Train Epoch: 13 [7200/7471 (96%)]\tLoss: 1616421.500000\n",
            "Train Epoch: 13 [7360/7471 (99%)]\tLoss: 1580130.625000\n",
            "Epoch 13 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 14 [160/7471 (2%)]\tLoss: 1573064.625000\n",
            "Train Epoch: 14 [320/7471 (4%)]\tLoss: 1570829.250000\n",
            "Train Epoch: 14 [480/7471 (6%)]\tLoss: 1579697.875000\n",
            "Train Epoch: 14 [640/7471 (9%)]\tLoss: 1573308.375000\n",
            "Train Epoch: 14 [800/7471 (11%)]\tLoss: 1642841.625000\n",
            "Train Epoch: 14 [960/7471 (13%)]\tLoss: 1626822.375000\n",
            "Train Epoch: 14 [1120/7471 (15%)]\tLoss: 1516003.250000\n",
            "Train Epoch: 14 [1280/7471 (17%)]\tLoss: 1616090.875000\n",
            "Train Epoch: 14 [1440/7471 (19%)]\tLoss: 1564093.750000\n",
            "Train Epoch: 14 [1600/7471 (21%)]\tLoss: 1593028.250000\n",
            "Train Epoch: 14 [1760/7471 (24%)]\tLoss: 1588522.750000\n",
            "Train Epoch: 14 [1920/7471 (26%)]\tLoss: 1589749.375000\n",
            "Train Epoch: 14 [2080/7471 (28%)]\tLoss: 1607672.000000\n",
            "Train Epoch: 14 [2240/7471 (30%)]\tLoss: 1596477.375000\n",
            "Train Epoch: 14 [2400/7471 (32%)]\tLoss: 1591850.000000\n",
            "Train Epoch: 14 [2560/7471 (34%)]\tLoss: 1586639.000000\n",
            "Train Epoch: 14 [2720/7471 (36%)]\tLoss: 1555976.875000\n",
            "Train Epoch: 14 [2880/7471 (39%)]\tLoss: 1498918.750000\n",
            "Train Epoch: 14 [3040/7471 (41%)]\tLoss: 1615206.250000\n",
            "Train Epoch: 14 [3200/7471 (43%)]\tLoss: 1595183.125000\n",
            "Train Epoch: 14 [3360/7471 (45%)]\tLoss: 1637917.625000\n",
            "Train Epoch: 14 [3520/7471 (47%)]\tLoss: 1629636.625000\n",
            "Train Epoch: 14 [3680/7471 (49%)]\tLoss: 1600554.500000\n",
            "Train Epoch: 14 [3840/7471 (51%)]\tLoss: 1500670.625000\n",
            "Train Epoch: 14 [4000/7471 (54%)]\tLoss: 1515715.000000\n",
            "Train Epoch: 14 [4160/7471 (56%)]\tLoss: 1599331.500000\n",
            "Train Epoch: 14 [4320/7471 (58%)]\tLoss: 1547939.375000\n",
            "Train Epoch: 14 [4480/7471 (60%)]\tLoss: 1631396.500000\n",
            "Train Epoch: 14 [4640/7471 (62%)]\tLoss: 1575332.875000\n",
            "Train Epoch: 14 [4800/7471 (64%)]\tLoss: 1608873.125000\n",
            "Train Epoch: 14 [4960/7471 (66%)]\tLoss: 1519020.750000\n",
            "Train Epoch: 14 [5120/7471 (69%)]\tLoss: 1585971.500000\n",
            "Train Epoch: 14 [5280/7471 (71%)]\tLoss: 1592318.000000\n",
            "Train Epoch: 14 [5440/7471 (73%)]\tLoss: 1602054.875000\n",
            "Train Epoch: 14 [5600/7471 (75%)]\tLoss: 1585735.500000\n",
            "Train Epoch: 14 [5760/7471 (77%)]\tLoss: 1525147.250000\n",
            "Train Epoch: 14 [5920/7471 (79%)]\tLoss: 1587630.625000\n",
            "Train Epoch: 14 [6080/7471 (81%)]\tLoss: 1602884.875000\n",
            "Train Epoch: 14 [6240/7471 (84%)]\tLoss: 1585550.500000\n",
            "Train Epoch: 14 [6400/7471 (86%)]\tLoss: 1653593.250000\n",
            "Train Epoch: 14 [6560/7471 (88%)]\tLoss: 1627971.750000\n",
            "Train Epoch: 14 [6720/7471 (90%)]\tLoss: 1586406.125000\n",
            "Train Epoch: 14 [6880/7471 (92%)]\tLoss: 1631766.875000\n",
            "Train Epoch: 14 [7040/7471 (94%)]\tLoss: 1617407.875000\n",
            "Train Epoch: 14 [7200/7471 (96%)]\tLoss: 1569229.750000\n",
            "Train Epoch: 14 [7360/7471 (99%)]\tLoss: 1526912.500000\n",
            "Epoch 14 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 15 [160/7471 (2%)]\tLoss: 1528888.375000\n",
            "Train Epoch: 15 [320/7471 (4%)]\tLoss: 1553042.250000\n",
            "Train Epoch: 15 [480/7471 (6%)]\tLoss: 1613986.875000\n",
            "Train Epoch: 15 [640/7471 (9%)]\tLoss: 1575144.875000\n",
            "Train Epoch: 15 [800/7471 (11%)]\tLoss: 1598630.375000\n",
            "Train Epoch: 15 [960/7471 (13%)]\tLoss: 1542250.875000\n",
            "Train Epoch: 15 [1120/7471 (15%)]\tLoss: 1581870.375000\n",
            "Train Epoch: 15 [1280/7471 (17%)]\tLoss: 1605282.875000\n",
            "Train Epoch: 15 [1440/7471 (19%)]\tLoss: 1585246.500000\n",
            "Train Epoch: 15 [1600/7471 (21%)]\tLoss: 1562331.375000\n",
            "Train Epoch: 15 [1760/7471 (24%)]\tLoss: 1604539.750000\n",
            "Train Epoch: 15 [1920/7471 (26%)]\tLoss: 1652528.375000\n",
            "Train Epoch: 15 [2080/7471 (28%)]\tLoss: 1620499.750000\n",
            "Train Epoch: 15 [2240/7471 (30%)]\tLoss: 1613464.000000\n",
            "Train Epoch: 15 [2400/7471 (32%)]\tLoss: 1520155.875000\n",
            "Train Epoch: 15 [2560/7471 (34%)]\tLoss: 1626181.875000\n",
            "Train Epoch: 15 [2720/7471 (36%)]\tLoss: 1592427.625000\n",
            "Train Epoch: 15 [2880/7471 (39%)]\tLoss: 1610230.125000\n",
            "Train Epoch: 15 [3040/7471 (41%)]\tLoss: 1624797.500000\n",
            "Train Epoch: 15 [3200/7471 (43%)]\tLoss: 1563354.625000\n",
            "Train Epoch: 15 [3360/7471 (45%)]\tLoss: 1588234.750000\n",
            "Train Epoch: 15 [3520/7471 (47%)]\tLoss: 1519803.375000\n",
            "Train Epoch: 15 [3680/7471 (49%)]\tLoss: 1513910.125000\n",
            "Train Epoch: 15 [3840/7471 (51%)]\tLoss: 1559364.750000\n",
            "Train Epoch: 15 [4000/7471 (54%)]\tLoss: 1564931.500000\n",
            "Train Epoch: 15 [4160/7471 (56%)]\tLoss: 1536549.875000\n",
            "Train Epoch: 15 [4320/7471 (58%)]\tLoss: 1597941.875000\n",
            "Train Epoch: 15 [4480/7471 (60%)]\tLoss: 1591725.375000\n",
            "Train Epoch: 15 [4640/7471 (62%)]\tLoss: 1612613.125000\n",
            "Train Epoch: 15 [4800/7471 (64%)]\tLoss: 1597326.250000\n",
            "Train Epoch: 15 [4960/7471 (66%)]\tLoss: 1635943.750000\n",
            "Train Epoch: 15 [5120/7471 (69%)]\tLoss: 1562647.875000\n",
            "Train Epoch: 15 [5280/7471 (71%)]\tLoss: 1565306.625000\n",
            "Train Epoch: 15 [5440/7471 (73%)]\tLoss: 1582150.250000\n",
            "Train Epoch: 15 [5600/7471 (75%)]\tLoss: 1586887.625000\n",
            "Train Epoch: 15 [5760/7471 (77%)]\tLoss: 1588524.875000\n",
            "Train Epoch: 15 [5920/7471 (79%)]\tLoss: 1567206.625000\n",
            "Train Epoch: 15 [6080/7471 (81%)]\tLoss: 1520679.750000\n",
            "Train Epoch: 15 [6240/7471 (84%)]\tLoss: 1589724.750000\n",
            "Train Epoch: 15 [6400/7471 (86%)]\tLoss: 1588518.625000\n",
            "Train Epoch: 15 [6560/7471 (88%)]\tLoss: 1574492.250000\n",
            "Train Epoch: 15 [6720/7471 (90%)]\tLoss: 1584410.500000\n",
            "Train Epoch: 15 [6880/7471 (92%)]\tLoss: 1601717.625000\n",
            "Train Epoch: 15 [7040/7471 (94%)]\tLoss: 1593180.625000\n",
            "Train Epoch: 15 [7200/7471 (96%)]\tLoss: 1556955.500000\n",
            "Train Epoch: 15 [7360/7471 (99%)]\tLoss: 1592908.875000\n",
            "Epoch 15 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 16 [160/7471 (2%)]\tLoss: 1599833.250000\n",
            "Train Epoch: 16 [320/7471 (4%)]\tLoss: 1544388.000000\n",
            "Train Epoch: 16 [480/7471 (6%)]\tLoss: 1579525.125000\n",
            "Train Epoch: 16 [640/7471 (9%)]\tLoss: 1516537.250000\n",
            "Train Epoch: 16 [800/7471 (11%)]\tLoss: 1646375.125000\n",
            "Train Epoch: 16 [960/7471 (13%)]\tLoss: 1635067.875000\n",
            "Train Epoch: 16 [1120/7471 (15%)]\tLoss: 1596071.000000\n",
            "Train Epoch: 16 [1280/7471 (17%)]\tLoss: 1572007.250000\n",
            "Train Epoch: 16 [1440/7471 (19%)]\tLoss: 1619227.000000\n",
            "Train Epoch: 16 [1600/7471 (21%)]\tLoss: 1509570.125000\n",
            "Train Epoch: 16 [1760/7471 (24%)]\tLoss: 1631570.750000\n",
            "Train Epoch: 16 [1920/7471 (26%)]\tLoss: 1513674.875000\n",
            "Train Epoch: 16 [2080/7471 (28%)]\tLoss: 1569701.625000\n",
            "Train Epoch: 16 [2240/7471 (30%)]\tLoss: 1593284.250000\n",
            "Train Epoch: 16 [2400/7471 (32%)]\tLoss: 1631089.750000\n",
            "Train Epoch: 16 [2560/7471 (34%)]\tLoss: 1532320.000000\n",
            "Train Epoch: 16 [2720/7471 (36%)]\tLoss: 1568948.500000\n",
            "Train Epoch: 16 [2880/7471 (39%)]\tLoss: 1627271.750000\n",
            "Train Epoch: 16 [3040/7471 (41%)]\tLoss: 1594059.875000\n",
            "Train Epoch: 16 [3200/7471 (43%)]\tLoss: 1618603.250000\n",
            "Train Epoch: 16 [3360/7471 (45%)]\tLoss: 1566061.375000\n",
            "Train Epoch: 16 [3520/7471 (47%)]\tLoss: 1599313.125000\n",
            "Train Epoch: 16 [3680/7471 (49%)]\tLoss: 1599686.625000\n",
            "Train Epoch: 16 [3840/7471 (51%)]\tLoss: 1632684.125000\n",
            "Train Epoch: 16 [4000/7471 (54%)]\tLoss: 1614397.125000\n",
            "Train Epoch: 16 [4160/7471 (56%)]\tLoss: 1578766.625000\n",
            "Train Epoch: 16 [4320/7471 (58%)]\tLoss: 1579415.750000\n",
            "Train Epoch: 16 [4480/7471 (60%)]\tLoss: 1615901.375000\n",
            "Train Epoch: 16 [4640/7471 (62%)]\tLoss: 1626552.000000\n",
            "Train Epoch: 16 [4800/7471 (64%)]\tLoss: 1621608.125000\n",
            "Train Epoch: 16 [4960/7471 (66%)]\tLoss: 1506755.875000\n",
            "Train Epoch: 16 [5120/7471 (69%)]\tLoss: 1559007.875000\n",
            "Train Epoch: 16 [5280/7471 (71%)]\tLoss: 1589881.000000\n",
            "Train Epoch: 16 [5440/7471 (73%)]\tLoss: 1554719.125000\n",
            "Train Epoch: 16 [5600/7471 (75%)]\tLoss: 1582615.875000\n",
            "Train Epoch: 16 [5760/7471 (77%)]\tLoss: 1611325.250000\n",
            "Train Epoch: 16 [5920/7471 (79%)]\tLoss: 1596269.250000\n",
            "Train Epoch: 16 [6080/7471 (81%)]\tLoss: 1623965.375000\n",
            "Train Epoch: 16 [6240/7471 (84%)]\tLoss: 1595731.000000\n",
            "Train Epoch: 16 [6400/7471 (86%)]\tLoss: 1580244.250000\n",
            "Train Epoch: 16 [6560/7471 (88%)]\tLoss: 1600511.875000\n",
            "Train Epoch: 16 [6720/7471 (90%)]\tLoss: 1529677.625000\n",
            "Train Epoch: 16 [6880/7471 (92%)]\tLoss: 1617535.250000\n",
            "Train Epoch: 16 [7040/7471 (94%)]\tLoss: 1617337.500000\n",
            "Train Epoch: 16 [7200/7471 (96%)]\tLoss: 1576537.000000\n",
            "Train Epoch: 16 [7360/7471 (99%)]\tLoss: 1598889.125000\n",
            "Epoch 16 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 17 [160/7471 (2%)]\tLoss: 1590434.500000\n",
            "Train Epoch: 17 [320/7471 (4%)]\tLoss: 1535442.625000\n",
            "Train Epoch: 17 [480/7471 (6%)]\tLoss: 1634152.125000\n",
            "Train Epoch: 17 [640/7471 (9%)]\tLoss: 1612819.875000\n",
            "Train Epoch: 17 [800/7471 (11%)]\tLoss: 1562790.125000\n",
            "Train Epoch: 17 [960/7471 (13%)]\tLoss: 1611723.500000\n",
            "Train Epoch: 17 [1120/7471 (15%)]\tLoss: 1525289.250000\n",
            "Train Epoch: 17 [1280/7471 (17%)]\tLoss: 1618976.625000\n",
            "Train Epoch: 17 [1440/7471 (19%)]\tLoss: 1554516.125000\n",
            "Train Epoch: 17 [1600/7471 (21%)]\tLoss: 1608997.000000\n",
            "Train Epoch: 17 [1760/7471 (24%)]\tLoss: 1604459.625000\n",
            "Train Epoch: 17 [1920/7471 (26%)]\tLoss: 1599694.250000\n",
            "Train Epoch: 17 [2080/7471 (28%)]\tLoss: 1581349.875000\n",
            "Train Epoch: 17 [2240/7471 (30%)]\tLoss: 1611130.250000\n",
            "Train Epoch: 17 [2400/7471 (32%)]\tLoss: 1638294.375000\n",
            "Train Epoch: 17 [2560/7471 (34%)]\tLoss: 1594746.375000\n",
            "Train Epoch: 17 [2720/7471 (36%)]\tLoss: 1553897.875000\n",
            "Train Epoch: 17 [2880/7471 (39%)]\tLoss: 1536002.625000\n",
            "Train Epoch: 17 [3040/7471 (41%)]\tLoss: 1519994.875000\n",
            "Train Epoch: 17 [3200/7471 (43%)]\tLoss: 1576896.000000\n",
            "Train Epoch: 17 [3360/7471 (45%)]\tLoss: 1561177.250000\n",
            "Train Epoch: 17 [3520/7471 (47%)]\tLoss: 1622485.875000\n",
            "Train Epoch: 17 [3680/7471 (49%)]\tLoss: 1587866.125000\n",
            "Train Epoch: 17 [3840/7471 (51%)]\tLoss: 1569649.250000\n",
            "Train Epoch: 17 [4000/7471 (54%)]\tLoss: 1572786.875000\n",
            "Train Epoch: 17 [4160/7471 (56%)]\tLoss: 1648237.875000\n",
            "Train Epoch: 17 [4320/7471 (58%)]\tLoss: 1500256.750000\n",
            "Train Epoch: 17 [4480/7471 (60%)]\tLoss: 1557519.625000\n",
            "Train Epoch: 17 [4640/7471 (62%)]\tLoss: 1628349.750000\n",
            "Train Epoch: 17 [4800/7471 (64%)]\tLoss: 1572941.375000\n",
            "Train Epoch: 17 [4960/7471 (66%)]\tLoss: 1637931.250000\n",
            "Train Epoch: 17 [5120/7471 (69%)]\tLoss: 1607256.625000\n",
            "Train Epoch: 17 [5280/7471 (71%)]\tLoss: 1530232.250000\n",
            "Train Epoch: 17 [5440/7471 (73%)]\tLoss: 1648159.000000\n",
            "Train Epoch: 17 [5600/7471 (75%)]\tLoss: 1592714.750000\n",
            "Train Epoch: 17 [5760/7471 (77%)]\tLoss: 1580215.750000\n",
            "Train Epoch: 17 [5920/7471 (79%)]\tLoss: 1603960.250000\n",
            "Train Epoch: 17 [6080/7471 (81%)]\tLoss: 1615801.000000\n",
            "Train Epoch: 17 [6240/7471 (84%)]\tLoss: 1505148.500000\n",
            "Train Epoch: 17 [6400/7471 (86%)]\tLoss: 1599358.750000\n",
            "Train Epoch: 17 [6560/7471 (88%)]\tLoss: 1628091.750000\n",
            "Train Epoch: 17 [6720/7471 (90%)]\tLoss: 1517679.625000\n",
            "Train Epoch: 17 [6880/7471 (92%)]\tLoss: 1578753.500000\n",
            "Train Epoch: 17 [7040/7471 (94%)]\tLoss: 1533023.875000\n",
            "Train Epoch: 17 [7200/7471 (96%)]\tLoss: 1607119.125000\n",
            "Train Epoch: 17 [7360/7471 (99%)]\tLoss: 1584719.750000\n",
            "Epoch 17 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 18 [160/7471 (2%)]\tLoss: 1564283.000000\n",
            "Train Epoch: 18 [320/7471 (4%)]\tLoss: 1639349.250000\n",
            "Train Epoch: 18 [480/7471 (6%)]\tLoss: 1515995.625000\n",
            "Train Epoch: 18 [640/7471 (9%)]\tLoss: 1615937.125000\n",
            "Train Epoch: 18 [800/7471 (11%)]\tLoss: 1562218.500000\n",
            "Train Epoch: 18 [960/7471 (13%)]\tLoss: 1586647.750000\n",
            "Train Epoch: 18 [1120/7471 (15%)]\tLoss: 1592447.875000\n",
            "Train Epoch: 18 [1280/7471 (17%)]\tLoss: 1561535.875000\n",
            "Train Epoch: 18 [1440/7471 (19%)]\tLoss: 1618942.375000\n",
            "Train Epoch: 18 [1600/7471 (21%)]\tLoss: 1622005.500000\n",
            "Train Epoch: 18 [1760/7471 (24%)]\tLoss: 1510490.375000\n",
            "Train Epoch: 18 [1920/7471 (26%)]\tLoss: 1570144.875000\n",
            "Train Epoch: 18 [2080/7471 (28%)]\tLoss: 1601686.625000\n",
            "Train Epoch: 18 [2240/7471 (30%)]\tLoss: 1618635.875000\n",
            "Train Epoch: 18 [2400/7471 (32%)]\tLoss: 1615793.750000\n",
            "Train Epoch: 18 [2560/7471 (34%)]\tLoss: 1544174.750000\n",
            "Train Epoch: 18 [2720/7471 (36%)]\tLoss: 1597196.750000\n",
            "Train Epoch: 18 [2880/7471 (39%)]\tLoss: 1629967.875000\n",
            "Train Epoch: 18 [3040/7471 (41%)]\tLoss: 1616295.125000\n",
            "Train Epoch: 18 [3200/7471 (43%)]\tLoss: 1591234.500000\n",
            "Train Epoch: 18 [3360/7471 (45%)]\tLoss: 1599927.875000\n",
            "Train Epoch: 18 [3520/7471 (47%)]\tLoss: 1605646.625000\n",
            "Train Epoch: 18 [3680/7471 (49%)]\tLoss: 1634355.750000\n",
            "Train Epoch: 18 [3840/7471 (51%)]\tLoss: 1548907.875000\n",
            "Train Epoch: 18 [4000/7471 (54%)]\tLoss: 1584318.750000\n",
            "Train Epoch: 18 [4160/7471 (56%)]\tLoss: 1654431.125000\n",
            "Train Epoch: 18 [4320/7471 (58%)]\tLoss: 1575183.125000\n",
            "Train Epoch: 18 [4480/7471 (60%)]\tLoss: 1603570.125000\n",
            "Train Epoch: 18 [4640/7471 (62%)]\tLoss: 1582381.500000\n",
            "Train Epoch: 18 [4800/7471 (64%)]\tLoss: 1568323.250000\n",
            "Train Epoch: 18 [4960/7471 (66%)]\tLoss: 1565656.125000\n",
            "Train Epoch: 18 [5120/7471 (69%)]\tLoss: 1632784.375000\n",
            "Train Epoch: 18 [5280/7471 (71%)]\tLoss: 1595303.375000\n",
            "Train Epoch: 18 [5440/7471 (73%)]\tLoss: 1538241.500000\n",
            "Train Epoch: 18 [5600/7471 (75%)]\tLoss: 1603323.500000\n",
            "Train Epoch: 18 [5760/7471 (77%)]\tLoss: 1565687.000000\n",
            "Train Epoch: 18 [5920/7471 (79%)]\tLoss: 1563219.875000\n",
            "Train Epoch: 18 [6080/7471 (81%)]\tLoss: 1605352.250000\n",
            "Train Epoch: 18 [6240/7471 (84%)]\tLoss: 1585501.125000\n",
            "Train Epoch: 18 [6400/7471 (86%)]\tLoss: 1571841.000000\n",
            "Train Epoch: 18 [6560/7471 (88%)]\tLoss: 1573402.750000\n",
            "Train Epoch: 18 [6720/7471 (90%)]\tLoss: 1505004.500000\n",
            "Train Epoch: 18 [6880/7471 (92%)]\tLoss: 1586942.250000\n",
            "Train Epoch: 18 [7040/7471 (94%)]\tLoss: 1525106.500000\n",
            "Train Epoch: 18 [7200/7471 (96%)]\tLoss: 1642645.000000\n",
            "Train Epoch: 18 [7360/7471 (99%)]\tLoss: 1596565.125000\n",
            "Epoch 18 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 19 [160/7471 (2%)]\tLoss: 1633419.875000\n",
            "Train Epoch: 19 [320/7471 (4%)]\tLoss: 1583055.500000\n",
            "Train Epoch: 19 [480/7471 (6%)]\tLoss: 1610883.500000\n",
            "Train Epoch: 19 [640/7471 (9%)]\tLoss: 1627058.625000\n",
            "Train Epoch: 19 [800/7471 (11%)]\tLoss: 1533657.625000\n",
            "Train Epoch: 19 [960/7471 (13%)]\tLoss: 1559804.250000\n",
            "Train Epoch: 19 [1120/7471 (15%)]\tLoss: 1629110.625000\n",
            "Train Epoch: 19 [1280/7471 (17%)]\tLoss: 1626034.250000\n",
            "Train Epoch: 19 [1440/7471 (19%)]\tLoss: 1587477.000000\n",
            "Train Epoch: 19 [1600/7471 (21%)]\tLoss: 1554465.750000\n",
            "Train Epoch: 19 [1760/7471 (24%)]\tLoss: 1593343.250000\n",
            "Train Epoch: 19 [1920/7471 (26%)]\tLoss: 1599093.875000\n",
            "Train Epoch: 19 [2080/7471 (28%)]\tLoss: 1573266.875000\n",
            "Train Epoch: 19 [2240/7471 (30%)]\tLoss: 1575141.500000\n",
            "Train Epoch: 19 [2400/7471 (32%)]\tLoss: 1539422.000000\n",
            "Train Epoch: 19 [2560/7471 (34%)]\tLoss: 1592825.125000\n",
            "Train Epoch: 19 [2720/7471 (36%)]\tLoss: 1608234.250000\n",
            "Train Epoch: 19 [2880/7471 (39%)]\tLoss: 1590011.625000\n",
            "Train Epoch: 19 [3040/7471 (41%)]\tLoss: 1560321.250000\n",
            "Train Epoch: 19 [3200/7471 (43%)]\tLoss: 1592191.375000\n",
            "Train Epoch: 19 [3360/7471 (45%)]\tLoss: 1501503.750000\n",
            "Train Epoch: 19 [3520/7471 (47%)]\tLoss: 1558999.875000\n",
            "Train Epoch: 19 [3680/7471 (49%)]\tLoss: 1595751.375000\n",
            "Train Epoch: 19 [3840/7471 (51%)]\tLoss: 1622972.750000\n",
            "Train Epoch: 19 [4000/7471 (54%)]\tLoss: 1571406.125000\n",
            "Train Epoch: 19 [4160/7471 (56%)]\tLoss: 1558556.875000\n",
            "Train Epoch: 19 [4320/7471 (58%)]\tLoss: 1598188.250000\n",
            "Train Epoch: 19 [4480/7471 (60%)]\tLoss: 1562318.625000\n",
            "Train Epoch: 19 [4640/7471 (62%)]\tLoss: 1589510.875000\n",
            "Train Epoch: 19 [4800/7471 (64%)]\tLoss: 1619693.000000\n",
            "Train Epoch: 19 [4960/7471 (66%)]\tLoss: 1543296.625000\n",
            "Train Epoch: 19 [5120/7471 (69%)]\tLoss: 1583020.750000\n",
            "Train Epoch: 19 [5280/7471 (71%)]\tLoss: 1612315.875000\n",
            "Train Epoch: 19 [5440/7471 (73%)]\tLoss: 1607390.875000\n",
            "Train Epoch: 19 [5600/7471 (75%)]\tLoss: 1625227.875000\n",
            "Train Epoch: 19 [5760/7471 (77%)]\tLoss: 1529790.125000\n",
            "Train Epoch: 19 [5920/7471 (79%)]\tLoss: 1588839.125000\n",
            "Train Epoch: 19 [6080/7471 (81%)]\tLoss: 1573262.750000\n",
            "Train Epoch: 19 [6240/7471 (84%)]\tLoss: 1563539.250000\n",
            "Train Epoch: 19 [6400/7471 (86%)]\tLoss: 1588065.000000\n",
            "Train Epoch: 19 [6560/7471 (88%)]\tLoss: 1587790.750000\n",
            "Train Epoch: 19 [6720/7471 (90%)]\tLoss: 1560032.500000\n",
            "Train Epoch: 19 [6880/7471 (92%)]\tLoss: 1550940.875000\n",
            "Train Epoch: 19 [7040/7471 (94%)]\tLoss: 1519064.250000\n",
            "Train Epoch: 19 [7200/7471 (96%)]\tLoss: 1539587.250000\n",
            "Train Epoch: 19 [7360/7471 (99%)]\tLoss: 1621540.375000\n",
            "Epoch 19 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 20 [160/7471 (2%)]\tLoss: 1577982.875000\n",
            "Train Epoch: 20 [320/7471 (4%)]\tLoss: 1622913.125000\n",
            "Train Epoch: 20 [480/7471 (6%)]\tLoss: 1575464.875000\n",
            "Train Epoch: 20 [640/7471 (9%)]\tLoss: 1581490.375000\n",
            "Train Epoch: 20 [800/7471 (11%)]\tLoss: 1558964.375000\n",
            "Train Epoch: 20 [960/7471 (13%)]\tLoss: 1627836.250000\n",
            "Train Epoch: 20 [1120/7471 (15%)]\tLoss: 1624319.500000\n",
            "Train Epoch: 20 [1280/7471 (17%)]\tLoss: 1607160.250000\n",
            "Train Epoch: 20 [1440/7471 (19%)]\tLoss: 1571864.500000\n",
            "Train Epoch: 20 [1600/7471 (21%)]\tLoss: 1610226.000000\n",
            "Train Epoch: 20 [1760/7471 (24%)]\tLoss: 1624510.250000\n",
            "Train Epoch: 20 [1920/7471 (26%)]\tLoss: 1600905.875000\n",
            "Train Epoch: 20 [2080/7471 (28%)]\tLoss: 1592094.625000\n",
            "Train Epoch: 20 [2240/7471 (30%)]\tLoss: 1624593.000000\n",
            "Train Epoch: 20 [2400/7471 (32%)]\tLoss: 1569831.000000\n",
            "Train Epoch: 20 [2560/7471 (34%)]\tLoss: 1574768.750000\n",
            "Train Epoch: 20 [2720/7471 (36%)]\tLoss: 1558606.500000\n",
            "Train Epoch: 20 [2880/7471 (39%)]\tLoss: 1569061.375000\n",
            "Train Epoch: 20 [3040/7471 (41%)]\tLoss: 1517823.500000\n",
            "Train Epoch: 20 [3200/7471 (43%)]\tLoss: 1587861.250000\n",
            "Train Epoch: 20 [3360/7471 (45%)]\tLoss: 1600460.875000\n",
            "Train Epoch: 20 [3520/7471 (47%)]\tLoss: 1582791.375000\n",
            "Train Epoch: 20 [3680/7471 (49%)]\tLoss: 1580026.875000\n",
            "Train Epoch: 20 [3840/7471 (51%)]\tLoss: 1580632.875000\n",
            "Train Epoch: 20 [4000/7471 (54%)]\tLoss: 1554281.625000\n",
            "Train Epoch: 20 [4160/7471 (56%)]\tLoss: 1623739.375000\n",
            "Train Epoch: 20 [4320/7471 (58%)]\tLoss: 1572662.125000\n",
            "Train Epoch: 20 [4480/7471 (60%)]\tLoss: 1602064.250000\n",
            "Train Epoch: 20 [4640/7471 (62%)]\tLoss: 1605487.500000\n",
            "Train Epoch: 20 [4800/7471 (64%)]\tLoss: 1618845.500000\n",
            "Train Epoch: 20 [4960/7471 (66%)]\tLoss: 1517509.250000\n",
            "Train Epoch: 20 [5120/7471 (69%)]\tLoss: 1607085.625000\n",
            "Train Epoch: 20 [5280/7471 (71%)]\tLoss: 1616318.750000\n",
            "Train Epoch: 20 [5440/7471 (73%)]\tLoss: 1559470.750000\n",
            "Train Epoch: 20 [5600/7471 (75%)]\tLoss: 1596057.875000\n",
            "Train Epoch: 20 [5760/7471 (77%)]\tLoss: 1583369.500000\n",
            "Train Epoch: 20 [5920/7471 (79%)]\tLoss: 1570722.750000\n",
            "Train Epoch: 20 [6080/7471 (81%)]\tLoss: 1634027.625000\n",
            "Train Epoch: 20 [6240/7471 (84%)]\tLoss: 1583339.375000\n",
            "Train Epoch: 20 [6400/7471 (86%)]\tLoss: 1615657.875000\n",
            "Train Epoch: 20 [6560/7471 (88%)]\tLoss: 1541800.250000\n",
            "Train Epoch: 20 [6720/7471 (90%)]\tLoss: 1582159.875000\n",
            "Train Epoch: 20 [6880/7471 (92%)]\tLoss: 1557304.875000\n",
            "Train Epoch: 20 [7040/7471 (94%)]\tLoss: 1623610.250000\n",
            "Train Epoch: 20 [7200/7471 (96%)]\tLoss: 1659801.500000\n",
            "Train Epoch: 20 [7360/7471 (99%)]\tLoss: 1625108.375000\n",
            "Epoch 20 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 21 [160/7471 (2%)]\tLoss: 1600198.875000\n",
            "Train Epoch: 21 [320/7471 (4%)]\tLoss: 1559775.875000\n",
            "Train Epoch: 21 [480/7471 (6%)]\tLoss: 1590422.125000\n",
            "Train Epoch: 21 [640/7471 (9%)]\tLoss: 1536476.250000\n",
            "Train Epoch: 21 [800/7471 (11%)]\tLoss: 1568477.625000\n",
            "Train Epoch: 21 [960/7471 (13%)]\tLoss: 1628093.125000\n",
            "Train Epoch: 21 [1120/7471 (15%)]\tLoss: 1543172.250000\n",
            "Train Epoch: 21 [1280/7471 (17%)]\tLoss: 1540384.750000\n",
            "Train Epoch: 21 [1440/7471 (19%)]\tLoss: 1599039.875000\n",
            "Train Epoch: 21 [1600/7471 (21%)]\tLoss: 1579281.375000\n",
            "Train Epoch: 21 [1760/7471 (24%)]\tLoss: 1589906.250000\n",
            "Train Epoch: 21 [1920/7471 (26%)]\tLoss: 1586280.625000\n",
            "Train Epoch: 21 [2080/7471 (28%)]\tLoss: 1556365.250000\n",
            "Train Epoch: 21 [2240/7471 (30%)]\tLoss: 1553857.375000\n",
            "Train Epoch: 21 [2400/7471 (32%)]\tLoss: 1549776.250000\n",
            "Train Epoch: 21 [2560/7471 (34%)]\tLoss: 1522210.375000\n",
            "Train Epoch: 21 [2720/7471 (36%)]\tLoss: 1545779.125000\n",
            "Train Epoch: 21 [2880/7471 (39%)]\tLoss: 1560503.000000\n",
            "Train Epoch: 21 [3040/7471 (41%)]\tLoss: 1589248.500000\n",
            "Train Epoch: 21 [3200/7471 (43%)]\tLoss: 1522510.500000\n",
            "Train Epoch: 21 [3360/7471 (45%)]\tLoss: 1569947.750000\n",
            "Train Epoch: 21 [3520/7471 (47%)]\tLoss: 1592872.625000\n",
            "Train Epoch: 21 [3680/7471 (49%)]\tLoss: 1620902.875000\n",
            "Train Epoch: 21 [3840/7471 (51%)]\tLoss: 1607369.250000\n",
            "Train Epoch: 21 [4000/7471 (54%)]\tLoss: 1578272.875000\n",
            "Train Epoch: 21 [4160/7471 (56%)]\tLoss: 1622052.375000\n",
            "Train Epoch: 21 [4320/7471 (58%)]\tLoss: 1582494.250000\n",
            "Train Epoch: 21 [4480/7471 (60%)]\tLoss: 1583718.875000\n",
            "Train Epoch: 21 [4640/7471 (62%)]\tLoss: 1617800.625000\n",
            "Train Epoch: 21 [4800/7471 (64%)]\tLoss: 1649110.625000\n",
            "Train Epoch: 21 [4960/7471 (66%)]\tLoss: 1642672.250000\n",
            "Train Epoch: 21 [5120/7471 (69%)]\tLoss: 1605321.500000\n",
            "Train Epoch: 21 [5280/7471 (71%)]\tLoss: 1600239.750000\n",
            "Train Epoch: 21 [5440/7471 (73%)]\tLoss: 1533126.250000\n",
            "Train Epoch: 21 [5600/7471 (75%)]\tLoss: 1605064.125000\n",
            "Train Epoch: 21 [5760/7471 (77%)]\tLoss: 1616413.750000\n",
            "Train Epoch: 21 [5920/7471 (79%)]\tLoss: 1602956.500000\n",
            "Train Epoch: 21 [6080/7471 (81%)]\tLoss: 1521510.875000\n",
            "Train Epoch: 21 [6240/7471 (84%)]\tLoss: 1595190.625000\n",
            "Train Epoch: 21 [6400/7471 (86%)]\tLoss: 1613442.125000\n",
            "Train Epoch: 21 [6560/7471 (88%)]\tLoss: 1582714.625000\n",
            "Train Epoch: 21 [6720/7471 (90%)]\tLoss: 1596008.500000\n",
            "Train Epoch: 21 [6880/7471 (92%)]\tLoss: 1631695.250000\n",
            "Train Epoch: 21 [7040/7471 (94%)]\tLoss: 1561249.750000\n",
            "Train Epoch: 21 [7200/7471 (96%)]\tLoss: 1571953.125000\n",
            "Train Epoch: 21 [7360/7471 (99%)]\tLoss: 1611108.875000\n",
            "Epoch 21 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 22 [160/7471 (2%)]\tLoss: 1620010.500000\n",
            "Train Epoch: 22 [320/7471 (4%)]\tLoss: 1524320.000000\n",
            "Train Epoch: 22 [480/7471 (6%)]\tLoss: 1600991.750000\n",
            "Train Epoch: 22 [640/7471 (9%)]\tLoss: 1559866.000000\n",
            "Train Epoch: 22 [800/7471 (11%)]\tLoss: 1563888.875000\n",
            "Train Epoch: 22 [960/7471 (13%)]\tLoss: 1590811.000000\n",
            "Train Epoch: 22 [1120/7471 (15%)]\tLoss: 1620457.125000\n",
            "Train Epoch: 22 [1280/7471 (17%)]\tLoss: 1548393.000000\n",
            "Train Epoch: 22 [1440/7471 (19%)]\tLoss: 1614052.125000\n",
            "Train Epoch: 22 [1600/7471 (21%)]\tLoss: 1614091.625000\n",
            "Train Epoch: 22 [1760/7471 (24%)]\tLoss: 1628209.500000\n",
            "Train Epoch: 22 [1920/7471 (26%)]\tLoss: 1601094.750000\n",
            "Train Epoch: 22 [2080/7471 (28%)]\tLoss: 1564787.000000\n",
            "Train Epoch: 22 [2240/7471 (30%)]\tLoss: 1585848.750000\n",
            "Train Epoch: 22 [2400/7471 (32%)]\tLoss: 1540245.250000\n",
            "Train Epoch: 22 [2560/7471 (34%)]\tLoss: 1618077.000000\n",
            "Train Epoch: 22 [2720/7471 (36%)]\tLoss: 1507231.250000\n",
            "Train Epoch: 22 [2880/7471 (39%)]\tLoss: 1555737.500000\n",
            "Train Epoch: 22 [3040/7471 (41%)]\tLoss: 1571502.250000\n",
            "Train Epoch: 22 [3200/7471 (43%)]\tLoss: 1608562.500000\n",
            "Train Epoch: 22 [3360/7471 (45%)]\tLoss: 1537348.750000\n",
            "Train Epoch: 22 [3520/7471 (47%)]\tLoss: 1592820.125000\n",
            "Train Epoch: 22 [3680/7471 (49%)]\tLoss: 1621128.125000\n",
            "Train Epoch: 22 [3840/7471 (51%)]\tLoss: 1589111.875000\n",
            "Train Epoch: 22 [4000/7471 (54%)]\tLoss: 1617310.125000\n",
            "Train Epoch: 22 [4160/7471 (56%)]\tLoss: 1593601.250000\n",
            "Train Epoch: 22 [4320/7471 (58%)]\tLoss: 1594830.500000\n",
            "Train Epoch: 22 [4480/7471 (60%)]\tLoss: 1581277.875000\n",
            "Train Epoch: 22 [4640/7471 (62%)]\tLoss: 1605333.875000\n",
            "Train Epoch: 22 [4800/7471 (64%)]\tLoss: 1548155.250000\n",
            "Train Epoch: 22 [4960/7471 (66%)]\tLoss: 1573432.750000\n",
            "Train Epoch: 22 [5120/7471 (69%)]\tLoss: 1527966.000000\n",
            "Train Epoch: 22 [5280/7471 (71%)]\tLoss: 1610581.000000\n",
            "Train Epoch: 22 [5440/7471 (73%)]\tLoss: 1532848.000000\n",
            "Train Epoch: 22 [5600/7471 (75%)]\tLoss: 1605505.625000\n",
            "Train Epoch: 22 [5760/7471 (77%)]\tLoss: 1617943.000000\n",
            "Train Epoch: 22 [5920/7471 (79%)]\tLoss: 1556951.125000\n",
            "Train Epoch: 22 [6080/7471 (81%)]\tLoss: 1586122.500000\n",
            "Train Epoch: 22 [6240/7471 (84%)]\tLoss: 1589201.625000\n",
            "Train Epoch: 22 [6400/7471 (86%)]\tLoss: 1565871.125000\n",
            "Train Epoch: 22 [6560/7471 (88%)]\tLoss: 1555125.375000\n",
            "Train Epoch: 22 [6720/7471 (90%)]\tLoss: 1625029.625000\n",
            "Train Epoch: 22 [6880/7471 (92%)]\tLoss: 1545955.125000\n",
            "Train Epoch: 22 [7040/7471 (94%)]\tLoss: 1553168.750000\n",
            "Train Epoch: 22 [7200/7471 (96%)]\tLoss: 1631073.125000\n",
            "Train Epoch: 22 [7360/7471 (99%)]\tLoss: 1579036.500000\n",
            "Epoch 22 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 23 [160/7471 (2%)]\tLoss: 1583660.375000\n",
            "Train Epoch: 23 [320/7471 (4%)]\tLoss: 1609126.250000\n",
            "Train Epoch: 23 [480/7471 (6%)]\tLoss: 1544185.250000\n",
            "Train Epoch: 23 [640/7471 (9%)]\tLoss: 1585593.625000\n",
            "Train Epoch: 23 [800/7471 (11%)]\tLoss: 1551313.375000\n",
            "Train Epoch: 23 [960/7471 (13%)]\tLoss: 1624818.625000\n",
            "Train Epoch: 23 [1120/7471 (15%)]\tLoss: 1581563.250000\n",
            "Train Epoch: 23 [1280/7471 (17%)]\tLoss: 1635987.000000\n",
            "Train Epoch: 23 [1440/7471 (19%)]\tLoss: 1538021.500000\n",
            "Train Epoch: 23 [1600/7471 (21%)]\tLoss: 1572194.875000\n",
            "Train Epoch: 23 [1760/7471 (24%)]\tLoss: 1526350.375000\n",
            "Train Epoch: 23 [1920/7471 (26%)]\tLoss: 1599671.500000\n",
            "Train Epoch: 23 [2080/7471 (28%)]\tLoss: 1567018.500000\n",
            "Train Epoch: 23 [2240/7471 (30%)]\tLoss: 1555378.375000\n",
            "Train Epoch: 23 [2400/7471 (32%)]\tLoss: 1500394.250000\n",
            "Train Epoch: 23 [2560/7471 (34%)]\tLoss: 1574995.750000\n",
            "Train Epoch: 23 [2720/7471 (36%)]\tLoss: 1615068.625000\n",
            "Train Epoch: 23 [2880/7471 (39%)]\tLoss: 1543530.125000\n",
            "Train Epoch: 23 [3040/7471 (41%)]\tLoss: 1586244.000000\n",
            "Train Epoch: 23 [3200/7471 (43%)]\tLoss: 1601403.875000\n",
            "Train Epoch: 23 [3360/7471 (45%)]\tLoss: 1594387.500000\n",
            "Train Epoch: 23 [3520/7471 (47%)]\tLoss: 1590558.750000\n",
            "Train Epoch: 23 [3680/7471 (49%)]\tLoss: 1593239.500000\n",
            "Train Epoch: 23 [3840/7471 (51%)]\tLoss: 1591594.000000\n",
            "Train Epoch: 23 [4000/7471 (54%)]\tLoss: 1627522.875000\n",
            "Train Epoch: 23 [4160/7471 (56%)]\tLoss: 1637407.250000\n",
            "Train Epoch: 23 [4320/7471 (58%)]\tLoss: 1615813.500000\n",
            "Train Epoch: 23 [4480/7471 (60%)]\tLoss: 1616437.750000\n",
            "Train Epoch: 23 [4640/7471 (62%)]\tLoss: 1536719.500000\n",
            "Train Epoch: 23 [4800/7471 (64%)]\tLoss: 1614723.750000\n",
            "Train Epoch: 23 [4960/7471 (66%)]\tLoss: 1540469.500000\n",
            "Train Epoch: 23 [5120/7471 (69%)]\tLoss: 1634024.250000\n",
            "Train Epoch: 23 [5280/7471 (71%)]\tLoss: 1589573.250000\n",
            "Train Epoch: 23 [5440/7471 (73%)]\tLoss: 1617629.375000\n",
            "Train Epoch: 23 [5600/7471 (75%)]\tLoss: 1593918.875000\n",
            "Train Epoch: 23 [5760/7471 (77%)]\tLoss: 1640370.375000\n",
            "Train Epoch: 23 [5920/7471 (79%)]\tLoss: 1613511.250000\n",
            "Train Epoch: 23 [6080/7471 (81%)]\tLoss: 1568180.125000\n",
            "Train Epoch: 23 [6240/7471 (84%)]\tLoss: 1569082.875000\n",
            "Train Epoch: 23 [6400/7471 (86%)]\tLoss: 1596247.125000\n",
            "Train Epoch: 23 [6560/7471 (88%)]\tLoss: 1642165.000000\n",
            "Train Epoch: 23 [6720/7471 (90%)]\tLoss: 1585645.750000\n",
            "Train Epoch: 23 [6880/7471 (92%)]\tLoss: 1581405.000000\n",
            "Train Epoch: 23 [7040/7471 (94%)]\tLoss: 1632807.750000\n",
            "Train Epoch: 23 [7200/7471 (96%)]\tLoss: 1591224.250000\n",
            "Train Epoch: 23 [7360/7471 (99%)]\tLoss: 1523659.250000\n",
            "Epoch 23 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 24 [160/7471 (2%)]\tLoss: 1590561.125000\n",
            "Train Epoch: 24 [320/7471 (4%)]\tLoss: 1515764.625000\n",
            "Train Epoch: 24 [480/7471 (6%)]\tLoss: 1553047.000000\n",
            "Train Epoch: 24 [640/7471 (9%)]\tLoss: 1576052.125000\n",
            "Train Epoch: 24 [800/7471 (11%)]\tLoss: 1556732.625000\n",
            "Train Epoch: 24 [960/7471 (13%)]\tLoss: 1631955.125000\n",
            "Train Epoch: 24 [1120/7471 (15%)]\tLoss: 1580229.750000\n",
            "Train Epoch: 24 [1280/7471 (17%)]\tLoss: 1494567.750000\n",
            "Train Epoch: 24 [1440/7471 (19%)]\tLoss: 1622526.250000\n",
            "Train Epoch: 24 [1600/7471 (21%)]\tLoss: 1626084.125000\n",
            "Train Epoch: 24 [1760/7471 (24%)]\tLoss: 1625940.375000\n",
            "Train Epoch: 24 [1920/7471 (26%)]\tLoss: 1611241.250000\n",
            "Train Epoch: 24 [2080/7471 (28%)]\tLoss: 1624619.625000\n",
            "Train Epoch: 24 [2240/7471 (30%)]\tLoss: 1558172.125000\n",
            "Train Epoch: 24 [2400/7471 (32%)]\tLoss: 1615635.250000\n",
            "Train Epoch: 24 [2560/7471 (34%)]\tLoss: 1540068.625000\n",
            "Train Epoch: 24 [2720/7471 (36%)]\tLoss: 1561818.125000\n",
            "Train Epoch: 24 [2880/7471 (39%)]\tLoss: 1579490.250000\n",
            "Train Epoch: 24 [3040/7471 (41%)]\tLoss: 1572991.000000\n",
            "Train Epoch: 24 [3200/7471 (43%)]\tLoss: 1559473.125000\n",
            "Train Epoch: 24 [3360/7471 (45%)]\tLoss: 1629740.875000\n",
            "Train Epoch: 24 [3520/7471 (47%)]\tLoss: 1626872.000000\n",
            "Train Epoch: 24 [3680/7471 (49%)]\tLoss: 1557303.750000\n",
            "Train Epoch: 24 [3840/7471 (51%)]\tLoss: 1570463.500000\n",
            "Train Epoch: 24 [4000/7471 (54%)]\tLoss: 1617146.625000\n",
            "Train Epoch: 24 [4160/7471 (56%)]\tLoss: 1621117.125000\n",
            "Train Epoch: 24 [4320/7471 (58%)]\tLoss: 1537317.000000\n",
            "Train Epoch: 24 [4480/7471 (60%)]\tLoss: 1589717.875000\n",
            "Train Epoch: 24 [4640/7471 (62%)]\tLoss: 1517229.250000\n",
            "Train Epoch: 24 [4800/7471 (64%)]\tLoss: 1603392.250000\n",
            "Train Epoch: 24 [4960/7471 (66%)]\tLoss: 1610095.875000\n",
            "Train Epoch: 24 [5120/7471 (69%)]\tLoss: 1573447.875000\n",
            "Train Epoch: 24 [5280/7471 (71%)]\tLoss: 1557226.750000\n",
            "Train Epoch: 24 [5440/7471 (73%)]\tLoss: 1504964.125000\n",
            "Train Epoch: 24 [5600/7471 (75%)]\tLoss: 1581156.500000\n",
            "Train Epoch: 24 [5760/7471 (77%)]\tLoss: 1609508.250000\n",
            "Train Epoch: 24 [5920/7471 (79%)]\tLoss: 1576976.750000\n",
            "Train Epoch: 24 [6080/7471 (81%)]\tLoss: 1626739.500000\n",
            "Train Epoch: 24 [6240/7471 (84%)]\tLoss: 1591411.500000\n",
            "Train Epoch: 24 [6400/7471 (86%)]\tLoss: 1598469.000000\n",
            "Train Epoch: 24 [6560/7471 (88%)]\tLoss: 1586589.125000\n",
            "Train Epoch: 24 [6720/7471 (90%)]\tLoss: 1621330.750000\n",
            "Train Epoch: 24 [6880/7471 (92%)]\tLoss: 1601105.375000\n",
            "Train Epoch: 24 [7040/7471 (94%)]\tLoss: 1627032.750000\n",
            "Train Epoch: 24 [7200/7471 (96%)]\tLoss: 1599168.250000\n",
            "Train Epoch: 24 [7360/7471 (99%)]\tLoss: 1567331.625000\n",
            "Epoch 24 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 25 [160/7471 (2%)]\tLoss: 1616227.625000\n",
            "Train Epoch: 25 [320/7471 (4%)]\tLoss: 1610847.875000\n",
            "Train Epoch: 25 [480/7471 (6%)]\tLoss: 1639008.250000\n",
            "Train Epoch: 25 [640/7471 (9%)]\tLoss: 1576231.250000\n",
            "Train Epoch: 25 [800/7471 (11%)]\tLoss: 1543584.125000\n",
            "Train Epoch: 25 [960/7471 (13%)]\tLoss: 1570320.000000\n",
            "Train Epoch: 25 [1120/7471 (15%)]\tLoss: 1591498.375000\n",
            "Train Epoch: 25 [1280/7471 (17%)]\tLoss: 1641868.750000\n",
            "Train Epoch: 25 [1440/7471 (19%)]\tLoss: 1630250.375000\n",
            "Train Epoch: 25 [1600/7471 (21%)]\tLoss: 1553346.750000\n",
            "Train Epoch: 25 [1760/7471 (24%)]\tLoss: 1596944.500000\n",
            "Train Epoch: 25 [1920/7471 (26%)]\tLoss: 1625777.875000\n",
            "Train Epoch: 25 [2080/7471 (28%)]\tLoss: 1610605.625000\n",
            "Train Epoch: 25 [2240/7471 (30%)]\tLoss: 1597316.250000\n",
            "Train Epoch: 25 [2400/7471 (32%)]\tLoss: 1604403.875000\n",
            "Train Epoch: 25 [2560/7471 (34%)]\tLoss: 1591594.625000\n",
            "Train Epoch: 25 [2720/7471 (36%)]\tLoss: 1601729.125000\n",
            "Train Epoch: 25 [2880/7471 (39%)]\tLoss: 1625471.625000\n",
            "Train Epoch: 25 [3040/7471 (41%)]\tLoss: 1539560.625000\n",
            "Train Epoch: 25 [3200/7471 (43%)]\tLoss: 1577094.625000\n",
            "Train Epoch: 25 [3360/7471 (45%)]\tLoss: 1598244.625000\n",
            "Train Epoch: 25 [3520/7471 (47%)]\tLoss: 1584225.250000\n",
            "Train Epoch: 25 [3680/7471 (49%)]\tLoss: 1593768.750000\n",
            "Train Epoch: 25 [3840/7471 (51%)]\tLoss: 1603505.875000\n",
            "Train Epoch: 25 [4000/7471 (54%)]\tLoss: 1552096.250000\n",
            "Train Epoch: 25 [4160/7471 (56%)]\tLoss: 1550736.250000\n",
            "Train Epoch: 25 [4320/7471 (58%)]\tLoss: 1593719.750000\n",
            "Train Epoch: 25 [4480/7471 (60%)]\tLoss: 1634656.875000\n",
            "Train Epoch: 25 [4640/7471 (62%)]\tLoss: 1525160.750000\n",
            "Train Epoch: 25 [4800/7471 (64%)]\tLoss: 1582801.000000\n",
            "Train Epoch: 25 [4960/7471 (66%)]\tLoss: 1611222.625000\n",
            "Train Epoch: 25 [5120/7471 (69%)]\tLoss: 1615703.500000\n",
            "Train Epoch: 25 [5280/7471 (71%)]\tLoss: 1555102.125000\n",
            "Train Epoch: 25 [5440/7471 (73%)]\tLoss: 1566505.375000\n",
            "Train Epoch: 25 [5600/7471 (75%)]\tLoss: 1518353.750000\n",
            "Train Epoch: 25 [5760/7471 (77%)]\tLoss: 1526427.000000\n",
            "Train Epoch: 25 [5920/7471 (79%)]\tLoss: 1626920.750000\n",
            "Train Epoch: 25 [6080/7471 (81%)]\tLoss: 1611972.375000\n",
            "Train Epoch: 25 [6240/7471 (84%)]\tLoss: 1518000.125000\n",
            "Train Epoch: 25 [6400/7471 (86%)]\tLoss: 1625701.500000\n",
            "Train Epoch: 25 [6560/7471 (88%)]\tLoss: 1564003.250000\n",
            "Train Epoch: 25 [6720/7471 (90%)]\tLoss: 1566932.500000\n",
            "Train Epoch: 25 [6880/7471 (92%)]\tLoss: 1585277.500000\n",
            "Train Epoch: 25 [7040/7471 (94%)]\tLoss: 1575378.500000\n",
            "Train Epoch: 25 [7200/7471 (96%)]\tLoss: 1647869.750000\n",
            "Train Epoch: 25 [7360/7471 (99%)]\tLoss: 1566393.375000\n",
            "Epoch 25 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 99112.9865\n",
            "\n",
            "Train Epoch: 26 [160/7471 (2%)]\tLoss: 1585473.625000\n",
            "Train Epoch: 26 [320/7471 (4%)]\tLoss: 1558632.875000\n",
            "Train Epoch: 26 [480/7471 (6%)]\tLoss: 1604970.750000\n",
            "Train Epoch: 26 [640/7471 (9%)]\tLoss: 1606024.625000\n",
            "Train Epoch: 26 [800/7471 (11%)]\tLoss: 1619187.375000\n",
            "Train Epoch: 26 [960/7471 (13%)]\tLoss: 1617979.875000\n",
            "Train Epoch: 26 [1120/7471 (15%)]\tLoss: 1544862.625000\n",
            "Train Epoch: 26 [1280/7471 (17%)]\tLoss: 1595689.250000\n",
            "Train Epoch: 26 [1440/7471 (19%)]\tLoss: 1545747.500000\n",
            "Train Epoch: 26 [1600/7471 (21%)]\tLoss: 1610574.500000\n",
            "Train Epoch: 26 [1760/7471 (24%)]\tLoss: 1535779.500000\n",
            "Train Epoch: 26 [1920/7471 (26%)]\tLoss: 1633128.125000\n",
            "Train Epoch: 26 [2080/7471 (28%)]\tLoss: 1631964.125000\n",
            "Train Epoch: 26 [2240/7471 (30%)]\tLoss: 1563158.625000\n",
            "Train Epoch: 26 [2400/7471 (32%)]\tLoss: 1591300.375000\n",
            "Train Epoch: 26 [2560/7471 (34%)]\tLoss: 1613517.375000\n",
            "Train Epoch: 26 [2720/7471 (36%)]\tLoss: 1595263.750000\n",
            "Train Epoch: 26 [2880/7471 (39%)]\tLoss: 1507680.750000\n",
            "Train Epoch: 26 [3040/7471 (41%)]\tLoss: 1588715.500000\n",
            "Train Epoch: 26 [3200/7471 (43%)]\tLoss: 1593561.625000\n",
            "Train Epoch: 26 [3360/7471 (45%)]\tLoss: 1592788.875000\n",
            "Train Epoch: 26 [3520/7471 (47%)]\tLoss: 1599587.375000\n",
            "Train Epoch: 26 [3680/7471 (49%)]\tLoss: 1549175.125000\n",
            "Train Epoch: 26 [3840/7471 (51%)]\tLoss: 1600797.875000\n",
            "Train Epoch: 26 [4000/7471 (54%)]\tLoss: 1587960.500000\n",
            "Train Epoch: 26 [4160/7471 (56%)]\tLoss: 1582829.250000\n",
            "Train Epoch: 26 [4320/7471 (58%)]\tLoss: 1596240.625000\n",
            "Train Epoch: 26 [4480/7471 (60%)]\tLoss: 1599618.125000\n",
            "Train Epoch: 26 [4640/7471 (62%)]\tLoss: 1604410.750000\n",
            "Train Epoch: 26 [4800/7471 (64%)]\tLoss: 1619898.625000\n",
            "Train Epoch: 26 [4960/7471 (66%)]\tLoss: 1634936.750000\n",
            "Train Epoch: 26 [5120/7471 (69%)]\tLoss: 1633808.000000\n",
            "Train Epoch: 26 [5280/7471 (71%)]\tLoss: 1527734.750000\n",
            "Train Epoch: 26 [5440/7471 (73%)]\tLoss: 1613764.500000\n",
            "Train Epoch: 26 [5600/7471 (75%)]\tLoss: 1590489.000000\n",
            "Train Epoch: 26 [5760/7471 (77%)]\tLoss: 1570831.250000\n",
            "Train Epoch: 26 [5920/7471 (79%)]\tLoss: 1580298.250000\n",
            "Train Epoch: 26 [6080/7471 (81%)]\tLoss: 1592188.000000\n",
            "Train Epoch: 26 [6240/7471 (84%)]\tLoss: 1639271.125000\n",
            "Train Epoch: 26 [6400/7471 (86%)]\tLoss: 1582390.625000\n",
            "Train Epoch: 26 [6560/7471 (88%)]\tLoss: 1567006.500000\n",
            "Train Epoch: 26 [6720/7471 (90%)]\tLoss: 1619691.375000\n",
            "Train Epoch: 26 [6880/7471 (92%)]\tLoss: 1600030.375000\n",
            "Train Epoch: 26 [7040/7471 (94%)]\tLoss: 1620915.375000\n",
            "Train Epoch: 26 [7200/7471 (96%)]\tLoss: 1566296.875000\n",
            "Train Epoch: 26 [7360/7471 (99%)]\tLoss: 1584645.625000\n",
            "Epoch 26 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 27 [160/7471 (2%)]\tLoss: 1548505.500000\n",
            "Train Epoch: 27 [320/7471 (4%)]\tLoss: 1638204.500000\n",
            "Train Epoch: 27 [480/7471 (6%)]\tLoss: 1638069.375000\n",
            "Train Epoch: 27 [640/7471 (9%)]\tLoss: 1577613.875000\n",
            "Train Epoch: 27 [800/7471 (11%)]\tLoss: 1614765.375000\n",
            "Train Epoch: 27 [960/7471 (13%)]\tLoss: 1640597.000000\n",
            "Train Epoch: 27 [1120/7471 (15%)]\tLoss: 1562803.250000\n",
            "Train Epoch: 27 [1280/7471 (17%)]\tLoss: 1624816.750000\n",
            "Train Epoch: 27 [1440/7471 (19%)]\tLoss: 1594742.750000\n",
            "Train Epoch: 27 [1600/7471 (21%)]\tLoss: 1603284.375000\n",
            "Train Epoch: 27 [1760/7471 (24%)]\tLoss: 1647574.125000\n",
            "Train Epoch: 27 [1920/7471 (26%)]\tLoss: 1622895.000000\n",
            "Train Epoch: 27 [2080/7471 (28%)]\tLoss: 1651681.375000\n",
            "Train Epoch: 27 [2240/7471 (30%)]\tLoss: 1578373.875000\n",
            "Train Epoch: 27 [2400/7471 (32%)]\tLoss: 1603808.500000\n",
            "Train Epoch: 27 [2560/7471 (34%)]\tLoss: 1620005.125000\n",
            "Train Epoch: 27 [2720/7471 (36%)]\tLoss: 1579744.375000\n",
            "Train Epoch: 27 [2880/7471 (39%)]\tLoss: 1591931.625000\n",
            "Train Epoch: 27 [3040/7471 (41%)]\tLoss: 1605574.000000\n",
            "Train Epoch: 27 [3200/7471 (43%)]\tLoss: 1567395.375000\n",
            "Train Epoch: 27 [3360/7471 (45%)]\tLoss: 1612714.750000\n",
            "Train Epoch: 27 [3520/7471 (47%)]\tLoss: 1572816.500000\n",
            "Train Epoch: 27 [3680/7471 (49%)]\tLoss: 1536642.750000\n",
            "Train Epoch: 27 [3840/7471 (51%)]\tLoss: 1471127.500000\n",
            "Train Epoch: 27 [4000/7471 (54%)]\tLoss: 1515883.375000\n",
            "Train Epoch: 27 [4160/7471 (56%)]\tLoss: 1562123.625000\n",
            "Train Epoch: 27 [4320/7471 (58%)]\tLoss: 1616099.250000\n",
            "Train Epoch: 27 [4480/7471 (60%)]\tLoss: 1547074.000000\n",
            "Train Epoch: 27 [4640/7471 (62%)]\tLoss: 1546451.500000\n",
            "Train Epoch: 27 [4800/7471 (64%)]\tLoss: 1603909.750000\n",
            "Train Epoch: 27 [4960/7471 (66%)]\tLoss: 1613655.250000\n",
            "Train Epoch: 27 [5120/7471 (69%)]\tLoss: 1534848.125000\n",
            "Train Epoch: 27 [5280/7471 (71%)]\tLoss: 1560816.250000\n",
            "Train Epoch: 27 [5440/7471 (73%)]\tLoss: 1563315.375000\n",
            "Train Epoch: 27 [5600/7471 (75%)]\tLoss: 1586976.500000\n",
            "Train Epoch: 27 [5760/7471 (77%)]\tLoss: 1590745.875000\n",
            "Train Epoch: 27 [5920/7471 (79%)]\tLoss: 1580414.125000\n",
            "Train Epoch: 27 [6080/7471 (81%)]\tLoss: 1589181.625000\n",
            "Train Epoch: 27 [6240/7471 (84%)]\tLoss: 1615162.750000\n",
            "Train Epoch: 27 [6400/7471 (86%)]\tLoss: 1558023.750000\n",
            "Train Epoch: 27 [6560/7471 (88%)]\tLoss: 1572974.250000\n",
            "Train Epoch: 27 [6720/7471 (90%)]\tLoss: 1562101.000000\n",
            "Train Epoch: 27 [6880/7471 (92%)]\tLoss: 1588747.750000\n",
            "Train Epoch: 27 [7040/7471 (94%)]\tLoss: 1582402.750000\n",
            "Train Epoch: 27 [7200/7471 (96%)]\tLoss: 1643842.625000\n",
            "Train Epoch: 27 [7360/7471 (99%)]\tLoss: 1574922.250000\n",
            "Epoch 27 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 28 [160/7471 (2%)]\tLoss: 1552265.875000\n",
            "Train Epoch: 28 [320/7471 (4%)]\tLoss: 1597789.500000\n",
            "Train Epoch: 28 [480/7471 (6%)]\tLoss: 1631642.375000\n",
            "Train Epoch: 28 [640/7471 (9%)]\tLoss: 1616043.500000\n",
            "Train Epoch: 28 [800/7471 (11%)]\tLoss: 1589985.750000\n",
            "Train Epoch: 28 [960/7471 (13%)]\tLoss: 1567141.375000\n",
            "Train Epoch: 28 [1120/7471 (15%)]\tLoss: 1607521.750000\n",
            "Train Epoch: 28 [1280/7471 (17%)]\tLoss: 1498058.875000\n",
            "Train Epoch: 28 [1440/7471 (19%)]\tLoss: 1492814.625000\n",
            "Train Epoch: 28 [1600/7471 (21%)]\tLoss: 1573812.125000\n",
            "Train Epoch: 28 [1760/7471 (24%)]\tLoss: 1508839.625000\n",
            "Train Epoch: 28 [1920/7471 (26%)]\tLoss: 1597981.000000\n",
            "Train Epoch: 28 [2080/7471 (28%)]\tLoss: 1556848.125000\n",
            "Train Epoch: 28 [2240/7471 (30%)]\tLoss: 1566678.750000\n",
            "Train Epoch: 28 [2400/7471 (32%)]\tLoss: 1514197.250000\n",
            "Train Epoch: 28 [2560/7471 (34%)]\tLoss: 1532758.875000\n",
            "Train Epoch: 28 [2720/7471 (36%)]\tLoss: 1580755.875000\n",
            "Train Epoch: 28 [2880/7471 (39%)]\tLoss: 1622000.875000\n",
            "Train Epoch: 28 [3040/7471 (41%)]\tLoss: 1566369.125000\n",
            "Train Epoch: 28 [3200/7471 (43%)]\tLoss: 1563621.000000\n",
            "Train Epoch: 28 [3360/7471 (45%)]\tLoss: 1543423.500000\n",
            "Train Epoch: 28 [3520/7471 (47%)]\tLoss: 1588647.625000\n",
            "Train Epoch: 28 [3680/7471 (49%)]\tLoss: 1587070.625000\n",
            "Train Epoch: 28 [3840/7471 (51%)]\tLoss: 1610230.125000\n",
            "Train Epoch: 28 [4000/7471 (54%)]\tLoss: 1628174.375000\n",
            "Train Epoch: 28 [4160/7471 (56%)]\tLoss: 1586573.750000\n",
            "Train Epoch: 28 [4320/7471 (58%)]\tLoss: 1653734.875000\n",
            "Train Epoch: 28 [4480/7471 (60%)]\tLoss: 1557531.250000\n",
            "Train Epoch: 28 [4640/7471 (62%)]\tLoss: 1568882.500000\n",
            "Train Epoch: 28 [4800/7471 (64%)]\tLoss: 1613395.875000\n",
            "Train Epoch: 28 [4960/7471 (66%)]\tLoss: 1566433.625000\n",
            "Train Epoch: 28 [5120/7471 (69%)]\tLoss: 1546214.625000\n",
            "Train Epoch: 28 [5280/7471 (71%)]\tLoss: 1558105.250000\n",
            "Train Epoch: 28 [5440/7471 (73%)]\tLoss: 1502504.500000\n",
            "Train Epoch: 28 [5600/7471 (75%)]\tLoss: 1627041.500000\n",
            "Train Epoch: 28 [5760/7471 (77%)]\tLoss: 1589691.375000\n",
            "Train Epoch: 28 [5920/7471 (79%)]\tLoss: 1619381.250000\n",
            "Train Epoch: 28 [6080/7471 (81%)]\tLoss: 1562801.000000\n",
            "Train Epoch: 28 [6240/7471 (84%)]\tLoss: 1612377.750000\n",
            "Train Epoch: 28 [6400/7471 (86%)]\tLoss: 1566282.500000\n",
            "Train Epoch: 28 [6560/7471 (88%)]\tLoss: 1524566.000000\n",
            "Train Epoch: 28 [6720/7471 (90%)]\tLoss: 1536252.250000\n",
            "Train Epoch: 28 [6880/7471 (92%)]\tLoss: 1550928.625000\n",
            "Train Epoch: 28 [7040/7471 (94%)]\tLoss: 1535884.375000\n",
            "Train Epoch: 28 [7200/7471 (96%)]\tLoss: 1557496.375000\n",
            "Train Epoch: 28 [7360/7471 (99%)]\tLoss: 1593716.750000\n",
            "Epoch 28 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 29 [160/7471 (2%)]\tLoss: 1631484.250000\n",
            "Train Epoch: 29 [320/7471 (4%)]\tLoss: 1612445.750000\n",
            "Train Epoch: 29 [480/7471 (6%)]\tLoss: 1563539.500000\n",
            "Train Epoch: 29 [640/7471 (9%)]\tLoss: 1552536.250000\n",
            "Train Epoch: 29 [800/7471 (11%)]\tLoss: 1537642.125000\n",
            "Train Epoch: 29 [960/7471 (13%)]\tLoss: 1644382.250000\n",
            "Train Epoch: 29 [1120/7471 (15%)]\tLoss: 1638512.125000\n",
            "Train Epoch: 29 [1280/7471 (17%)]\tLoss: 1552298.500000\n",
            "Train Epoch: 29 [1440/7471 (19%)]\tLoss: 1559952.500000\n",
            "Train Epoch: 29 [1600/7471 (21%)]\tLoss: 1581292.500000\n",
            "Train Epoch: 29 [1760/7471 (24%)]\tLoss: 1552098.500000\n",
            "Train Epoch: 29 [1920/7471 (26%)]\tLoss: 1542109.375000\n",
            "Train Epoch: 29 [2080/7471 (28%)]\tLoss: 1601541.500000\n",
            "Train Epoch: 29 [2240/7471 (30%)]\tLoss: 1610233.750000\n",
            "Train Epoch: 29 [2400/7471 (32%)]\tLoss: 1572339.625000\n",
            "Train Epoch: 29 [2560/7471 (34%)]\tLoss: 1564902.500000\n",
            "Train Epoch: 29 [2720/7471 (36%)]\tLoss: 1612492.625000\n",
            "Train Epoch: 29 [2880/7471 (39%)]\tLoss: 1534305.625000\n",
            "Train Epoch: 29 [3040/7471 (41%)]\tLoss: 1575177.125000\n",
            "Train Epoch: 29 [3200/7471 (43%)]\tLoss: 1623259.125000\n",
            "Train Epoch: 29 [3360/7471 (45%)]\tLoss: 1573879.250000\n",
            "Train Epoch: 29 [3520/7471 (47%)]\tLoss: 1594026.000000\n",
            "Train Epoch: 29 [3680/7471 (49%)]\tLoss: 1604019.250000\n",
            "Train Epoch: 29 [3840/7471 (51%)]\tLoss: 1599413.250000\n",
            "Train Epoch: 29 [4000/7471 (54%)]\tLoss: 1551970.500000\n",
            "Train Epoch: 29 [4160/7471 (56%)]\tLoss: 1626112.125000\n",
            "Train Epoch: 29 [4320/7471 (58%)]\tLoss: 1552315.250000\n",
            "Train Epoch: 29 [4480/7471 (60%)]\tLoss: 1593775.250000\n",
            "Train Epoch: 29 [4640/7471 (62%)]\tLoss: 1617416.125000\n",
            "Train Epoch: 29 [4800/7471 (64%)]\tLoss: 1601348.375000\n",
            "Train Epoch: 29 [4960/7471 (66%)]\tLoss: 1532113.875000\n",
            "Train Epoch: 29 [5120/7471 (69%)]\tLoss: 1538123.375000\n",
            "Train Epoch: 29 [5280/7471 (71%)]\tLoss: 1615191.000000\n",
            "Train Epoch: 29 [5440/7471 (73%)]\tLoss: 1569751.250000\n",
            "Train Epoch: 29 [5600/7471 (75%)]\tLoss: 1635807.750000\n",
            "Train Epoch: 29 [5760/7471 (77%)]\tLoss: 1568726.750000\n",
            "Train Epoch: 29 [5920/7471 (79%)]\tLoss: 1598137.250000\n",
            "Train Epoch: 29 [6080/7471 (81%)]\tLoss: 1615723.250000\n",
            "Train Epoch: 29 [6240/7471 (84%)]\tLoss: 1554051.000000\n",
            "Train Epoch: 29 [6400/7471 (86%)]\tLoss: 1491685.750000\n",
            "Train Epoch: 29 [6560/7471 (88%)]\tLoss: 1635252.500000\n",
            "Train Epoch: 29 [6720/7471 (90%)]\tLoss: 1623542.250000\n",
            "Train Epoch: 29 [6880/7471 (92%)]\tLoss: 1589533.875000\n",
            "Train Epoch: 29 [7040/7471 (94%)]\tLoss: 1602172.250000\n",
            "Train Epoch: 29 [7200/7471 (96%)]\tLoss: 1558970.750000\n",
            "Train Epoch: 29 [7360/7471 (99%)]\tLoss: 1597274.750000\n",
            "Epoch 29 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 30 [160/7471 (2%)]\tLoss: 1578465.750000\n",
            "Train Epoch: 30 [320/7471 (4%)]\tLoss: 1618643.000000\n",
            "Train Epoch: 30 [480/7471 (6%)]\tLoss: 1631933.125000\n",
            "Train Epoch: 30 [640/7471 (9%)]\tLoss: 1548556.500000\n",
            "Train Epoch: 30 [800/7471 (11%)]\tLoss: 1568497.500000\n",
            "Train Epoch: 30 [960/7471 (13%)]\tLoss: 1532670.750000\n",
            "Train Epoch: 30 [1120/7471 (15%)]\tLoss: 1541585.375000\n",
            "Train Epoch: 30 [1280/7471 (17%)]\tLoss: 1574575.125000\n",
            "Train Epoch: 30 [1440/7471 (19%)]\tLoss: 1539603.375000\n",
            "Train Epoch: 30 [1600/7471 (21%)]\tLoss: 1592164.875000\n",
            "Train Epoch: 30 [1760/7471 (24%)]\tLoss: 1591782.625000\n",
            "Train Epoch: 30 [1920/7471 (26%)]\tLoss: 1624378.250000\n",
            "Train Epoch: 30 [2080/7471 (28%)]\tLoss: 1566449.500000\n",
            "Train Epoch: 30 [2240/7471 (30%)]\tLoss: 1550551.750000\n",
            "Train Epoch: 30 [2400/7471 (32%)]\tLoss: 1505334.875000\n",
            "Train Epoch: 30 [2560/7471 (34%)]\tLoss: 1574010.375000\n",
            "Train Epoch: 30 [2720/7471 (36%)]\tLoss: 1608979.625000\n",
            "Train Epoch: 30 [2880/7471 (39%)]\tLoss: 1561140.125000\n",
            "Train Epoch: 30 [3040/7471 (41%)]\tLoss: 1603429.375000\n",
            "Train Epoch: 30 [3200/7471 (43%)]\tLoss: 1570827.125000\n",
            "Train Epoch: 30 [3360/7471 (45%)]\tLoss: 1544406.375000\n",
            "Train Epoch: 30 [3520/7471 (47%)]\tLoss: 1596345.125000\n",
            "Train Epoch: 30 [3680/7471 (49%)]\tLoss: 1563632.250000\n",
            "Train Epoch: 30 [3840/7471 (51%)]\tLoss: 1629888.750000\n",
            "Train Epoch: 30 [4000/7471 (54%)]\tLoss: 1587790.500000\n",
            "Train Epoch: 30 [4160/7471 (56%)]\tLoss: 1571991.000000\n",
            "Train Epoch: 30 [4320/7471 (58%)]\tLoss: 1513766.750000\n",
            "Train Epoch: 30 [4480/7471 (60%)]\tLoss: 1604342.125000\n",
            "Train Epoch: 30 [4640/7471 (62%)]\tLoss: 1568022.625000\n",
            "Train Epoch: 30 [4800/7471 (64%)]\tLoss: 1601851.500000\n",
            "Train Epoch: 30 [4960/7471 (66%)]\tLoss: 1647777.875000\n",
            "Train Epoch: 30 [5120/7471 (69%)]\tLoss: 1556865.875000\n",
            "Train Epoch: 30 [5280/7471 (71%)]\tLoss: 1527313.750000\n",
            "Train Epoch: 30 [5440/7471 (73%)]\tLoss: 1570404.750000\n",
            "Train Epoch: 30 [5600/7471 (75%)]\tLoss: 1575758.875000\n",
            "Train Epoch: 30 [5760/7471 (77%)]\tLoss: 1590149.000000\n",
            "Train Epoch: 30 [5920/7471 (79%)]\tLoss: 1612281.500000\n",
            "Train Epoch: 30 [6080/7471 (81%)]\tLoss: 1529396.750000\n",
            "Train Epoch: 30 [6240/7471 (84%)]\tLoss: 1612901.875000\n",
            "Train Epoch: 30 [6400/7471 (86%)]\tLoss: 1551288.125000\n",
            "Train Epoch: 30 [6560/7471 (88%)]\tLoss: 1644063.750000\n",
            "Train Epoch: 30 [6720/7471 (90%)]\tLoss: 1521367.500000\n",
            "Train Epoch: 30 [6880/7471 (92%)]\tLoss: 1614062.375000\n",
            "Train Epoch: 30 [7040/7471 (94%)]\tLoss: 1604467.500000\n",
            "Train Epoch: 30 [7200/7471 (96%)]\tLoss: 1628192.500000\n",
            "Train Epoch: 30 [7360/7471 (99%)]\tLoss: 1553088.875000\n",
            "Epoch 30 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 31 [160/7471 (2%)]\tLoss: 1638215.000000\n",
            "Train Epoch: 31 [320/7471 (4%)]\tLoss: 1625368.125000\n",
            "Train Epoch: 31 [480/7471 (6%)]\tLoss: 1595051.875000\n",
            "Train Epoch: 31 [640/7471 (9%)]\tLoss: 1626051.875000\n",
            "Train Epoch: 31 [800/7471 (11%)]\tLoss: 1545758.500000\n",
            "Train Epoch: 31 [960/7471 (13%)]\tLoss: 1561526.875000\n",
            "Train Epoch: 31 [1120/7471 (15%)]\tLoss: 1560966.250000\n",
            "Train Epoch: 31 [1280/7471 (17%)]\tLoss: 1557192.000000\n",
            "Train Epoch: 31 [1440/7471 (19%)]\tLoss: 1571599.250000\n",
            "Train Epoch: 31 [1600/7471 (21%)]\tLoss: 1533409.375000\n",
            "Train Epoch: 31 [1760/7471 (24%)]\tLoss: 1601587.000000\n",
            "Train Epoch: 31 [1920/7471 (26%)]\tLoss: 1579533.000000\n",
            "Train Epoch: 31 [2080/7471 (28%)]\tLoss: 1557205.125000\n",
            "Train Epoch: 31 [2240/7471 (30%)]\tLoss: 1594633.125000\n",
            "Train Epoch: 31 [2400/7471 (32%)]\tLoss: 1557207.875000\n",
            "Train Epoch: 31 [2560/7471 (34%)]\tLoss: 1586236.625000\n",
            "Train Epoch: 31 [2720/7471 (36%)]\tLoss: 1634816.000000\n",
            "Train Epoch: 31 [2880/7471 (39%)]\tLoss: 1611049.250000\n",
            "Train Epoch: 31 [3040/7471 (41%)]\tLoss: 1572779.000000\n",
            "Train Epoch: 31 [3200/7471 (43%)]\tLoss: 1571362.875000\n",
            "Train Epoch: 31 [3360/7471 (45%)]\tLoss: 1553106.875000\n",
            "Train Epoch: 31 [3520/7471 (47%)]\tLoss: 1562040.750000\n",
            "Train Epoch: 31 [3680/7471 (49%)]\tLoss: 1627640.625000\n",
            "Train Epoch: 31 [3840/7471 (51%)]\tLoss: 1563142.875000\n",
            "Train Epoch: 31 [4000/7471 (54%)]\tLoss: 1574080.250000\n",
            "Train Epoch: 31 [4160/7471 (56%)]\tLoss: 1544412.000000\n",
            "Train Epoch: 31 [4320/7471 (58%)]\tLoss: 1523018.875000\n",
            "Train Epoch: 31 [4480/7471 (60%)]\tLoss: 1597814.000000\n",
            "Train Epoch: 31 [4640/7471 (62%)]\tLoss: 1605341.875000\n",
            "Train Epoch: 31 [4800/7471 (64%)]\tLoss: 1594135.250000\n",
            "Train Epoch: 31 [4960/7471 (66%)]\tLoss: 1553531.875000\n",
            "Train Epoch: 31 [5120/7471 (69%)]\tLoss: 1556373.250000\n",
            "Train Epoch: 31 [5280/7471 (71%)]\tLoss: 1635660.500000\n",
            "Train Epoch: 31 [5440/7471 (73%)]\tLoss: 1599184.375000\n",
            "Train Epoch: 31 [5600/7471 (75%)]\tLoss: 1611203.000000\n",
            "Train Epoch: 31 [5760/7471 (77%)]\tLoss: 1587424.875000\n",
            "Train Epoch: 31 [5920/7471 (79%)]\tLoss: 1599029.750000\n",
            "Train Epoch: 31 [6080/7471 (81%)]\tLoss: 1590155.500000\n",
            "Train Epoch: 31 [6240/7471 (84%)]\tLoss: 1612477.625000\n",
            "Train Epoch: 31 [6400/7471 (86%)]\tLoss: 1609666.625000\n",
            "Train Epoch: 31 [6560/7471 (88%)]\tLoss: 1555545.875000\n",
            "Train Epoch: 31 [6720/7471 (90%)]\tLoss: 1546606.375000\n",
            "Train Epoch: 31 [6880/7471 (92%)]\tLoss: 1594124.125000\n",
            "Train Epoch: 31 [7040/7471 (94%)]\tLoss: 1580784.875000\n",
            "Train Epoch: 31 [7200/7471 (96%)]\tLoss: 1610844.000000\n",
            "Train Epoch: 31 [7360/7471 (99%)]\tLoss: 1599699.500000\n",
            "Epoch 31 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 32 [160/7471 (2%)]\tLoss: 1583772.000000\n",
            "Train Epoch: 32 [320/7471 (4%)]\tLoss: 1542797.375000\n",
            "Train Epoch: 32 [480/7471 (6%)]\tLoss: 1559996.250000\n",
            "Train Epoch: 32 [640/7471 (9%)]\tLoss: 1512281.875000\n",
            "Train Epoch: 32 [800/7471 (11%)]\tLoss: 1598265.875000\n",
            "Train Epoch: 32 [960/7471 (13%)]\tLoss: 1620449.375000\n",
            "Train Epoch: 32 [1120/7471 (15%)]\tLoss: 1566640.875000\n",
            "Train Epoch: 32 [1280/7471 (17%)]\tLoss: 1614038.250000\n",
            "Train Epoch: 32 [1440/7471 (19%)]\tLoss: 1626531.750000\n",
            "Train Epoch: 32 [1600/7471 (21%)]\tLoss: 1562486.250000\n",
            "Train Epoch: 32 [1760/7471 (24%)]\tLoss: 1531558.000000\n",
            "Train Epoch: 32 [1920/7471 (26%)]\tLoss: 1574842.875000\n",
            "Train Epoch: 32 [2080/7471 (28%)]\tLoss: 1576484.750000\n",
            "Train Epoch: 32 [2240/7471 (30%)]\tLoss: 1592002.750000\n",
            "Train Epoch: 32 [2400/7471 (32%)]\tLoss: 1605065.750000\n",
            "Train Epoch: 32 [2560/7471 (34%)]\tLoss: 1593841.250000\n",
            "Train Epoch: 32 [2720/7471 (36%)]\tLoss: 1621075.250000\n",
            "Train Epoch: 32 [2880/7471 (39%)]\tLoss: 1583533.375000\n",
            "Train Epoch: 32 [3040/7471 (41%)]\tLoss: 1600093.500000\n",
            "Train Epoch: 32 [3200/7471 (43%)]\tLoss: 1619037.375000\n",
            "Train Epoch: 32 [3360/7471 (45%)]\tLoss: 1621077.750000\n",
            "Train Epoch: 32 [3520/7471 (47%)]\tLoss: 1608164.000000\n",
            "Train Epoch: 32 [3680/7471 (49%)]\tLoss: 1631745.875000\n",
            "Train Epoch: 32 [3840/7471 (51%)]\tLoss: 1602874.750000\n",
            "Train Epoch: 32 [4000/7471 (54%)]\tLoss: 1497706.375000\n",
            "Train Epoch: 32 [4160/7471 (56%)]\tLoss: 1608600.250000\n",
            "Train Epoch: 32 [4320/7471 (58%)]\tLoss: 1604146.375000\n",
            "Train Epoch: 32 [4480/7471 (60%)]\tLoss: 1626866.500000\n",
            "Train Epoch: 32 [4640/7471 (62%)]\tLoss: 1624606.375000\n",
            "Train Epoch: 32 [4800/7471 (64%)]\tLoss: 1569519.000000\n",
            "Train Epoch: 32 [4960/7471 (66%)]\tLoss: 1577886.500000\n",
            "Train Epoch: 32 [5120/7471 (69%)]\tLoss: 1616102.250000\n",
            "Train Epoch: 32 [5280/7471 (71%)]\tLoss: 1640917.875000\n",
            "Train Epoch: 32 [5440/7471 (73%)]\tLoss: 1584856.750000\n",
            "Train Epoch: 32 [5600/7471 (75%)]\tLoss: 1613092.625000\n",
            "Train Epoch: 32 [5760/7471 (77%)]\tLoss: 1576291.250000\n",
            "Train Epoch: 32 [5920/7471 (79%)]\tLoss: 1539006.375000\n",
            "Train Epoch: 32 [6080/7471 (81%)]\tLoss: 1595043.625000\n",
            "Train Epoch: 32 [6240/7471 (84%)]\tLoss: 1589357.125000\n",
            "Train Epoch: 32 [6400/7471 (86%)]\tLoss: 1557485.250000\n",
            "Train Epoch: 32 [6560/7471 (88%)]\tLoss: 1600017.000000\n",
            "Train Epoch: 32 [6720/7471 (90%)]\tLoss: 1594668.500000\n",
            "Train Epoch: 32 [6880/7471 (92%)]\tLoss: 1597166.375000\n",
            "Train Epoch: 32 [7040/7471 (94%)]\tLoss: 1587653.750000\n",
            "Train Epoch: 32 [7200/7471 (96%)]\tLoss: 1571494.125000\n",
            "Train Epoch: 32 [7360/7471 (99%)]\tLoss: 1612121.250000\n",
            "Epoch 32 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 33 [160/7471 (2%)]\tLoss: 1614972.125000\n",
            "Train Epoch: 33 [320/7471 (4%)]\tLoss: 1591135.375000\n",
            "Train Epoch: 33 [480/7471 (6%)]\tLoss: 1601891.750000\n",
            "Train Epoch: 33 [640/7471 (9%)]\tLoss: 1575779.375000\n",
            "Train Epoch: 33 [800/7471 (11%)]\tLoss: 1570269.250000\n",
            "Train Epoch: 33 [960/7471 (13%)]\tLoss: 1510083.000000\n",
            "Train Epoch: 33 [1120/7471 (15%)]\tLoss: 1551091.625000\n",
            "Train Epoch: 33 [1280/7471 (17%)]\tLoss: 1490609.250000\n",
            "Train Epoch: 33 [1440/7471 (19%)]\tLoss: 1620044.250000\n",
            "Train Epoch: 33 [1600/7471 (21%)]\tLoss: 1478997.875000\n",
            "Train Epoch: 33 [1760/7471 (24%)]\tLoss: 1585512.750000\n",
            "Train Epoch: 33 [1920/7471 (26%)]\tLoss: 1641792.625000\n",
            "Train Epoch: 33 [2080/7471 (28%)]\tLoss: 1552723.250000\n",
            "Train Epoch: 33 [2240/7471 (30%)]\tLoss: 1618048.375000\n",
            "Train Epoch: 33 [2400/7471 (32%)]\tLoss: 1598132.875000\n",
            "Train Epoch: 33 [2560/7471 (34%)]\tLoss: 1568924.625000\n",
            "Train Epoch: 33 [2720/7471 (36%)]\tLoss: 1569725.875000\n",
            "Train Epoch: 33 [2880/7471 (39%)]\tLoss: 1559426.500000\n",
            "Train Epoch: 33 [3040/7471 (41%)]\tLoss: 1534306.875000\n",
            "Train Epoch: 33 [3200/7471 (43%)]\tLoss: 1616234.625000\n",
            "Train Epoch: 33 [3360/7471 (45%)]\tLoss: 1578816.750000\n",
            "Train Epoch: 33 [3520/7471 (47%)]\tLoss: 1550154.250000\n",
            "Train Epoch: 33 [3680/7471 (49%)]\tLoss: 1641933.375000\n",
            "Train Epoch: 33 [3840/7471 (51%)]\tLoss: 1573523.375000\n",
            "Train Epoch: 33 [4000/7471 (54%)]\tLoss: 1597623.500000\n",
            "Train Epoch: 33 [4160/7471 (56%)]\tLoss: 1553280.875000\n",
            "Train Epoch: 33 [4320/7471 (58%)]\tLoss: 1622427.375000\n",
            "Train Epoch: 33 [4480/7471 (60%)]\tLoss: 1521105.375000\n",
            "Train Epoch: 33 [4640/7471 (62%)]\tLoss: 1623031.875000\n",
            "Train Epoch: 33 [4800/7471 (64%)]\tLoss: 1627304.750000\n",
            "Train Epoch: 33 [4960/7471 (66%)]\tLoss: 1564096.750000\n",
            "Train Epoch: 33 [5120/7471 (69%)]\tLoss: 1562700.250000\n",
            "Train Epoch: 33 [5280/7471 (71%)]\tLoss: 1572657.750000\n",
            "Train Epoch: 33 [5440/7471 (73%)]\tLoss: 1563743.375000\n",
            "Train Epoch: 33 [5600/7471 (75%)]\tLoss: 1620823.500000\n",
            "Train Epoch: 33 [5760/7471 (77%)]\tLoss: 1590950.875000\n",
            "Train Epoch: 33 [5920/7471 (79%)]\tLoss: 1620925.750000\n",
            "Train Epoch: 33 [6080/7471 (81%)]\tLoss: 1599591.000000\n",
            "Train Epoch: 33 [6240/7471 (84%)]\tLoss: 1611338.125000\n",
            "Train Epoch: 33 [6400/7471 (86%)]\tLoss: 1538228.500000\n",
            "Train Epoch: 33 [6560/7471 (88%)]\tLoss: 1613924.000000\n",
            "Train Epoch: 33 [6720/7471 (90%)]\tLoss: 1591221.875000\n",
            "Train Epoch: 33 [6880/7471 (92%)]\tLoss: 1578477.500000\n",
            "Train Epoch: 33 [7040/7471 (94%)]\tLoss: 1560292.375000\n",
            "Train Epoch: 33 [7200/7471 (96%)]\tLoss: 1617221.250000\n",
            "Train Epoch: 33 [7360/7471 (99%)]\tLoss: 1592354.250000\n",
            "Epoch 33 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 34 [160/7471 (2%)]\tLoss: 1610135.250000\n",
            "Train Epoch: 34 [320/7471 (4%)]\tLoss: 1587111.250000\n",
            "Train Epoch: 34 [480/7471 (6%)]\tLoss: 1583014.000000\n",
            "Train Epoch: 34 [640/7471 (9%)]\tLoss: 1561543.750000\n",
            "Train Epoch: 34 [800/7471 (11%)]\tLoss: 1587553.750000\n",
            "Train Epoch: 34 [960/7471 (13%)]\tLoss: 1625151.625000\n",
            "Train Epoch: 34 [1120/7471 (15%)]\tLoss: 1570262.625000\n",
            "Train Epoch: 34 [1280/7471 (17%)]\tLoss: 1584233.875000\n",
            "Train Epoch: 34 [1440/7471 (19%)]\tLoss: 1588959.125000\n",
            "Train Epoch: 34 [1600/7471 (21%)]\tLoss: 1590838.875000\n",
            "Train Epoch: 34 [1760/7471 (24%)]\tLoss: 1564437.875000\n",
            "Train Epoch: 34 [1920/7471 (26%)]\tLoss: 1620543.625000\n",
            "Train Epoch: 34 [2080/7471 (28%)]\tLoss: 1582874.875000\n",
            "Train Epoch: 34 [2240/7471 (30%)]\tLoss: 1603756.125000\n",
            "Train Epoch: 34 [2400/7471 (32%)]\tLoss: 1633785.875000\n",
            "Train Epoch: 34 [2560/7471 (34%)]\tLoss: 1635469.125000\n",
            "Train Epoch: 34 [2720/7471 (36%)]\tLoss: 1578507.750000\n",
            "Train Epoch: 34 [2880/7471 (39%)]\tLoss: 1527988.000000\n",
            "Train Epoch: 34 [3040/7471 (41%)]\tLoss: 1633098.375000\n",
            "Train Epoch: 34 [3200/7471 (43%)]\tLoss: 1609212.375000\n",
            "Train Epoch: 34 [3360/7471 (45%)]\tLoss: 1583686.000000\n",
            "Train Epoch: 34 [3520/7471 (47%)]\tLoss: 1552323.500000\n",
            "Train Epoch: 34 [3680/7471 (49%)]\tLoss: 1567115.500000\n",
            "Train Epoch: 34 [3840/7471 (51%)]\tLoss: 1619882.875000\n",
            "Train Epoch: 34 [4000/7471 (54%)]\tLoss: 1544029.250000\n",
            "Train Epoch: 34 [4160/7471 (56%)]\tLoss: 1628577.125000\n",
            "Train Epoch: 34 [4320/7471 (58%)]\tLoss: 1589134.375000\n",
            "Train Epoch: 34 [4480/7471 (60%)]\tLoss: 1622194.125000\n",
            "Train Epoch: 34 [4640/7471 (62%)]\tLoss: 1564861.750000\n",
            "Train Epoch: 34 [4800/7471 (64%)]\tLoss: 1554570.375000\n",
            "Train Epoch: 34 [4960/7471 (66%)]\tLoss: 1533003.875000\n",
            "Train Epoch: 34 [5120/7471 (69%)]\tLoss: 1577700.625000\n",
            "Train Epoch: 34 [5280/7471 (71%)]\tLoss: 1466727.000000\n",
            "Train Epoch: 34 [5440/7471 (73%)]\tLoss: 1564732.625000\n",
            "Train Epoch: 34 [5600/7471 (75%)]\tLoss: 1578875.375000\n",
            "Train Epoch: 34 [5760/7471 (77%)]\tLoss: 1599308.750000\n",
            "Train Epoch: 34 [5920/7471 (79%)]\tLoss: 1595599.625000\n",
            "Train Epoch: 34 [6080/7471 (81%)]\tLoss: 1603304.250000\n",
            "Train Epoch: 34 [6240/7471 (84%)]\tLoss: 1548152.625000\n",
            "Train Epoch: 34 [6400/7471 (86%)]\tLoss: 1614805.750000\n",
            "Train Epoch: 34 [6560/7471 (88%)]\tLoss: 1566336.250000\n",
            "Train Epoch: 34 [6720/7471 (90%)]\tLoss: 1536119.750000\n",
            "Train Epoch: 34 [6880/7471 (92%)]\tLoss: 1553986.500000\n",
            "Train Epoch: 34 [7040/7471 (94%)]\tLoss: 1591920.750000\n",
            "Train Epoch: 34 [7200/7471 (96%)]\tLoss: 1618896.500000\n",
            "Train Epoch: 34 [7360/7471 (99%)]\tLoss: 1595491.000000\n",
            "Epoch 34 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 35 [160/7471 (2%)]\tLoss: 1602654.000000\n",
            "Train Epoch: 35 [320/7471 (4%)]\tLoss: 1593845.875000\n",
            "Train Epoch: 35 [480/7471 (6%)]\tLoss: 1566622.875000\n",
            "Train Epoch: 35 [640/7471 (9%)]\tLoss: 1567717.000000\n",
            "Train Epoch: 35 [800/7471 (11%)]\tLoss: 1567530.875000\n",
            "Train Epoch: 35 [960/7471 (13%)]\tLoss: 1599161.625000\n",
            "Train Epoch: 35 [1120/7471 (15%)]\tLoss: 1542673.875000\n",
            "Train Epoch: 35 [1280/7471 (17%)]\tLoss: 1575246.875000\n",
            "Train Epoch: 35 [1440/7471 (19%)]\tLoss: 1572496.375000\n",
            "Train Epoch: 35 [1600/7471 (21%)]\tLoss: 1614162.000000\n",
            "Train Epoch: 35 [1760/7471 (24%)]\tLoss: 1567744.750000\n",
            "Train Epoch: 35 [1920/7471 (26%)]\tLoss: 1569052.250000\n",
            "Train Epoch: 35 [2080/7471 (28%)]\tLoss: 1572815.000000\n",
            "Train Epoch: 35 [2240/7471 (30%)]\tLoss: 1603309.000000\n",
            "Train Epoch: 35 [2400/7471 (32%)]\tLoss: 1550178.250000\n",
            "Train Epoch: 35 [2560/7471 (34%)]\tLoss: 1574753.250000\n",
            "Train Epoch: 35 [2720/7471 (36%)]\tLoss: 1589648.500000\n",
            "Train Epoch: 35 [2880/7471 (39%)]\tLoss: 1584663.500000\n",
            "Train Epoch: 35 [3040/7471 (41%)]\tLoss: 1621052.625000\n",
            "Train Epoch: 35 [3200/7471 (43%)]\tLoss: 1599831.750000\n",
            "Train Epoch: 35 [3360/7471 (45%)]\tLoss: 1612055.750000\n",
            "Train Epoch: 35 [3520/7471 (47%)]\tLoss: 1633655.375000\n",
            "Train Epoch: 35 [3680/7471 (49%)]\tLoss: 1559772.875000\n",
            "Train Epoch: 35 [3840/7471 (51%)]\tLoss: 1569829.125000\n",
            "Train Epoch: 35 [4000/7471 (54%)]\tLoss: 1554366.500000\n",
            "Train Epoch: 35 [4160/7471 (56%)]\tLoss: 1643785.250000\n",
            "Train Epoch: 35 [4320/7471 (58%)]\tLoss: 1549367.750000\n",
            "Train Epoch: 35 [4480/7471 (60%)]\tLoss: 1553232.875000\n",
            "Train Epoch: 35 [4640/7471 (62%)]\tLoss: 1572254.500000\n",
            "Train Epoch: 35 [4800/7471 (64%)]\tLoss: 1639961.625000\n",
            "Train Epoch: 35 [4960/7471 (66%)]\tLoss: 1561066.250000\n",
            "Train Epoch: 35 [5120/7471 (69%)]\tLoss: 1584760.875000\n",
            "Train Epoch: 35 [5280/7471 (71%)]\tLoss: 1500592.500000\n",
            "Train Epoch: 35 [5440/7471 (73%)]\tLoss: 1563905.750000\n",
            "Train Epoch: 35 [5600/7471 (75%)]\tLoss: 1563564.250000\n",
            "Train Epoch: 35 [5760/7471 (77%)]\tLoss: 1532179.125000\n",
            "Train Epoch: 35 [5920/7471 (79%)]\tLoss: 1586198.625000\n",
            "Train Epoch: 35 [6080/7471 (81%)]\tLoss: 1495896.750000\n",
            "Train Epoch: 35 [6240/7471 (84%)]\tLoss: 1536057.875000\n",
            "Train Epoch: 35 [6400/7471 (86%)]\tLoss: 1574138.000000\n",
            "Train Epoch: 35 [6560/7471 (88%)]\tLoss: 1542358.375000\n",
            "Train Epoch: 35 [6720/7471 (90%)]\tLoss: 1627076.375000\n",
            "Train Epoch: 35 [6880/7471 (92%)]\tLoss: 1558191.375000\n",
            "Train Epoch: 35 [7040/7471 (94%)]\tLoss: 1541981.500000\n",
            "Train Epoch: 35 [7200/7471 (96%)]\tLoss: 1606336.500000\n",
            "Train Epoch: 35 [7360/7471 (99%)]\tLoss: 1574970.000000\n",
            "Epoch 35 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 36 [160/7471 (2%)]\tLoss: 1591544.125000\n",
            "Train Epoch: 36 [320/7471 (4%)]\tLoss: 1573415.125000\n",
            "Train Epoch: 36 [480/7471 (6%)]\tLoss: 1608086.750000\n",
            "Train Epoch: 36 [640/7471 (9%)]\tLoss: 1579212.875000\n",
            "Train Epoch: 36 [800/7471 (11%)]\tLoss: 1568528.250000\n",
            "Train Epoch: 36 [960/7471 (13%)]\tLoss: 1607742.250000\n",
            "Train Epoch: 36 [1120/7471 (15%)]\tLoss: 1570543.375000\n",
            "Train Epoch: 36 [1280/7471 (17%)]\tLoss: 1530377.375000\n",
            "Train Epoch: 36 [1440/7471 (19%)]\tLoss: 1528669.750000\n",
            "Train Epoch: 36 [1600/7471 (21%)]\tLoss: 1595823.750000\n",
            "Train Epoch: 36 [1760/7471 (24%)]\tLoss: 1573791.750000\n",
            "Train Epoch: 36 [1920/7471 (26%)]\tLoss: 1570393.375000\n",
            "Train Epoch: 36 [2080/7471 (28%)]\tLoss: 1541139.250000\n",
            "Train Epoch: 36 [2240/7471 (30%)]\tLoss: 1573780.375000\n",
            "Train Epoch: 36 [2400/7471 (32%)]\tLoss: 1585793.625000\n",
            "Train Epoch: 36 [2560/7471 (34%)]\tLoss: 1529957.625000\n",
            "Train Epoch: 36 [2720/7471 (36%)]\tLoss: 1586210.875000\n",
            "Train Epoch: 36 [2880/7471 (39%)]\tLoss: 1529923.750000\n",
            "Train Epoch: 36 [3040/7471 (41%)]\tLoss: 1618334.500000\n",
            "Train Epoch: 36 [3200/7471 (43%)]\tLoss: 1561322.125000\n",
            "Train Epoch: 36 [3360/7471 (45%)]\tLoss: 1609900.625000\n",
            "Train Epoch: 36 [3520/7471 (47%)]\tLoss: 1586762.875000\n",
            "Train Epoch: 36 [3680/7471 (49%)]\tLoss: 1554119.125000\n",
            "Train Epoch: 36 [3840/7471 (51%)]\tLoss: 1531047.875000\n",
            "Train Epoch: 36 [4000/7471 (54%)]\tLoss: 1558265.500000\n",
            "Train Epoch: 36 [4160/7471 (56%)]\tLoss: 1645918.750000\n",
            "Train Epoch: 36 [4320/7471 (58%)]\tLoss: 1581476.750000\n",
            "Train Epoch: 36 [4480/7471 (60%)]\tLoss: 1536492.375000\n",
            "Train Epoch: 36 [4640/7471 (62%)]\tLoss: 1573340.625000\n",
            "Train Epoch: 36 [4800/7471 (64%)]\tLoss: 1622582.625000\n",
            "Train Epoch: 36 [4960/7471 (66%)]\tLoss: 1577932.750000\n",
            "Train Epoch: 36 [5120/7471 (69%)]\tLoss: 1617450.625000\n",
            "Train Epoch: 36 [5280/7471 (71%)]\tLoss: 1593010.375000\n",
            "Train Epoch: 36 [5440/7471 (73%)]\tLoss: 1572910.375000\n",
            "Train Epoch: 36 [5600/7471 (75%)]\tLoss: 1575023.250000\n",
            "Train Epoch: 36 [5760/7471 (77%)]\tLoss: 1561354.500000\n",
            "Train Epoch: 36 [5920/7471 (79%)]\tLoss: 1568918.500000\n",
            "Train Epoch: 36 [6080/7471 (81%)]\tLoss: 1595858.500000\n",
            "Train Epoch: 36 [6240/7471 (84%)]\tLoss: 1588432.750000\n",
            "Train Epoch: 36 [6400/7471 (86%)]\tLoss: 1584851.625000\n",
            "Train Epoch: 36 [6560/7471 (88%)]\tLoss: 1605520.250000\n",
            "Train Epoch: 36 [6720/7471 (90%)]\tLoss: 1563424.500000\n",
            "Train Epoch: 36 [6880/7471 (92%)]\tLoss: 1591328.875000\n",
            "Train Epoch: 36 [7040/7471 (94%)]\tLoss: 1587063.750000\n",
            "Train Epoch: 36 [7200/7471 (96%)]\tLoss: 1570373.000000\n",
            "Train Epoch: 36 [7360/7471 (99%)]\tLoss: 1588442.750000\n",
            "Epoch 36 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 37 [160/7471 (2%)]\tLoss: 1577080.250000\n",
            "Train Epoch: 37 [320/7471 (4%)]\tLoss: 1635886.500000\n",
            "Train Epoch: 37 [480/7471 (6%)]\tLoss: 1597608.875000\n",
            "Train Epoch: 37 [640/7471 (9%)]\tLoss: 1567954.375000\n",
            "Train Epoch: 37 [800/7471 (11%)]\tLoss: 1612215.125000\n",
            "Train Epoch: 37 [960/7471 (13%)]\tLoss: 1556079.625000\n",
            "Train Epoch: 37 [1120/7471 (15%)]\tLoss: 1634680.875000\n",
            "Train Epoch: 37 [1280/7471 (17%)]\tLoss: 1544069.750000\n",
            "Train Epoch: 37 [1440/7471 (19%)]\tLoss: 1612438.250000\n",
            "Train Epoch: 37 [1600/7471 (21%)]\tLoss: 1588642.875000\n",
            "Train Epoch: 37 [1760/7471 (24%)]\tLoss: 1558381.125000\n",
            "Train Epoch: 37 [1920/7471 (26%)]\tLoss: 1513681.125000\n",
            "Train Epoch: 37 [2080/7471 (28%)]\tLoss: 1593600.125000\n",
            "Train Epoch: 37 [2240/7471 (30%)]\tLoss: 1538439.000000\n",
            "Train Epoch: 37 [2400/7471 (32%)]\tLoss: 1600685.750000\n",
            "Train Epoch: 37 [2560/7471 (34%)]\tLoss: 1631997.875000\n",
            "Train Epoch: 37 [2720/7471 (36%)]\tLoss: 1541540.625000\n",
            "Train Epoch: 37 [2880/7471 (39%)]\tLoss: 1584719.875000\n",
            "Train Epoch: 37 [3040/7471 (41%)]\tLoss: 1621221.375000\n",
            "Train Epoch: 37 [3200/7471 (43%)]\tLoss: 1564343.000000\n",
            "Train Epoch: 37 [3360/7471 (45%)]\tLoss: 1611506.000000\n",
            "Train Epoch: 37 [3520/7471 (47%)]\tLoss: 1553181.875000\n",
            "Train Epoch: 37 [3680/7471 (49%)]\tLoss: 1636249.625000\n",
            "Train Epoch: 37 [3840/7471 (51%)]\tLoss: 1510271.125000\n",
            "Train Epoch: 37 [4000/7471 (54%)]\tLoss: 1621209.375000\n",
            "Train Epoch: 37 [4160/7471 (56%)]\tLoss: 1595234.375000\n",
            "Train Epoch: 37 [4320/7471 (58%)]\tLoss: 1546042.750000\n",
            "Train Epoch: 37 [4480/7471 (60%)]\tLoss: 1494812.375000\n",
            "Train Epoch: 37 [4640/7471 (62%)]\tLoss: 1632224.000000\n",
            "Train Epoch: 37 [4800/7471 (64%)]\tLoss: 1554654.000000\n",
            "Train Epoch: 37 [4960/7471 (66%)]\tLoss: 1568586.250000\n",
            "Train Epoch: 37 [5120/7471 (69%)]\tLoss: 1649292.000000\n",
            "Train Epoch: 37 [5280/7471 (71%)]\tLoss: 1629887.125000\n",
            "Train Epoch: 37 [5440/7471 (73%)]\tLoss: 1585930.250000\n",
            "Train Epoch: 37 [5600/7471 (75%)]\tLoss: 1541343.000000\n",
            "Train Epoch: 37 [5760/7471 (77%)]\tLoss: 1596631.500000\n",
            "Train Epoch: 37 [5920/7471 (79%)]\tLoss: 1571838.625000\n",
            "Train Epoch: 37 [6080/7471 (81%)]\tLoss: 1599777.750000\n",
            "Train Epoch: 37 [6240/7471 (84%)]\tLoss: 1565314.375000\n",
            "Train Epoch: 37 [6400/7471 (86%)]\tLoss: 1572580.375000\n",
            "Train Epoch: 37 [6560/7471 (88%)]\tLoss: 1576414.750000\n",
            "Train Epoch: 37 [6720/7471 (90%)]\tLoss: 1604391.625000\n",
            "Train Epoch: 37 [6880/7471 (92%)]\tLoss: 1577205.000000\n",
            "Train Epoch: 37 [7040/7471 (94%)]\tLoss: 1575501.000000\n",
            "Train Epoch: 37 [7200/7471 (96%)]\tLoss: 1587665.375000\n",
            "Train Epoch: 37 [7360/7471 (99%)]\tLoss: 1581613.250000\n",
            "Epoch 37 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 38 [160/7471 (2%)]\tLoss: 1487404.500000\n",
            "Train Epoch: 38 [320/7471 (4%)]\tLoss: 1607705.625000\n",
            "Train Epoch: 38 [480/7471 (6%)]\tLoss: 1619911.375000\n",
            "Train Epoch: 38 [640/7471 (9%)]\tLoss: 1623461.500000\n",
            "Train Epoch: 38 [800/7471 (11%)]\tLoss: 1559177.250000\n",
            "Train Epoch: 38 [960/7471 (13%)]\tLoss: 1574582.000000\n",
            "Train Epoch: 38 [1120/7471 (15%)]\tLoss: 1557700.500000\n",
            "Train Epoch: 38 [1280/7471 (17%)]\tLoss: 1621687.750000\n",
            "Train Epoch: 38 [1440/7471 (19%)]\tLoss: 1568685.500000\n",
            "Train Epoch: 38 [1600/7471 (21%)]\tLoss: 1537031.000000\n",
            "Train Epoch: 38 [1760/7471 (24%)]\tLoss: 1552193.250000\n",
            "Train Epoch: 38 [1920/7471 (26%)]\tLoss: 1516815.750000\n",
            "Train Epoch: 38 [2080/7471 (28%)]\tLoss: 1503304.750000\n",
            "Train Epoch: 38 [2240/7471 (30%)]\tLoss: 1540356.625000\n",
            "Train Epoch: 38 [2400/7471 (32%)]\tLoss: 1588452.875000\n",
            "Train Epoch: 38 [2560/7471 (34%)]\tLoss: 1600320.750000\n",
            "Train Epoch: 38 [2720/7471 (36%)]\tLoss: 1492796.250000\n",
            "Train Epoch: 38 [2880/7471 (39%)]\tLoss: 1594607.500000\n",
            "Train Epoch: 38 [3040/7471 (41%)]\tLoss: 1623568.375000\n",
            "Train Epoch: 38 [3200/7471 (43%)]\tLoss: 1550621.625000\n",
            "Train Epoch: 38 [3360/7471 (45%)]\tLoss: 1468897.500000\n",
            "Train Epoch: 38 [3520/7471 (47%)]\tLoss: 1556236.500000\n",
            "Train Epoch: 38 [3680/7471 (49%)]\tLoss: 1591226.375000\n",
            "Train Epoch: 38 [3840/7471 (51%)]\tLoss: 1592912.375000\n",
            "Train Epoch: 38 [4000/7471 (54%)]\tLoss: 1545655.750000\n",
            "Train Epoch: 38 [4160/7471 (56%)]\tLoss: 1536377.750000\n",
            "Train Epoch: 38 [4320/7471 (58%)]\tLoss: 1612977.125000\n",
            "Train Epoch: 38 [4480/7471 (60%)]\tLoss: 1616597.875000\n",
            "Train Epoch: 38 [4640/7471 (62%)]\tLoss: 1544913.750000\n",
            "Train Epoch: 38 [4800/7471 (64%)]\tLoss: 1602027.000000\n",
            "Train Epoch: 38 [4960/7471 (66%)]\tLoss: 1609984.625000\n",
            "Train Epoch: 38 [5120/7471 (69%)]\tLoss: 1627886.875000\n",
            "Train Epoch: 38 [5280/7471 (71%)]\tLoss: 1534940.500000\n",
            "Train Epoch: 38 [5440/7471 (73%)]\tLoss: 1589668.875000\n",
            "Train Epoch: 38 [5600/7471 (75%)]\tLoss: 1532972.000000\n",
            "Train Epoch: 38 [5760/7471 (77%)]\tLoss: 1603078.000000\n",
            "Train Epoch: 38 [5920/7471 (79%)]\tLoss: 1615971.875000\n",
            "Train Epoch: 38 [6080/7471 (81%)]\tLoss: 1542239.750000\n",
            "Train Epoch: 38 [6240/7471 (84%)]\tLoss: 1555319.000000\n",
            "Train Epoch: 38 [6400/7471 (86%)]\tLoss: 1577536.500000\n",
            "Train Epoch: 38 [6560/7471 (88%)]\tLoss: 1604092.625000\n",
            "Train Epoch: 38 [6720/7471 (90%)]\tLoss: 1551200.750000\n",
            "Train Epoch: 38 [6880/7471 (92%)]\tLoss: 1624101.125000\n",
            "Train Epoch: 38 [7040/7471 (94%)]\tLoss: 1620891.750000\n",
            "Train Epoch: 38 [7200/7471 (96%)]\tLoss: 1647330.625000\n",
            "Train Epoch: 38 [7360/7471 (99%)]\tLoss: 1643371.500000\n",
            "Epoch 38 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 39 [160/7471 (2%)]\tLoss: 1584199.875000\n",
            "Train Epoch: 39 [320/7471 (4%)]\tLoss: 1632724.625000\n",
            "Train Epoch: 39 [480/7471 (6%)]\tLoss: 1575262.875000\n",
            "Train Epoch: 39 [640/7471 (9%)]\tLoss: 1615697.875000\n",
            "Train Epoch: 39 [800/7471 (11%)]\tLoss: 1584572.750000\n",
            "Train Epoch: 39 [960/7471 (13%)]\tLoss: 1606917.500000\n",
            "Train Epoch: 39 [1120/7471 (15%)]\tLoss: 1600058.000000\n",
            "Train Epoch: 39 [1280/7471 (17%)]\tLoss: 1595191.375000\n",
            "Train Epoch: 39 [1440/7471 (19%)]\tLoss: 1598148.875000\n",
            "Train Epoch: 39 [1600/7471 (21%)]\tLoss: 1581320.500000\n",
            "Train Epoch: 39 [1760/7471 (24%)]\tLoss: 1642636.000000\n",
            "Train Epoch: 39 [1920/7471 (26%)]\tLoss: 1541178.000000\n",
            "Train Epoch: 39 [2080/7471 (28%)]\tLoss: 1614974.000000\n",
            "Train Epoch: 39 [2240/7471 (30%)]\tLoss: 1570415.625000\n",
            "Train Epoch: 39 [2400/7471 (32%)]\tLoss: 1626857.250000\n",
            "Train Epoch: 39 [2560/7471 (34%)]\tLoss: 1635724.625000\n",
            "Train Epoch: 39 [2720/7471 (36%)]\tLoss: 1609058.750000\n",
            "Train Epoch: 39 [2880/7471 (39%)]\tLoss: 1552864.875000\n",
            "Train Epoch: 39 [3040/7471 (41%)]\tLoss: 1620386.250000\n",
            "Train Epoch: 39 [3200/7471 (43%)]\tLoss: 1547463.000000\n",
            "Train Epoch: 39 [3360/7471 (45%)]\tLoss: 1517724.000000\n",
            "Train Epoch: 39 [3520/7471 (47%)]\tLoss: 1577359.250000\n",
            "Train Epoch: 39 [3680/7471 (49%)]\tLoss: 1556665.000000\n",
            "Train Epoch: 39 [3840/7471 (51%)]\tLoss: 1627121.500000\n",
            "Train Epoch: 39 [4000/7471 (54%)]\tLoss: 1620301.125000\n",
            "Train Epoch: 39 [4160/7471 (56%)]\tLoss: 1525976.625000\n",
            "Train Epoch: 39 [4320/7471 (58%)]\tLoss: 1577686.500000\n",
            "Train Epoch: 39 [4480/7471 (60%)]\tLoss: 1590472.750000\n",
            "Train Epoch: 39 [4640/7471 (62%)]\tLoss: 1571694.125000\n",
            "Train Epoch: 39 [4800/7471 (64%)]\tLoss: 1618903.500000\n",
            "Train Epoch: 39 [4960/7471 (66%)]\tLoss: 1594700.875000\n",
            "Train Epoch: 39 [5120/7471 (69%)]\tLoss: 1555693.625000\n",
            "Train Epoch: 39 [5280/7471 (71%)]\tLoss: 1570264.500000\n",
            "Train Epoch: 39 [5440/7471 (73%)]\tLoss: 1594777.750000\n",
            "Train Epoch: 39 [5600/7471 (75%)]\tLoss: 1574520.375000\n",
            "Train Epoch: 39 [5760/7471 (77%)]\tLoss: 1617313.125000\n",
            "Train Epoch: 39 [5920/7471 (79%)]\tLoss: 1599566.000000\n",
            "Train Epoch: 39 [6080/7471 (81%)]\tLoss: 1538314.875000\n",
            "Train Epoch: 39 [6240/7471 (84%)]\tLoss: 1571964.000000\n",
            "Train Epoch: 39 [6400/7471 (86%)]\tLoss: 1549481.375000\n",
            "Train Epoch: 39 [6560/7471 (88%)]\tLoss: 1618550.750000\n",
            "Train Epoch: 39 [6720/7471 (90%)]\tLoss: 1522901.875000\n",
            "Train Epoch: 39 [6880/7471 (92%)]\tLoss: 1580120.875000\n",
            "Train Epoch: 39 [7040/7471 (94%)]\tLoss: 1578921.375000\n",
            "Train Epoch: 39 [7200/7471 (96%)]\tLoss: 1575726.125000\n",
            "Train Epoch: 39 [7360/7471 (99%)]\tLoss: 1547848.250000\n",
            "Epoch 39 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 40 [160/7471 (2%)]\tLoss: 1515913.000000\n",
            "Train Epoch: 40 [320/7471 (4%)]\tLoss: 1609100.500000\n",
            "Train Epoch: 40 [480/7471 (6%)]\tLoss: 1527935.875000\n",
            "Train Epoch: 40 [640/7471 (9%)]\tLoss: 1591983.750000\n",
            "Train Epoch: 40 [800/7471 (11%)]\tLoss: 1574587.500000\n",
            "Train Epoch: 40 [960/7471 (13%)]\tLoss: 1535299.500000\n",
            "Train Epoch: 40 [1120/7471 (15%)]\tLoss: 1594549.375000\n",
            "Train Epoch: 40 [1280/7471 (17%)]\tLoss: 1647330.750000\n",
            "Train Epoch: 40 [1440/7471 (19%)]\tLoss: 1584410.125000\n",
            "Train Epoch: 40 [1600/7471 (21%)]\tLoss: 1615363.500000\n",
            "Train Epoch: 40 [1760/7471 (24%)]\tLoss: 1514393.625000\n",
            "Train Epoch: 40 [1920/7471 (26%)]\tLoss: 1515106.625000\n",
            "Train Epoch: 40 [2080/7471 (28%)]\tLoss: 1577202.375000\n",
            "Train Epoch: 40 [2240/7471 (30%)]\tLoss: 1551203.750000\n",
            "Train Epoch: 40 [2400/7471 (32%)]\tLoss: 1581491.250000\n",
            "Train Epoch: 40 [2560/7471 (34%)]\tLoss: 1621069.625000\n",
            "Train Epoch: 40 [2720/7471 (36%)]\tLoss: 1633496.000000\n",
            "Train Epoch: 40 [2880/7471 (39%)]\tLoss: 1576860.375000\n",
            "Train Epoch: 40 [3040/7471 (41%)]\tLoss: 1591822.000000\n",
            "Train Epoch: 40 [3200/7471 (43%)]\tLoss: 1522671.875000\n",
            "Train Epoch: 40 [3360/7471 (45%)]\tLoss: 1525350.250000\n",
            "Train Epoch: 40 [3520/7471 (47%)]\tLoss: 1531830.000000\n",
            "Train Epoch: 40 [3680/7471 (49%)]\tLoss: 1519172.750000\n",
            "Train Epoch: 40 [3840/7471 (51%)]\tLoss: 1568838.125000\n",
            "Train Epoch: 40 [4000/7471 (54%)]\tLoss: 1557292.750000\n",
            "Train Epoch: 40 [4160/7471 (56%)]\tLoss: 1622466.375000\n",
            "Train Epoch: 40 [4320/7471 (58%)]\tLoss: 1539611.500000\n",
            "Train Epoch: 40 [4480/7471 (60%)]\tLoss: 1602999.125000\n",
            "Train Epoch: 40 [4640/7471 (62%)]\tLoss: 1562748.250000\n",
            "Train Epoch: 40 [4800/7471 (64%)]\tLoss: 1597124.000000\n",
            "Train Epoch: 40 [4960/7471 (66%)]\tLoss: 1583933.375000\n",
            "Train Epoch: 40 [5120/7471 (69%)]\tLoss: 1584622.625000\n",
            "Train Epoch: 40 [5280/7471 (71%)]\tLoss: 1571642.000000\n",
            "Train Epoch: 40 [5440/7471 (73%)]\tLoss: 1580969.250000\n",
            "Train Epoch: 40 [5600/7471 (75%)]\tLoss: 1591375.375000\n",
            "Train Epoch: 40 [5760/7471 (77%)]\tLoss: 1587092.875000\n",
            "Train Epoch: 40 [5920/7471 (79%)]\tLoss: 1612994.375000\n",
            "Train Epoch: 40 [6080/7471 (81%)]\tLoss: 1598726.125000\n",
            "Train Epoch: 40 [6240/7471 (84%)]\tLoss: 1584525.000000\n",
            "Train Epoch: 40 [6400/7471 (86%)]\tLoss: 1572903.875000\n",
            "Train Epoch: 40 [6560/7471 (88%)]\tLoss: 1586291.375000\n",
            "Train Epoch: 40 [6720/7471 (90%)]\tLoss: 1586268.750000\n",
            "Train Epoch: 40 [6880/7471 (92%)]\tLoss: 1594089.125000\n",
            "Train Epoch: 40 [7040/7471 (94%)]\tLoss: 1605068.875000\n",
            "Train Epoch: 40 [7200/7471 (96%)]\tLoss: 1600151.750000\n",
            "Train Epoch: 40 [7360/7471 (99%)]\tLoss: 1592719.250000\n",
            "Epoch 40 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 41 [160/7471 (2%)]\tLoss: 1596354.000000\n",
            "Train Epoch: 41 [320/7471 (4%)]\tLoss: 1573093.625000\n",
            "Train Epoch: 41 [480/7471 (6%)]\tLoss: 1550367.750000\n",
            "Train Epoch: 41 [640/7471 (9%)]\tLoss: 1603727.750000\n",
            "Train Epoch: 41 [800/7471 (11%)]\tLoss: 1597140.000000\n",
            "Train Epoch: 41 [960/7471 (13%)]\tLoss: 1561171.875000\n",
            "Train Epoch: 41 [1120/7471 (15%)]\tLoss: 1617321.375000\n",
            "Train Epoch: 41 [1280/7471 (17%)]\tLoss: 1557603.250000\n",
            "Train Epoch: 41 [1440/7471 (19%)]\tLoss: 1536359.875000\n",
            "Train Epoch: 41 [1600/7471 (21%)]\tLoss: 1535952.875000\n",
            "Train Epoch: 41 [1760/7471 (24%)]\tLoss: 1607643.125000\n",
            "Train Epoch: 41 [1920/7471 (26%)]\tLoss: 1628559.750000\n",
            "Train Epoch: 41 [2080/7471 (28%)]\tLoss: 1625188.625000\n",
            "Train Epoch: 41 [2240/7471 (30%)]\tLoss: 1510601.875000\n",
            "Train Epoch: 41 [2400/7471 (32%)]\tLoss: 1577260.750000\n",
            "Train Epoch: 41 [2560/7471 (34%)]\tLoss: 1616540.375000\n",
            "Train Epoch: 41 [2720/7471 (36%)]\tLoss: 1527543.125000\n",
            "Train Epoch: 41 [2880/7471 (39%)]\tLoss: 1557345.875000\n",
            "Train Epoch: 41 [3040/7471 (41%)]\tLoss: 1586392.375000\n",
            "Train Epoch: 41 [3200/7471 (43%)]\tLoss: 1514268.000000\n",
            "Train Epoch: 41 [3360/7471 (45%)]\tLoss: 1571158.875000\n",
            "Train Epoch: 41 [3520/7471 (47%)]\tLoss: 1585434.250000\n",
            "Train Epoch: 41 [3680/7471 (49%)]\tLoss: 1616875.750000\n",
            "Train Epoch: 41 [3840/7471 (51%)]\tLoss: 1556308.375000\n",
            "Train Epoch: 41 [4000/7471 (54%)]\tLoss: 1487462.875000\n",
            "Train Epoch: 41 [4160/7471 (56%)]\tLoss: 1627140.250000\n",
            "Train Epoch: 41 [4320/7471 (58%)]\tLoss: 1602108.875000\n",
            "Train Epoch: 41 [4480/7471 (60%)]\tLoss: 1597927.250000\n",
            "Train Epoch: 41 [4640/7471 (62%)]\tLoss: 1545253.625000\n",
            "Train Epoch: 41 [4800/7471 (64%)]\tLoss: 1545019.875000\n",
            "Train Epoch: 41 [4960/7471 (66%)]\tLoss: 1555417.000000\n",
            "Train Epoch: 41 [5120/7471 (69%)]\tLoss: 1578382.000000\n",
            "Train Epoch: 41 [5280/7471 (71%)]\tLoss: 1633052.500000\n",
            "Train Epoch: 41 [5440/7471 (73%)]\tLoss: 1594752.125000\n",
            "Train Epoch: 41 [5600/7471 (75%)]\tLoss: 1579062.375000\n",
            "Train Epoch: 41 [5760/7471 (77%)]\tLoss: 1538280.500000\n",
            "Train Epoch: 41 [5920/7471 (79%)]\tLoss: 1622441.500000\n",
            "Train Epoch: 41 [6080/7471 (81%)]\tLoss: 1637004.125000\n",
            "Train Epoch: 41 [6240/7471 (84%)]\tLoss: 1579051.500000\n",
            "Train Epoch: 41 [6400/7471 (86%)]\tLoss: 1581515.500000\n",
            "Train Epoch: 41 [6560/7471 (88%)]\tLoss: 1599097.000000\n",
            "Train Epoch: 41 [6720/7471 (90%)]\tLoss: 1592709.125000\n",
            "Train Epoch: 41 [6880/7471 (92%)]\tLoss: 1506493.875000\n",
            "Train Epoch: 41 [7040/7471 (94%)]\tLoss: 1601089.000000\n",
            "Train Epoch: 41 [7200/7471 (96%)]\tLoss: 1575978.500000\n",
            "Train Epoch: 41 [7360/7471 (99%)]\tLoss: 1576498.125000\n",
            "Epoch 41 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 42 [160/7471 (2%)]\tLoss: 1579228.500000\n",
            "Train Epoch: 42 [320/7471 (4%)]\tLoss: 1536213.250000\n",
            "Train Epoch: 42 [480/7471 (6%)]\tLoss: 1575887.125000\n",
            "Train Epoch: 42 [640/7471 (9%)]\tLoss: 1630641.625000\n",
            "Train Epoch: 42 [800/7471 (11%)]\tLoss: 1621255.000000\n",
            "Train Epoch: 42 [960/7471 (13%)]\tLoss: 1534528.750000\n",
            "Train Epoch: 42 [1120/7471 (15%)]\tLoss: 1558416.500000\n",
            "Train Epoch: 42 [1280/7471 (17%)]\tLoss: 1579809.625000\n",
            "Train Epoch: 42 [1440/7471 (19%)]\tLoss: 1622578.250000\n",
            "Train Epoch: 42 [1600/7471 (21%)]\tLoss: 1566249.000000\n",
            "Train Epoch: 42 [1760/7471 (24%)]\tLoss: 1552914.375000\n",
            "Train Epoch: 42 [1920/7471 (26%)]\tLoss: 1606757.875000\n",
            "Train Epoch: 42 [2080/7471 (28%)]\tLoss: 1548058.125000\n",
            "Train Epoch: 42 [2240/7471 (30%)]\tLoss: 1611313.250000\n",
            "Train Epoch: 42 [2400/7471 (32%)]\tLoss: 1620138.250000\n",
            "Train Epoch: 42 [2560/7471 (34%)]\tLoss: 1610334.000000\n",
            "Train Epoch: 42 [2720/7471 (36%)]\tLoss: 1563979.250000\n",
            "Train Epoch: 42 [2880/7471 (39%)]\tLoss: 1587410.750000\n",
            "Train Epoch: 42 [3040/7471 (41%)]\tLoss: 1619577.375000\n",
            "Train Epoch: 42 [3200/7471 (43%)]\tLoss: 1588123.625000\n",
            "Train Epoch: 42 [3360/7471 (45%)]\tLoss: 1597382.250000\n",
            "Train Epoch: 42 [3520/7471 (47%)]\tLoss: 1612066.250000\n",
            "Train Epoch: 42 [3680/7471 (49%)]\tLoss: 1629859.125000\n",
            "Train Epoch: 42 [3840/7471 (51%)]\tLoss: 1514374.875000\n",
            "Train Epoch: 42 [4000/7471 (54%)]\tLoss: 1629496.375000\n",
            "Train Epoch: 42 [4160/7471 (56%)]\tLoss: 1555819.000000\n",
            "Train Epoch: 42 [4320/7471 (58%)]\tLoss: 1578704.250000\n",
            "Train Epoch: 42 [4480/7471 (60%)]\tLoss: 1521884.750000\n",
            "Train Epoch: 42 [4640/7471 (62%)]\tLoss: 1582593.500000\n",
            "Train Epoch: 42 [4800/7471 (64%)]\tLoss: 1560536.500000\n",
            "Train Epoch: 42 [4960/7471 (66%)]\tLoss: 1621925.125000\n",
            "Train Epoch: 42 [5120/7471 (69%)]\tLoss: 1599476.250000\n",
            "Train Epoch: 42 [5280/7471 (71%)]\tLoss: 1585783.875000\n",
            "Train Epoch: 42 [5440/7471 (73%)]\tLoss: 1619947.250000\n",
            "Train Epoch: 42 [5600/7471 (75%)]\tLoss: 1585415.125000\n",
            "Train Epoch: 42 [5760/7471 (77%)]\tLoss: 1561478.000000\n",
            "Train Epoch: 42 [5920/7471 (79%)]\tLoss: 1592449.500000\n",
            "Train Epoch: 42 [6080/7471 (81%)]\tLoss: 1516540.250000\n",
            "Train Epoch: 42 [6240/7471 (84%)]\tLoss: 1655135.000000\n",
            "Train Epoch: 42 [6400/7471 (86%)]\tLoss: 1559312.250000\n",
            "Train Epoch: 42 [6560/7471 (88%)]\tLoss: 1602361.125000\n",
            "Train Epoch: 42 [6720/7471 (90%)]\tLoss: 1611253.875000\n",
            "Train Epoch: 42 [6880/7471 (92%)]\tLoss: 1582957.250000\n",
            "Train Epoch: 42 [7040/7471 (94%)]\tLoss: 1625370.125000\n",
            "Train Epoch: 42 [7200/7471 (96%)]\tLoss: 1545555.250000\n",
            "Train Epoch: 42 [7360/7471 (99%)]\tLoss: 1493300.500000\n",
            "Epoch 42 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 464997510447131136.0000\n",
            "\n",
            "Train Epoch: 43 [160/7471 (2%)]\tLoss: 1582178.000000\n",
            "Train Epoch: 43 [320/7471 (4%)]\tLoss: 1565687.625000\n",
            "Train Epoch: 43 [480/7471 (6%)]\tLoss: 1547876.875000\n",
            "Train Epoch: 43 [640/7471 (9%)]\tLoss: 1498752.500000\n",
            "Train Epoch: 43 [800/7471 (11%)]\tLoss: 1606051.500000\n",
            "Train Epoch: 43 [960/7471 (13%)]\tLoss: 1608957.500000\n",
            "Train Epoch: 43 [1120/7471 (15%)]\tLoss: 1581211.625000\n",
            "Train Epoch: 43 [1280/7471 (17%)]\tLoss: 1588525.375000\n",
            "Train Epoch: 43 [1440/7471 (19%)]\tLoss: 1631727.375000\n",
            "Train Epoch: 43 [1600/7471 (21%)]\tLoss: 1598245.750000\n",
            "Train Epoch: 43 [1760/7471 (24%)]\tLoss: 1588858.875000\n",
            "Train Epoch: 43 [1920/7471 (26%)]\tLoss: 1608645.625000\n",
            "Train Epoch: 43 [2080/7471 (28%)]\tLoss: 1627062.375000\n",
            "Train Epoch: 43 [2240/7471 (30%)]\tLoss: 1608986.625000\n",
            "Train Epoch: 43 [2400/7471 (32%)]\tLoss: 1585409.125000\n",
            "Train Epoch: 43 [2560/7471 (34%)]\tLoss: 1639731.625000\n",
            "Train Epoch: 43 [2720/7471 (36%)]\tLoss: 1536528.250000\n",
            "Train Epoch: 43 [2880/7471 (39%)]\tLoss: 1481327.000000\n",
            "Train Epoch: 43 [3040/7471 (41%)]\tLoss: 1581743.250000\n",
            "Train Epoch: 43 [3200/7471 (43%)]\tLoss: 1543671.750000\n",
            "Train Epoch: 43 [3360/7471 (45%)]\tLoss: 1565802.500000\n",
            "Train Epoch: 43 [3520/7471 (47%)]\tLoss: 1625828.750000\n",
            "Train Epoch: 43 [3680/7471 (49%)]\tLoss: 1554370.250000\n",
            "Train Epoch: 43 [3840/7471 (51%)]\tLoss: 1584598.250000\n",
            "Train Epoch: 43 [4000/7471 (54%)]\tLoss: 1613277.125000\n",
            "Train Epoch: 43 [4160/7471 (56%)]\tLoss: 1560223.375000\n",
            "Train Epoch: 43 [4320/7471 (58%)]\tLoss: 1630150.625000\n",
            "Train Epoch: 43 [4480/7471 (60%)]\tLoss: 1523328.000000\n",
            "Train Epoch: 43 [4640/7471 (62%)]\tLoss: 1517238.750000\n",
            "Train Epoch: 43 [4800/7471 (64%)]\tLoss: 1543541.000000\n",
            "Train Epoch: 43 [4960/7471 (66%)]\tLoss: 1620248.250000\n",
            "Train Epoch: 43 [5120/7471 (69%)]\tLoss: 1626207.500000\n",
            "Train Epoch: 43 [5280/7471 (71%)]\tLoss: 1519952.750000\n",
            "Train Epoch: 43 [5440/7471 (73%)]\tLoss: 1553866.625000\n",
            "Train Epoch: 43 [5600/7471 (75%)]\tLoss: 1568376.750000\n",
            "Train Epoch: 43 [5760/7471 (77%)]\tLoss: 1629952.625000\n",
            "Train Epoch: 43 [5920/7471 (79%)]\tLoss: 1598584.250000\n",
            "Train Epoch: 43 [6080/7471 (81%)]\tLoss: 1569554.250000\n",
            "Train Epoch: 43 [6240/7471 (84%)]\tLoss: 1476076.625000\n",
            "Train Epoch: 43 [6400/7471 (86%)]\tLoss: 1608547.625000\n",
            "Train Epoch: 43 [6560/7471 (88%)]\tLoss: 1567386.000000\n",
            "Train Epoch: 43 [6720/7471 (90%)]\tLoss: 1593423.625000\n",
            "Train Epoch: 43 [6880/7471 (92%)]\tLoss: 1605983.125000\n",
            "Train Epoch: 43 [7040/7471 (94%)]\tLoss: 1575765.750000\n",
            "Train Epoch: 43 [7200/7471 (96%)]\tLoss: 1552765.750000\n",
            "Train Epoch: 43 [7360/7471 (99%)]\tLoss: 1600835.500000\n",
            "Epoch 43 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98696.1006\n",
            "\n",
            "Train Epoch: 44 [160/7471 (2%)]\tLoss: 1641140.875000\n",
            "Train Epoch: 44 [320/7471 (4%)]\tLoss: 1601313.750000\n",
            "Train Epoch: 44 [480/7471 (6%)]\tLoss: 1580851.375000\n",
            "Train Epoch: 44 [640/7471 (9%)]\tLoss: 1568941.875000\n",
            "Train Epoch: 44 [800/7471 (11%)]\tLoss: 1626766.625000\n",
            "Train Epoch: 44 [960/7471 (13%)]\tLoss: 1599350.750000\n",
            "Train Epoch: 44 [1120/7471 (15%)]\tLoss: 1612609.875000\n",
            "Train Epoch: 44 [1280/7471 (17%)]\tLoss: 1615502.375000\n",
            "Train Epoch: 44 [1440/7471 (19%)]\tLoss: 1633796.750000\n",
            "Train Epoch: 44 [1600/7471 (21%)]\tLoss: 1612037.250000\n",
            "Train Epoch: 44 [1760/7471 (24%)]\tLoss: 1554842.875000\n",
            "Train Epoch: 44 [1920/7471 (26%)]\tLoss: 1600403.250000\n",
            "Train Epoch: 44 [2080/7471 (28%)]\tLoss: 1573406.875000\n",
            "Train Epoch: 44 [2240/7471 (30%)]\tLoss: 1570535.500000\n",
            "Train Epoch: 44 [2400/7471 (32%)]\tLoss: 1580743.625000\n",
            "Train Epoch: 44 [2560/7471 (34%)]\tLoss: 1563004.250000\n",
            "Train Epoch: 44 [2720/7471 (36%)]\tLoss: 1551479.750000\n",
            "Train Epoch: 44 [2880/7471 (39%)]\tLoss: 1528837.125000\n",
            "Train Epoch: 44 [3040/7471 (41%)]\tLoss: 1564517.875000\n",
            "Train Epoch: 44 [3200/7471 (43%)]\tLoss: 1518001.125000\n",
            "Train Epoch: 44 [3360/7471 (45%)]\tLoss: 1559354.875000\n",
            "Train Epoch: 44 [3520/7471 (47%)]\tLoss: 1617936.500000\n",
            "Train Epoch: 44 [3680/7471 (49%)]\tLoss: 1616289.625000\n",
            "Train Epoch: 44 [3840/7471 (51%)]\tLoss: 1554000.375000\n",
            "Train Epoch: 44 [4000/7471 (54%)]\tLoss: 1577969.625000\n",
            "Train Epoch: 44 [4160/7471 (56%)]\tLoss: 1584533.000000\n",
            "Train Epoch: 44 [4320/7471 (58%)]\tLoss: 1616898.250000\n",
            "Train Epoch: 44 [4480/7471 (60%)]\tLoss: 1498230.000000\n",
            "Train Epoch: 44 [4640/7471 (62%)]\tLoss: 1569943.750000\n",
            "Train Epoch: 44 [4800/7471 (64%)]\tLoss: 1583052.875000\n",
            "Train Epoch: 44 [4960/7471 (66%)]\tLoss: 1532858.625000\n",
            "Train Epoch: 44 [5120/7471 (69%)]\tLoss: 1606628.375000\n",
            "Train Epoch: 44 [5280/7471 (71%)]\tLoss: 1611994.000000\n",
            "Train Epoch: 44 [5440/7471 (73%)]\tLoss: 1535562.500000\n",
            "Train Epoch: 44 [5600/7471 (75%)]\tLoss: 1488003.250000\n",
            "Train Epoch: 44 [5760/7471 (77%)]\tLoss: 1586057.375000\n",
            "Train Epoch: 44 [5920/7471 (79%)]\tLoss: 1616736.500000\n",
            "Train Epoch: 44 [6080/7471 (81%)]\tLoss: 1599318.875000\n",
            "Train Epoch: 44 [6240/7471 (84%)]\tLoss: 1577753.375000\n",
            "Train Epoch: 44 [6400/7471 (86%)]\tLoss: 1585333.875000\n",
            "Train Epoch: 44 [6560/7471 (88%)]\tLoss: 1563551.500000\n",
            "Train Epoch: 44 [6720/7471 (90%)]\tLoss: 1621390.500000\n",
            "Train Epoch: 44 [6880/7471 (92%)]\tLoss: 1607486.250000\n",
            "Train Epoch: 44 [7040/7471 (94%)]\tLoss: 1585815.750000\n",
            "Train Epoch: 44 [7200/7471 (96%)]\tLoss: 1593638.375000\n",
            "Train Epoch: 44 [7360/7471 (99%)]\tLoss: 1608059.125000\n",
            "Epoch 44 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98670.2425\n",
            "\n",
            "Train Epoch: 45 [160/7471 (2%)]\tLoss: 1590721.500000\n",
            "Train Epoch: 45 [320/7471 (4%)]\tLoss: 1527302.125000\n",
            "Train Epoch: 45 [480/7471 (6%)]\tLoss: 1617389.375000\n",
            "Train Epoch: 45 [640/7471 (9%)]\tLoss: 1558698.875000\n",
            "Train Epoch: 45 [800/7471 (11%)]\tLoss: 1650449.625000\n",
            "Train Epoch: 45 [960/7471 (13%)]\tLoss: 1555403.500000\n",
            "Train Epoch: 45 [1120/7471 (15%)]\tLoss: 1614083.500000\n",
            "Train Epoch: 45 [1280/7471 (17%)]\tLoss: 1611478.125000\n",
            "Train Epoch: 45 [1440/7471 (19%)]\tLoss: 1506693.875000\n",
            "Train Epoch: 45 [1600/7471 (21%)]\tLoss: 1607278.375000\n",
            "Train Epoch: 45 [1760/7471 (24%)]\tLoss: 1563738.625000\n",
            "Train Epoch: 45 [1920/7471 (26%)]\tLoss: 1607334.250000\n",
            "Train Epoch: 45 [2080/7471 (28%)]\tLoss: 1591564.750000\n",
            "Train Epoch: 45 [2240/7471 (30%)]\tLoss: 1598045.250000\n",
            "Train Epoch: 45 [2400/7471 (32%)]\tLoss: 1595200.500000\n",
            "Train Epoch: 45 [2560/7471 (34%)]\tLoss: 1584240.750000\n",
            "Train Epoch: 45 [2720/7471 (36%)]\tLoss: 1605934.750000\n",
            "Train Epoch: 45 [2880/7471 (39%)]\tLoss: 1550187.000000\n",
            "Train Epoch: 45 [3040/7471 (41%)]\tLoss: 1593446.750000\n",
            "Train Epoch: 45 [3200/7471 (43%)]\tLoss: 1615400.625000\n",
            "Train Epoch: 45 [3360/7471 (45%)]\tLoss: 1572350.375000\n",
            "Train Epoch: 45 [3520/7471 (47%)]\tLoss: 1567616.625000\n",
            "Train Epoch: 45 [3680/7471 (49%)]\tLoss: 1528384.125000\n",
            "Train Epoch: 45 [3840/7471 (51%)]\tLoss: 1575123.625000\n",
            "Train Epoch: 45 [4000/7471 (54%)]\tLoss: 1593890.375000\n",
            "Train Epoch: 45 [4160/7471 (56%)]\tLoss: 1630023.125000\n",
            "Train Epoch: 45 [4320/7471 (58%)]\tLoss: 1475841.750000\n",
            "Train Epoch: 45 [4480/7471 (60%)]\tLoss: 1596393.125000\n",
            "Train Epoch: 45 [4640/7471 (62%)]\tLoss: 1625030.250000\n",
            "Train Epoch: 45 [4800/7471 (64%)]\tLoss: 1550602.875000\n",
            "Train Epoch: 45 [4960/7471 (66%)]\tLoss: 1559076.750000\n",
            "Train Epoch: 45 [5120/7471 (69%)]\tLoss: 1616080.250000\n",
            "Train Epoch: 45 [5280/7471 (71%)]\tLoss: 1569953.000000\n",
            "Train Epoch: 45 [5440/7471 (73%)]\tLoss: 1593911.500000\n",
            "Train Epoch: 45 [5600/7471 (75%)]\tLoss: 1502086.000000\n",
            "Train Epoch: 45 [5760/7471 (77%)]\tLoss: 1603405.375000\n",
            "Train Epoch: 45 [5920/7471 (79%)]\tLoss: 1526464.375000\n",
            "Train Epoch: 45 [6080/7471 (81%)]\tLoss: 1610375.500000\n",
            "Train Epoch: 45 [6240/7471 (84%)]\tLoss: 1613847.625000\n",
            "Train Epoch: 45 [6400/7471 (86%)]\tLoss: 1593108.250000\n",
            "Train Epoch: 45 [6560/7471 (88%)]\tLoss: 1624981.250000\n",
            "Train Epoch: 45 [6720/7471 (90%)]\tLoss: 1556312.625000\n",
            "Train Epoch: 45 [6880/7471 (92%)]\tLoss: 1587251.000000\n",
            "Train Epoch: 45 [7040/7471 (94%)]\tLoss: 1596697.750000\n",
            "Train Epoch: 45 [7200/7471 (96%)]\tLoss: 1592813.250000\n",
            "Train Epoch: 45 [7360/7471 (99%)]\tLoss: 1544927.000000\n",
            "Epoch 45 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98641.9925\n",
            "\n",
            "Train Epoch: 46 [160/7471 (2%)]\tLoss: 1576996.000000\n",
            "Train Epoch: 46 [320/7471 (4%)]\tLoss: 1600741.250000\n",
            "Train Epoch: 46 [480/7471 (6%)]\tLoss: 1648447.625000\n",
            "Train Epoch: 46 [640/7471 (9%)]\tLoss: 1620327.625000\n",
            "Train Epoch: 46 [800/7471 (11%)]\tLoss: 1592771.125000\n",
            "Train Epoch: 46 [960/7471 (13%)]\tLoss: 1541522.250000\n",
            "Train Epoch: 46 [1120/7471 (15%)]\tLoss: 1555896.000000\n",
            "Train Epoch: 46 [1280/7471 (17%)]\tLoss: 1596968.125000\n",
            "Train Epoch: 46 [1440/7471 (19%)]\tLoss: 1604418.375000\n",
            "Train Epoch: 46 [1600/7471 (21%)]\tLoss: 1551365.375000\n",
            "Train Epoch: 46 [1760/7471 (24%)]\tLoss: 1570521.875000\n",
            "Train Epoch: 46 [1920/7471 (26%)]\tLoss: 1567248.375000\n",
            "Train Epoch: 46 [2080/7471 (28%)]\tLoss: 1578973.000000\n",
            "Train Epoch: 46 [2240/7471 (30%)]\tLoss: 1612973.375000\n",
            "Train Epoch: 46 [2400/7471 (32%)]\tLoss: 1587004.375000\n",
            "Train Epoch: 46 [2560/7471 (34%)]\tLoss: 1563945.000000\n",
            "Train Epoch: 46 [2720/7471 (36%)]\tLoss: 1582721.500000\n",
            "Train Epoch: 46 [2880/7471 (39%)]\tLoss: 1587414.625000\n",
            "Train Epoch: 46 [3040/7471 (41%)]\tLoss: 1625098.750000\n",
            "Train Epoch: 46 [3200/7471 (43%)]\tLoss: 1536686.500000\n",
            "Train Epoch: 46 [3360/7471 (45%)]\tLoss: 1629588.875000\n",
            "Train Epoch: 46 [3520/7471 (47%)]\tLoss: 1563563.500000\n",
            "Train Epoch: 46 [3680/7471 (49%)]\tLoss: 1597204.375000\n",
            "Train Epoch: 46 [3840/7471 (51%)]\tLoss: 1490971.625000\n",
            "Train Epoch: 46 [4000/7471 (54%)]\tLoss: 1552796.500000\n",
            "Train Epoch: 46 [4160/7471 (56%)]\tLoss: 1589037.750000\n",
            "Train Epoch: 46 [4320/7471 (58%)]\tLoss: 1605481.000000\n",
            "Train Epoch: 46 [4480/7471 (60%)]\tLoss: 1614937.375000\n",
            "Train Epoch: 46 [4640/7471 (62%)]\tLoss: 1580260.625000\n",
            "Train Epoch: 46 [4800/7471 (64%)]\tLoss: 1615962.125000\n",
            "Train Epoch: 46 [4960/7471 (66%)]\tLoss: 1607879.500000\n",
            "Train Epoch: 46 [5120/7471 (69%)]\tLoss: 1578212.375000\n",
            "Train Epoch: 46 [5280/7471 (71%)]\tLoss: 1622106.125000\n",
            "Train Epoch: 46 [5440/7471 (73%)]\tLoss: 1552165.125000\n",
            "Train Epoch: 46 [5600/7471 (75%)]\tLoss: 1498178.875000\n",
            "Train Epoch: 46 [5760/7471 (77%)]\tLoss: 1560205.500000\n",
            "Train Epoch: 46 [5920/7471 (79%)]\tLoss: 1550162.250000\n",
            "Train Epoch: 46 [6080/7471 (81%)]\tLoss: 1624521.250000\n",
            "Train Epoch: 46 [6240/7471 (84%)]\tLoss: 1637525.250000\n",
            "Train Epoch: 46 [6400/7471 (86%)]\tLoss: 1530318.125000\n",
            "Train Epoch: 46 [6560/7471 (88%)]\tLoss: 1607932.625000\n",
            "Train Epoch: 46 [6720/7471 (90%)]\tLoss: 1587088.375000\n",
            "Train Epoch: 46 [6880/7471 (92%)]\tLoss: 1609609.750000\n",
            "Train Epoch: 46 [7040/7471 (94%)]\tLoss: 1534373.375000\n",
            "Train Epoch: 46 [7200/7471 (96%)]\tLoss: 1578822.375000\n",
            "Train Epoch: 46 [7360/7471 (99%)]\tLoss: 1626248.875000\n",
            "Epoch 46 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98643.0577\n",
            "\n",
            "Train Epoch: 47 [160/7471 (2%)]\tLoss: 1544179.125000\n",
            "Train Epoch: 47 [320/7471 (4%)]\tLoss: 1571171.625000\n",
            "Train Epoch: 47 [480/7471 (6%)]\tLoss: 1487019.875000\n",
            "Train Epoch: 47 [640/7471 (9%)]\tLoss: 1613742.375000\n",
            "Train Epoch: 47 [800/7471 (11%)]\tLoss: 1533853.250000\n",
            "Train Epoch: 47 [960/7471 (13%)]\tLoss: 1545417.500000\n",
            "Train Epoch: 47 [1120/7471 (15%)]\tLoss: 1606167.250000\n",
            "Train Epoch: 47 [1280/7471 (17%)]\tLoss: 1600137.000000\n",
            "Train Epoch: 47 [1440/7471 (19%)]\tLoss: 1608841.625000\n",
            "Train Epoch: 47 [1600/7471 (21%)]\tLoss: 1532955.750000\n",
            "Train Epoch: 47 [1760/7471 (24%)]\tLoss: 1619625.000000\n",
            "Train Epoch: 47 [1920/7471 (26%)]\tLoss: 1608920.000000\n",
            "Train Epoch: 47 [2080/7471 (28%)]\tLoss: 1556452.625000\n",
            "Train Epoch: 47 [2240/7471 (30%)]\tLoss: 1463787.000000\n",
            "Train Epoch: 47 [2400/7471 (32%)]\tLoss: 1605644.000000\n",
            "Train Epoch: 47 [2560/7471 (34%)]\tLoss: 1605019.000000\n",
            "Train Epoch: 47 [2720/7471 (36%)]\tLoss: 1601971.250000\n",
            "Train Epoch: 47 [2880/7471 (39%)]\tLoss: 1563786.750000\n",
            "Train Epoch: 47 [3040/7471 (41%)]\tLoss: 1571373.125000\n",
            "Train Epoch: 47 [3200/7471 (43%)]\tLoss: 1584784.250000\n",
            "Train Epoch: 47 [3360/7471 (45%)]\tLoss: 1605798.125000\n",
            "Train Epoch: 47 [3520/7471 (47%)]\tLoss: 1592580.125000\n",
            "Train Epoch: 47 [3680/7471 (49%)]\tLoss: 1641048.500000\n",
            "Train Epoch: 47 [3840/7471 (51%)]\tLoss: 1607973.750000\n",
            "Train Epoch: 47 [4000/7471 (54%)]\tLoss: 1611942.500000\n",
            "Train Epoch: 47 [4160/7471 (56%)]\tLoss: 1592058.625000\n",
            "Train Epoch: 47 [4320/7471 (58%)]\tLoss: 1544278.500000\n",
            "Train Epoch: 47 [4480/7471 (60%)]\tLoss: 1602897.875000\n",
            "Train Epoch: 47 [4640/7471 (62%)]\tLoss: 1635447.625000\n",
            "Train Epoch: 47 [4800/7471 (64%)]\tLoss: 1602877.375000\n",
            "Train Epoch: 47 [4960/7471 (66%)]\tLoss: 1599890.625000\n",
            "Train Epoch: 47 [5120/7471 (69%)]\tLoss: 1612028.000000\n",
            "Train Epoch: 47 [5280/7471 (71%)]\tLoss: 1525908.000000\n",
            "Train Epoch: 47 [5440/7471 (73%)]\tLoss: 1597098.625000\n",
            "Train Epoch: 47 [5600/7471 (75%)]\tLoss: 1557575.375000\n",
            "Train Epoch: 47 [5760/7471 (77%)]\tLoss: 1542935.375000\n",
            "Train Epoch: 47 [5920/7471 (79%)]\tLoss: 1520707.875000\n",
            "Train Epoch: 47 [6080/7471 (81%)]\tLoss: 1532944.125000\n",
            "Train Epoch: 47 [6240/7471 (84%)]\tLoss: 1604263.125000\n",
            "Train Epoch: 47 [6400/7471 (86%)]\tLoss: 1559815.125000\n",
            "Train Epoch: 47 [6560/7471 (88%)]\tLoss: 1630736.625000\n",
            "Train Epoch: 47 [6720/7471 (90%)]\tLoss: 1643466.875000\n",
            "Train Epoch: 47 [6880/7471 (92%)]\tLoss: 1502669.250000\n",
            "Train Epoch: 47 [7040/7471 (94%)]\tLoss: 1558784.000000\n",
            "Train Epoch: 47 [7200/7471 (96%)]\tLoss: 1511889.625000\n",
            "Train Epoch: 47 [7360/7471 (99%)]\tLoss: 1562543.375000\n",
            "Epoch 47 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98889.9452\n",
            "\n",
            "Train Epoch: 48 [160/7471 (2%)]\tLoss: 1628644.625000\n",
            "Train Epoch: 48 [320/7471 (4%)]\tLoss: 1608522.625000\n",
            "Train Epoch: 48 [480/7471 (6%)]\tLoss: 1547956.000000\n",
            "Train Epoch: 48 [640/7471 (9%)]\tLoss: 1610513.125000\n",
            "Train Epoch: 48 [800/7471 (11%)]\tLoss: 1587166.125000\n",
            "Train Epoch: 48 [960/7471 (13%)]\tLoss: 1538055.625000\n",
            "Train Epoch: 48 [1120/7471 (15%)]\tLoss: 1622694.250000\n",
            "Train Epoch: 48 [1280/7471 (17%)]\tLoss: 1612359.125000\n",
            "Train Epoch: 48 [1440/7471 (19%)]\tLoss: 1570942.125000\n",
            "Train Epoch: 48 [1600/7471 (21%)]\tLoss: 1609106.375000\n",
            "Train Epoch: 48 [1760/7471 (24%)]\tLoss: 1598989.500000\n",
            "Train Epoch: 48 [1920/7471 (26%)]\tLoss: 1570139.125000\n",
            "Train Epoch: 48 [2080/7471 (28%)]\tLoss: 1561135.375000\n",
            "Train Epoch: 48 [2240/7471 (30%)]\tLoss: 1577000.375000\n",
            "Train Epoch: 48 [2400/7471 (32%)]\tLoss: 1573323.750000\n",
            "Train Epoch: 48 [2560/7471 (34%)]\tLoss: 1619975.125000\n",
            "Train Epoch: 48 [2720/7471 (36%)]\tLoss: 1593913.000000\n",
            "Train Epoch: 48 [2880/7471 (39%)]\tLoss: 1557236.750000\n",
            "Train Epoch: 48 [3040/7471 (41%)]\tLoss: 1555309.375000\n",
            "Train Epoch: 48 [3200/7471 (43%)]\tLoss: 1598712.125000\n",
            "Train Epoch: 48 [3360/7471 (45%)]\tLoss: 1581639.875000\n",
            "Train Epoch: 48 [3520/7471 (47%)]\tLoss: 1571445.250000\n",
            "Train Epoch: 48 [3680/7471 (49%)]\tLoss: 1565004.000000\n",
            "Train Epoch: 48 [3840/7471 (51%)]\tLoss: 1601053.250000\n",
            "Train Epoch: 48 [4000/7471 (54%)]\tLoss: 1641309.875000\n",
            "Train Epoch: 48 [4160/7471 (56%)]\tLoss: 1584083.875000\n",
            "Train Epoch: 48 [4320/7471 (58%)]\tLoss: 1554798.875000\n",
            "Train Epoch: 48 [4480/7471 (60%)]\tLoss: 1575482.875000\n",
            "Train Epoch: 48 [4640/7471 (62%)]\tLoss: 1612756.375000\n",
            "Train Epoch: 48 [4800/7471 (64%)]\tLoss: 1572782.250000\n",
            "Train Epoch: 48 [4960/7471 (66%)]\tLoss: 1555854.125000\n",
            "Train Epoch: 48 [5120/7471 (69%)]\tLoss: 1592386.000000\n",
            "Train Epoch: 48 [5280/7471 (71%)]\tLoss: 1576855.000000\n",
            "Train Epoch: 48 [5440/7471 (73%)]\tLoss: 1558962.250000\n",
            "Train Epoch: 48 [5600/7471 (75%)]\tLoss: 1609177.375000\n",
            "Train Epoch: 48 [5760/7471 (77%)]\tLoss: 1601519.750000\n",
            "Train Epoch: 48 [5920/7471 (79%)]\tLoss: 1549710.375000\n",
            "Train Epoch: 48 [6080/7471 (81%)]\tLoss: 1589382.750000\n",
            "Train Epoch: 48 [6240/7471 (84%)]\tLoss: 1597648.000000\n",
            "Train Epoch: 48 [6400/7471 (86%)]\tLoss: 1569824.000000\n",
            "Train Epoch: 48 [6560/7471 (88%)]\tLoss: 1559397.375000\n",
            "Train Epoch: 48 [6720/7471 (90%)]\tLoss: 1599203.500000\n",
            "Train Epoch: 48 [6880/7471 (92%)]\tLoss: 1631887.375000\n",
            "Train Epoch: 48 [7040/7471 (94%)]\tLoss: 1555965.375000\n",
            "Train Epoch: 48 [7200/7471 (96%)]\tLoss: 1639110.500000\n",
            "Train Epoch: 48 [7360/7471 (99%)]\tLoss: 1591172.750000\n",
            "Epoch 48 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 99106.1395\n",
            "\n",
            "Train Epoch: 49 [160/7471 (2%)]\tLoss: 1555179.000000\n",
            "Train Epoch: 49 [320/7471 (4%)]\tLoss: 1608024.125000\n",
            "Train Epoch: 49 [480/7471 (6%)]\tLoss: 1644089.375000\n",
            "Train Epoch: 49 [640/7471 (9%)]\tLoss: 1598810.000000\n",
            "Train Epoch: 49 [800/7471 (11%)]\tLoss: 1627471.625000\n",
            "Train Epoch: 49 [960/7471 (13%)]\tLoss: 1622053.500000\n",
            "Train Epoch: 49 [1120/7471 (15%)]\tLoss: 1568197.125000\n",
            "Train Epoch: 49 [1280/7471 (17%)]\tLoss: 1515400.375000\n",
            "Train Epoch: 49 [1440/7471 (19%)]\tLoss: 1600317.125000\n",
            "Train Epoch: 49 [1600/7471 (21%)]\tLoss: 1625315.500000\n",
            "Train Epoch: 49 [1760/7471 (24%)]\tLoss: 1611097.375000\n",
            "Train Epoch: 49 [1920/7471 (26%)]\tLoss: 1605055.125000\n",
            "Train Epoch: 49 [2080/7471 (28%)]\tLoss: 1485101.000000\n",
            "Train Epoch: 49 [2240/7471 (30%)]\tLoss: 1578825.875000\n",
            "Train Epoch: 49 [2400/7471 (32%)]\tLoss: 1620203.250000\n",
            "Train Epoch: 49 [2560/7471 (34%)]\tLoss: 1607283.000000\n",
            "Train Epoch: 49 [2720/7471 (36%)]\tLoss: 1583672.375000\n",
            "Train Epoch: 49 [2880/7471 (39%)]\tLoss: 1596731.875000\n",
            "Train Epoch: 49 [3040/7471 (41%)]\tLoss: 1593184.125000\n",
            "Train Epoch: 49 [3200/7471 (43%)]\tLoss: 1564504.375000\n",
            "Train Epoch: 49 [3360/7471 (45%)]\tLoss: 1625015.500000\n",
            "Train Epoch: 49 [3520/7471 (47%)]\tLoss: 1601564.750000\n",
            "Train Epoch: 49 [3680/7471 (49%)]\tLoss: 1506765.750000\n",
            "Train Epoch: 49 [3840/7471 (51%)]\tLoss: 1631887.000000\n",
            "Train Epoch: 49 [4000/7471 (54%)]\tLoss: 1607942.000000\n",
            "Train Epoch: 49 [4160/7471 (56%)]\tLoss: 1605026.125000\n",
            "Train Epoch: 49 [4320/7471 (58%)]\tLoss: 1621498.250000\n",
            "Train Epoch: 49 [4480/7471 (60%)]\tLoss: 1599889.375000\n",
            "Train Epoch: 49 [4640/7471 (62%)]\tLoss: 1613995.875000\n",
            "Train Epoch: 49 [4800/7471 (64%)]\tLoss: 1545820.875000\n",
            "Train Epoch: 49 [4960/7471 (66%)]\tLoss: 1544253.750000\n",
            "Train Epoch: 49 [5120/7471 (69%)]\tLoss: 1589514.625000\n",
            "Train Epoch: 49 [5280/7471 (71%)]\tLoss: 1521946.000000\n",
            "Train Epoch: 49 [5440/7471 (73%)]\tLoss: 1554908.500000\n",
            "Train Epoch: 49 [5600/7471 (75%)]\tLoss: 1584115.250000\n",
            "Train Epoch: 49 [5760/7471 (77%)]\tLoss: 1561549.125000\n",
            "Train Epoch: 49 [5920/7471 (79%)]\tLoss: 1553825.125000\n",
            "Train Epoch: 49 [6080/7471 (81%)]\tLoss: 1594818.750000\n",
            "Train Epoch: 49 [6240/7471 (84%)]\tLoss: 1388290.250000\n",
            "Train Epoch: 49 [6400/7471 (86%)]\tLoss: 1631772.625000\n",
            "Train Epoch: 49 [6560/7471 (88%)]\tLoss: 1520496.500000\n",
            "Train Epoch: 49 [6720/7471 (90%)]\tLoss: 1614994.500000\n",
            "Train Epoch: 49 [6880/7471 (92%)]\tLoss: 1617952.750000\n",
            "Train Epoch: 49 [7040/7471 (94%)]\tLoss: 1546907.375000\n",
            "Train Epoch: 49 [7200/7471 (96%)]\tLoss: 1607328.625000\n",
            "Train Epoch: 49 [7360/7471 (99%)]\tLoss: 1620234.125000\n",
            "Epoch 49 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98603.7253\n",
            "\n",
            "Train Epoch: 50 [160/7471 (2%)]\tLoss: 1620111.875000\n",
            "Train Epoch: 50 [320/7471 (4%)]\tLoss: 1640216.000000\n",
            "Train Epoch: 50 [480/7471 (6%)]\tLoss: 1575056.625000\n",
            "Train Epoch: 50 [640/7471 (9%)]\tLoss: 1547548.875000\n",
            "Train Epoch: 50 [800/7471 (11%)]\tLoss: 1579250.875000\n",
            "Train Epoch: 50 [960/7471 (13%)]\tLoss: 1611081.000000\n",
            "Train Epoch: 50 [1120/7471 (15%)]\tLoss: 1533414.375000\n",
            "Train Epoch: 50 [1280/7471 (17%)]\tLoss: 1569378.000000\n",
            "Train Epoch: 50 [1440/7471 (19%)]\tLoss: 1598009.375000\n",
            "Train Epoch: 50 [1600/7471 (21%)]\tLoss: 1545798.625000\n",
            "Train Epoch: 50 [1760/7471 (24%)]\tLoss: 1631457.000000\n",
            "Train Epoch: 50 [1920/7471 (26%)]\tLoss: 1553384.625000\n",
            "Train Epoch: 50 [2080/7471 (28%)]\tLoss: 1543299.625000\n",
            "Train Epoch: 50 [2240/7471 (30%)]\tLoss: 1585402.250000\n",
            "Train Epoch: 50 [2400/7471 (32%)]\tLoss: 1514298.875000\n",
            "Train Epoch: 50 [2560/7471 (34%)]\tLoss: 1620287.750000\n",
            "Train Epoch: 50 [2720/7471 (36%)]\tLoss: 1616872.125000\n",
            "Train Epoch: 50 [2880/7471 (39%)]\tLoss: 1561115.875000\n",
            "Train Epoch: 50 [3040/7471 (41%)]\tLoss: 1557945.500000\n",
            "Train Epoch: 50 [3200/7471 (43%)]\tLoss: 1620918.250000\n",
            "Train Epoch: 50 [3360/7471 (45%)]\tLoss: 1586703.000000\n",
            "Train Epoch: 50 [3520/7471 (47%)]\tLoss: 1585004.125000\n",
            "Train Epoch: 50 [3680/7471 (49%)]\tLoss: 1589503.875000\n",
            "Train Epoch: 50 [3840/7471 (51%)]\tLoss: 1553622.500000\n",
            "Train Epoch: 50 [4000/7471 (54%)]\tLoss: 1569042.125000\n",
            "Train Epoch: 50 [4160/7471 (56%)]\tLoss: 1608705.000000\n",
            "Train Epoch: 50 [4320/7471 (58%)]\tLoss: 1602956.750000\n",
            "Train Epoch: 50 [4480/7471 (60%)]\tLoss: 1525430.875000\n",
            "Train Epoch: 50 [4640/7471 (62%)]\tLoss: 1560402.250000\n",
            "Train Epoch: 50 [4800/7471 (64%)]\tLoss: 1538984.750000\n",
            "Train Epoch: 50 [4960/7471 (66%)]\tLoss: 1537379.750000\n",
            "Train Epoch: 50 [5120/7471 (69%)]\tLoss: 1558039.125000\n",
            "Train Epoch: 50 [5280/7471 (71%)]\tLoss: 1531314.500000\n",
            "Train Epoch: 50 [5440/7471 (73%)]\tLoss: 1611209.250000\n",
            "Train Epoch: 50 [5600/7471 (75%)]\tLoss: 1621486.625000\n",
            "Train Epoch: 50 [5760/7471 (77%)]\tLoss: 1537647.500000\n",
            "Train Epoch: 50 [5920/7471 (79%)]\tLoss: 1611742.375000\n",
            "Train Epoch: 50 [6080/7471 (81%)]\tLoss: 1529586.500000\n",
            "Train Epoch: 50 [6240/7471 (84%)]\tLoss: 1546954.875000\n",
            "Train Epoch: 50 [6400/7471 (86%)]\tLoss: 1579989.625000\n",
            "Train Epoch: 50 [6560/7471 (88%)]\tLoss: 1606646.500000\n",
            "Train Epoch: 50 [6720/7471 (90%)]\tLoss: 1572132.625000\n",
            "Train Epoch: 50 [6880/7471 (92%)]\tLoss: 1563099.250000\n",
            "Train Epoch: 50 [7040/7471 (94%)]\tLoss: 1567068.375000\n",
            "Train Epoch: 50 [7200/7471 (96%)]\tLoss: 1588597.125000\n",
            "Train Epoch: 50 [7360/7471 (99%)]\tLoss: 1639485.750000\n",
            "Epoch 50 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98592.2494\n",
            "\n",
            "Train Epoch: 51 [160/7471 (2%)]\tLoss: 1545507.125000\n",
            "Train Epoch: 51 [320/7471 (4%)]\tLoss: 1598190.000000\n",
            "Train Epoch: 51 [480/7471 (6%)]\tLoss: 1593003.250000\n",
            "Train Epoch: 51 [640/7471 (9%)]\tLoss: 1591991.125000\n",
            "Train Epoch: 51 [800/7471 (11%)]\tLoss: 1544646.750000\n",
            "Train Epoch: 51 [960/7471 (13%)]\tLoss: 1533177.375000\n",
            "Train Epoch: 51 [1120/7471 (15%)]\tLoss: 1605477.500000\n",
            "Train Epoch: 51 [1280/7471 (17%)]\tLoss: 1621611.125000\n",
            "Train Epoch: 51 [1440/7471 (19%)]\tLoss: 1574692.250000\n",
            "Train Epoch: 51 [1600/7471 (21%)]\tLoss: 1622129.250000\n",
            "Train Epoch: 51 [1760/7471 (24%)]\tLoss: 1602614.375000\n",
            "Train Epoch: 51 [1920/7471 (26%)]\tLoss: 1628435.000000\n",
            "Train Epoch: 51 [2080/7471 (28%)]\tLoss: 1617743.750000\n",
            "Train Epoch: 51 [2240/7471 (30%)]\tLoss: 1611372.125000\n",
            "Train Epoch: 51 [2400/7471 (32%)]\tLoss: 1628986.750000\n",
            "Train Epoch: 51 [2560/7471 (34%)]\tLoss: 1488620.125000\n",
            "Train Epoch: 51 [2720/7471 (36%)]\tLoss: 1562236.250000\n",
            "Train Epoch: 51 [2880/7471 (39%)]\tLoss: 1637709.375000\n",
            "Train Epoch: 51 [3040/7471 (41%)]\tLoss: 1563164.250000\n",
            "Train Epoch: 51 [3200/7471 (43%)]\tLoss: 1569640.375000\n",
            "Train Epoch: 51 [3360/7471 (45%)]\tLoss: 1578378.375000\n",
            "Train Epoch: 51 [3520/7471 (47%)]\tLoss: 1596096.875000\n",
            "Train Epoch: 51 [3680/7471 (49%)]\tLoss: 1602500.500000\n",
            "Train Epoch: 51 [3840/7471 (51%)]\tLoss: 1579296.125000\n",
            "Train Epoch: 51 [4000/7471 (54%)]\tLoss: 1627398.750000\n",
            "Train Epoch: 51 [4160/7471 (56%)]\tLoss: 1573194.625000\n",
            "Train Epoch: 51 [4320/7471 (58%)]\tLoss: 1603911.375000\n",
            "Train Epoch: 51 [4480/7471 (60%)]\tLoss: 1612024.375000\n",
            "Train Epoch: 51 [4640/7471 (62%)]\tLoss: 1549814.000000\n",
            "Train Epoch: 51 [4800/7471 (64%)]\tLoss: 1509927.000000\n",
            "Train Epoch: 51 [4960/7471 (66%)]\tLoss: 1646668.125000\n",
            "Train Epoch: 51 [5120/7471 (69%)]\tLoss: 1562836.125000\n",
            "Train Epoch: 51 [5280/7471 (71%)]\tLoss: 1597987.000000\n",
            "Train Epoch: 51 [5440/7471 (73%)]\tLoss: 1609987.625000\n",
            "Train Epoch: 51 [5600/7471 (75%)]\tLoss: 1535576.250000\n",
            "Train Epoch: 51 [5760/7471 (77%)]\tLoss: 1600053.500000\n",
            "Train Epoch: 51 [5920/7471 (79%)]\tLoss: 1544519.125000\n",
            "Train Epoch: 51 [6080/7471 (81%)]\tLoss: 1569017.375000\n",
            "Train Epoch: 51 [6240/7471 (84%)]\tLoss: 1575848.625000\n",
            "Train Epoch: 51 [6400/7471 (86%)]\tLoss: 1584058.125000\n",
            "Train Epoch: 51 [6560/7471 (88%)]\tLoss: 1591562.875000\n",
            "Train Epoch: 51 [6720/7471 (90%)]\tLoss: 1528447.000000\n",
            "Train Epoch: 51 [6880/7471 (92%)]\tLoss: 1611568.625000\n",
            "Train Epoch: 51 [7040/7471 (94%)]\tLoss: 1533924.125000\n",
            "Train Epoch: 51 [7200/7471 (96%)]\tLoss: 1571466.500000\n",
            "Train Epoch: 51 [7360/7471 (99%)]\tLoss: 1566466.750000\n",
            "Epoch 51 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98606.9897\n",
            "\n",
            "Train Epoch: 52 [160/7471 (2%)]\tLoss: 1601572.750000\n",
            "Train Epoch: 52 [320/7471 (4%)]\tLoss: 1614755.000000\n",
            "Train Epoch: 52 [480/7471 (6%)]\tLoss: 1608372.500000\n",
            "Train Epoch: 52 [640/7471 (9%)]\tLoss: 1553574.500000\n",
            "Train Epoch: 52 [800/7471 (11%)]\tLoss: 1637379.625000\n",
            "Train Epoch: 52 [960/7471 (13%)]\tLoss: 1571000.250000\n",
            "Train Epoch: 52 [1120/7471 (15%)]\tLoss: 1551815.625000\n",
            "Train Epoch: 52 [1280/7471 (17%)]\tLoss: 1492663.125000\n",
            "Train Epoch: 52 [1440/7471 (19%)]\tLoss: 1595307.250000\n",
            "Train Epoch: 52 [1600/7471 (21%)]\tLoss: 1603084.500000\n",
            "Train Epoch: 52 [1760/7471 (24%)]\tLoss: 1561270.625000\n",
            "Train Epoch: 52 [1920/7471 (26%)]\tLoss: 1558607.500000\n",
            "Train Epoch: 52 [2080/7471 (28%)]\tLoss: 1571844.375000\n",
            "Train Epoch: 52 [2240/7471 (30%)]\tLoss: 1583203.500000\n",
            "Train Epoch: 52 [2400/7471 (32%)]\tLoss: 1602480.500000\n",
            "Train Epoch: 52 [2560/7471 (34%)]\tLoss: 1542979.750000\n",
            "Train Epoch: 52 [2720/7471 (36%)]\tLoss: 1546553.000000\n",
            "Train Epoch: 52 [2880/7471 (39%)]\tLoss: 1593529.250000\n",
            "Train Epoch: 52 [3040/7471 (41%)]\tLoss: 1460760.625000\n",
            "Train Epoch: 52 [3200/7471 (43%)]\tLoss: 1590982.875000\n",
            "Train Epoch: 52 [3360/7471 (45%)]\tLoss: 1617678.875000\n",
            "Train Epoch: 52 [3520/7471 (47%)]\tLoss: 1589595.625000\n",
            "Train Epoch: 52 [3680/7471 (49%)]\tLoss: 1588098.250000\n",
            "Train Epoch: 52 [3840/7471 (51%)]\tLoss: 1628166.625000\n",
            "Train Epoch: 52 [4000/7471 (54%)]\tLoss: 1623358.375000\n",
            "Train Epoch: 52 [4160/7471 (56%)]\tLoss: 1575955.375000\n",
            "Train Epoch: 52 [4320/7471 (58%)]\tLoss: 1601357.500000\n",
            "Train Epoch: 52 [4480/7471 (60%)]\tLoss: 1514750.750000\n",
            "Train Epoch: 52 [4640/7471 (62%)]\tLoss: 1514072.000000\n",
            "Train Epoch: 52 [4800/7471 (64%)]\tLoss: 1615916.500000\n",
            "Train Epoch: 52 [4960/7471 (66%)]\tLoss: 1549871.375000\n",
            "Train Epoch: 52 [5120/7471 (69%)]\tLoss: 1606356.750000\n",
            "Train Epoch: 52 [5280/7471 (71%)]\tLoss: 1607861.500000\n",
            "Train Epoch: 52 [5440/7471 (73%)]\tLoss: 1568715.750000\n",
            "Train Epoch: 52 [5600/7471 (75%)]\tLoss: 1597772.500000\n",
            "Train Epoch: 52 [5760/7471 (77%)]\tLoss: 1619700.875000\n",
            "Train Epoch: 52 [5920/7471 (79%)]\tLoss: 1653830.875000\n",
            "Train Epoch: 52 [6080/7471 (81%)]\tLoss: 1605100.875000\n",
            "Train Epoch: 52 [6240/7471 (84%)]\tLoss: 1614223.875000\n",
            "Train Epoch: 52 [6400/7471 (86%)]\tLoss: 1593505.250000\n",
            "Train Epoch: 52 [6560/7471 (88%)]\tLoss: 1640365.375000\n",
            "Train Epoch: 52 [6720/7471 (90%)]\tLoss: 1506536.875000\n",
            "Train Epoch: 52 [6880/7471 (92%)]\tLoss: 1607338.500000\n",
            "Train Epoch: 52 [7040/7471 (94%)]\tLoss: 1611552.500000\n",
            "Train Epoch: 52 [7200/7471 (96%)]\tLoss: 1637168.500000\n",
            "Train Epoch: 52 [7360/7471 (99%)]\tLoss: 1539757.875000\n",
            "Epoch 52 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98673.9283\n",
            "\n",
            "Train Epoch: 53 [160/7471 (2%)]\tLoss: 1555717.125000\n",
            "Train Epoch: 53 [320/7471 (4%)]\tLoss: 1551914.000000\n",
            "Train Epoch: 53 [480/7471 (6%)]\tLoss: 1595029.750000\n",
            "Train Epoch: 53 [640/7471 (9%)]\tLoss: 1581352.500000\n",
            "Train Epoch: 53 [800/7471 (11%)]\tLoss: 1541470.500000\n",
            "Train Epoch: 53 [960/7471 (13%)]\tLoss: 1543414.500000\n",
            "Train Epoch: 53 [1120/7471 (15%)]\tLoss: 1639724.500000\n",
            "Train Epoch: 53 [1280/7471 (17%)]\tLoss: 1619119.875000\n",
            "Train Epoch: 53 [1440/7471 (19%)]\tLoss: 1584877.375000\n",
            "Train Epoch: 53 [1600/7471 (21%)]\tLoss: 1584422.000000\n",
            "Train Epoch: 53 [1760/7471 (24%)]\tLoss: 1495611.625000\n",
            "Train Epoch: 53 [1920/7471 (26%)]\tLoss: 1522331.000000\n",
            "Train Epoch: 53 [2080/7471 (28%)]\tLoss: 1586252.875000\n",
            "Train Epoch: 53 [2240/7471 (30%)]\tLoss: 1619389.375000\n",
            "Train Epoch: 53 [2400/7471 (32%)]\tLoss: 1586039.750000\n",
            "Train Epoch: 53 [2560/7471 (34%)]\tLoss: 1538611.250000\n",
            "Train Epoch: 53 [2720/7471 (36%)]\tLoss: 1597077.875000\n",
            "Train Epoch: 53 [2880/7471 (39%)]\tLoss: 1611255.625000\n",
            "Train Epoch: 53 [3040/7471 (41%)]\tLoss: 1577120.500000\n",
            "Train Epoch: 53 [3200/7471 (43%)]\tLoss: 1602650.250000\n",
            "Train Epoch: 53 [3360/7471 (45%)]\tLoss: 1603524.000000\n",
            "Train Epoch: 53 [3520/7471 (47%)]\tLoss: 1570258.000000\n",
            "Train Epoch: 53 [3680/7471 (49%)]\tLoss: 1467973.875000\n",
            "Train Epoch: 53 [3840/7471 (51%)]\tLoss: 1507083.125000\n",
            "Train Epoch: 53 [4000/7471 (54%)]\tLoss: 1625100.250000\n",
            "Train Epoch: 53 [4160/7471 (56%)]\tLoss: 1623047.125000\n",
            "Train Epoch: 53 [4320/7471 (58%)]\tLoss: 1535148.750000\n",
            "Train Epoch: 53 [4480/7471 (60%)]\tLoss: 1621311.625000\n",
            "Train Epoch: 53 [4640/7471 (62%)]\tLoss: 1612079.250000\n",
            "Train Epoch: 53 [4800/7471 (64%)]\tLoss: 1567489.500000\n",
            "Train Epoch: 53 [4960/7471 (66%)]\tLoss: 1611600.375000\n",
            "Train Epoch: 53 [5120/7471 (69%)]\tLoss: 1623826.625000\n",
            "Train Epoch: 53 [5280/7471 (71%)]\tLoss: 1600143.375000\n",
            "Train Epoch: 53 [5440/7471 (73%)]\tLoss: 1568674.000000\n",
            "Train Epoch: 53 [5600/7471 (75%)]\tLoss: 1549429.250000\n",
            "Train Epoch: 53 [5760/7471 (77%)]\tLoss: 1638874.250000\n",
            "Train Epoch: 53 [5920/7471 (79%)]\tLoss: 1577698.875000\n",
            "Train Epoch: 53 [6080/7471 (81%)]\tLoss: 1589683.000000\n",
            "Train Epoch: 53 [6240/7471 (84%)]\tLoss: 1557739.500000\n",
            "Train Epoch: 53 [6400/7471 (86%)]\tLoss: 1531233.875000\n",
            "Train Epoch: 53 [6560/7471 (88%)]\tLoss: 1611268.875000\n",
            "Train Epoch: 53 [6720/7471 (90%)]\tLoss: 1602802.750000\n",
            "Train Epoch: 53 [6880/7471 (92%)]\tLoss: 1551344.750000\n",
            "Train Epoch: 53 [7040/7471 (94%)]\tLoss: 1535859.375000\n",
            "Train Epoch: 53 [7200/7471 (96%)]\tLoss: 1613938.375000\n",
            "Train Epoch: 53 [7360/7471 (99%)]\tLoss: 1599985.625000\n",
            "Epoch 53 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98901.1277\n",
            "\n",
            "Train Epoch: 54 [160/7471 (2%)]\tLoss: 1559581.625000\n",
            "Train Epoch: 54 [320/7471 (4%)]\tLoss: 1587398.750000\n",
            "Train Epoch: 54 [480/7471 (6%)]\tLoss: 1532066.625000\n",
            "Train Epoch: 54 [640/7471 (9%)]\tLoss: 1566675.500000\n",
            "Train Epoch: 54 [800/7471 (11%)]\tLoss: 1588218.500000\n",
            "Train Epoch: 54 [960/7471 (13%)]\tLoss: 1613783.500000\n",
            "Train Epoch: 54 [1120/7471 (15%)]\tLoss: 1528939.875000\n",
            "Train Epoch: 54 [1280/7471 (17%)]\tLoss: 1609115.125000\n",
            "Train Epoch: 54 [1440/7471 (19%)]\tLoss: 1558668.625000\n",
            "Train Epoch: 54 [1600/7471 (21%)]\tLoss: 1612624.250000\n",
            "Train Epoch: 54 [1760/7471 (24%)]\tLoss: 1594462.125000\n",
            "Train Epoch: 54 [1920/7471 (26%)]\tLoss: 1643339.750000\n",
            "Train Epoch: 54 [2080/7471 (28%)]\tLoss: 1573850.375000\n",
            "Train Epoch: 54 [2240/7471 (30%)]\tLoss: 1584530.375000\n",
            "Train Epoch: 54 [2400/7471 (32%)]\tLoss: 1525132.500000\n",
            "Train Epoch: 54 [2560/7471 (34%)]\tLoss: 1589924.625000\n",
            "Train Epoch: 54 [2720/7471 (36%)]\tLoss: 1560872.000000\n",
            "Train Epoch: 54 [2880/7471 (39%)]\tLoss: 1569026.250000\n",
            "Train Epoch: 54 [3040/7471 (41%)]\tLoss: 1574885.750000\n",
            "Train Epoch: 54 [3200/7471 (43%)]\tLoss: 1501230.000000\n",
            "Train Epoch: 54 [3360/7471 (45%)]\tLoss: 1615478.625000\n",
            "Train Epoch: 54 [3520/7471 (47%)]\tLoss: 1526249.125000\n",
            "Train Epoch: 54 [3680/7471 (49%)]\tLoss: 1607618.875000\n",
            "Train Epoch: 54 [3840/7471 (51%)]\tLoss: 1601525.125000\n",
            "Train Epoch: 54 [4000/7471 (54%)]\tLoss: 1619305.250000\n",
            "Train Epoch: 54 [4160/7471 (56%)]\tLoss: 1541421.375000\n",
            "Train Epoch: 54 [4320/7471 (58%)]\tLoss: 1582620.875000\n",
            "Train Epoch: 54 [4480/7471 (60%)]\tLoss: 1545361.750000\n",
            "Train Epoch: 54 [4640/7471 (62%)]\tLoss: 1600800.000000\n",
            "Train Epoch: 54 [4800/7471 (64%)]\tLoss: 1628873.250000\n",
            "Train Epoch: 54 [4960/7471 (66%)]\tLoss: 1555583.375000\n",
            "Train Epoch: 54 [5120/7471 (69%)]\tLoss: 1593052.625000\n",
            "Train Epoch: 54 [5280/7471 (71%)]\tLoss: 1583850.125000\n",
            "Train Epoch: 54 [5440/7471 (73%)]\tLoss: 1554910.125000\n",
            "Train Epoch: 54 [5600/7471 (75%)]\tLoss: 1602736.750000\n",
            "Train Epoch: 54 [5760/7471 (77%)]\tLoss: 1578763.250000\n",
            "Train Epoch: 54 [5920/7471 (79%)]\tLoss: 1558581.625000\n",
            "Train Epoch: 54 [6080/7471 (81%)]\tLoss: 1568688.000000\n",
            "Train Epoch: 54 [6240/7471 (84%)]\tLoss: 1583554.125000\n",
            "Train Epoch: 54 [6400/7471 (86%)]\tLoss: 1609308.375000\n",
            "Train Epoch: 54 [6560/7471 (88%)]\tLoss: 1594125.250000\n",
            "Train Epoch: 54 [6720/7471 (90%)]\tLoss: 1578373.625000\n",
            "Train Epoch: 54 [6880/7471 (92%)]\tLoss: 1570674.375000\n",
            "Train Epoch: 54 [7040/7471 (94%)]\tLoss: 1585669.500000\n",
            "Train Epoch: 54 [7200/7471 (96%)]\tLoss: 1536459.000000\n",
            "Train Epoch: 54 [7360/7471 (99%)]\tLoss: 1589563.625000\n",
            "Epoch 54 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98609.7878\n",
            "\n",
            "Train Epoch: 55 [160/7471 (2%)]\tLoss: 1560911.000000\n",
            "Train Epoch: 55 [320/7471 (4%)]\tLoss: 1551171.625000\n",
            "Train Epoch: 55 [480/7471 (6%)]\tLoss: 1598628.625000\n",
            "Train Epoch: 55 [640/7471 (9%)]\tLoss: 1629318.500000\n",
            "Train Epoch: 55 [800/7471 (11%)]\tLoss: 1585198.250000\n",
            "Train Epoch: 55 [960/7471 (13%)]\tLoss: 1538247.750000\n",
            "Train Epoch: 55 [1120/7471 (15%)]\tLoss: 1595349.750000\n",
            "Train Epoch: 55 [1280/7471 (17%)]\tLoss: 1547313.750000\n",
            "Train Epoch: 55 [1440/7471 (19%)]\tLoss: 1597579.250000\n",
            "Train Epoch: 55 [1600/7471 (21%)]\tLoss: 1522829.125000\n",
            "Train Epoch: 55 [1760/7471 (24%)]\tLoss: 1570212.875000\n",
            "Train Epoch: 55 [1920/7471 (26%)]\tLoss: 1601122.875000\n",
            "Train Epoch: 55 [2080/7471 (28%)]\tLoss: 1617820.500000\n",
            "Train Epoch: 55 [2240/7471 (30%)]\tLoss: 1571184.625000\n",
            "Train Epoch: 55 [2400/7471 (32%)]\tLoss: 1614786.375000\n",
            "Train Epoch: 55 [2560/7471 (34%)]\tLoss: 1444993.000000\n",
            "Train Epoch: 55 [2720/7471 (36%)]\tLoss: 1562986.875000\n",
            "Train Epoch: 55 [2880/7471 (39%)]\tLoss: 1508375.125000\n",
            "Train Epoch: 55 [3040/7471 (41%)]\tLoss: 1639723.000000\n",
            "Train Epoch: 55 [3200/7471 (43%)]\tLoss: 1627053.625000\n",
            "Train Epoch: 55 [3360/7471 (45%)]\tLoss: 1620217.250000\n",
            "Train Epoch: 55 [3520/7471 (47%)]\tLoss: 1573061.000000\n",
            "Train Epoch: 55 [3680/7471 (49%)]\tLoss: 1582025.750000\n",
            "Train Epoch: 55 [3840/7471 (51%)]\tLoss: 1597578.000000\n",
            "Train Epoch: 55 [4000/7471 (54%)]\tLoss: 1608164.000000\n",
            "Train Epoch: 55 [4160/7471 (56%)]\tLoss: 1585720.000000\n",
            "Train Epoch: 55 [4320/7471 (58%)]\tLoss: 1591149.625000\n",
            "Train Epoch: 55 [4480/7471 (60%)]\tLoss: 1574526.750000\n",
            "Train Epoch: 55 [4640/7471 (62%)]\tLoss: 1594393.000000\n",
            "Train Epoch: 55 [4800/7471 (64%)]\tLoss: 1574724.625000\n",
            "Train Epoch: 55 [4960/7471 (66%)]\tLoss: 1567489.500000\n",
            "Train Epoch: 55 [5120/7471 (69%)]\tLoss: 1586632.750000\n",
            "Train Epoch: 55 [5280/7471 (71%)]\tLoss: 1596419.125000\n",
            "Train Epoch: 55 [5440/7471 (73%)]\tLoss: 1556864.250000\n",
            "Train Epoch: 55 [5600/7471 (75%)]\tLoss: 1521355.125000\n",
            "Train Epoch: 55 [5760/7471 (77%)]\tLoss: 1539538.250000\n",
            "Train Epoch: 55 [5920/7471 (79%)]\tLoss: 1627608.375000\n",
            "Train Epoch: 55 [6080/7471 (81%)]\tLoss: 1604466.000000\n",
            "Train Epoch: 55 [6240/7471 (84%)]\tLoss: 1598955.625000\n",
            "Train Epoch: 55 [6400/7471 (86%)]\tLoss: 1573514.875000\n",
            "Train Epoch: 55 [6560/7471 (88%)]\tLoss: 1610992.625000\n",
            "Train Epoch: 55 [6720/7471 (90%)]\tLoss: 1563661.750000\n",
            "Train Epoch: 55 [6880/7471 (92%)]\tLoss: 1560164.250000\n",
            "Train Epoch: 55 [7040/7471 (94%)]\tLoss: 1615914.375000\n",
            "Train Epoch: 55 [7200/7471 (96%)]\tLoss: 1626710.125000\n",
            "Train Epoch: 55 [7360/7471 (99%)]\tLoss: 1626796.750000\n",
            "Epoch 55 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98621.3919\n",
            "\n",
            "Train Epoch: 56 [160/7471 (2%)]\tLoss: 1597075.125000\n",
            "Train Epoch: 56 [320/7471 (4%)]\tLoss: 1657889.125000\n",
            "Train Epoch: 56 [480/7471 (6%)]\tLoss: 1559448.875000\n",
            "Train Epoch: 56 [640/7471 (9%)]\tLoss: 1568495.125000\n",
            "Train Epoch: 56 [800/7471 (11%)]\tLoss: 1606716.000000\n",
            "Train Epoch: 56 [960/7471 (13%)]\tLoss: 1592957.125000\n",
            "Train Epoch: 56 [1120/7471 (15%)]\tLoss: 1519969.250000\n",
            "Train Epoch: 56 [1280/7471 (17%)]\tLoss: 1525032.375000\n",
            "Train Epoch: 56 [1440/7471 (19%)]\tLoss: 1550016.750000\n",
            "Train Epoch: 56 [1600/7471 (21%)]\tLoss: 1610154.125000\n",
            "Train Epoch: 56 [1760/7471 (24%)]\tLoss: 1570467.750000\n",
            "Train Epoch: 56 [1920/7471 (26%)]\tLoss: 1590605.375000\n",
            "Train Epoch: 56 [2080/7471 (28%)]\tLoss: 1476956.125000\n",
            "Train Epoch: 56 [2240/7471 (30%)]\tLoss: 1597669.375000\n",
            "Train Epoch: 56 [2400/7471 (32%)]\tLoss: 1649476.250000\n",
            "Train Epoch: 56 [2560/7471 (34%)]\tLoss: 1567092.750000\n",
            "Train Epoch: 56 [2720/7471 (36%)]\tLoss: 1544365.875000\n",
            "Train Epoch: 56 [2880/7471 (39%)]\tLoss: 1557029.500000\n",
            "Train Epoch: 56 [3040/7471 (41%)]\tLoss: 1614865.875000\n",
            "Train Epoch: 56 [3200/7471 (43%)]\tLoss: 1587127.750000\n",
            "Train Epoch: 56 [3360/7471 (45%)]\tLoss: 1546827.375000\n",
            "Train Epoch: 56 [3520/7471 (47%)]\tLoss: 1593554.375000\n",
            "Train Epoch: 56 [3680/7471 (49%)]\tLoss: 1643182.000000\n",
            "Train Epoch: 56 [3840/7471 (51%)]\tLoss: 1579069.250000\n",
            "Train Epoch: 56 [4000/7471 (54%)]\tLoss: 1602214.875000\n",
            "Train Epoch: 56 [4160/7471 (56%)]\tLoss: 1626907.000000\n",
            "Train Epoch: 56 [4320/7471 (58%)]\tLoss: 1588841.625000\n",
            "Train Epoch: 56 [4480/7471 (60%)]\tLoss: 1620462.625000\n",
            "Train Epoch: 56 [4640/7471 (62%)]\tLoss: 1504633.750000\n",
            "Train Epoch: 56 [4800/7471 (64%)]\tLoss: 1648927.875000\n",
            "Train Epoch: 56 [4960/7471 (66%)]\tLoss: 1599379.125000\n",
            "Train Epoch: 56 [5120/7471 (69%)]\tLoss: 1504390.500000\n",
            "Train Epoch: 56 [5280/7471 (71%)]\tLoss: 1626926.125000\n",
            "Train Epoch: 56 [5440/7471 (73%)]\tLoss: 1525977.250000\n",
            "Train Epoch: 56 [5600/7471 (75%)]\tLoss: 1560920.125000\n",
            "Train Epoch: 56 [5760/7471 (77%)]\tLoss: 1559933.000000\n",
            "Train Epoch: 56 [5920/7471 (79%)]\tLoss: 1571362.375000\n",
            "Train Epoch: 56 [6080/7471 (81%)]\tLoss: 1582832.125000\n",
            "Train Epoch: 56 [6240/7471 (84%)]\tLoss: 1544591.000000\n",
            "Train Epoch: 56 [6400/7471 (86%)]\tLoss: 1520650.875000\n",
            "Train Epoch: 56 [6560/7471 (88%)]\tLoss: 1599550.250000\n",
            "Train Epoch: 56 [6720/7471 (90%)]\tLoss: 1635131.250000\n",
            "Train Epoch: 56 [6880/7471 (92%)]\tLoss: 1526688.125000\n",
            "Train Epoch: 56 [7040/7471 (94%)]\tLoss: 1578709.250000\n",
            "Train Epoch: 56 [7200/7471 (96%)]\tLoss: 1533039.250000\n",
            "Train Epoch: 56 [7360/7471 (99%)]\tLoss: 1618232.500000\n",
            "Epoch 56 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98598.1963\n",
            "\n",
            "Train Epoch: 57 [160/7471 (2%)]\tLoss: 1572212.875000\n",
            "Train Epoch: 57 [320/7471 (4%)]\tLoss: 1555545.500000\n",
            "Train Epoch: 57 [480/7471 (6%)]\tLoss: 1581884.250000\n",
            "Train Epoch: 57 [640/7471 (9%)]\tLoss: 1626617.125000\n",
            "Train Epoch: 57 [800/7471 (11%)]\tLoss: 1584222.625000\n",
            "Train Epoch: 57 [960/7471 (13%)]\tLoss: 1537974.250000\n",
            "Train Epoch: 57 [1120/7471 (15%)]\tLoss: 1494748.000000\n",
            "Train Epoch: 57 [1280/7471 (17%)]\tLoss: 1528745.375000\n",
            "Train Epoch: 57 [1440/7471 (19%)]\tLoss: 1596751.250000\n",
            "Train Epoch: 57 [1600/7471 (21%)]\tLoss: 1640793.375000\n",
            "Train Epoch: 57 [1760/7471 (24%)]\tLoss: 1556347.375000\n",
            "Train Epoch: 57 [1920/7471 (26%)]\tLoss: 1602971.000000\n",
            "Train Epoch: 57 [2080/7471 (28%)]\tLoss: 1609091.125000\n",
            "Train Epoch: 57 [2240/7471 (30%)]\tLoss: 1554674.250000\n",
            "Train Epoch: 57 [2400/7471 (32%)]\tLoss: 1572425.125000\n",
            "Train Epoch: 57 [2560/7471 (34%)]\tLoss: 1552727.250000\n",
            "Train Epoch: 57 [2720/7471 (36%)]\tLoss: 1595777.625000\n",
            "Train Epoch: 57 [2880/7471 (39%)]\tLoss: 1608742.750000\n",
            "Train Epoch: 57 [3040/7471 (41%)]\tLoss: 1562902.375000\n",
            "Train Epoch: 57 [3200/7471 (43%)]\tLoss: 1603074.125000\n",
            "Train Epoch: 57 [3360/7471 (45%)]\tLoss: 1617112.625000\n",
            "Train Epoch: 57 [3520/7471 (47%)]\tLoss: 1620204.500000\n",
            "Train Epoch: 57 [3680/7471 (49%)]\tLoss: 1591914.125000\n",
            "Train Epoch: 57 [3840/7471 (51%)]\tLoss: 1558233.500000\n",
            "Train Epoch: 57 [4000/7471 (54%)]\tLoss: 1563282.875000\n",
            "Train Epoch: 57 [4160/7471 (56%)]\tLoss: 1611685.750000\n",
            "Train Epoch: 57 [4320/7471 (58%)]\tLoss: 1553912.000000\n",
            "Train Epoch: 57 [4480/7471 (60%)]\tLoss: 1515292.125000\n",
            "Train Epoch: 57 [4640/7471 (62%)]\tLoss: 1623388.875000\n",
            "Train Epoch: 57 [4800/7471 (64%)]\tLoss: 1566662.000000\n",
            "Train Epoch: 57 [4960/7471 (66%)]\tLoss: 1524004.000000\n",
            "Train Epoch: 57 [5120/7471 (69%)]\tLoss: 1607987.250000\n",
            "Train Epoch: 57 [5280/7471 (71%)]\tLoss: 1524865.250000\n",
            "Train Epoch: 57 [5440/7471 (73%)]\tLoss: 1626453.875000\n",
            "Train Epoch: 57 [5600/7471 (75%)]\tLoss: 1640496.375000\n",
            "Train Epoch: 57 [5760/7471 (77%)]\tLoss: 1620647.750000\n",
            "Train Epoch: 57 [5920/7471 (79%)]\tLoss: 1520981.375000\n",
            "Train Epoch: 57 [6080/7471 (81%)]\tLoss: 1554905.000000\n",
            "Train Epoch: 57 [6240/7471 (84%)]\tLoss: 1634283.500000\n",
            "Train Epoch: 57 [6400/7471 (86%)]\tLoss: 1564017.250000\n",
            "Train Epoch: 57 [6560/7471 (88%)]\tLoss: 1586993.375000\n",
            "Train Epoch: 57 [6720/7471 (90%)]\tLoss: 1619238.875000\n",
            "Train Epoch: 57 [6880/7471 (92%)]\tLoss: 1580614.375000\n",
            "Train Epoch: 57 [7040/7471 (94%)]\tLoss: 1539775.125000\n",
            "Train Epoch: 57 [7200/7471 (96%)]\tLoss: 1590242.125000\n",
            "Train Epoch: 57 [7360/7471 (99%)]\tLoss: 1541983.125000\n",
            "Epoch 57 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98584.1948\n",
            "\n",
            "Train Epoch: 58 [160/7471 (2%)]\tLoss: 1549913.000000\n",
            "Train Epoch: 58 [320/7471 (4%)]\tLoss: 1545476.875000\n",
            "Train Epoch: 58 [480/7471 (6%)]\tLoss: 1593402.875000\n",
            "Train Epoch: 58 [640/7471 (9%)]\tLoss: 1609287.500000\n",
            "Train Epoch: 58 [800/7471 (11%)]\tLoss: 1600863.500000\n",
            "Train Epoch: 58 [960/7471 (13%)]\tLoss: 1550538.250000\n",
            "Train Epoch: 58 [1120/7471 (15%)]\tLoss: 1566993.250000\n",
            "Train Epoch: 58 [1280/7471 (17%)]\tLoss: 1593992.250000\n",
            "Train Epoch: 58 [1440/7471 (19%)]\tLoss: 1570781.875000\n",
            "Train Epoch: 58 [1600/7471 (21%)]\tLoss: 1624733.000000\n",
            "Train Epoch: 58 [1760/7471 (24%)]\tLoss: 1552496.250000\n",
            "Train Epoch: 58 [1920/7471 (26%)]\tLoss: 1569610.625000\n",
            "Train Epoch: 58 [2080/7471 (28%)]\tLoss: 1546566.500000\n",
            "Train Epoch: 58 [2240/7471 (30%)]\tLoss: 1608491.375000\n",
            "Train Epoch: 58 [2400/7471 (32%)]\tLoss: 1564913.375000\n",
            "Train Epoch: 58 [2560/7471 (34%)]\tLoss: 1630935.250000\n",
            "Train Epoch: 58 [2720/7471 (36%)]\tLoss: 1559623.250000\n",
            "Train Epoch: 58 [2880/7471 (39%)]\tLoss: 1622428.375000\n",
            "Train Epoch: 58 [3040/7471 (41%)]\tLoss: 1601566.125000\n",
            "Train Epoch: 58 [3200/7471 (43%)]\tLoss: 1635933.625000\n",
            "Train Epoch: 58 [3360/7471 (45%)]\tLoss: 1582119.125000\n",
            "Train Epoch: 58 [3520/7471 (47%)]\tLoss: 1586135.125000\n",
            "Train Epoch: 58 [3680/7471 (49%)]\tLoss: 1600863.750000\n",
            "Train Epoch: 58 [3840/7471 (51%)]\tLoss: 1578074.750000\n",
            "Train Epoch: 58 [4000/7471 (54%)]\tLoss: 1524630.250000\n",
            "Train Epoch: 58 [4160/7471 (56%)]\tLoss: 1571538.250000\n",
            "Train Epoch: 58 [4320/7471 (58%)]\tLoss: 1566635.000000\n",
            "Train Epoch: 58 [4480/7471 (60%)]\tLoss: 1592891.500000\n",
            "Train Epoch: 58 [4640/7471 (62%)]\tLoss: 1609792.250000\n",
            "Train Epoch: 58 [4800/7471 (64%)]\tLoss: 1591127.750000\n",
            "Train Epoch: 58 [4960/7471 (66%)]\tLoss: 1546163.500000\n",
            "Train Epoch: 58 [5120/7471 (69%)]\tLoss: 1576816.500000\n",
            "Train Epoch: 58 [5280/7471 (71%)]\tLoss: 1588179.875000\n",
            "Train Epoch: 58 [5440/7471 (73%)]\tLoss: 1579636.500000\n",
            "Train Epoch: 58 [5600/7471 (75%)]\tLoss: 1568389.250000\n",
            "Train Epoch: 58 [5760/7471 (77%)]\tLoss: 1629198.875000\n",
            "Train Epoch: 58 [5920/7471 (79%)]\tLoss: 1563530.000000\n",
            "Train Epoch: 58 [6080/7471 (81%)]\tLoss: 1600997.750000\n",
            "Train Epoch: 58 [6240/7471 (84%)]\tLoss: 1608527.125000\n",
            "Train Epoch: 58 [6400/7471 (86%)]\tLoss: 1566345.000000\n",
            "Train Epoch: 58 [6560/7471 (88%)]\tLoss: 1616374.000000\n",
            "Train Epoch: 58 [6720/7471 (90%)]\tLoss: 1583143.375000\n",
            "Train Epoch: 58 [6880/7471 (92%)]\tLoss: 1553582.250000\n",
            "Train Epoch: 58 [7040/7471 (94%)]\tLoss: 1559102.500000\n",
            "Train Epoch: 58 [7200/7471 (96%)]\tLoss: 1528204.875000\n",
            "Train Epoch: 58 [7360/7471 (99%)]\tLoss: 1579766.750000\n",
            "Epoch 58 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98671.6995\n",
            "\n",
            "Train Epoch: 59 [160/7471 (2%)]\tLoss: 1522576.500000\n",
            "Train Epoch: 59 [320/7471 (4%)]\tLoss: 1595441.500000\n",
            "Train Epoch: 59 [480/7471 (6%)]\tLoss: 1599500.500000\n",
            "Train Epoch: 59 [640/7471 (9%)]\tLoss: 1623048.625000\n",
            "Train Epoch: 59 [800/7471 (11%)]\tLoss: 1537039.625000\n",
            "Train Epoch: 59 [960/7471 (13%)]\tLoss: 1612376.750000\n",
            "Train Epoch: 59 [1120/7471 (15%)]\tLoss: 1515690.250000\n",
            "Train Epoch: 59 [1280/7471 (17%)]\tLoss: 1574136.500000\n",
            "Train Epoch: 59 [1440/7471 (19%)]\tLoss: 1562379.125000\n",
            "Train Epoch: 59 [1600/7471 (21%)]\tLoss: 1562160.000000\n",
            "Train Epoch: 59 [1760/7471 (24%)]\tLoss: 1583966.125000\n",
            "Train Epoch: 59 [1920/7471 (26%)]\tLoss: 1600788.000000\n",
            "Train Epoch: 59 [2080/7471 (28%)]\tLoss: 1501805.375000\n",
            "Train Epoch: 59 [2240/7471 (30%)]\tLoss: 1564758.875000\n",
            "Train Epoch: 59 [2400/7471 (32%)]\tLoss: 1581284.500000\n",
            "Train Epoch: 59 [2560/7471 (34%)]\tLoss: 1542471.750000\n",
            "Train Epoch: 59 [2720/7471 (36%)]\tLoss: 1534741.500000\n",
            "Train Epoch: 59 [2880/7471 (39%)]\tLoss: 1549051.125000\n",
            "Train Epoch: 59 [3040/7471 (41%)]\tLoss: 1560492.125000\n",
            "Train Epoch: 59 [3200/7471 (43%)]\tLoss: 1586109.750000\n",
            "Train Epoch: 59 [3360/7471 (45%)]\tLoss: 1612132.125000\n",
            "Train Epoch: 59 [3520/7471 (47%)]\tLoss: 1511895.875000\n",
            "Train Epoch: 59 [3680/7471 (49%)]\tLoss: 1595003.625000\n",
            "Train Epoch: 59 [3840/7471 (51%)]\tLoss: 1609806.875000\n",
            "Train Epoch: 59 [4000/7471 (54%)]\tLoss: 1527434.875000\n",
            "Train Epoch: 59 [4160/7471 (56%)]\tLoss: 1545098.875000\n",
            "Train Epoch: 59 [4320/7471 (58%)]\tLoss: 1580372.750000\n",
            "Train Epoch: 59 [4480/7471 (60%)]\tLoss: 1542532.500000\n",
            "Train Epoch: 59 [4640/7471 (62%)]\tLoss: 1592521.500000\n",
            "Train Epoch: 59 [4800/7471 (64%)]\tLoss: 1552819.125000\n",
            "Train Epoch: 59 [4960/7471 (66%)]\tLoss: 1563264.375000\n",
            "Train Epoch: 59 [5120/7471 (69%)]\tLoss: 1582020.625000\n",
            "Train Epoch: 59 [5280/7471 (71%)]\tLoss: 1575154.250000\n",
            "Train Epoch: 59 [5440/7471 (73%)]\tLoss: 1566490.375000\n",
            "Train Epoch: 59 [5600/7471 (75%)]\tLoss: 1586694.250000\n",
            "Train Epoch: 59 [5760/7471 (77%)]\tLoss: 1606059.375000\n",
            "Train Epoch: 59 [5920/7471 (79%)]\tLoss: 1625530.250000\n",
            "Train Epoch: 59 [6080/7471 (81%)]\tLoss: 1615997.625000\n",
            "Train Epoch: 59 [6240/7471 (84%)]\tLoss: 1542348.250000\n",
            "Train Epoch: 59 [6400/7471 (86%)]\tLoss: 1629745.000000\n",
            "Train Epoch: 59 [6560/7471 (88%)]\tLoss: 1584383.750000\n",
            "Train Epoch: 59 [6720/7471 (90%)]\tLoss: 1578286.375000\n",
            "Train Epoch: 59 [6880/7471 (92%)]\tLoss: 1538938.750000\n",
            "Train Epoch: 59 [7040/7471 (94%)]\tLoss: 1564959.875000\n",
            "Train Epoch: 59 [7200/7471 (96%)]\tLoss: 1594112.375000\n",
            "Train Epoch: 59 [7360/7471 (99%)]\tLoss: 1547347.500000\n",
            "Epoch 59 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98566.4475\n",
            "\n",
            "Train Epoch: 60 [160/7471 (2%)]\tLoss: 1616310.125000\n",
            "Train Epoch: 60 [320/7471 (4%)]\tLoss: 1599511.750000\n",
            "Train Epoch: 60 [480/7471 (6%)]\tLoss: 1622157.125000\n",
            "Train Epoch: 60 [640/7471 (9%)]\tLoss: 1613219.000000\n",
            "Train Epoch: 60 [800/7471 (11%)]\tLoss: 1566761.875000\n",
            "Train Epoch: 60 [960/7471 (13%)]\tLoss: 1530043.375000\n",
            "Train Epoch: 60 [1120/7471 (15%)]\tLoss: 1588026.375000\n",
            "Train Epoch: 60 [1280/7471 (17%)]\tLoss: 1601484.875000\n",
            "Train Epoch: 60 [1440/7471 (19%)]\tLoss: 1529981.500000\n",
            "Train Epoch: 60 [1600/7471 (21%)]\tLoss: 1553933.250000\n",
            "Train Epoch: 60 [1760/7471 (24%)]\tLoss: 1534061.375000\n",
            "Train Epoch: 60 [1920/7471 (26%)]\tLoss: 1550703.250000\n",
            "Train Epoch: 60 [2080/7471 (28%)]\tLoss: 1562380.250000\n",
            "Train Epoch: 60 [2240/7471 (30%)]\tLoss: 1626125.250000\n",
            "Train Epoch: 60 [2400/7471 (32%)]\tLoss: 1597388.625000\n",
            "Train Epoch: 60 [2560/7471 (34%)]\tLoss: 1498892.750000\n",
            "Train Epoch: 60 [2720/7471 (36%)]\tLoss: 1526304.500000\n",
            "Train Epoch: 60 [2880/7471 (39%)]\tLoss: 1598174.875000\n",
            "Train Epoch: 60 [3040/7471 (41%)]\tLoss: 1561219.875000\n",
            "Train Epoch: 60 [3200/7471 (43%)]\tLoss: 1594421.875000\n",
            "Train Epoch: 60 [3360/7471 (45%)]\tLoss: 1587053.500000\n",
            "Train Epoch: 60 [3520/7471 (47%)]\tLoss: 1532564.750000\n",
            "Train Epoch: 60 [3680/7471 (49%)]\tLoss: 1591417.250000\n",
            "Train Epoch: 60 [3840/7471 (51%)]\tLoss: 1625815.125000\n",
            "Train Epoch: 60 [4000/7471 (54%)]\tLoss: 1508454.500000\n",
            "Train Epoch: 60 [4160/7471 (56%)]\tLoss: 1552652.125000\n",
            "Train Epoch: 60 [4320/7471 (58%)]\tLoss: 1603968.750000\n",
            "Train Epoch: 60 [4480/7471 (60%)]\tLoss: 1571734.625000\n",
            "Train Epoch: 60 [4640/7471 (62%)]\tLoss: 1542038.375000\n",
            "Train Epoch: 60 [4800/7471 (64%)]\tLoss: 1539668.875000\n",
            "Train Epoch: 60 [4960/7471 (66%)]\tLoss: 1600006.875000\n",
            "Train Epoch: 60 [5120/7471 (69%)]\tLoss: 1575169.625000\n",
            "Train Epoch: 60 [5280/7471 (71%)]\tLoss: 1619464.375000\n",
            "Train Epoch: 60 [5440/7471 (73%)]\tLoss: 1522759.250000\n",
            "Train Epoch: 60 [5600/7471 (75%)]\tLoss: 1573575.875000\n",
            "Train Epoch: 60 [5760/7471 (77%)]\tLoss: 1622343.250000\n",
            "Train Epoch: 60 [5920/7471 (79%)]\tLoss: 1611878.000000\n",
            "Train Epoch: 60 [6080/7471 (81%)]\tLoss: 1531475.625000\n",
            "Train Epoch: 60 [6240/7471 (84%)]\tLoss: 1552444.625000\n",
            "Train Epoch: 60 [6400/7471 (86%)]\tLoss: 1543035.500000\n",
            "Train Epoch: 60 [6560/7471 (88%)]\tLoss: 1591894.500000\n",
            "Train Epoch: 60 [6720/7471 (90%)]\tLoss: 1578150.875000\n",
            "Train Epoch: 60 [6880/7471 (92%)]\tLoss: 1564408.875000\n",
            "Train Epoch: 60 [7040/7471 (94%)]\tLoss: 1544952.625000\n",
            "Train Epoch: 60 [7200/7471 (96%)]\tLoss: 1546758.000000\n",
            "Train Epoch: 60 [7360/7471 (99%)]\tLoss: 1610163.500000\n",
            "Epoch 60 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98590.6832\n",
            "\n",
            "Train Epoch: 61 [160/7471 (2%)]\tLoss: 1626120.750000\n",
            "Train Epoch: 61 [320/7471 (4%)]\tLoss: 1595785.375000\n",
            "Train Epoch: 61 [480/7471 (6%)]\tLoss: 1512356.500000\n",
            "Train Epoch: 61 [640/7471 (9%)]\tLoss: 1629493.250000\n",
            "Train Epoch: 61 [800/7471 (11%)]\tLoss: 1566918.125000\n",
            "Train Epoch: 61 [960/7471 (13%)]\tLoss: 1577209.375000\n",
            "Train Epoch: 61 [1120/7471 (15%)]\tLoss: 1538974.500000\n",
            "Train Epoch: 61 [1280/7471 (17%)]\tLoss: 1558791.500000\n",
            "Train Epoch: 61 [1440/7471 (19%)]\tLoss: 1628241.750000\n",
            "Train Epoch: 61 [1600/7471 (21%)]\tLoss: 1618684.250000\n",
            "Train Epoch: 61 [1760/7471 (24%)]\tLoss: 1569906.750000\n",
            "Train Epoch: 61 [1920/7471 (26%)]\tLoss: 1566386.875000\n",
            "Train Epoch: 61 [2080/7471 (28%)]\tLoss: 1581768.500000\n",
            "Train Epoch: 61 [2240/7471 (30%)]\tLoss: 1594148.000000\n",
            "Train Epoch: 61 [2400/7471 (32%)]\tLoss: 1568149.875000\n",
            "Train Epoch: 61 [2560/7471 (34%)]\tLoss: 1597597.125000\n",
            "Train Epoch: 61 [2720/7471 (36%)]\tLoss: 1589714.875000\n",
            "Train Epoch: 61 [2880/7471 (39%)]\tLoss: 1543605.625000\n",
            "Train Epoch: 61 [3040/7471 (41%)]\tLoss: 1616698.500000\n",
            "Train Epoch: 61 [3200/7471 (43%)]\tLoss: 1617438.250000\n",
            "Train Epoch: 61 [3360/7471 (45%)]\tLoss: 1624937.000000\n",
            "Train Epoch: 61 [3520/7471 (47%)]\tLoss: 1571450.375000\n",
            "Train Epoch: 61 [3680/7471 (49%)]\tLoss: 1543937.375000\n",
            "Train Epoch: 61 [3840/7471 (51%)]\tLoss: 1566256.750000\n",
            "Train Epoch: 61 [4000/7471 (54%)]\tLoss: 1600753.125000\n",
            "Train Epoch: 61 [4160/7471 (56%)]\tLoss: 1578124.250000\n",
            "Train Epoch: 61 [4320/7471 (58%)]\tLoss: 1615225.375000\n",
            "Train Epoch: 61 [4480/7471 (60%)]\tLoss: 1609885.375000\n",
            "Train Epoch: 61 [4640/7471 (62%)]\tLoss: 1611924.250000\n",
            "Train Epoch: 61 [4800/7471 (64%)]\tLoss: 1579668.750000\n",
            "Train Epoch: 61 [4960/7471 (66%)]\tLoss: 1602186.625000\n",
            "Train Epoch: 61 [5120/7471 (69%)]\tLoss: 1589091.125000\n",
            "Train Epoch: 61 [5280/7471 (71%)]\tLoss: 1633823.250000\n",
            "Train Epoch: 61 [5440/7471 (73%)]\tLoss: 1520223.500000\n",
            "Train Epoch: 61 [5600/7471 (75%)]\tLoss: 1604707.625000\n",
            "Train Epoch: 61 [5760/7471 (77%)]\tLoss: 1618964.875000\n",
            "Train Epoch: 61 [5920/7471 (79%)]\tLoss: 1608344.750000\n",
            "Train Epoch: 61 [6080/7471 (81%)]\tLoss: 1564239.125000\n",
            "Train Epoch: 61 [6240/7471 (84%)]\tLoss: 1529174.500000\n",
            "Train Epoch: 61 [6400/7471 (86%)]\tLoss: 1608838.125000\n",
            "Train Epoch: 61 [6560/7471 (88%)]\tLoss: 1539675.875000\n",
            "Train Epoch: 61 [6720/7471 (90%)]\tLoss: 1617205.000000\n",
            "Train Epoch: 61 [6880/7471 (92%)]\tLoss: 1627341.000000\n",
            "Train Epoch: 61 [7040/7471 (94%)]\tLoss: 1601694.750000\n",
            "Train Epoch: 61 [7200/7471 (96%)]\tLoss: 1500294.250000\n",
            "Train Epoch: 61 [7360/7471 (99%)]\tLoss: 1604112.125000\n",
            "Epoch 61 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98534.8911\n",
            "\n",
            "Train Epoch: 62 [160/7471 (2%)]\tLoss: 1594415.000000\n",
            "Train Epoch: 62 [320/7471 (4%)]\tLoss: 1528453.250000\n",
            "Train Epoch: 62 [480/7471 (6%)]\tLoss: 1515119.375000\n",
            "Train Epoch: 62 [640/7471 (9%)]\tLoss: 1650063.375000\n",
            "Train Epoch: 62 [800/7471 (11%)]\tLoss: 1534624.625000\n",
            "Train Epoch: 62 [960/7471 (13%)]\tLoss: 1605384.375000\n",
            "Train Epoch: 62 [1120/7471 (15%)]\tLoss: 1499732.500000\n",
            "Train Epoch: 62 [1280/7471 (17%)]\tLoss: 1590701.750000\n",
            "Train Epoch: 62 [1440/7471 (19%)]\tLoss: 1568343.375000\n",
            "Train Epoch: 62 [1600/7471 (21%)]\tLoss: 1581985.625000\n",
            "Train Epoch: 62 [1760/7471 (24%)]\tLoss: 1511473.375000\n",
            "Train Epoch: 62 [1920/7471 (26%)]\tLoss: 1582584.875000\n",
            "Train Epoch: 62 [2080/7471 (28%)]\tLoss: 1528797.000000\n",
            "Train Epoch: 62 [2240/7471 (30%)]\tLoss: 1580281.625000\n",
            "Train Epoch: 62 [2400/7471 (32%)]\tLoss: 1592196.125000\n",
            "Train Epoch: 62 [2560/7471 (34%)]\tLoss: 1597916.000000\n",
            "Train Epoch: 62 [2720/7471 (36%)]\tLoss: 1589399.875000\n",
            "Train Epoch: 62 [2880/7471 (39%)]\tLoss: 1526424.000000\n",
            "Train Epoch: 62 [3040/7471 (41%)]\tLoss: 1604601.750000\n",
            "Train Epoch: 62 [3200/7471 (43%)]\tLoss: 1534370.375000\n",
            "Train Epoch: 62 [3360/7471 (45%)]\tLoss: 1607981.125000\n",
            "Train Epoch: 62 [3520/7471 (47%)]\tLoss: 1557078.125000\n",
            "Train Epoch: 62 [3680/7471 (49%)]\tLoss: 1608506.375000\n",
            "Train Epoch: 62 [3840/7471 (51%)]\tLoss: 1575430.500000\n",
            "Train Epoch: 62 [4000/7471 (54%)]\tLoss: 1626652.000000\n",
            "Train Epoch: 62 [4160/7471 (56%)]\tLoss: 1615517.000000\n",
            "Train Epoch: 62 [4320/7471 (58%)]\tLoss: 1574646.125000\n",
            "Train Epoch: 62 [4480/7471 (60%)]\tLoss: 1595710.750000\n",
            "Train Epoch: 62 [4640/7471 (62%)]\tLoss: 1573339.000000\n",
            "Train Epoch: 62 [4800/7471 (64%)]\tLoss: 1564299.250000\n",
            "Train Epoch: 62 [4960/7471 (66%)]\tLoss: 1618410.125000\n",
            "Train Epoch: 62 [5120/7471 (69%)]\tLoss: 1632161.625000\n",
            "Train Epoch: 62 [5280/7471 (71%)]\tLoss: 1578105.125000\n",
            "Train Epoch: 62 [5440/7471 (73%)]\tLoss: 1503605.500000\n",
            "Train Epoch: 62 [5600/7471 (75%)]\tLoss: 1521217.250000\n",
            "Train Epoch: 62 [5760/7471 (77%)]\tLoss: 1589960.375000\n",
            "Train Epoch: 62 [5920/7471 (79%)]\tLoss: 1555083.875000\n",
            "Train Epoch: 62 [6080/7471 (81%)]\tLoss: 1575207.375000\n",
            "Train Epoch: 62 [6240/7471 (84%)]\tLoss: 1598837.750000\n",
            "Train Epoch: 62 [6400/7471 (86%)]\tLoss: 1543928.250000\n",
            "Train Epoch: 62 [6560/7471 (88%)]\tLoss: 1602317.500000\n",
            "Train Epoch: 62 [6720/7471 (90%)]\tLoss: 1566478.750000\n",
            "Train Epoch: 62 [6880/7471 (92%)]\tLoss: 1624488.250000\n",
            "Train Epoch: 62 [7040/7471 (94%)]\tLoss: 1552079.625000\n",
            "Train Epoch: 62 [7200/7471 (96%)]\tLoss: 1608069.500000\n",
            "Train Epoch: 62 [7360/7471 (99%)]\tLoss: 1575081.625000\n",
            "Epoch 62 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98521.2109\n",
            "\n",
            "Train Epoch: 63 [160/7471 (2%)]\tLoss: 1586357.500000\n",
            "Train Epoch: 63 [320/7471 (4%)]\tLoss: 1549684.750000\n",
            "Train Epoch: 63 [480/7471 (6%)]\tLoss: 1621389.250000\n",
            "Train Epoch: 63 [640/7471 (9%)]\tLoss: 1578824.875000\n",
            "Train Epoch: 63 [800/7471 (11%)]\tLoss: 1626627.750000\n",
            "Train Epoch: 63 [960/7471 (13%)]\tLoss: 1579660.250000\n",
            "Train Epoch: 63 [1120/7471 (15%)]\tLoss: 1566030.375000\n",
            "Train Epoch: 63 [1280/7471 (17%)]\tLoss: 1625935.000000\n",
            "Train Epoch: 63 [1440/7471 (19%)]\tLoss: 1615292.625000\n",
            "Train Epoch: 63 [1600/7471 (21%)]\tLoss: 1560971.750000\n",
            "Train Epoch: 63 [1760/7471 (24%)]\tLoss: 1522508.875000\n",
            "Train Epoch: 63 [1920/7471 (26%)]\tLoss: 1580523.125000\n",
            "Train Epoch: 63 [2080/7471 (28%)]\tLoss: 1588127.000000\n",
            "Train Epoch: 63 [2240/7471 (30%)]\tLoss: 1584694.125000\n",
            "Train Epoch: 63 [2400/7471 (32%)]\tLoss: 1582879.750000\n",
            "Train Epoch: 63 [2560/7471 (34%)]\tLoss: 1584647.750000\n",
            "Train Epoch: 63 [2720/7471 (36%)]\tLoss: 1559974.375000\n",
            "Train Epoch: 63 [2880/7471 (39%)]\tLoss: 1570271.000000\n",
            "Train Epoch: 63 [3040/7471 (41%)]\tLoss: 1495432.500000\n",
            "Train Epoch: 63 [3200/7471 (43%)]\tLoss: 1569905.375000\n",
            "Train Epoch: 63 [3360/7471 (45%)]\tLoss: 1632400.125000\n",
            "Train Epoch: 63 [3520/7471 (47%)]\tLoss: 1627480.625000\n",
            "Train Epoch: 63 [3680/7471 (49%)]\tLoss: 1569944.125000\n",
            "Train Epoch: 63 [3840/7471 (51%)]\tLoss: 1570277.500000\n",
            "Train Epoch: 63 [4000/7471 (54%)]\tLoss: 1556156.125000\n",
            "Train Epoch: 63 [4160/7471 (56%)]\tLoss: 1552488.875000\n",
            "Train Epoch: 63 [4320/7471 (58%)]\tLoss: 1532401.750000\n",
            "Train Epoch: 63 [4480/7471 (60%)]\tLoss: 1592886.875000\n",
            "Train Epoch: 63 [4640/7471 (62%)]\tLoss: 1602952.625000\n",
            "Train Epoch: 63 [4800/7471 (64%)]\tLoss: 1570187.875000\n",
            "Train Epoch: 63 [4960/7471 (66%)]\tLoss: 1632122.125000\n",
            "Train Epoch: 63 [5120/7471 (69%)]\tLoss: 1604307.375000\n",
            "Train Epoch: 63 [5280/7471 (71%)]\tLoss: 1615001.375000\n",
            "Train Epoch: 63 [5440/7471 (73%)]\tLoss: 1597046.250000\n",
            "Train Epoch: 63 [5600/7471 (75%)]\tLoss: 1602162.125000\n",
            "Train Epoch: 63 [5760/7471 (77%)]\tLoss: 1595481.875000\n",
            "Train Epoch: 63 [5920/7471 (79%)]\tLoss: 1637029.375000\n",
            "Train Epoch: 63 [6080/7471 (81%)]\tLoss: 1582139.250000\n",
            "Train Epoch: 63 [6240/7471 (84%)]\tLoss: 1621515.500000\n",
            "Train Epoch: 63 [6400/7471 (86%)]\tLoss: 1538853.625000\n",
            "Train Epoch: 63 [6560/7471 (88%)]\tLoss: 1579242.000000\n",
            "Train Epoch: 63 [6720/7471 (90%)]\tLoss: 1530285.000000\n",
            "Train Epoch: 63 [6880/7471 (92%)]\tLoss: 1567759.375000\n",
            "Train Epoch: 63 [7040/7471 (94%)]\tLoss: 1552351.250000\n",
            "Train Epoch: 63 [7200/7471 (96%)]\tLoss: 1594416.250000\n",
            "Train Epoch: 63 [7360/7471 (99%)]\tLoss: 1595598.250000\n",
            "Epoch 63 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98574.8725\n",
            "\n",
            "Train Epoch: 64 [160/7471 (2%)]\tLoss: 1613497.375000\n",
            "Train Epoch: 64 [320/7471 (4%)]\tLoss: 1583599.375000\n",
            "Train Epoch: 64 [480/7471 (6%)]\tLoss: 1588875.125000\n",
            "Train Epoch: 64 [640/7471 (9%)]\tLoss: 1568137.625000\n",
            "Train Epoch: 64 [800/7471 (11%)]\tLoss: 1516114.500000\n",
            "Train Epoch: 64 [960/7471 (13%)]\tLoss: 1567470.375000\n",
            "Train Epoch: 64 [1120/7471 (15%)]\tLoss: 1598300.375000\n",
            "Train Epoch: 64 [1280/7471 (17%)]\tLoss: 1617963.250000\n",
            "Train Epoch: 64 [1440/7471 (19%)]\tLoss: 1625808.250000\n",
            "Train Epoch: 64 [1600/7471 (21%)]\tLoss: 1591862.500000\n",
            "Train Epoch: 64 [1760/7471 (24%)]\tLoss: 1557563.750000\n",
            "Train Epoch: 64 [1920/7471 (26%)]\tLoss: 1596936.125000\n",
            "Train Epoch: 64 [2080/7471 (28%)]\tLoss: 1583402.500000\n",
            "Train Epoch: 64 [2240/7471 (30%)]\tLoss: 1604676.375000\n",
            "Train Epoch: 64 [2400/7471 (32%)]\tLoss: 1566019.000000\n",
            "Train Epoch: 64 [2560/7471 (34%)]\tLoss: 1579500.250000\n",
            "Train Epoch: 64 [2720/7471 (36%)]\tLoss: 1587231.125000\n",
            "Train Epoch: 64 [2880/7471 (39%)]\tLoss: 1588505.250000\n",
            "Train Epoch: 64 [3040/7471 (41%)]\tLoss: 1560711.000000\n",
            "Train Epoch: 64 [3200/7471 (43%)]\tLoss: 1557356.250000\n",
            "Train Epoch: 64 [3360/7471 (45%)]\tLoss: 1620185.250000\n",
            "Train Epoch: 64 [3520/7471 (47%)]\tLoss: 1608733.125000\n",
            "Train Epoch: 64 [3680/7471 (49%)]\tLoss: 1493484.375000\n",
            "Train Epoch: 64 [3840/7471 (51%)]\tLoss: 1560059.250000\n",
            "Train Epoch: 64 [4000/7471 (54%)]\tLoss: 1564269.125000\n",
            "Train Epoch: 64 [4160/7471 (56%)]\tLoss: 1544529.750000\n",
            "Train Epoch: 64 [4320/7471 (58%)]\tLoss: 1615078.375000\n",
            "Train Epoch: 64 [4480/7471 (60%)]\tLoss: 1581826.250000\n",
            "Train Epoch: 64 [4640/7471 (62%)]\tLoss: 1585245.375000\n",
            "Train Epoch: 64 [4800/7471 (64%)]\tLoss: 1587598.000000\n",
            "Train Epoch: 64 [4960/7471 (66%)]\tLoss: 1608852.875000\n",
            "Train Epoch: 64 [5120/7471 (69%)]\tLoss: 1577378.375000\n",
            "Train Epoch: 64 [5280/7471 (71%)]\tLoss: 1622759.875000\n",
            "Train Epoch: 64 [5440/7471 (73%)]\tLoss: 1561058.000000\n",
            "Train Epoch: 64 [5600/7471 (75%)]\tLoss: 1518686.000000\n",
            "Train Epoch: 64 [5760/7471 (77%)]\tLoss: 1539594.000000\n",
            "Train Epoch: 64 [5920/7471 (79%)]\tLoss: 1622682.500000\n",
            "Train Epoch: 64 [6080/7471 (81%)]\tLoss: 1624458.875000\n",
            "Train Epoch: 64 [6240/7471 (84%)]\tLoss: 1512251.500000\n",
            "Train Epoch: 64 [6400/7471 (86%)]\tLoss: 1570385.375000\n",
            "Train Epoch: 64 [6560/7471 (88%)]\tLoss: 1571234.625000\n",
            "Train Epoch: 64 [6720/7471 (90%)]\tLoss: 1616430.250000\n",
            "Train Epoch: 64 [6880/7471 (92%)]\tLoss: 1610618.500000\n",
            "Train Epoch: 64 [7040/7471 (94%)]\tLoss: 1583030.250000\n",
            "Train Epoch: 64 [7200/7471 (96%)]\tLoss: 1541065.250000\n",
            "Train Epoch: 64 [7360/7471 (99%)]\tLoss: 1590209.125000\n",
            "Epoch 64 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98532.6321\n",
            "\n",
            "Train Epoch: 65 [160/7471 (2%)]\tLoss: 1520193.750000\n",
            "Train Epoch: 65 [320/7471 (4%)]\tLoss: 1617007.250000\n",
            "Train Epoch: 65 [480/7471 (6%)]\tLoss: 1590205.000000\n",
            "Train Epoch: 65 [640/7471 (9%)]\tLoss: 1591745.750000\n",
            "Train Epoch: 65 [800/7471 (11%)]\tLoss: 1582867.625000\n",
            "Train Epoch: 65 [960/7471 (13%)]\tLoss: 1606095.625000\n",
            "Train Epoch: 65 [1120/7471 (15%)]\tLoss: 1551547.875000\n",
            "Train Epoch: 65 [1280/7471 (17%)]\tLoss: 1562165.375000\n",
            "Train Epoch: 65 [1440/7471 (19%)]\tLoss: 1615086.000000\n",
            "Train Epoch: 65 [1600/7471 (21%)]\tLoss: 1597669.625000\n",
            "Train Epoch: 65 [1760/7471 (24%)]\tLoss: 1575215.000000\n",
            "Train Epoch: 65 [1920/7471 (26%)]\tLoss: 1624208.125000\n",
            "Train Epoch: 65 [2080/7471 (28%)]\tLoss: 1509318.875000\n",
            "Train Epoch: 65 [2240/7471 (30%)]\tLoss: 1506027.375000\n",
            "Train Epoch: 65 [2400/7471 (32%)]\tLoss: 1618193.625000\n",
            "Train Epoch: 65 [2560/7471 (34%)]\tLoss: 1611752.375000\n",
            "Train Epoch: 65 [2720/7471 (36%)]\tLoss: 1598119.875000\n",
            "Train Epoch: 65 [2880/7471 (39%)]\tLoss: 1493191.250000\n",
            "Train Epoch: 65 [3040/7471 (41%)]\tLoss: 1547006.125000\n",
            "Train Epoch: 65 [3200/7471 (43%)]\tLoss: 1574465.750000\n",
            "Train Epoch: 65 [3360/7471 (45%)]\tLoss: 1626991.500000\n",
            "Train Epoch: 65 [3520/7471 (47%)]\tLoss: 1606502.000000\n",
            "Train Epoch: 65 [3680/7471 (49%)]\tLoss: 1595504.375000\n",
            "Train Epoch: 65 [3840/7471 (51%)]\tLoss: 1603261.750000\n",
            "Train Epoch: 65 [4000/7471 (54%)]\tLoss: 1510692.000000\n",
            "Train Epoch: 65 [4160/7471 (56%)]\tLoss: 1583486.875000\n",
            "Train Epoch: 65 [4320/7471 (58%)]\tLoss: 1618794.250000\n",
            "Train Epoch: 65 [4480/7471 (60%)]\tLoss: 1538513.625000\n",
            "Train Epoch: 65 [4640/7471 (62%)]\tLoss: 1539436.000000\n",
            "Train Epoch: 65 [4800/7471 (64%)]\tLoss: 1564907.625000\n",
            "Train Epoch: 65 [4960/7471 (66%)]\tLoss: 1592330.875000\n",
            "Train Epoch: 65 [5120/7471 (69%)]\tLoss: 1567379.750000\n",
            "Train Epoch: 65 [5280/7471 (71%)]\tLoss: 1471354.000000\n",
            "Train Epoch: 65 [5440/7471 (73%)]\tLoss: 1542351.375000\n",
            "Train Epoch: 65 [5600/7471 (75%)]\tLoss: 1552682.500000\n",
            "Train Epoch: 65 [5760/7471 (77%)]\tLoss: 1610305.000000\n",
            "Train Epoch: 65 [5920/7471 (79%)]\tLoss: 1600940.250000\n",
            "Train Epoch: 65 [6080/7471 (81%)]\tLoss: 1604177.875000\n",
            "Train Epoch: 65 [6240/7471 (84%)]\tLoss: 1647075.500000\n",
            "Train Epoch: 65 [6400/7471 (86%)]\tLoss: 1634561.125000\n",
            "Train Epoch: 65 [6560/7471 (88%)]\tLoss: 1538909.125000\n",
            "Train Epoch: 65 [6720/7471 (90%)]\tLoss: 1601800.875000\n",
            "Train Epoch: 65 [6880/7471 (92%)]\tLoss: 1600202.625000\n",
            "Train Epoch: 65 [7040/7471 (94%)]\tLoss: 1610417.250000\n",
            "Train Epoch: 65 [7200/7471 (96%)]\tLoss: 1537169.500000\n",
            "Train Epoch: 65 [7360/7471 (99%)]\tLoss: 1629184.875000\n",
            "Epoch 65 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98532.1190\n",
            "\n",
            "Train Epoch: 66 [160/7471 (2%)]\tLoss: 1547710.000000\n",
            "Train Epoch: 66 [320/7471 (4%)]\tLoss: 1589372.375000\n",
            "Train Epoch: 66 [480/7471 (6%)]\tLoss: 1573938.875000\n",
            "Train Epoch: 66 [640/7471 (9%)]\tLoss: 1572166.500000\n",
            "Train Epoch: 66 [800/7471 (11%)]\tLoss: 1566636.000000\n",
            "Train Epoch: 66 [960/7471 (13%)]\tLoss: 1555889.000000\n",
            "Train Epoch: 66 [1120/7471 (15%)]\tLoss: 1566964.750000\n",
            "Train Epoch: 66 [1280/7471 (17%)]\tLoss: 1641898.875000\n",
            "Train Epoch: 66 [1440/7471 (19%)]\tLoss: 1576158.625000\n",
            "Train Epoch: 66 [1600/7471 (21%)]\tLoss: 1589236.500000\n",
            "Train Epoch: 66 [1760/7471 (24%)]\tLoss: 1557789.500000\n",
            "Train Epoch: 66 [1920/7471 (26%)]\tLoss: 1613963.250000\n",
            "Train Epoch: 66 [2080/7471 (28%)]\tLoss: 1582955.875000\n",
            "Train Epoch: 66 [2240/7471 (30%)]\tLoss: 1595179.750000\n",
            "Train Epoch: 66 [2400/7471 (32%)]\tLoss: 1586198.875000\n",
            "Train Epoch: 66 [2560/7471 (34%)]\tLoss: 1526503.375000\n",
            "Train Epoch: 66 [2720/7471 (36%)]\tLoss: 1583947.500000\n",
            "Train Epoch: 66 [2880/7471 (39%)]\tLoss: 1585599.500000\n",
            "Train Epoch: 66 [3040/7471 (41%)]\tLoss: 1570502.250000\n",
            "Train Epoch: 66 [3200/7471 (43%)]\tLoss: 1579343.500000\n",
            "Train Epoch: 66 [3360/7471 (45%)]\tLoss: 1622542.500000\n",
            "Train Epoch: 66 [3520/7471 (47%)]\tLoss: 1548073.125000\n",
            "Train Epoch: 66 [3680/7471 (49%)]\tLoss: 1638262.625000\n",
            "Train Epoch: 66 [3840/7471 (51%)]\tLoss: 1599753.000000\n",
            "Train Epoch: 66 [4000/7471 (54%)]\tLoss: 1558612.625000\n",
            "Train Epoch: 66 [4160/7471 (56%)]\tLoss: 1583866.500000\n",
            "Train Epoch: 66 [4320/7471 (58%)]\tLoss: 1530977.625000\n",
            "Train Epoch: 66 [4480/7471 (60%)]\tLoss: 1539600.500000\n",
            "Train Epoch: 66 [4640/7471 (62%)]\tLoss: 1532538.250000\n",
            "Train Epoch: 66 [4800/7471 (64%)]\tLoss: 1537391.875000\n",
            "Train Epoch: 66 [4960/7471 (66%)]\tLoss: 1592712.375000\n",
            "Train Epoch: 66 [5120/7471 (69%)]\tLoss: 1630930.375000\n",
            "Train Epoch: 66 [5280/7471 (71%)]\tLoss: 1622720.625000\n",
            "Train Epoch: 66 [5440/7471 (73%)]\tLoss: 1554485.375000\n",
            "Train Epoch: 66 [5600/7471 (75%)]\tLoss: 1546092.375000\n",
            "Train Epoch: 66 [5760/7471 (77%)]\tLoss: 1545988.875000\n",
            "Train Epoch: 66 [5920/7471 (79%)]\tLoss: 1556826.250000\n",
            "Train Epoch: 66 [6080/7471 (81%)]\tLoss: 1606325.500000\n",
            "Train Epoch: 66 [6240/7471 (84%)]\tLoss: 1645263.625000\n",
            "Train Epoch: 66 [6400/7471 (86%)]\tLoss: 1509169.750000\n",
            "Train Epoch: 66 [6560/7471 (88%)]\tLoss: 1605368.875000\n",
            "Train Epoch: 66 [6720/7471 (90%)]\tLoss: 1603617.250000\n",
            "Train Epoch: 66 [6880/7471 (92%)]\tLoss: 1623859.500000\n",
            "Train Epoch: 66 [7040/7471 (94%)]\tLoss: 1585406.125000\n",
            "Train Epoch: 66 [7200/7471 (96%)]\tLoss: 1522449.375000\n",
            "Train Epoch: 66 [7360/7471 (99%)]\tLoss: 1612648.875000\n",
            "Epoch 66 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98513.7038\n",
            "\n",
            "Train Epoch: 67 [160/7471 (2%)]\tLoss: 1544207.125000\n",
            "Train Epoch: 67 [320/7471 (4%)]\tLoss: 1627045.125000\n",
            "Train Epoch: 67 [480/7471 (6%)]\tLoss: 1547202.000000\n",
            "Train Epoch: 67 [640/7471 (9%)]\tLoss: 1580680.500000\n",
            "Train Epoch: 67 [800/7471 (11%)]\tLoss: 1613385.625000\n",
            "Train Epoch: 67 [960/7471 (13%)]\tLoss: 1495078.250000\n",
            "Train Epoch: 67 [1120/7471 (15%)]\tLoss: 1556826.125000\n",
            "Train Epoch: 67 [1280/7471 (17%)]\tLoss: 1612847.875000\n",
            "Train Epoch: 67 [1440/7471 (19%)]\tLoss: 1533439.750000\n",
            "Train Epoch: 67 [1600/7471 (21%)]\tLoss: 1653761.750000\n",
            "Train Epoch: 67 [1760/7471 (24%)]\tLoss: 1550309.625000\n",
            "Train Epoch: 67 [1920/7471 (26%)]\tLoss: 1570523.625000\n",
            "Train Epoch: 67 [2080/7471 (28%)]\tLoss: 1566290.750000\n",
            "Train Epoch: 67 [2240/7471 (30%)]\tLoss: 1574813.500000\n",
            "Train Epoch: 67 [2400/7471 (32%)]\tLoss: 1509841.875000\n",
            "Train Epoch: 67 [2560/7471 (34%)]\tLoss: 1568609.875000\n",
            "Train Epoch: 67 [2720/7471 (36%)]\tLoss: 1586413.000000\n",
            "Train Epoch: 67 [2880/7471 (39%)]\tLoss: 1590198.750000\n",
            "Train Epoch: 67 [3040/7471 (41%)]\tLoss: 1532111.250000\n",
            "Train Epoch: 67 [3200/7471 (43%)]\tLoss: 1574761.750000\n",
            "Train Epoch: 67 [3360/7471 (45%)]\tLoss: 1622578.125000\n",
            "Train Epoch: 67 [3520/7471 (47%)]\tLoss: 1596637.500000\n",
            "Train Epoch: 67 [3680/7471 (49%)]\tLoss: 1617943.500000\n",
            "Train Epoch: 67 [3840/7471 (51%)]\tLoss: 1615956.125000\n",
            "Train Epoch: 67 [4000/7471 (54%)]\tLoss: 1567422.375000\n",
            "Train Epoch: 67 [4160/7471 (56%)]\tLoss: 1607013.500000\n",
            "Train Epoch: 67 [4320/7471 (58%)]\tLoss: 1542187.375000\n",
            "Train Epoch: 67 [4480/7471 (60%)]\tLoss: 1620050.750000\n",
            "Train Epoch: 67 [4640/7471 (62%)]\tLoss: 1498992.500000\n",
            "Train Epoch: 67 [4800/7471 (64%)]\tLoss: 1551521.500000\n",
            "Train Epoch: 67 [4960/7471 (66%)]\tLoss: 1506346.875000\n",
            "Train Epoch: 67 [5120/7471 (69%)]\tLoss: 1605809.375000\n",
            "Train Epoch: 67 [5280/7471 (71%)]\tLoss: 1624512.375000\n",
            "Train Epoch: 67 [5440/7471 (73%)]\tLoss: 1529372.250000\n",
            "Train Epoch: 67 [5600/7471 (75%)]\tLoss: 1603167.125000\n",
            "Train Epoch: 67 [5760/7471 (77%)]\tLoss: 1617831.000000\n",
            "Train Epoch: 67 [5920/7471 (79%)]\tLoss: 1573380.750000\n",
            "Train Epoch: 67 [6080/7471 (81%)]\tLoss: 1557391.500000\n",
            "Train Epoch: 67 [6240/7471 (84%)]\tLoss: 1571176.250000\n",
            "Train Epoch: 67 [6400/7471 (86%)]\tLoss: 1538272.625000\n",
            "Train Epoch: 67 [6560/7471 (88%)]\tLoss: 1579974.000000\n",
            "Train Epoch: 67 [6720/7471 (90%)]\tLoss: 1572672.625000\n",
            "Train Epoch: 67 [6880/7471 (92%)]\tLoss: 1576264.250000\n",
            "Train Epoch: 67 [7040/7471 (94%)]\tLoss: 1513821.875000\n",
            "Train Epoch: 67 [7200/7471 (96%)]\tLoss: 1592886.000000\n",
            "Train Epoch: 67 [7360/7471 (99%)]\tLoss: 1619928.000000\n",
            "Epoch 67 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98585.6745\n",
            "\n",
            "Train Epoch: 68 [160/7471 (2%)]\tLoss: 1517024.250000\n",
            "Train Epoch: 68 [320/7471 (4%)]\tLoss: 1586429.250000\n",
            "Train Epoch: 68 [480/7471 (6%)]\tLoss: 1561531.625000\n",
            "Train Epoch: 68 [640/7471 (9%)]\tLoss: 1596098.375000\n",
            "Train Epoch: 68 [800/7471 (11%)]\tLoss: 1540614.000000\n",
            "Train Epoch: 68 [960/7471 (13%)]\tLoss: 1556590.125000\n",
            "Train Epoch: 68 [1120/7471 (15%)]\tLoss: 1526057.250000\n",
            "Train Epoch: 68 [1280/7471 (17%)]\tLoss: 1603173.750000\n",
            "Train Epoch: 68 [1440/7471 (19%)]\tLoss: 1600586.750000\n",
            "Train Epoch: 68 [1600/7471 (21%)]\tLoss: 1597122.750000\n",
            "Train Epoch: 68 [1760/7471 (24%)]\tLoss: 1495667.750000\n",
            "Train Epoch: 68 [1920/7471 (26%)]\tLoss: 1593118.000000\n",
            "Train Epoch: 68 [2080/7471 (28%)]\tLoss: 1517703.875000\n",
            "Train Epoch: 68 [2240/7471 (30%)]\tLoss: 1554085.000000\n",
            "Train Epoch: 68 [2400/7471 (32%)]\tLoss: 1583210.625000\n",
            "Train Epoch: 68 [2560/7471 (34%)]\tLoss: 1542621.250000\n",
            "Train Epoch: 68 [2720/7471 (36%)]\tLoss: 1566199.750000\n",
            "Train Epoch: 68 [2880/7471 (39%)]\tLoss: 1521425.125000\n",
            "Train Epoch: 68 [3040/7471 (41%)]\tLoss: 1610111.250000\n",
            "Train Epoch: 68 [3200/7471 (43%)]\tLoss: 1559262.750000\n",
            "Train Epoch: 68 [3360/7471 (45%)]\tLoss: 1611253.375000\n",
            "Train Epoch: 68 [3520/7471 (47%)]\tLoss: 1555148.375000\n",
            "Train Epoch: 68 [3680/7471 (49%)]\tLoss: 1615423.250000\n",
            "Train Epoch: 68 [3840/7471 (51%)]\tLoss: 1576643.000000\n",
            "Train Epoch: 68 [4000/7471 (54%)]\tLoss: 1536250.375000\n",
            "Train Epoch: 68 [4160/7471 (56%)]\tLoss: 1594509.625000\n",
            "Train Epoch: 68 [4320/7471 (58%)]\tLoss: 1538457.125000\n",
            "Train Epoch: 68 [4480/7471 (60%)]\tLoss: 1558231.625000\n",
            "Train Epoch: 68 [4640/7471 (62%)]\tLoss: 1621311.125000\n",
            "Train Epoch: 68 [4800/7471 (64%)]\tLoss: 1546304.750000\n",
            "Train Epoch: 68 [4960/7471 (66%)]\tLoss: 1634198.500000\n",
            "Train Epoch: 68 [5120/7471 (69%)]\tLoss: 1594655.375000\n",
            "Train Epoch: 68 [5280/7471 (71%)]\tLoss: 1577973.750000\n",
            "Train Epoch: 68 [5440/7471 (73%)]\tLoss: 1610684.875000\n",
            "Train Epoch: 68 [5600/7471 (75%)]\tLoss: 1617628.875000\n",
            "Train Epoch: 68 [5760/7471 (77%)]\tLoss: 1550393.250000\n",
            "Train Epoch: 68 [5920/7471 (79%)]\tLoss: 1540146.125000\n",
            "Train Epoch: 68 [6080/7471 (81%)]\tLoss: 1597158.500000\n",
            "Train Epoch: 68 [6240/7471 (84%)]\tLoss: 1599516.875000\n",
            "Train Epoch: 68 [6400/7471 (86%)]\tLoss: 1585939.875000\n",
            "Train Epoch: 68 [6560/7471 (88%)]\tLoss: 1562108.625000\n",
            "Train Epoch: 68 [6720/7471 (90%)]\tLoss: 1588041.250000\n",
            "Train Epoch: 68 [6880/7471 (92%)]\tLoss: 1599906.875000\n",
            "Train Epoch: 68 [7040/7471 (94%)]\tLoss: 1626754.750000\n",
            "Train Epoch: 68 [7200/7471 (96%)]\tLoss: 1607671.500000\n",
            "Train Epoch: 68 [7360/7471 (99%)]\tLoss: 1523298.000000\n",
            "Epoch 68 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98593.5321\n",
            "\n",
            "Train Epoch: 69 [160/7471 (2%)]\tLoss: 1568612.875000\n",
            "Train Epoch: 69 [320/7471 (4%)]\tLoss: 1560236.125000\n",
            "Train Epoch: 69 [480/7471 (6%)]\tLoss: 1607405.375000\n",
            "Train Epoch: 69 [640/7471 (9%)]\tLoss: 1594780.500000\n",
            "Train Epoch: 69 [800/7471 (11%)]\tLoss: 1575582.375000\n",
            "Train Epoch: 69 [960/7471 (13%)]\tLoss: 1572638.375000\n",
            "Train Epoch: 69 [1120/7471 (15%)]\tLoss: 1564486.375000\n",
            "Train Epoch: 69 [1280/7471 (17%)]\tLoss: 1607511.500000\n",
            "Train Epoch: 69 [1440/7471 (19%)]\tLoss: 1605836.375000\n",
            "Train Epoch: 69 [1600/7471 (21%)]\tLoss: 1556159.375000\n",
            "Train Epoch: 69 [1760/7471 (24%)]\tLoss: 1509847.750000\n",
            "Train Epoch: 69 [1920/7471 (26%)]\tLoss: 1500546.625000\n",
            "Train Epoch: 69 [2080/7471 (28%)]\tLoss: 1560809.125000\n",
            "Train Epoch: 69 [2240/7471 (30%)]\tLoss: 1556483.000000\n",
            "Train Epoch: 69 [2400/7471 (32%)]\tLoss: 1591322.625000\n",
            "Train Epoch: 69 [2560/7471 (34%)]\tLoss: 1585673.250000\n",
            "Train Epoch: 69 [2720/7471 (36%)]\tLoss: 1531209.000000\n",
            "Train Epoch: 69 [2880/7471 (39%)]\tLoss: 1619352.250000\n",
            "Train Epoch: 69 [3040/7471 (41%)]\tLoss: 1598102.125000\n",
            "Train Epoch: 69 [3200/7471 (43%)]\tLoss: 1627748.625000\n",
            "Train Epoch: 69 [3360/7471 (45%)]\tLoss: 1613596.500000\n",
            "Train Epoch: 69 [3520/7471 (47%)]\tLoss: 1603738.750000\n",
            "Train Epoch: 69 [3680/7471 (49%)]\tLoss: 1629453.750000\n",
            "Train Epoch: 69 [3840/7471 (51%)]\tLoss: 1628557.125000\n",
            "Train Epoch: 69 [4000/7471 (54%)]\tLoss: 1574972.625000\n",
            "Train Epoch: 69 [4160/7471 (56%)]\tLoss: 1544134.000000\n",
            "Train Epoch: 69 [4320/7471 (58%)]\tLoss: 1594677.125000\n",
            "Train Epoch: 69 [4480/7471 (60%)]\tLoss: 1625565.000000\n",
            "Train Epoch: 69 [4640/7471 (62%)]\tLoss: 1534249.250000\n",
            "Train Epoch: 69 [4800/7471 (64%)]\tLoss: 1484202.500000\n",
            "Train Epoch: 69 [4960/7471 (66%)]\tLoss: 1612048.250000\n",
            "Train Epoch: 69 [5120/7471 (69%)]\tLoss: 1629820.500000\n",
            "Train Epoch: 69 [5280/7471 (71%)]\tLoss: 1560301.625000\n",
            "Train Epoch: 69 [5440/7471 (73%)]\tLoss: 1537326.625000\n",
            "Train Epoch: 69 [5600/7471 (75%)]\tLoss: 1545804.125000\n",
            "Train Epoch: 69 [5760/7471 (77%)]\tLoss: 1595558.625000\n",
            "Train Epoch: 69 [5920/7471 (79%)]\tLoss: 1569201.500000\n",
            "Train Epoch: 69 [6080/7471 (81%)]\tLoss: 1535587.375000\n",
            "Train Epoch: 69 [6240/7471 (84%)]\tLoss: 1607934.250000\n",
            "Train Epoch: 69 [6400/7471 (86%)]\tLoss: 1545861.500000\n",
            "Train Epoch: 69 [6560/7471 (88%)]\tLoss: 1600782.125000\n",
            "Train Epoch: 69 [6720/7471 (90%)]\tLoss: 1589371.875000\n",
            "Train Epoch: 69 [6880/7471 (92%)]\tLoss: 1562103.875000\n",
            "Train Epoch: 69 [7040/7471 (94%)]\tLoss: 1607187.125000\n",
            "Train Epoch: 69 [7200/7471 (96%)]\tLoss: 1544706.500000\n",
            "Train Epoch: 69 [7360/7471 (99%)]\tLoss: 1573849.875000\n",
            "Epoch 69 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98553.6405\n",
            "\n",
            "Train Epoch: 70 [160/7471 (2%)]\tLoss: 1587213.125000\n",
            "Train Epoch: 70 [320/7471 (4%)]\tLoss: 1571982.375000\n",
            "Train Epoch: 70 [480/7471 (6%)]\tLoss: 1592889.125000\n",
            "Train Epoch: 70 [640/7471 (9%)]\tLoss: 1629779.250000\n",
            "Train Epoch: 70 [800/7471 (11%)]\tLoss: 1611395.125000\n",
            "Train Epoch: 70 [960/7471 (13%)]\tLoss: 1577501.625000\n",
            "Train Epoch: 70 [1120/7471 (15%)]\tLoss: 1546368.500000\n",
            "Train Epoch: 70 [1280/7471 (17%)]\tLoss: 1612689.500000\n",
            "Train Epoch: 70 [1440/7471 (19%)]\tLoss: 1544777.000000\n",
            "Train Epoch: 70 [1600/7471 (21%)]\tLoss: 1547279.250000\n",
            "Train Epoch: 70 [1760/7471 (24%)]\tLoss: 1623437.625000\n",
            "Train Epoch: 70 [1920/7471 (26%)]\tLoss: 1536172.125000\n",
            "Train Epoch: 70 [2080/7471 (28%)]\tLoss: 1586823.500000\n",
            "Train Epoch: 70 [2240/7471 (30%)]\tLoss: 1542635.875000\n",
            "Train Epoch: 70 [2400/7471 (32%)]\tLoss: 1610006.125000\n",
            "Train Epoch: 70 [2560/7471 (34%)]\tLoss: 1585775.625000\n",
            "Train Epoch: 70 [2720/7471 (36%)]\tLoss: 1593253.875000\n",
            "Train Epoch: 70 [2880/7471 (39%)]\tLoss: 1612371.125000\n",
            "Train Epoch: 70 [3040/7471 (41%)]\tLoss: 1548234.750000\n",
            "Train Epoch: 70 [3200/7471 (43%)]\tLoss: 1547650.375000\n",
            "Train Epoch: 70 [3360/7471 (45%)]\tLoss: 1644845.750000\n",
            "Train Epoch: 70 [3520/7471 (47%)]\tLoss: 1572392.750000\n",
            "Train Epoch: 70 [3680/7471 (49%)]\tLoss: 1606200.250000\n",
            "Train Epoch: 70 [3840/7471 (51%)]\tLoss: 1560342.500000\n",
            "Train Epoch: 70 [4000/7471 (54%)]\tLoss: 1617912.500000\n",
            "Train Epoch: 70 [4160/7471 (56%)]\tLoss: 1575742.000000\n",
            "Train Epoch: 70 [4320/7471 (58%)]\tLoss: 1586802.250000\n",
            "Train Epoch: 70 [4480/7471 (60%)]\tLoss: 1579385.250000\n",
            "Train Epoch: 70 [4640/7471 (62%)]\tLoss: 1492386.000000\n",
            "Train Epoch: 70 [4800/7471 (64%)]\tLoss: 1562373.000000\n",
            "Train Epoch: 70 [4960/7471 (66%)]\tLoss: 1536081.125000\n",
            "Train Epoch: 70 [5120/7471 (69%)]\tLoss: 1599380.250000\n",
            "Train Epoch: 70 [5280/7471 (71%)]\tLoss: 1612036.625000\n",
            "Train Epoch: 70 [5440/7471 (73%)]\tLoss: 1577070.000000\n",
            "Train Epoch: 70 [5600/7471 (75%)]\tLoss: 1546230.625000\n",
            "Train Epoch: 70 [5760/7471 (77%)]\tLoss: 1557090.750000\n",
            "Train Epoch: 70 [5920/7471 (79%)]\tLoss: 1530592.250000\n",
            "Train Epoch: 70 [6080/7471 (81%)]\tLoss: 1582414.125000\n",
            "Train Epoch: 70 [6240/7471 (84%)]\tLoss: 1577390.125000\n",
            "Train Epoch: 70 [6400/7471 (86%)]\tLoss: 1617354.250000\n",
            "Train Epoch: 70 [6560/7471 (88%)]\tLoss: 1579825.500000\n",
            "Train Epoch: 70 [6720/7471 (90%)]\tLoss: 1607866.625000\n",
            "Train Epoch: 70 [6880/7471 (92%)]\tLoss: 1554806.250000\n",
            "Train Epoch: 70 [7040/7471 (94%)]\tLoss: 1592966.375000\n",
            "Train Epoch: 70 [7200/7471 (96%)]\tLoss: 1611084.250000\n",
            "Train Epoch: 70 [7360/7471 (99%)]\tLoss: 1556811.125000\n",
            "Epoch 70 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98488.4595\n",
            "\n",
            "Train Epoch: 71 [160/7471 (2%)]\tLoss: 1592646.125000\n",
            "Train Epoch: 71 [320/7471 (4%)]\tLoss: 1588022.375000\n",
            "Train Epoch: 71 [480/7471 (6%)]\tLoss: 1608342.375000\n",
            "Train Epoch: 71 [640/7471 (9%)]\tLoss: 1550004.500000\n",
            "Train Epoch: 71 [800/7471 (11%)]\tLoss: 1583011.000000\n",
            "Train Epoch: 71 [960/7471 (13%)]\tLoss: 1614771.750000\n",
            "Train Epoch: 71 [1120/7471 (15%)]\tLoss: 1624190.875000\n",
            "Train Epoch: 71 [1280/7471 (17%)]\tLoss: 1596235.250000\n",
            "Train Epoch: 71 [1440/7471 (19%)]\tLoss: 1590329.375000\n",
            "Train Epoch: 71 [1600/7471 (21%)]\tLoss: 1581016.500000\n",
            "Train Epoch: 71 [1760/7471 (24%)]\tLoss: 1589321.375000\n",
            "Train Epoch: 71 [1920/7471 (26%)]\tLoss: 1625150.875000\n",
            "Train Epoch: 71 [2080/7471 (28%)]\tLoss: 1587509.625000\n",
            "Train Epoch: 71 [2240/7471 (30%)]\tLoss: 1545725.000000\n",
            "Train Epoch: 71 [2400/7471 (32%)]\tLoss: 1588646.750000\n",
            "Train Epoch: 71 [2560/7471 (34%)]\tLoss: 1556990.250000\n",
            "Train Epoch: 71 [2720/7471 (36%)]\tLoss: 1624873.375000\n",
            "Train Epoch: 71 [2880/7471 (39%)]\tLoss: 1579992.500000\n",
            "Train Epoch: 71 [3040/7471 (41%)]\tLoss: 1559854.625000\n",
            "Train Epoch: 71 [3200/7471 (43%)]\tLoss: 1609634.875000\n",
            "Train Epoch: 71 [3360/7471 (45%)]\tLoss: 1498806.875000\n",
            "Train Epoch: 71 [3520/7471 (47%)]\tLoss: 1641013.125000\n",
            "Train Epoch: 71 [3680/7471 (49%)]\tLoss: 1494185.875000\n",
            "Train Epoch: 71 [3840/7471 (51%)]\tLoss: 1568229.375000\n",
            "Train Epoch: 71 [4000/7471 (54%)]\tLoss: 1526252.375000\n",
            "Train Epoch: 71 [4160/7471 (56%)]\tLoss: 1569650.000000\n",
            "Train Epoch: 71 [4320/7471 (58%)]\tLoss: 1560847.750000\n",
            "Train Epoch: 71 [4480/7471 (60%)]\tLoss: 1547043.875000\n",
            "Train Epoch: 71 [4640/7471 (62%)]\tLoss: 1593931.500000\n",
            "Train Epoch: 71 [4800/7471 (64%)]\tLoss: 1585344.750000\n",
            "Train Epoch: 71 [4960/7471 (66%)]\tLoss: 1571191.375000\n",
            "Train Epoch: 71 [5120/7471 (69%)]\tLoss: 1568386.875000\n",
            "Train Epoch: 71 [5280/7471 (71%)]\tLoss: 1550300.875000\n",
            "Train Epoch: 71 [5440/7471 (73%)]\tLoss: 1559691.750000\n",
            "Train Epoch: 71 [5600/7471 (75%)]\tLoss: 1542002.500000\n",
            "Train Epoch: 71 [5760/7471 (77%)]\tLoss: 1559136.000000\n",
            "Train Epoch: 71 [5920/7471 (79%)]\tLoss: 1610969.625000\n",
            "Train Epoch: 71 [6080/7471 (81%)]\tLoss: 1596700.500000\n",
            "Train Epoch: 71 [6240/7471 (84%)]\tLoss: 1588207.625000\n",
            "Train Epoch: 71 [6400/7471 (86%)]\tLoss: 1556318.875000\n",
            "Train Epoch: 71 [6560/7471 (88%)]\tLoss: 1563113.750000\n",
            "Train Epoch: 71 [6720/7471 (90%)]\tLoss: 1601742.500000\n",
            "Train Epoch: 71 [6880/7471 (92%)]\tLoss: 1613868.000000\n",
            "Train Epoch: 71 [7040/7471 (94%)]\tLoss: 1580105.125000\n",
            "Train Epoch: 71 [7200/7471 (96%)]\tLoss: 1543274.875000\n",
            "Train Epoch: 71 [7360/7471 (99%)]\tLoss: 1587711.375000\n",
            "Epoch 71 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98497.9273\n",
            "\n",
            "Train Epoch: 72 [160/7471 (2%)]\tLoss: 1615221.125000\n",
            "Train Epoch: 72 [320/7471 (4%)]\tLoss: 1556410.125000\n",
            "Train Epoch: 72 [480/7471 (6%)]\tLoss: 1568325.375000\n",
            "Train Epoch: 72 [640/7471 (9%)]\tLoss: 1565840.000000\n",
            "Train Epoch: 72 [800/7471 (11%)]\tLoss: 1618634.750000\n",
            "Train Epoch: 72 [960/7471 (13%)]\tLoss: 1554349.750000\n",
            "Train Epoch: 72 [1120/7471 (15%)]\tLoss: 1631630.875000\n",
            "Train Epoch: 72 [1280/7471 (17%)]\tLoss: 1568440.500000\n",
            "Train Epoch: 72 [1440/7471 (19%)]\tLoss: 1545515.875000\n",
            "Train Epoch: 72 [1600/7471 (21%)]\tLoss: 1484479.125000\n",
            "Train Epoch: 72 [1760/7471 (24%)]\tLoss: 1555950.625000\n",
            "Train Epoch: 72 [1920/7471 (26%)]\tLoss: 1628082.000000\n",
            "Train Epoch: 72 [2080/7471 (28%)]\tLoss: 1557620.625000\n",
            "Train Epoch: 72 [2240/7471 (30%)]\tLoss: 1515346.000000\n",
            "Train Epoch: 72 [2400/7471 (32%)]\tLoss: 1542033.000000\n",
            "Train Epoch: 72 [2560/7471 (34%)]\tLoss: 1529635.000000\n",
            "Train Epoch: 72 [2720/7471 (36%)]\tLoss: 1561431.500000\n",
            "Train Epoch: 72 [2880/7471 (39%)]\tLoss: 1494097.000000\n",
            "Train Epoch: 72 [3040/7471 (41%)]\tLoss: 1588675.750000\n",
            "Train Epoch: 72 [3200/7471 (43%)]\tLoss: 1499401.750000\n",
            "Train Epoch: 72 [3360/7471 (45%)]\tLoss: 1570175.375000\n",
            "Train Epoch: 72 [3520/7471 (47%)]\tLoss: 1533629.875000\n",
            "Train Epoch: 72 [3680/7471 (49%)]\tLoss: 1610478.375000\n",
            "Train Epoch: 72 [3840/7471 (51%)]\tLoss: 1567785.375000\n",
            "Train Epoch: 72 [4000/7471 (54%)]\tLoss: 1558448.250000\n",
            "Train Epoch: 72 [4160/7471 (56%)]\tLoss: 1505112.875000\n",
            "Train Epoch: 72 [4320/7471 (58%)]\tLoss: 1617402.375000\n",
            "Train Epoch: 72 [4480/7471 (60%)]\tLoss: 1566586.875000\n",
            "Train Epoch: 72 [4640/7471 (62%)]\tLoss: 1571993.625000\n",
            "Train Epoch: 72 [4800/7471 (64%)]\tLoss: 1582165.750000\n",
            "Train Epoch: 72 [4960/7471 (66%)]\tLoss: 1596908.500000\n",
            "Train Epoch: 72 [5120/7471 (69%)]\tLoss: 1548479.500000\n",
            "Train Epoch: 72 [5280/7471 (71%)]\tLoss: 1547965.375000\n",
            "Train Epoch: 72 [5440/7471 (73%)]\tLoss: 1530389.125000\n",
            "Train Epoch: 72 [5600/7471 (75%)]\tLoss: 1584295.375000\n",
            "Train Epoch: 72 [5760/7471 (77%)]\tLoss: 1552734.500000\n",
            "Train Epoch: 72 [5920/7471 (79%)]\tLoss: 1597945.000000\n",
            "Train Epoch: 72 [6080/7471 (81%)]\tLoss: 1608739.000000\n",
            "Train Epoch: 72 [6240/7471 (84%)]\tLoss: 1548331.625000\n",
            "Train Epoch: 72 [6400/7471 (86%)]\tLoss: 1527353.875000\n",
            "Train Epoch: 72 [6560/7471 (88%)]\tLoss: 1639134.125000\n",
            "Train Epoch: 72 [6720/7471 (90%)]\tLoss: 1548698.500000\n",
            "Train Epoch: 72 [6880/7471 (92%)]\tLoss: 1577973.125000\n",
            "Train Epoch: 72 [7040/7471 (94%)]\tLoss: 1600128.500000\n",
            "Train Epoch: 72 [7200/7471 (96%)]\tLoss: 1610837.000000\n",
            "Train Epoch: 72 [7360/7471 (99%)]\tLoss: 1494770.375000\n",
            "Epoch 72 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98511.3831\n",
            "\n",
            "Train Epoch: 73 [160/7471 (2%)]\tLoss: 1538096.750000\n",
            "Train Epoch: 73 [320/7471 (4%)]\tLoss: 1501041.125000\n",
            "Train Epoch: 73 [480/7471 (6%)]\tLoss: 1565545.375000\n",
            "Train Epoch: 73 [640/7471 (9%)]\tLoss: 1537300.750000\n",
            "Train Epoch: 73 [800/7471 (11%)]\tLoss: 1561558.000000\n",
            "Train Epoch: 73 [960/7471 (13%)]\tLoss: 1541237.000000\n",
            "Train Epoch: 73 [1120/7471 (15%)]\tLoss: 1527250.000000\n",
            "Train Epoch: 73 [1280/7471 (17%)]\tLoss: 1532577.750000\n",
            "Train Epoch: 73 [1440/7471 (19%)]\tLoss: 1479346.000000\n",
            "Train Epoch: 73 [1600/7471 (21%)]\tLoss: 1607326.750000\n",
            "Train Epoch: 73 [1760/7471 (24%)]\tLoss: 1518652.875000\n",
            "Train Epoch: 73 [1920/7471 (26%)]\tLoss: 1626327.500000\n",
            "Train Epoch: 73 [2080/7471 (28%)]\tLoss: 1531127.000000\n",
            "Train Epoch: 73 [2240/7471 (30%)]\tLoss: 1599618.875000\n",
            "Train Epoch: 73 [2400/7471 (32%)]\tLoss: 1557538.250000\n",
            "Train Epoch: 73 [2560/7471 (34%)]\tLoss: 1606235.625000\n",
            "Train Epoch: 73 [2720/7471 (36%)]\tLoss: 1445092.750000\n",
            "Train Epoch: 73 [2880/7471 (39%)]\tLoss: 1604247.250000\n",
            "Train Epoch: 73 [3040/7471 (41%)]\tLoss: 1614210.000000\n",
            "Train Epoch: 73 [3200/7471 (43%)]\tLoss: 1602448.625000\n",
            "Train Epoch: 73 [3360/7471 (45%)]\tLoss: 1551032.500000\n",
            "Train Epoch: 73 [3520/7471 (47%)]\tLoss: 1584389.125000\n",
            "Train Epoch: 73 [3680/7471 (49%)]\tLoss: 1599425.375000\n",
            "Train Epoch: 73 [3840/7471 (51%)]\tLoss: 1540348.500000\n",
            "Train Epoch: 73 [4000/7471 (54%)]\tLoss: 1503586.750000\n",
            "Train Epoch: 73 [4160/7471 (56%)]\tLoss: 1566590.375000\n",
            "Train Epoch: 73 [4320/7471 (58%)]\tLoss: 1619568.125000\n",
            "Train Epoch: 73 [4480/7471 (60%)]\tLoss: 1602599.000000\n",
            "Train Epoch: 73 [4640/7471 (62%)]\tLoss: 1561937.375000\n",
            "Train Epoch: 73 [4800/7471 (64%)]\tLoss: 1587909.250000\n",
            "Train Epoch: 73 [4960/7471 (66%)]\tLoss: 1509232.375000\n",
            "Train Epoch: 73 [5120/7471 (69%)]\tLoss: 1597098.375000\n",
            "Train Epoch: 73 [5280/7471 (71%)]\tLoss: 1626868.625000\n",
            "Train Epoch: 73 [5440/7471 (73%)]\tLoss: 1579243.000000\n",
            "Train Epoch: 73 [5600/7471 (75%)]\tLoss: 1606973.125000\n",
            "Train Epoch: 73 [5760/7471 (77%)]\tLoss: 1629024.375000\n",
            "Train Epoch: 73 [5920/7471 (79%)]\tLoss: 1623509.000000\n",
            "Train Epoch: 73 [6080/7471 (81%)]\tLoss: 1604848.000000\n",
            "Train Epoch: 73 [6240/7471 (84%)]\tLoss: 1583302.000000\n",
            "Train Epoch: 73 [6400/7471 (86%)]\tLoss: 1545320.750000\n",
            "Train Epoch: 73 [6560/7471 (88%)]\tLoss: 1584724.750000\n",
            "Train Epoch: 73 [6720/7471 (90%)]\tLoss: 1567591.750000\n",
            "Train Epoch: 73 [6880/7471 (92%)]\tLoss: 1544342.625000\n",
            "Train Epoch: 73 [7040/7471 (94%)]\tLoss: 1587386.750000\n",
            "Train Epoch: 73 [7200/7471 (96%)]\tLoss: 1597458.875000\n",
            "Train Epoch: 73 [7360/7471 (99%)]\tLoss: 1514675.375000\n",
            "Epoch 73 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98550.0923\n",
            "\n",
            "Train Epoch: 74 [160/7471 (2%)]\tLoss: 1616927.875000\n",
            "Train Epoch: 74 [320/7471 (4%)]\tLoss: 1584419.375000\n",
            "Train Epoch: 74 [480/7471 (6%)]\tLoss: 1587973.125000\n",
            "Train Epoch: 74 [640/7471 (9%)]\tLoss: 1537636.375000\n",
            "Train Epoch: 74 [800/7471 (11%)]\tLoss: 1586388.375000\n",
            "Train Epoch: 74 [960/7471 (13%)]\tLoss: 1566614.000000\n",
            "Train Epoch: 74 [1120/7471 (15%)]\tLoss: 1576090.375000\n",
            "Train Epoch: 74 [1280/7471 (17%)]\tLoss: 1617365.750000\n",
            "Train Epoch: 74 [1440/7471 (19%)]\tLoss: 1515035.625000\n",
            "Train Epoch: 74 [1600/7471 (21%)]\tLoss: 1509535.875000\n",
            "Train Epoch: 74 [1760/7471 (24%)]\tLoss: 1507078.375000\n",
            "Train Epoch: 74 [1920/7471 (26%)]\tLoss: 1603720.250000\n",
            "Train Epoch: 74 [2080/7471 (28%)]\tLoss: 1499028.000000\n",
            "Train Epoch: 74 [2240/7471 (30%)]\tLoss: 1614207.875000\n",
            "Train Epoch: 74 [2400/7471 (32%)]\tLoss: 1512971.375000\n",
            "Train Epoch: 74 [2560/7471 (34%)]\tLoss: 1570925.500000\n",
            "Train Epoch: 74 [2720/7471 (36%)]\tLoss: 1567491.875000\n",
            "Train Epoch: 74 [2880/7471 (39%)]\tLoss: 1574919.500000\n",
            "Train Epoch: 74 [3040/7471 (41%)]\tLoss: 1608094.250000\n",
            "Train Epoch: 74 [3200/7471 (43%)]\tLoss: 1610733.500000\n",
            "Train Epoch: 74 [3360/7471 (45%)]\tLoss: 1566705.000000\n",
            "Train Epoch: 74 [3520/7471 (47%)]\tLoss: 1464720.500000\n",
            "Train Epoch: 74 [3680/7471 (49%)]\tLoss: 1534691.000000\n",
            "Train Epoch: 74 [3840/7471 (51%)]\tLoss: 1598671.875000\n",
            "Train Epoch: 74 [4000/7471 (54%)]\tLoss: 1599923.000000\n",
            "Train Epoch: 74 [4160/7471 (56%)]\tLoss: 1603666.750000\n",
            "Train Epoch: 74 [4320/7471 (58%)]\tLoss: 1523582.750000\n",
            "Train Epoch: 74 [4480/7471 (60%)]\tLoss: 1533118.250000\n",
            "Train Epoch: 74 [4640/7471 (62%)]\tLoss: 1615185.500000\n",
            "Train Epoch: 74 [4800/7471 (64%)]\tLoss: 1586096.125000\n",
            "Train Epoch: 74 [4960/7471 (66%)]\tLoss: 1611904.875000\n",
            "Train Epoch: 74 [5120/7471 (69%)]\tLoss: 1551448.250000\n",
            "Train Epoch: 74 [5280/7471 (71%)]\tLoss: 1624222.000000\n",
            "Train Epoch: 74 [5440/7471 (73%)]\tLoss: 1567477.625000\n",
            "Train Epoch: 74 [5600/7471 (75%)]\tLoss: 1488205.750000\n",
            "Train Epoch: 74 [5760/7471 (77%)]\tLoss: 1627813.250000\n",
            "Train Epoch: 74 [5920/7471 (79%)]\tLoss: 1585261.250000\n",
            "Train Epoch: 74 [6080/7471 (81%)]\tLoss: 1591836.500000\n",
            "Train Epoch: 74 [6240/7471 (84%)]\tLoss: 1607989.375000\n",
            "Train Epoch: 74 [6400/7471 (86%)]\tLoss: 1497846.875000\n",
            "Train Epoch: 74 [6560/7471 (88%)]\tLoss: 1575699.000000\n",
            "Train Epoch: 74 [6720/7471 (90%)]\tLoss: 1578797.500000\n",
            "Train Epoch: 74 [6880/7471 (92%)]\tLoss: 1627025.875000\n",
            "Train Epoch: 74 [7040/7471 (94%)]\tLoss: 1601494.625000\n",
            "Train Epoch: 74 [7200/7471 (96%)]\tLoss: 1551324.500000\n",
            "Train Epoch: 74 [7360/7471 (99%)]\tLoss: 1616236.000000\n",
            "Epoch 74 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98462.9215\n",
            "\n",
            "Train Epoch: 75 [160/7471 (2%)]\tLoss: 1576053.625000\n",
            "Train Epoch: 75 [320/7471 (4%)]\tLoss: 1567635.125000\n",
            "Train Epoch: 75 [480/7471 (6%)]\tLoss: 1570378.250000\n",
            "Train Epoch: 75 [640/7471 (9%)]\tLoss: 1617431.000000\n",
            "Train Epoch: 75 [800/7471 (11%)]\tLoss: 1581302.125000\n",
            "Train Epoch: 75 [960/7471 (13%)]\tLoss: 1603487.500000\n",
            "Train Epoch: 75 [1120/7471 (15%)]\tLoss: 1609792.375000\n",
            "Train Epoch: 75 [1280/7471 (17%)]\tLoss: 1562960.750000\n",
            "Train Epoch: 75 [1440/7471 (19%)]\tLoss: 1498117.500000\n",
            "Train Epoch: 75 [1600/7471 (21%)]\tLoss: 1509005.625000\n",
            "Train Epoch: 75 [1760/7471 (24%)]\tLoss: 1622654.375000\n",
            "Train Epoch: 75 [1920/7471 (26%)]\tLoss: 1539534.500000\n",
            "Train Epoch: 75 [2080/7471 (28%)]\tLoss: 1533045.125000\n",
            "Train Epoch: 75 [2240/7471 (30%)]\tLoss: 1581028.500000\n",
            "Train Epoch: 75 [2400/7471 (32%)]\tLoss: 1632489.750000\n",
            "Train Epoch: 75 [2560/7471 (34%)]\tLoss: 1565987.375000\n",
            "Train Epoch: 75 [2720/7471 (36%)]\tLoss: 1570346.000000\n",
            "Train Epoch: 75 [2880/7471 (39%)]\tLoss: 1542521.000000\n",
            "Train Epoch: 75 [3040/7471 (41%)]\tLoss: 1609477.625000\n",
            "Train Epoch: 75 [3200/7471 (43%)]\tLoss: 1582618.875000\n",
            "Train Epoch: 75 [3360/7471 (45%)]\tLoss: 1640143.500000\n",
            "Train Epoch: 75 [3520/7471 (47%)]\tLoss: 1552897.625000\n",
            "Train Epoch: 75 [3680/7471 (49%)]\tLoss: 1598907.875000\n",
            "Train Epoch: 75 [3840/7471 (51%)]\tLoss: 1628730.875000\n",
            "Train Epoch: 75 [4000/7471 (54%)]\tLoss: 1548550.625000\n",
            "Train Epoch: 75 [4160/7471 (56%)]\tLoss: 1595901.250000\n",
            "Train Epoch: 75 [4320/7471 (58%)]\tLoss: 1566130.625000\n",
            "Train Epoch: 75 [4480/7471 (60%)]\tLoss: 1577923.125000\n",
            "Train Epoch: 75 [4640/7471 (62%)]\tLoss: 1609705.500000\n",
            "Train Epoch: 75 [4800/7471 (64%)]\tLoss: 1612482.875000\n",
            "Train Epoch: 75 [4960/7471 (66%)]\tLoss: 1587183.875000\n",
            "Train Epoch: 75 [5120/7471 (69%)]\tLoss: 1534464.250000\n",
            "Train Epoch: 75 [5280/7471 (71%)]\tLoss: 1568148.125000\n",
            "Train Epoch: 75 [5440/7471 (73%)]\tLoss: 1505655.375000\n",
            "Train Epoch: 75 [5600/7471 (75%)]\tLoss: 1600038.750000\n",
            "Train Epoch: 75 [5760/7471 (77%)]\tLoss: 1611290.125000\n",
            "Train Epoch: 75 [5920/7471 (79%)]\tLoss: 1589015.500000\n",
            "Train Epoch: 75 [6080/7471 (81%)]\tLoss: 1610555.125000\n",
            "Train Epoch: 75 [6240/7471 (84%)]\tLoss: 1615030.625000\n",
            "Train Epoch: 75 [6400/7471 (86%)]\tLoss: 1555033.500000\n",
            "Train Epoch: 75 [6560/7471 (88%)]\tLoss: 1556508.875000\n",
            "Train Epoch: 75 [6720/7471 (90%)]\tLoss: 1637429.875000\n",
            "Train Epoch: 75 [6880/7471 (92%)]\tLoss: 1521980.625000\n",
            "Train Epoch: 75 [7040/7471 (94%)]\tLoss: 1559904.750000\n",
            "Train Epoch: 75 [7200/7471 (96%)]\tLoss: 1564559.000000\n",
            "Train Epoch: 75 [7360/7471 (99%)]\tLoss: 1561194.375000\n",
            "Epoch 75 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98471.5464\n",
            "\n",
            "Train Epoch: 76 [160/7471 (2%)]\tLoss: 1557706.250000\n",
            "Train Epoch: 76 [320/7471 (4%)]\tLoss: 1611413.875000\n",
            "Train Epoch: 76 [480/7471 (6%)]\tLoss: 1617819.000000\n",
            "Train Epoch: 76 [640/7471 (9%)]\tLoss: 1596911.125000\n",
            "Train Epoch: 76 [800/7471 (11%)]\tLoss: 1601965.250000\n",
            "Train Epoch: 76 [960/7471 (13%)]\tLoss: 1628800.875000\n",
            "Train Epoch: 76 [1120/7471 (15%)]\tLoss: 1629856.375000\n",
            "Train Epoch: 76 [1280/7471 (17%)]\tLoss: 1572419.500000\n",
            "Train Epoch: 76 [1440/7471 (19%)]\tLoss: 1546637.500000\n",
            "Train Epoch: 76 [1600/7471 (21%)]\tLoss: 1593876.375000\n",
            "Train Epoch: 76 [1760/7471 (24%)]\tLoss: 1590647.375000\n",
            "Train Epoch: 76 [1920/7471 (26%)]\tLoss: 1575727.375000\n",
            "Train Epoch: 76 [2080/7471 (28%)]\tLoss: 1556300.375000\n",
            "Train Epoch: 76 [2240/7471 (30%)]\tLoss: 1539090.125000\n",
            "Train Epoch: 76 [2400/7471 (32%)]\tLoss: 1570570.375000\n",
            "Train Epoch: 76 [2560/7471 (34%)]\tLoss: 1561054.500000\n",
            "Train Epoch: 76 [2720/7471 (36%)]\tLoss: 1548865.375000\n",
            "Train Epoch: 76 [2880/7471 (39%)]\tLoss: 1588357.000000\n",
            "Train Epoch: 76 [3040/7471 (41%)]\tLoss: 1587873.750000\n",
            "Train Epoch: 76 [3200/7471 (43%)]\tLoss: 1601214.375000\n",
            "Train Epoch: 76 [3360/7471 (45%)]\tLoss: 1599785.250000\n",
            "Train Epoch: 76 [3520/7471 (47%)]\tLoss: 1586047.000000\n",
            "Train Epoch: 76 [3680/7471 (49%)]\tLoss: 1622086.875000\n",
            "Train Epoch: 76 [3840/7471 (51%)]\tLoss: 1554056.375000\n",
            "Train Epoch: 76 [4000/7471 (54%)]\tLoss: 1616798.750000\n",
            "Train Epoch: 76 [4160/7471 (56%)]\tLoss: 1590372.000000\n",
            "Train Epoch: 76 [4320/7471 (58%)]\tLoss: 1556515.375000\n",
            "Train Epoch: 76 [4480/7471 (60%)]\tLoss: 1519115.125000\n",
            "Train Epoch: 76 [4640/7471 (62%)]\tLoss: 1616728.625000\n",
            "Train Epoch: 76 [4800/7471 (64%)]\tLoss: 1562257.625000\n",
            "Train Epoch: 76 [4960/7471 (66%)]\tLoss: 1525420.250000\n",
            "Train Epoch: 76 [5120/7471 (69%)]\tLoss: 1527426.625000\n",
            "Train Epoch: 76 [5280/7471 (71%)]\tLoss: 1503523.875000\n",
            "Train Epoch: 76 [5440/7471 (73%)]\tLoss: 1563998.875000\n",
            "Train Epoch: 76 [5600/7471 (75%)]\tLoss: 1551152.750000\n",
            "Train Epoch: 76 [5760/7471 (77%)]\tLoss: 1600882.875000\n",
            "Train Epoch: 76 [5920/7471 (79%)]\tLoss: 1623056.375000\n",
            "Train Epoch: 76 [6080/7471 (81%)]\tLoss: 1576111.125000\n",
            "Train Epoch: 76 [6240/7471 (84%)]\tLoss: 1572070.625000\n",
            "Train Epoch: 76 [6400/7471 (86%)]\tLoss: 1605803.625000\n",
            "Train Epoch: 76 [6560/7471 (88%)]\tLoss: 1527801.125000\n",
            "Train Epoch: 76 [6720/7471 (90%)]\tLoss: 1529703.000000\n",
            "Train Epoch: 76 [6880/7471 (92%)]\tLoss: 1607289.875000\n",
            "Train Epoch: 76 [7040/7471 (94%)]\tLoss: 1627141.375000\n",
            "Train Epoch: 76 [7200/7471 (96%)]\tLoss: 1549499.625000\n",
            "Train Epoch: 76 [7360/7471 (99%)]\tLoss: 1610946.000000\n",
            "Epoch 76 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98485.1026\n",
            "\n",
            "Train Epoch: 77 [160/7471 (2%)]\tLoss: 1616493.125000\n",
            "Train Epoch: 77 [320/7471 (4%)]\tLoss: 1531444.625000\n",
            "Train Epoch: 77 [480/7471 (6%)]\tLoss: 1491344.250000\n",
            "Train Epoch: 77 [640/7471 (9%)]\tLoss: 1543605.500000\n",
            "Train Epoch: 77 [800/7471 (11%)]\tLoss: 1623383.500000\n",
            "Train Epoch: 77 [960/7471 (13%)]\tLoss: 1604734.125000\n",
            "Train Epoch: 77 [1120/7471 (15%)]\tLoss: 1477951.875000\n",
            "Train Epoch: 77 [1280/7471 (17%)]\tLoss: 1547071.250000\n",
            "Train Epoch: 77 [1440/7471 (19%)]\tLoss: 1591276.250000\n",
            "Train Epoch: 77 [1600/7471 (21%)]\tLoss: 1534355.250000\n",
            "Train Epoch: 77 [1760/7471 (24%)]\tLoss: 1575552.250000\n",
            "Train Epoch: 77 [1920/7471 (26%)]\tLoss: 1499208.125000\n",
            "Train Epoch: 77 [2080/7471 (28%)]\tLoss: 1610800.375000\n",
            "Train Epoch: 77 [2240/7471 (30%)]\tLoss: 1539927.000000\n",
            "Train Epoch: 77 [2400/7471 (32%)]\tLoss: 1578316.125000\n",
            "Train Epoch: 77 [2560/7471 (34%)]\tLoss: 1579287.125000\n",
            "Train Epoch: 77 [2720/7471 (36%)]\tLoss: 1536202.500000\n",
            "Train Epoch: 77 [2880/7471 (39%)]\tLoss: 1563675.750000\n",
            "Train Epoch: 77 [3040/7471 (41%)]\tLoss: 1565413.875000\n",
            "Train Epoch: 77 [3200/7471 (43%)]\tLoss: 1577777.375000\n",
            "Train Epoch: 77 [3360/7471 (45%)]\tLoss: 1611335.875000\n",
            "Train Epoch: 77 [3520/7471 (47%)]\tLoss: 1610559.625000\n",
            "Train Epoch: 77 [3680/7471 (49%)]\tLoss: 1586114.875000\n",
            "Train Epoch: 77 [3840/7471 (51%)]\tLoss: 1539485.125000\n",
            "Train Epoch: 77 [4000/7471 (54%)]\tLoss: 1574020.875000\n",
            "Train Epoch: 77 [4160/7471 (56%)]\tLoss: 1608858.250000\n",
            "Train Epoch: 77 [4320/7471 (58%)]\tLoss: 1594117.125000\n",
            "Train Epoch: 77 [4480/7471 (60%)]\tLoss: 1588778.375000\n",
            "Train Epoch: 77 [4640/7471 (62%)]\tLoss: 1557717.125000\n",
            "Train Epoch: 77 [4800/7471 (64%)]\tLoss: 1612415.875000\n",
            "Train Epoch: 77 [4960/7471 (66%)]\tLoss: 1516790.500000\n",
            "Train Epoch: 77 [5120/7471 (69%)]\tLoss: 1613671.625000\n",
            "Train Epoch: 77 [5280/7471 (71%)]\tLoss: 1583099.125000\n",
            "Train Epoch: 77 [5440/7471 (73%)]\tLoss: 1592548.500000\n",
            "Train Epoch: 77 [5600/7471 (75%)]\tLoss: 1575346.625000\n",
            "Train Epoch: 77 [5760/7471 (77%)]\tLoss: 1628780.625000\n",
            "Train Epoch: 77 [5920/7471 (79%)]\tLoss: 1622577.000000\n",
            "Train Epoch: 77 [6080/7471 (81%)]\tLoss: 1613501.750000\n",
            "Train Epoch: 77 [6240/7471 (84%)]\tLoss: 1557100.750000\n",
            "Train Epoch: 77 [6400/7471 (86%)]\tLoss: 1594000.125000\n",
            "Train Epoch: 77 [6560/7471 (88%)]\tLoss: 1619838.250000\n",
            "Train Epoch: 77 [6720/7471 (90%)]\tLoss: 1607382.250000\n",
            "Train Epoch: 77 [6880/7471 (92%)]\tLoss: 1528027.625000\n",
            "Train Epoch: 77 [7040/7471 (94%)]\tLoss: 1569533.750000\n",
            "Train Epoch: 77 [7200/7471 (96%)]\tLoss: 1544813.875000\n",
            "Train Epoch: 77 [7360/7471 (99%)]\tLoss: 1570149.125000\n",
            "Epoch 77 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98475.7658\n",
            "\n",
            "Train Epoch: 78 [160/7471 (2%)]\tLoss: 1553348.875000\n",
            "Train Epoch: 78 [320/7471 (4%)]\tLoss: 1582764.500000\n",
            "Train Epoch: 78 [480/7471 (6%)]\tLoss: 1631015.750000\n",
            "Train Epoch: 78 [640/7471 (9%)]\tLoss: 1546909.125000\n",
            "Train Epoch: 78 [800/7471 (11%)]\tLoss: 1633212.250000\n",
            "Train Epoch: 78 [960/7471 (13%)]\tLoss: 1597358.500000\n",
            "Train Epoch: 78 [1120/7471 (15%)]\tLoss: 1514821.625000\n",
            "Train Epoch: 78 [1280/7471 (17%)]\tLoss: 1530423.500000\n",
            "Train Epoch: 78 [1440/7471 (19%)]\tLoss: 1517529.750000\n",
            "Train Epoch: 78 [1600/7471 (21%)]\tLoss: 1590077.250000\n",
            "Train Epoch: 78 [1760/7471 (24%)]\tLoss: 1595486.250000\n",
            "Train Epoch: 78 [1920/7471 (26%)]\tLoss: 1593269.875000\n",
            "Train Epoch: 78 [2080/7471 (28%)]\tLoss: 1543205.375000\n",
            "Train Epoch: 78 [2240/7471 (30%)]\tLoss: 1611863.250000\n",
            "Train Epoch: 78 [2400/7471 (32%)]\tLoss: 1612799.250000\n",
            "Train Epoch: 78 [2560/7471 (34%)]\tLoss: 1530137.875000\n",
            "Train Epoch: 78 [2720/7471 (36%)]\tLoss: 1553341.500000\n",
            "Train Epoch: 78 [2880/7471 (39%)]\tLoss: 1561585.375000\n",
            "Train Epoch: 78 [3040/7471 (41%)]\tLoss: 1579177.875000\n",
            "Train Epoch: 78 [3200/7471 (43%)]\tLoss: 1545685.625000\n",
            "Train Epoch: 78 [3360/7471 (45%)]\tLoss: 1571989.750000\n",
            "Train Epoch: 78 [3520/7471 (47%)]\tLoss: 1600993.625000\n",
            "Train Epoch: 78 [3680/7471 (49%)]\tLoss: 1636234.000000\n",
            "Train Epoch: 78 [3840/7471 (51%)]\tLoss: 1604608.875000\n",
            "Train Epoch: 78 [4000/7471 (54%)]\tLoss: 1545391.000000\n",
            "Train Epoch: 78 [4160/7471 (56%)]\tLoss: 1567288.500000\n",
            "Train Epoch: 78 [4320/7471 (58%)]\tLoss: 1530688.375000\n",
            "Train Epoch: 78 [4480/7471 (60%)]\tLoss: 1594742.625000\n",
            "Train Epoch: 78 [4640/7471 (62%)]\tLoss: 1585699.375000\n",
            "Train Epoch: 78 [4800/7471 (64%)]\tLoss: 1589350.250000\n",
            "Train Epoch: 78 [4960/7471 (66%)]\tLoss: 1565256.375000\n",
            "Train Epoch: 78 [5120/7471 (69%)]\tLoss: 1583525.375000\n",
            "Train Epoch: 78 [5280/7471 (71%)]\tLoss: 1514799.250000\n",
            "Train Epoch: 78 [5440/7471 (73%)]\tLoss: 1591569.500000\n",
            "Train Epoch: 78 [5600/7471 (75%)]\tLoss: 1596273.625000\n",
            "Train Epoch: 78 [5760/7471 (77%)]\tLoss: 1512898.375000\n",
            "Train Epoch: 78 [5920/7471 (79%)]\tLoss: 1521757.000000\n",
            "Train Epoch: 78 [6080/7471 (81%)]\tLoss: 1565531.375000\n",
            "Train Epoch: 78 [6240/7471 (84%)]\tLoss: 1524885.500000\n",
            "Train Epoch: 78 [6400/7471 (86%)]\tLoss: 1584368.000000\n",
            "Train Epoch: 78 [6560/7471 (88%)]\tLoss: 1595796.750000\n",
            "Train Epoch: 78 [6720/7471 (90%)]\tLoss: 1552813.500000\n",
            "Train Epoch: 78 [6880/7471 (92%)]\tLoss: 1530221.375000\n",
            "Train Epoch: 78 [7040/7471 (94%)]\tLoss: 1586029.875000\n",
            "Train Epoch: 78 [7200/7471 (96%)]\tLoss: 1622835.625000\n",
            "Train Epoch: 78 [7360/7471 (99%)]\tLoss: 1555076.500000\n",
            "Epoch 78 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98506.3151\n",
            "\n",
            "Train Epoch: 79 [160/7471 (2%)]\tLoss: 1612423.750000\n",
            "Train Epoch: 79 [320/7471 (4%)]\tLoss: 1584480.750000\n",
            "Train Epoch: 79 [480/7471 (6%)]\tLoss: 1577067.375000\n",
            "Train Epoch: 79 [640/7471 (9%)]\tLoss: 1573894.750000\n",
            "Train Epoch: 79 [800/7471 (11%)]\tLoss: 1529640.000000\n",
            "Train Epoch: 79 [960/7471 (13%)]\tLoss: 1583491.750000\n",
            "Train Epoch: 79 [1120/7471 (15%)]\tLoss: 1609954.375000\n",
            "Train Epoch: 79 [1280/7471 (17%)]\tLoss: 1520107.750000\n",
            "Train Epoch: 79 [1440/7471 (19%)]\tLoss: 1573206.125000\n",
            "Train Epoch: 79 [1600/7471 (21%)]\tLoss: 1572334.500000\n",
            "Train Epoch: 79 [1760/7471 (24%)]\tLoss: 1567634.375000\n",
            "Train Epoch: 79 [1920/7471 (26%)]\tLoss: 1606181.625000\n",
            "Train Epoch: 79 [2080/7471 (28%)]\tLoss: 1561636.500000\n",
            "Train Epoch: 79 [2240/7471 (30%)]\tLoss: 1596780.750000\n",
            "Train Epoch: 79 [2400/7471 (32%)]\tLoss: 1634803.625000\n",
            "Train Epoch: 79 [2560/7471 (34%)]\tLoss: 1627043.250000\n",
            "Train Epoch: 79 [2720/7471 (36%)]\tLoss: 1568064.375000\n",
            "Train Epoch: 79 [2880/7471 (39%)]\tLoss: 1533839.750000\n",
            "Train Epoch: 79 [3040/7471 (41%)]\tLoss: 1596802.875000\n",
            "Train Epoch: 79 [3200/7471 (43%)]\tLoss: 1584244.500000\n",
            "Train Epoch: 79 [3360/7471 (45%)]\tLoss: 1587462.500000\n",
            "Train Epoch: 79 [3520/7471 (47%)]\tLoss: 1613699.000000\n",
            "Train Epoch: 79 [3680/7471 (49%)]\tLoss: 1527730.375000\n",
            "Train Epoch: 79 [3840/7471 (51%)]\tLoss: 1610748.500000\n",
            "Train Epoch: 79 [4000/7471 (54%)]\tLoss: 1545823.125000\n",
            "Train Epoch: 79 [4160/7471 (56%)]\tLoss: 1564022.000000\n",
            "Train Epoch: 79 [4320/7471 (58%)]\tLoss: 1547582.750000\n",
            "Train Epoch: 79 [4480/7471 (60%)]\tLoss: 1503474.625000\n",
            "Train Epoch: 79 [4640/7471 (62%)]\tLoss: 1520722.000000\n",
            "Train Epoch: 79 [4800/7471 (64%)]\tLoss: 1539781.000000\n",
            "Train Epoch: 79 [4960/7471 (66%)]\tLoss: 1584487.250000\n",
            "Train Epoch: 79 [5120/7471 (69%)]\tLoss: 1489424.750000\n",
            "Train Epoch: 79 [5280/7471 (71%)]\tLoss: 1584143.000000\n",
            "Train Epoch: 79 [5440/7471 (73%)]\tLoss: 1598693.625000\n",
            "Train Epoch: 79 [5600/7471 (75%)]\tLoss: 1596102.375000\n",
            "Train Epoch: 79 [5760/7471 (77%)]\tLoss: 1617715.875000\n",
            "Train Epoch: 79 [5920/7471 (79%)]\tLoss: 1519298.375000\n",
            "Train Epoch: 79 [6080/7471 (81%)]\tLoss: 1577686.875000\n",
            "Train Epoch: 79 [6240/7471 (84%)]\tLoss: 1555738.875000\n",
            "Train Epoch: 79 [6400/7471 (86%)]\tLoss: 1590445.375000\n",
            "Train Epoch: 79 [6560/7471 (88%)]\tLoss: 1566718.000000\n",
            "Train Epoch: 79 [6720/7471 (90%)]\tLoss: 1555021.125000\n",
            "Train Epoch: 79 [6880/7471 (92%)]\tLoss: 1583214.875000\n",
            "Train Epoch: 79 [7040/7471 (94%)]\tLoss: 1583644.875000\n",
            "Train Epoch: 79 [7200/7471 (96%)]\tLoss: 1608733.375000\n",
            "Train Epoch: 79 [7360/7471 (99%)]\tLoss: 1585396.125000\n",
            "Epoch 79 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98482.8543\n",
            "\n",
            "Train Epoch: 80 [160/7471 (2%)]\tLoss: 1583927.750000\n",
            "Train Epoch: 80 [320/7471 (4%)]\tLoss: 1615514.500000\n",
            "Train Epoch: 80 [480/7471 (6%)]\tLoss: 1616017.125000\n",
            "Train Epoch: 80 [640/7471 (9%)]\tLoss: 1600304.875000\n",
            "Train Epoch: 80 [800/7471 (11%)]\tLoss: 1520297.750000\n",
            "Train Epoch: 80 [960/7471 (13%)]\tLoss: 1547245.000000\n",
            "Train Epoch: 80 [1120/7471 (15%)]\tLoss: 1565920.125000\n",
            "Train Epoch: 80 [1280/7471 (17%)]\tLoss: 1502564.000000\n",
            "Train Epoch: 80 [1440/7471 (19%)]\tLoss: 1515926.000000\n",
            "Train Epoch: 80 [1600/7471 (21%)]\tLoss: 1573698.000000\n",
            "Train Epoch: 80 [1760/7471 (24%)]\tLoss: 1587537.125000\n",
            "Train Epoch: 80 [1920/7471 (26%)]\tLoss: 1601540.875000\n",
            "Train Epoch: 80 [2080/7471 (28%)]\tLoss: 1596580.125000\n",
            "Train Epoch: 80 [2240/7471 (30%)]\tLoss: 1612447.625000\n",
            "Train Epoch: 80 [2400/7471 (32%)]\tLoss: 1572698.250000\n",
            "Train Epoch: 80 [2560/7471 (34%)]\tLoss: 1470451.500000\n",
            "Train Epoch: 80 [2720/7471 (36%)]\tLoss: 1558505.875000\n",
            "Train Epoch: 80 [2880/7471 (39%)]\tLoss: 1629816.250000\n",
            "Train Epoch: 80 [3040/7471 (41%)]\tLoss: 1580590.375000\n",
            "Train Epoch: 80 [3200/7471 (43%)]\tLoss: 1520088.000000\n",
            "Train Epoch: 80 [3360/7471 (45%)]\tLoss: 1555471.500000\n",
            "Train Epoch: 80 [3520/7471 (47%)]\tLoss: 1595465.375000\n",
            "Train Epoch: 80 [3680/7471 (49%)]\tLoss: 1615760.500000\n",
            "Train Epoch: 80 [3840/7471 (51%)]\tLoss: 1633082.625000\n",
            "Train Epoch: 80 [4000/7471 (54%)]\tLoss: 1587166.625000\n",
            "Train Epoch: 80 [4160/7471 (56%)]\tLoss: 1503607.375000\n",
            "Train Epoch: 80 [4320/7471 (58%)]\tLoss: 1502401.750000\n",
            "Train Epoch: 80 [4480/7471 (60%)]\tLoss: 1565214.625000\n",
            "Train Epoch: 80 [4640/7471 (62%)]\tLoss: 1601330.000000\n",
            "Train Epoch: 80 [4800/7471 (64%)]\tLoss: 1613935.250000\n",
            "Train Epoch: 80 [4960/7471 (66%)]\tLoss: 1496106.875000\n",
            "Train Epoch: 80 [5120/7471 (69%)]\tLoss: 1574309.750000\n",
            "Train Epoch: 80 [5280/7471 (71%)]\tLoss: 1591109.500000\n",
            "Train Epoch: 80 [5440/7471 (73%)]\tLoss: 1566015.750000\n",
            "Train Epoch: 80 [5600/7471 (75%)]\tLoss: 1582940.500000\n",
            "Train Epoch: 80 [5760/7471 (77%)]\tLoss: 1601049.625000\n",
            "Train Epoch: 80 [5920/7471 (79%)]\tLoss: 1549037.500000\n",
            "Train Epoch: 80 [6080/7471 (81%)]\tLoss: 1593685.750000\n",
            "Train Epoch: 80 [6240/7471 (84%)]\tLoss: 1616389.125000\n",
            "Train Epoch: 80 [6400/7471 (86%)]\tLoss: 1581626.625000\n",
            "Train Epoch: 80 [6560/7471 (88%)]\tLoss: 1594715.000000\n",
            "Train Epoch: 80 [6720/7471 (90%)]\tLoss: 1607388.375000\n",
            "Train Epoch: 80 [6880/7471 (92%)]\tLoss: 1603074.250000\n",
            "Train Epoch: 80 [7040/7471 (94%)]\tLoss: 1626044.625000\n",
            "Train Epoch: 80 [7200/7471 (96%)]\tLoss: 1571719.125000\n",
            "Train Epoch: 80 [7360/7471 (99%)]\tLoss: 1596523.750000\n",
            "Epoch 80 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98524.2495\n",
            "\n",
            "Train Epoch: 81 [160/7471 (2%)]\tLoss: 1577399.250000\n",
            "Train Epoch: 81 [320/7471 (4%)]\tLoss: 1546306.875000\n",
            "Train Epoch: 81 [480/7471 (6%)]\tLoss: 1560568.125000\n",
            "Train Epoch: 81 [640/7471 (9%)]\tLoss: 1550509.000000\n",
            "Train Epoch: 81 [800/7471 (11%)]\tLoss: 1615546.750000\n",
            "Train Epoch: 81 [960/7471 (13%)]\tLoss: 1574862.375000\n",
            "Train Epoch: 81 [1120/7471 (15%)]\tLoss: 1562901.625000\n",
            "Train Epoch: 81 [1280/7471 (17%)]\tLoss: 1610620.500000\n",
            "Train Epoch: 81 [1440/7471 (19%)]\tLoss: 1543880.250000\n",
            "Train Epoch: 81 [1600/7471 (21%)]\tLoss: 1601976.250000\n",
            "Train Epoch: 81 [1760/7471 (24%)]\tLoss: 1556120.000000\n",
            "Train Epoch: 81 [1920/7471 (26%)]\tLoss: 1610946.750000\n",
            "Train Epoch: 81 [2080/7471 (28%)]\tLoss: 1588512.875000\n",
            "Train Epoch: 81 [2240/7471 (30%)]\tLoss: 1614995.500000\n",
            "Train Epoch: 81 [2400/7471 (32%)]\tLoss: 1629048.000000\n",
            "Train Epoch: 81 [2560/7471 (34%)]\tLoss: 1561204.000000\n",
            "Train Epoch: 81 [2720/7471 (36%)]\tLoss: 1580699.625000\n",
            "Train Epoch: 81 [2880/7471 (39%)]\tLoss: 1527643.500000\n",
            "Train Epoch: 81 [3040/7471 (41%)]\tLoss: 1586189.625000\n",
            "Train Epoch: 81 [3200/7471 (43%)]\tLoss: 1607546.375000\n",
            "Train Epoch: 81 [3360/7471 (45%)]\tLoss: 1589011.000000\n",
            "Train Epoch: 81 [3520/7471 (47%)]\tLoss: 1529034.875000\n",
            "Train Epoch: 81 [3680/7471 (49%)]\tLoss: 1561313.750000\n",
            "Train Epoch: 81 [3840/7471 (51%)]\tLoss: 1625114.625000\n",
            "Train Epoch: 81 [4000/7471 (54%)]\tLoss: 1609508.375000\n",
            "Train Epoch: 81 [4160/7471 (56%)]\tLoss: 1554033.000000\n",
            "Train Epoch: 81 [4320/7471 (58%)]\tLoss: 1621369.875000\n",
            "Train Epoch: 81 [4480/7471 (60%)]\tLoss: 1616121.750000\n",
            "Train Epoch: 81 [4640/7471 (62%)]\tLoss: 1591220.625000\n",
            "Train Epoch: 81 [4800/7471 (64%)]\tLoss: 1585743.125000\n",
            "Train Epoch: 81 [4960/7471 (66%)]\tLoss: 1536815.375000\n",
            "Train Epoch: 81 [5120/7471 (69%)]\tLoss: 1542932.625000\n",
            "Train Epoch: 81 [5280/7471 (71%)]\tLoss: 1596147.250000\n",
            "Train Epoch: 81 [5440/7471 (73%)]\tLoss: 1570459.125000\n",
            "Train Epoch: 81 [5600/7471 (75%)]\tLoss: 1556943.875000\n",
            "Train Epoch: 81 [5760/7471 (77%)]\tLoss: 1554301.375000\n",
            "Train Epoch: 81 [5920/7471 (79%)]\tLoss: 1607869.875000\n",
            "Train Epoch: 81 [6080/7471 (81%)]\tLoss: 1631752.125000\n",
            "Train Epoch: 81 [6240/7471 (84%)]\tLoss: 1539074.750000\n",
            "Train Epoch: 81 [6400/7471 (86%)]\tLoss: 1548135.500000\n",
            "Train Epoch: 81 [6560/7471 (88%)]\tLoss: 1548362.750000\n",
            "Train Epoch: 81 [6720/7471 (90%)]\tLoss: 1543696.000000\n",
            "Train Epoch: 81 [6880/7471 (92%)]\tLoss: 1457218.875000\n",
            "Train Epoch: 81 [7040/7471 (94%)]\tLoss: 1638122.625000\n",
            "Train Epoch: 81 [7200/7471 (96%)]\tLoss: 1564676.500000\n",
            "Train Epoch: 81 [7360/7471 (99%)]\tLoss: 1596966.125000\n",
            "Epoch 81 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98480.2659\n",
            "\n",
            "Train Epoch: 82 [160/7471 (2%)]\tLoss: 1584362.250000\n",
            "Train Epoch: 82 [320/7471 (4%)]\tLoss: 1523651.500000\n",
            "Train Epoch: 82 [480/7471 (6%)]\tLoss: 1593926.875000\n",
            "Train Epoch: 82 [640/7471 (9%)]\tLoss: 1581987.750000\n",
            "Train Epoch: 82 [800/7471 (11%)]\tLoss: 1609361.125000\n",
            "Train Epoch: 82 [960/7471 (13%)]\tLoss: 1550637.500000\n",
            "Train Epoch: 82 [1120/7471 (15%)]\tLoss: 1592675.125000\n",
            "Train Epoch: 82 [1280/7471 (17%)]\tLoss: 1536761.000000\n",
            "Train Epoch: 82 [1440/7471 (19%)]\tLoss: 1600872.875000\n",
            "Train Epoch: 82 [1600/7471 (21%)]\tLoss: 1628331.625000\n",
            "Train Epoch: 82 [1760/7471 (24%)]\tLoss: 1578002.000000\n",
            "Train Epoch: 82 [1920/7471 (26%)]\tLoss: 1606298.375000\n",
            "Train Epoch: 82 [2080/7471 (28%)]\tLoss: 1538032.500000\n",
            "Train Epoch: 82 [2240/7471 (30%)]\tLoss: 1600554.000000\n",
            "Train Epoch: 82 [2400/7471 (32%)]\tLoss: 1595684.625000\n",
            "Train Epoch: 82 [2560/7471 (34%)]\tLoss: 1623559.250000\n",
            "Train Epoch: 82 [2720/7471 (36%)]\tLoss: 1560412.375000\n",
            "Train Epoch: 82 [2880/7471 (39%)]\tLoss: 1601478.250000\n",
            "Train Epoch: 82 [3040/7471 (41%)]\tLoss: 1579391.375000\n",
            "Train Epoch: 82 [3200/7471 (43%)]\tLoss: 1499405.375000\n",
            "Train Epoch: 82 [3360/7471 (45%)]\tLoss: 1544220.625000\n",
            "Train Epoch: 82 [3520/7471 (47%)]\tLoss: 1644165.250000\n",
            "Train Epoch: 82 [3680/7471 (49%)]\tLoss: 1592986.250000\n",
            "Train Epoch: 82 [3840/7471 (51%)]\tLoss: 1563309.375000\n",
            "Train Epoch: 82 [4000/7471 (54%)]\tLoss: 1547019.500000\n",
            "Train Epoch: 82 [4160/7471 (56%)]\tLoss: 1553375.625000\n",
            "Train Epoch: 82 [4320/7471 (58%)]\tLoss: 1594421.750000\n",
            "Train Epoch: 82 [4480/7471 (60%)]\tLoss: 1613992.875000\n",
            "Train Epoch: 82 [4640/7471 (62%)]\tLoss: 1585421.375000\n",
            "Train Epoch: 82 [4800/7471 (64%)]\tLoss: 1606914.000000\n",
            "Train Epoch: 82 [4960/7471 (66%)]\tLoss: 1489671.625000\n",
            "Train Epoch: 82 [5120/7471 (69%)]\tLoss: 1582491.750000\n",
            "Train Epoch: 82 [5280/7471 (71%)]\tLoss: 1528520.375000\n",
            "Train Epoch: 82 [5440/7471 (73%)]\tLoss: 1566139.125000\n",
            "Train Epoch: 82 [5600/7471 (75%)]\tLoss: 1512980.500000\n",
            "Train Epoch: 82 [5760/7471 (77%)]\tLoss: 1555964.375000\n",
            "Train Epoch: 82 [5920/7471 (79%)]\tLoss: 1584941.000000\n",
            "Train Epoch: 82 [6080/7471 (81%)]\tLoss: 1594666.000000\n",
            "Train Epoch: 82 [6240/7471 (84%)]\tLoss: 1553238.250000\n",
            "Train Epoch: 82 [6400/7471 (86%)]\tLoss: 1562228.625000\n",
            "Train Epoch: 82 [6560/7471 (88%)]\tLoss: 1557898.500000\n",
            "Train Epoch: 82 [6720/7471 (90%)]\tLoss: 1506736.375000\n",
            "Train Epoch: 82 [6880/7471 (92%)]\tLoss: 1574904.375000\n",
            "Train Epoch: 82 [7040/7471 (94%)]\tLoss: 1581203.875000\n",
            "Train Epoch: 82 [7200/7471 (96%)]\tLoss: 1495748.750000\n",
            "Train Epoch: 82 [7360/7471 (99%)]\tLoss: 1566338.375000\n",
            "Epoch 82 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98549.2924\n",
            "\n",
            "Train Epoch: 83 [160/7471 (2%)]\tLoss: 1623741.125000\n",
            "Train Epoch: 83 [320/7471 (4%)]\tLoss: 1579273.750000\n",
            "Train Epoch: 83 [480/7471 (6%)]\tLoss: 1570093.750000\n",
            "Train Epoch: 83 [640/7471 (9%)]\tLoss: 1569408.250000\n",
            "Train Epoch: 83 [800/7471 (11%)]\tLoss: 1610023.625000\n",
            "Train Epoch: 83 [960/7471 (13%)]\tLoss: 1571353.875000\n",
            "Train Epoch: 83 [1120/7471 (15%)]\tLoss: 1587601.750000\n",
            "Train Epoch: 83 [1280/7471 (17%)]\tLoss: 1552954.625000\n",
            "Train Epoch: 83 [1440/7471 (19%)]\tLoss: 1614767.500000\n",
            "Train Epoch: 83 [1600/7471 (21%)]\tLoss: 1518389.500000\n",
            "Train Epoch: 83 [1760/7471 (24%)]\tLoss: 1515214.875000\n",
            "Train Epoch: 83 [1920/7471 (26%)]\tLoss: 1610385.750000\n",
            "Train Epoch: 83 [2080/7471 (28%)]\tLoss: 1582681.000000\n",
            "Train Epoch: 83 [2240/7471 (30%)]\tLoss: 1568577.500000\n",
            "Train Epoch: 83 [2400/7471 (32%)]\tLoss: 1587180.125000\n",
            "Train Epoch: 83 [2560/7471 (34%)]\tLoss: 1546247.750000\n",
            "Train Epoch: 83 [2720/7471 (36%)]\tLoss: 1545640.125000\n",
            "Train Epoch: 83 [2880/7471 (39%)]\tLoss: 1623004.750000\n",
            "Train Epoch: 83 [3040/7471 (41%)]\tLoss: 1627225.125000\n",
            "Train Epoch: 83 [3200/7471 (43%)]\tLoss: 1606322.625000\n",
            "Train Epoch: 83 [3360/7471 (45%)]\tLoss: 1567418.250000\n",
            "Train Epoch: 83 [3520/7471 (47%)]\tLoss: 1599969.750000\n",
            "Train Epoch: 83 [3680/7471 (49%)]\tLoss: 1555796.875000\n",
            "Train Epoch: 83 [3840/7471 (51%)]\tLoss: 1569963.250000\n",
            "Train Epoch: 83 [4000/7471 (54%)]\tLoss: 1623122.375000\n",
            "Train Epoch: 83 [4160/7471 (56%)]\tLoss: 1592151.750000\n",
            "Train Epoch: 83 [4320/7471 (58%)]\tLoss: 1556882.750000\n",
            "Train Epoch: 83 [4480/7471 (60%)]\tLoss: 1576774.000000\n",
            "Train Epoch: 83 [4640/7471 (62%)]\tLoss: 1520948.750000\n",
            "Train Epoch: 83 [4800/7471 (64%)]\tLoss: 1580715.625000\n",
            "Train Epoch: 83 [4960/7471 (66%)]\tLoss: 1575210.875000\n",
            "Train Epoch: 83 [5120/7471 (69%)]\tLoss: 1533005.875000\n",
            "Train Epoch: 83 [5280/7471 (71%)]\tLoss: 1607015.750000\n",
            "Train Epoch: 83 [5440/7471 (73%)]\tLoss: 1619940.625000\n",
            "Train Epoch: 83 [5600/7471 (75%)]\tLoss: 1609851.500000\n",
            "Train Epoch: 83 [5760/7471 (77%)]\tLoss: 1558092.250000\n",
            "Train Epoch: 83 [5920/7471 (79%)]\tLoss: 1539472.875000\n",
            "Train Epoch: 83 [6080/7471 (81%)]\tLoss: 1611940.250000\n",
            "Train Epoch: 83 [6240/7471 (84%)]\tLoss: 1577003.125000\n",
            "Train Epoch: 83 [6400/7471 (86%)]\tLoss: 1607269.125000\n",
            "Train Epoch: 83 [6560/7471 (88%)]\tLoss: 1528324.250000\n",
            "Train Epoch: 83 [6720/7471 (90%)]\tLoss: 1574347.125000\n",
            "Train Epoch: 83 [6880/7471 (92%)]\tLoss: 1555097.750000\n",
            "Train Epoch: 83 [7040/7471 (94%)]\tLoss: 1590334.375000\n",
            "Train Epoch: 83 [7200/7471 (96%)]\tLoss: 1550014.125000\n",
            "Train Epoch: 83 [7360/7471 (99%)]\tLoss: 1571051.875000\n",
            "Epoch 83 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98505.6253\n",
            "\n",
            "Train Epoch: 84 [160/7471 (2%)]\tLoss: 1547541.000000\n",
            "Train Epoch: 84 [320/7471 (4%)]\tLoss: 1611054.750000\n",
            "Train Epoch: 84 [480/7471 (6%)]\tLoss: 1591324.750000\n",
            "Train Epoch: 84 [640/7471 (9%)]\tLoss: 1584342.000000\n",
            "Train Epoch: 84 [800/7471 (11%)]\tLoss: 1577366.500000\n",
            "Train Epoch: 84 [960/7471 (13%)]\tLoss: 1611276.125000\n",
            "Train Epoch: 84 [1120/7471 (15%)]\tLoss: 1580636.875000\n",
            "Train Epoch: 84 [1280/7471 (17%)]\tLoss: 1583390.500000\n",
            "Train Epoch: 84 [1440/7471 (19%)]\tLoss: 1617385.375000\n",
            "Train Epoch: 84 [1600/7471 (21%)]\tLoss: 1569882.750000\n",
            "Train Epoch: 84 [1760/7471 (24%)]\tLoss: 1588981.250000\n",
            "Train Epoch: 84 [1920/7471 (26%)]\tLoss: 1568565.625000\n",
            "Train Epoch: 84 [2080/7471 (28%)]\tLoss: 1638637.000000\n",
            "Train Epoch: 84 [2240/7471 (30%)]\tLoss: 1574648.750000\n",
            "Train Epoch: 84 [2400/7471 (32%)]\tLoss: 1619118.500000\n",
            "Train Epoch: 84 [2560/7471 (34%)]\tLoss: 1572107.250000\n",
            "Train Epoch: 84 [2720/7471 (36%)]\tLoss: 1496760.875000\n",
            "Train Epoch: 84 [2880/7471 (39%)]\tLoss: 1611861.500000\n",
            "Train Epoch: 84 [3040/7471 (41%)]\tLoss: 1582562.750000\n",
            "Train Epoch: 84 [3200/7471 (43%)]\tLoss: 1546631.875000\n",
            "Train Epoch: 84 [3360/7471 (45%)]\tLoss: 1575118.500000\n",
            "Train Epoch: 84 [3520/7471 (47%)]\tLoss: 1607516.000000\n",
            "Train Epoch: 84 [3680/7471 (49%)]\tLoss: 1612561.500000\n",
            "Train Epoch: 84 [3840/7471 (51%)]\tLoss: 1574698.500000\n",
            "Train Epoch: 84 [4000/7471 (54%)]\tLoss: 1590083.000000\n",
            "Train Epoch: 84 [4160/7471 (56%)]\tLoss: 1501902.125000\n",
            "Train Epoch: 84 [4320/7471 (58%)]\tLoss: 1586143.750000\n",
            "Train Epoch: 84 [4480/7471 (60%)]\tLoss: 1608838.500000\n",
            "Train Epoch: 84 [4640/7471 (62%)]\tLoss: 1577081.000000\n",
            "Train Epoch: 84 [4800/7471 (64%)]\tLoss: 1622873.625000\n",
            "Train Epoch: 84 [4960/7471 (66%)]\tLoss: 1599261.250000\n",
            "Train Epoch: 84 [5120/7471 (69%)]\tLoss: 1622628.500000\n",
            "Train Epoch: 84 [5280/7471 (71%)]\tLoss: 1559650.750000\n",
            "Train Epoch: 84 [5440/7471 (73%)]\tLoss: 1626918.875000\n",
            "Train Epoch: 84 [5600/7471 (75%)]\tLoss: 1581701.250000\n",
            "Train Epoch: 84 [5760/7471 (77%)]\tLoss: 1557813.000000\n",
            "Train Epoch: 84 [5920/7471 (79%)]\tLoss: 1621357.125000\n",
            "Train Epoch: 84 [6080/7471 (81%)]\tLoss: 1592972.000000\n",
            "Train Epoch: 84 [6240/7471 (84%)]\tLoss: 1597546.250000\n",
            "Train Epoch: 84 [6400/7471 (86%)]\tLoss: 1516983.125000\n",
            "Train Epoch: 84 [6560/7471 (88%)]\tLoss: 1472791.125000\n",
            "Train Epoch: 84 [6720/7471 (90%)]\tLoss: 1540932.750000\n",
            "Train Epoch: 84 [6880/7471 (92%)]\tLoss: 1577836.125000\n",
            "Train Epoch: 84 [7040/7471 (94%)]\tLoss: 1537255.500000\n",
            "Train Epoch: 84 [7200/7471 (96%)]\tLoss: 1578442.500000\n",
            "Train Epoch: 84 [7360/7471 (99%)]\tLoss: 1574974.250000\n",
            "Epoch 84 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98450.1005\n",
            "\n",
            "Train Epoch: 85 [160/7471 (2%)]\tLoss: 1518179.750000\n",
            "Train Epoch: 85 [320/7471 (4%)]\tLoss: 1599835.375000\n",
            "Train Epoch: 85 [480/7471 (6%)]\tLoss: 1598115.250000\n",
            "Train Epoch: 85 [640/7471 (9%)]\tLoss: 1626772.750000\n",
            "Train Epoch: 85 [800/7471 (11%)]\tLoss: 1630179.250000\n",
            "Train Epoch: 85 [960/7471 (13%)]\tLoss: 1603124.750000\n",
            "Train Epoch: 85 [1120/7471 (15%)]\tLoss: 1559357.875000\n",
            "Train Epoch: 85 [1280/7471 (17%)]\tLoss: 1550649.375000\n",
            "Train Epoch: 85 [1440/7471 (19%)]\tLoss: 1585918.875000\n",
            "Train Epoch: 85 [1600/7471 (21%)]\tLoss: 1574477.125000\n",
            "Train Epoch: 85 [1760/7471 (24%)]\tLoss: 1604829.375000\n",
            "Train Epoch: 85 [1920/7471 (26%)]\tLoss: 1563927.625000\n",
            "Train Epoch: 85 [2080/7471 (28%)]\tLoss: 1619480.000000\n",
            "Train Epoch: 85 [2240/7471 (30%)]\tLoss: 1529499.375000\n",
            "Train Epoch: 85 [2400/7471 (32%)]\tLoss: 1581851.500000\n",
            "Train Epoch: 85 [2560/7471 (34%)]\tLoss: 1562012.250000\n",
            "Train Epoch: 85 [2720/7471 (36%)]\tLoss: 1621358.875000\n",
            "Train Epoch: 85 [2880/7471 (39%)]\tLoss: 1600879.125000\n",
            "Train Epoch: 85 [3040/7471 (41%)]\tLoss: 1583291.875000\n",
            "Train Epoch: 85 [3200/7471 (43%)]\tLoss: 1598146.500000\n",
            "Train Epoch: 85 [3360/7471 (45%)]\tLoss: 1522839.875000\n",
            "Train Epoch: 85 [3520/7471 (47%)]\tLoss: 1590861.250000\n",
            "Train Epoch: 85 [3680/7471 (49%)]\tLoss: 1569245.125000\n",
            "Train Epoch: 85 [3840/7471 (51%)]\tLoss: 1493879.625000\n",
            "Train Epoch: 85 [4000/7471 (54%)]\tLoss: 1634240.500000\n",
            "Train Epoch: 85 [4160/7471 (56%)]\tLoss: 1537521.125000\n",
            "Train Epoch: 85 [4320/7471 (58%)]\tLoss: 1535391.750000\n",
            "Train Epoch: 85 [4480/7471 (60%)]\tLoss: 1618039.500000\n",
            "Train Epoch: 85 [4640/7471 (62%)]\tLoss: 1574309.250000\n",
            "Train Epoch: 85 [4800/7471 (64%)]\tLoss: 1580296.875000\n",
            "Train Epoch: 85 [4960/7471 (66%)]\tLoss: 1546345.125000\n",
            "Train Epoch: 85 [5120/7471 (69%)]\tLoss: 1468635.000000\n",
            "Train Epoch: 85 [5280/7471 (71%)]\tLoss: 1539149.250000\n",
            "Train Epoch: 85 [5440/7471 (73%)]\tLoss: 1589696.625000\n",
            "Train Epoch: 85 [5600/7471 (75%)]\tLoss: 1538575.250000\n",
            "Train Epoch: 85 [5760/7471 (77%)]\tLoss: 1608118.250000\n",
            "Train Epoch: 85 [5920/7471 (79%)]\tLoss: 1624446.125000\n",
            "Train Epoch: 85 [6080/7471 (81%)]\tLoss: 1602742.250000\n",
            "Train Epoch: 85 [6240/7471 (84%)]\tLoss: 1578147.500000\n",
            "Train Epoch: 85 [6400/7471 (86%)]\tLoss: 1633280.875000\n",
            "Train Epoch: 85 [6560/7471 (88%)]\tLoss: 1570277.500000\n",
            "Train Epoch: 85 [6720/7471 (90%)]\tLoss: 1612424.125000\n",
            "Train Epoch: 85 [6880/7471 (92%)]\tLoss: 1613671.000000\n",
            "Train Epoch: 85 [7040/7471 (94%)]\tLoss: 1560547.250000\n",
            "Train Epoch: 85 [7200/7471 (96%)]\tLoss: 1555033.500000\n",
            "Train Epoch: 85 [7360/7471 (99%)]\tLoss: 1484191.500000\n",
            "Epoch 85 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98507.3227\n",
            "\n",
            "Train Epoch: 86 [160/7471 (2%)]\tLoss: 1597028.875000\n",
            "Train Epoch: 86 [320/7471 (4%)]\tLoss: 1615188.750000\n",
            "Train Epoch: 86 [480/7471 (6%)]\tLoss: 1615842.625000\n",
            "Train Epoch: 86 [640/7471 (9%)]\tLoss: 1553689.875000\n",
            "Train Epoch: 86 [800/7471 (11%)]\tLoss: 1593413.250000\n",
            "Train Epoch: 86 [960/7471 (13%)]\tLoss: 1523021.625000\n",
            "Train Epoch: 86 [1120/7471 (15%)]\tLoss: 1534948.375000\n",
            "Train Epoch: 86 [1280/7471 (17%)]\tLoss: 1539848.125000\n",
            "Train Epoch: 86 [1440/7471 (19%)]\tLoss: 1513952.250000\n",
            "Train Epoch: 86 [1600/7471 (21%)]\tLoss: 1587914.250000\n",
            "Train Epoch: 86 [1760/7471 (24%)]\tLoss: 1512523.625000\n",
            "Train Epoch: 86 [1920/7471 (26%)]\tLoss: 1570445.375000\n",
            "Train Epoch: 86 [2080/7471 (28%)]\tLoss: 1539501.500000\n",
            "Train Epoch: 86 [2240/7471 (30%)]\tLoss: 1618300.500000\n",
            "Train Epoch: 86 [2400/7471 (32%)]\tLoss: 1548213.500000\n",
            "Train Epoch: 86 [2560/7471 (34%)]\tLoss: 1599806.000000\n",
            "Train Epoch: 86 [2720/7471 (36%)]\tLoss: 1587456.750000\n",
            "Train Epoch: 86 [2880/7471 (39%)]\tLoss: 1580894.500000\n",
            "Train Epoch: 86 [3040/7471 (41%)]\tLoss: 1591703.625000\n",
            "Train Epoch: 86 [3200/7471 (43%)]\tLoss: 1530023.375000\n",
            "Train Epoch: 86 [3360/7471 (45%)]\tLoss: 1562955.250000\n",
            "Train Epoch: 86 [3520/7471 (47%)]\tLoss: 1616975.125000\n",
            "Train Epoch: 86 [3680/7471 (49%)]\tLoss: 1598393.875000\n",
            "Train Epoch: 86 [3840/7471 (51%)]\tLoss: 1623487.250000\n",
            "Train Epoch: 86 [4000/7471 (54%)]\tLoss: 1550512.375000\n",
            "Train Epoch: 86 [4160/7471 (56%)]\tLoss: 1467673.625000\n",
            "Train Epoch: 86 [4320/7471 (58%)]\tLoss: 1597624.125000\n",
            "Train Epoch: 86 [4480/7471 (60%)]\tLoss: 1503784.000000\n",
            "Train Epoch: 86 [4640/7471 (62%)]\tLoss: 1622902.625000\n",
            "Train Epoch: 86 [4800/7471 (64%)]\tLoss: 1593547.875000\n",
            "Train Epoch: 86 [4960/7471 (66%)]\tLoss: 1558259.250000\n",
            "Train Epoch: 86 [5120/7471 (69%)]\tLoss: 1591593.500000\n",
            "Train Epoch: 86 [5280/7471 (71%)]\tLoss: 1452949.500000\n",
            "Train Epoch: 86 [5440/7471 (73%)]\tLoss: 1590785.500000\n",
            "Train Epoch: 86 [5600/7471 (75%)]\tLoss: 1538380.375000\n",
            "Train Epoch: 86 [5760/7471 (77%)]\tLoss: 1624021.125000\n",
            "Train Epoch: 86 [5920/7471 (79%)]\tLoss: 1617932.875000\n",
            "Train Epoch: 86 [6080/7471 (81%)]\tLoss: 1634083.125000\n",
            "Train Epoch: 86 [6240/7471 (84%)]\tLoss: 1565198.375000\n",
            "Train Epoch: 86 [6400/7471 (86%)]\tLoss: 1616291.875000\n",
            "Train Epoch: 86 [6560/7471 (88%)]\tLoss: 1613145.875000\n",
            "Train Epoch: 86 [6720/7471 (90%)]\tLoss: 1587139.500000\n",
            "Train Epoch: 86 [6880/7471 (92%)]\tLoss: 1598133.375000\n",
            "Train Epoch: 86 [7040/7471 (94%)]\tLoss: 1575372.000000\n",
            "Train Epoch: 86 [7200/7471 (96%)]\tLoss: 1587852.875000\n",
            "Train Epoch: 86 [7360/7471 (99%)]\tLoss: 1594381.000000\n",
            "Epoch 86 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98466.0906\n",
            "\n",
            "Train Epoch: 87 [160/7471 (2%)]\tLoss: 1546360.625000\n",
            "Train Epoch: 87 [320/7471 (4%)]\tLoss: 1608239.125000\n",
            "Train Epoch: 87 [480/7471 (6%)]\tLoss: 1594885.625000\n",
            "Train Epoch: 87 [640/7471 (9%)]\tLoss: 1567896.125000\n",
            "Train Epoch: 87 [800/7471 (11%)]\tLoss: 1611130.625000\n",
            "Train Epoch: 87 [960/7471 (13%)]\tLoss: 1590829.375000\n",
            "Train Epoch: 87 [1120/7471 (15%)]\tLoss: 1524003.000000\n",
            "Train Epoch: 87 [1280/7471 (17%)]\tLoss: 1613007.625000\n",
            "Train Epoch: 87 [1440/7471 (19%)]\tLoss: 1557791.875000\n",
            "Train Epoch: 87 [1600/7471 (21%)]\tLoss: 1619387.250000\n",
            "Train Epoch: 87 [1760/7471 (24%)]\tLoss: 1560739.250000\n",
            "Train Epoch: 87 [1920/7471 (26%)]\tLoss: 1492247.375000\n",
            "Train Epoch: 87 [2080/7471 (28%)]\tLoss: 1581337.250000\n",
            "Train Epoch: 87 [2240/7471 (30%)]\tLoss: 1602032.750000\n",
            "Train Epoch: 87 [2400/7471 (32%)]\tLoss: 1606378.500000\n",
            "Train Epoch: 87 [2560/7471 (34%)]\tLoss: 1626186.875000\n",
            "Train Epoch: 87 [2720/7471 (36%)]\tLoss: 1574828.750000\n",
            "Train Epoch: 87 [2880/7471 (39%)]\tLoss: 1603496.000000\n",
            "Train Epoch: 87 [3040/7471 (41%)]\tLoss: 1614505.625000\n",
            "Train Epoch: 87 [3200/7471 (43%)]\tLoss: 1598230.250000\n",
            "Train Epoch: 87 [3360/7471 (45%)]\tLoss: 1572187.750000\n",
            "Train Epoch: 87 [3520/7471 (47%)]\tLoss: 1575185.500000\n",
            "Train Epoch: 87 [3680/7471 (49%)]\tLoss: 1556133.125000\n",
            "Train Epoch: 87 [3840/7471 (51%)]\tLoss: 1593483.625000\n",
            "Train Epoch: 87 [4000/7471 (54%)]\tLoss: 1601395.000000\n",
            "Train Epoch: 87 [4160/7471 (56%)]\tLoss: 1588065.125000\n",
            "Train Epoch: 87 [4320/7471 (58%)]\tLoss: 1603421.500000\n",
            "Train Epoch: 87 [4480/7471 (60%)]\tLoss: 1559590.000000\n",
            "Train Epoch: 87 [4640/7471 (62%)]\tLoss: 1628795.875000\n",
            "Train Epoch: 87 [4800/7471 (64%)]\tLoss: 1589360.000000\n",
            "Train Epoch: 87 [4960/7471 (66%)]\tLoss: 1584171.375000\n",
            "Train Epoch: 87 [5120/7471 (69%)]\tLoss: 1524179.750000\n",
            "Train Epoch: 87 [5280/7471 (71%)]\tLoss: 1608439.625000\n",
            "Train Epoch: 87 [5440/7471 (73%)]\tLoss: 1580338.500000\n",
            "Train Epoch: 87 [5600/7471 (75%)]\tLoss: 1575006.875000\n",
            "Train Epoch: 87 [5760/7471 (77%)]\tLoss: 1618752.375000\n",
            "Train Epoch: 87 [5920/7471 (79%)]\tLoss: 1614086.750000\n",
            "Train Epoch: 87 [6080/7471 (81%)]\tLoss: 1618862.625000\n",
            "Train Epoch: 87 [6240/7471 (84%)]\tLoss: 1532108.000000\n",
            "Train Epoch: 87 [6400/7471 (86%)]\tLoss: 1510169.750000\n",
            "Train Epoch: 87 [6560/7471 (88%)]\tLoss: 1554415.125000\n",
            "Train Epoch: 87 [6720/7471 (90%)]\tLoss: 1540541.875000\n",
            "Train Epoch: 87 [6880/7471 (92%)]\tLoss: 1556799.125000\n",
            "Train Epoch: 87 [7040/7471 (94%)]\tLoss: 1606871.250000\n",
            "Train Epoch: 87 [7200/7471 (96%)]\tLoss: 1552578.625000\n",
            "Train Epoch: 87 [7360/7471 (99%)]\tLoss: 1609180.125000\n",
            "Epoch 87 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98476.5147\n",
            "\n",
            "Train Epoch: 88 [160/7471 (2%)]\tLoss: 1599397.750000\n",
            "Train Epoch: 88 [320/7471 (4%)]\tLoss: 1592910.500000\n",
            "Train Epoch: 88 [480/7471 (6%)]\tLoss: 1479291.500000\n",
            "Train Epoch: 88 [640/7471 (9%)]\tLoss: 1628376.625000\n",
            "Train Epoch: 88 [800/7471 (11%)]\tLoss: 1597183.750000\n",
            "Train Epoch: 88 [960/7471 (13%)]\tLoss: 1559517.625000\n",
            "Train Epoch: 88 [1120/7471 (15%)]\tLoss: 1632966.875000\n",
            "Train Epoch: 88 [1280/7471 (17%)]\tLoss: 1607754.250000\n",
            "Train Epoch: 88 [1440/7471 (19%)]\tLoss: 1562722.250000\n",
            "Train Epoch: 88 [1600/7471 (21%)]\tLoss: 1587572.625000\n",
            "Train Epoch: 88 [1760/7471 (24%)]\tLoss: 1622936.000000\n",
            "Train Epoch: 88 [1920/7471 (26%)]\tLoss: 1560106.250000\n",
            "Train Epoch: 88 [2080/7471 (28%)]\tLoss: 1568418.375000\n",
            "Train Epoch: 88 [2240/7471 (30%)]\tLoss: 1498868.125000\n",
            "Train Epoch: 88 [2400/7471 (32%)]\tLoss: 1558863.125000\n",
            "Train Epoch: 88 [2560/7471 (34%)]\tLoss: 1635278.625000\n",
            "Train Epoch: 88 [2720/7471 (36%)]\tLoss: 1539454.000000\n",
            "Train Epoch: 88 [2880/7471 (39%)]\tLoss: 1584402.375000\n",
            "Train Epoch: 88 [3040/7471 (41%)]\tLoss: 1496897.500000\n",
            "Train Epoch: 88 [3200/7471 (43%)]\tLoss: 1570439.875000\n",
            "Train Epoch: 88 [3360/7471 (45%)]\tLoss: 1589065.625000\n",
            "Train Epoch: 88 [3520/7471 (47%)]\tLoss: 1598213.625000\n",
            "Train Epoch: 88 [3680/7471 (49%)]\tLoss: 1517627.625000\n",
            "Train Epoch: 88 [3840/7471 (51%)]\tLoss: 1611328.875000\n",
            "Train Epoch: 88 [4000/7471 (54%)]\tLoss: 1519245.500000\n",
            "Train Epoch: 88 [4160/7471 (56%)]\tLoss: 1597887.750000\n",
            "Train Epoch: 88 [4320/7471 (58%)]\tLoss: 1628403.500000\n",
            "Train Epoch: 88 [4480/7471 (60%)]\tLoss: 1540903.375000\n",
            "Train Epoch: 88 [4640/7471 (62%)]\tLoss: 1482134.500000\n",
            "Train Epoch: 88 [4800/7471 (64%)]\tLoss: 1548918.000000\n",
            "Train Epoch: 88 [4960/7471 (66%)]\tLoss: 1417743.000000\n",
            "Train Epoch: 88 [5120/7471 (69%)]\tLoss: 1572141.500000\n",
            "Train Epoch: 88 [5280/7471 (71%)]\tLoss: 1573680.000000\n",
            "Train Epoch: 88 [5440/7471 (73%)]\tLoss: 1541394.250000\n",
            "Train Epoch: 88 [5600/7471 (75%)]\tLoss: 1615438.875000\n",
            "Train Epoch: 88 [5760/7471 (77%)]\tLoss: 1581971.375000\n",
            "Train Epoch: 88 [5920/7471 (79%)]\tLoss: 1578495.250000\n",
            "Train Epoch: 88 [6080/7471 (81%)]\tLoss: 1549288.625000\n",
            "Train Epoch: 88 [6240/7471 (84%)]\tLoss: 1584275.000000\n",
            "Train Epoch: 88 [6400/7471 (86%)]\tLoss: 1529769.750000\n",
            "Train Epoch: 88 [6560/7471 (88%)]\tLoss: 1598800.750000\n",
            "Train Epoch: 88 [6720/7471 (90%)]\tLoss: 1554227.500000\n",
            "Train Epoch: 88 [6880/7471 (92%)]\tLoss: 1605949.875000\n",
            "Train Epoch: 88 [7040/7471 (94%)]\tLoss: 1532701.250000\n",
            "Train Epoch: 88 [7200/7471 (96%)]\tLoss: 1512796.000000\n",
            "Train Epoch: 88 [7360/7471 (99%)]\tLoss: 1612351.125000\n",
            "Epoch 88 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98433.7928\n",
            "\n",
            "Train Epoch: 89 [160/7471 (2%)]\tLoss: 1543269.125000\n",
            "Train Epoch: 89 [320/7471 (4%)]\tLoss: 1609692.750000\n",
            "Train Epoch: 89 [480/7471 (6%)]\tLoss: 1604511.000000\n",
            "Train Epoch: 89 [640/7471 (9%)]\tLoss: 1554401.250000\n",
            "Train Epoch: 89 [800/7471 (11%)]\tLoss: 1470431.000000\n",
            "Train Epoch: 89 [960/7471 (13%)]\tLoss: 1609683.625000\n",
            "Train Epoch: 89 [1120/7471 (15%)]\tLoss: 1633988.125000\n",
            "Train Epoch: 89 [1280/7471 (17%)]\tLoss: 1527409.500000\n",
            "Train Epoch: 89 [1440/7471 (19%)]\tLoss: 1571382.125000\n",
            "Train Epoch: 89 [1600/7471 (21%)]\tLoss: 1619583.250000\n",
            "Train Epoch: 89 [1760/7471 (24%)]\tLoss: 1584955.875000\n",
            "Train Epoch: 89 [1920/7471 (26%)]\tLoss: 1618465.875000\n",
            "Train Epoch: 89 [2080/7471 (28%)]\tLoss: 1582989.125000\n",
            "Train Epoch: 89 [2240/7471 (30%)]\tLoss: 1562199.250000\n",
            "Train Epoch: 89 [2400/7471 (32%)]\tLoss: 1627650.250000\n",
            "Train Epoch: 89 [2560/7471 (34%)]\tLoss: 1582782.625000\n",
            "Train Epoch: 89 [2720/7471 (36%)]\tLoss: 1612421.625000\n",
            "Train Epoch: 89 [2880/7471 (39%)]\tLoss: 1626647.250000\n",
            "Train Epoch: 89 [3040/7471 (41%)]\tLoss: 1591710.750000\n",
            "Train Epoch: 89 [3200/7471 (43%)]\tLoss: 1545631.625000\n",
            "Train Epoch: 89 [3360/7471 (45%)]\tLoss: 1605580.000000\n",
            "Train Epoch: 89 [3520/7471 (47%)]\tLoss: 1600222.000000\n",
            "Train Epoch: 89 [3680/7471 (49%)]\tLoss: 1587566.375000\n",
            "Train Epoch: 89 [3840/7471 (51%)]\tLoss: 1544179.625000\n",
            "Train Epoch: 89 [4000/7471 (54%)]\tLoss: 1492735.250000\n",
            "Train Epoch: 89 [4160/7471 (56%)]\tLoss: 1593933.875000\n",
            "Train Epoch: 89 [4320/7471 (58%)]\tLoss: 1570397.125000\n",
            "Train Epoch: 89 [4480/7471 (60%)]\tLoss: 1617943.875000\n",
            "Train Epoch: 89 [4640/7471 (62%)]\tLoss: 1503263.750000\n",
            "Train Epoch: 89 [4800/7471 (64%)]\tLoss: 1573418.500000\n",
            "Train Epoch: 89 [4960/7471 (66%)]\tLoss: 1592846.000000\n",
            "Train Epoch: 89 [5120/7471 (69%)]\tLoss: 1563313.500000\n",
            "Train Epoch: 89 [5280/7471 (71%)]\tLoss: 1534630.125000\n",
            "Train Epoch: 89 [5440/7471 (73%)]\tLoss: 1577041.000000\n",
            "Train Epoch: 89 [5600/7471 (75%)]\tLoss: 1578226.000000\n",
            "Train Epoch: 89 [5760/7471 (77%)]\tLoss: 1560525.125000\n",
            "Train Epoch: 89 [5920/7471 (79%)]\tLoss: 1600989.000000\n",
            "Train Epoch: 89 [6080/7471 (81%)]\tLoss: 1516444.875000\n",
            "Train Epoch: 89 [6240/7471 (84%)]\tLoss: 1594647.375000\n",
            "Train Epoch: 89 [6400/7471 (86%)]\tLoss: 1547038.625000\n",
            "Train Epoch: 89 [6560/7471 (88%)]\tLoss: 1599565.000000\n",
            "Train Epoch: 89 [6720/7471 (90%)]\tLoss: 1596027.625000\n",
            "Train Epoch: 89 [6880/7471 (92%)]\tLoss: 1545694.125000\n",
            "Train Epoch: 89 [7040/7471 (94%)]\tLoss: 1578138.625000\n",
            "Train Epoch: 89 [7200/7471 (96%)]\tLoss: 1583309.375000\n",
            "Train Epoch: 89 [7360/7471 (99%)]\tLoss: 1589730.250000\n",
            "Epoch 89 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98442.6701\n",
            "\n",
            "Train Epoch: 90 [160/7471 (2%)]\tLoss: 1582542.125000\n",
            "Train Epoch: 90 [320/7471 (4%)]\tLoss: 1545481.750000\n",
            "Train Epoch: 90 [480/7471 (6%)]\tLoss: 1577856.375000\n",
            "Train Epoch: 90 [640/7471 (9%)]\tLoss: 1564905.250000\n",
            "Train Epoch: 90 [800/7471 (11%)]\tLoss: 1606089.750000\n",
            "Train Epoch: 90 [960/7471 (13%)]\tLoss: 1622695.750000\n",
            "Train Epoch: 90 [1120/7471 (15%)]\tLoss: 1582087.875000\n",
            "Train Epoch: 90 [1280/7471 (17%)]\tLoss: 1524199.125000\n",
            "Train Epoch: 90 [1440/7471 (19%)]\tLoss: 1545055.500000\n",
            "Train Epoch: 90 [1600/7471 (21%)]\tLoss: 1562655.875000\n",
            "Train Epoch: 90 [1760/7471 (24%)]\tLoss: 1610046.625000\n",
            "Train Epoch: 90 [1920/7471 (26%)]\tLoss: 1508068.750000\n",
            "Train Epoch: 90 [2080/7471 (28%)]\tLoss: 1556983.250000\n",
            "Train Epoch: 90 [2240/7471 (30%)]\tLoss: 1632632.125000\n",
            "Train Epoch: 90 [2400/7471 (32%)]\tLoss: 1568108.750000\n",
            "Train Epoch: 90 [2560/7471 (34%)]\tLoss: 1577853.250000\n",
            "Train Epoch: 90 [2720/7471 (36%)]\tLoss: 1561643.125000\n",
            "Train Epoch: 90 [2880/7471 (39%)]\tLoss: 1535627.875000\n",
            "Train Epoch: 90 [3040/7471 (41%)]\tLoss: 1593610.875000\n",
            "Train Epoch: 90 [3200/7471 (43%)]\tLoss: 1579314.375000\n",
            "Train Epoch: 90 [3360/7471 (45%)]\tLoss: 1571885.375000\n",
            "Train Epoch: 90 [3520/7471 (47%)]\tLoss: 1585744.250000\n",
            "Train Epoch: 90 [3680/7471 (49%)]\tLoss: 1572111.250000\n",
            "Train Epoch: 90 [3840/7471 (51%)]\tLoss: 1565837.375000\n",
            "Train Epoch: 90 [4000/7471 (54%)]\tLoss: 1617641.875000\n",
            "Train Epoch: 90 [4160/7471 (56%)]\tLoss: 1474609.000000\n",
            "Train Epoch: 90 [4320/7471 (58%)]\tLoss: 1559529.625000\n",
            "Train Epoch: 90 [4480/7471 (60%)]\tLoss: 1588875.000000\n",
            "Train Epoch: 90 [4640/7471 (62%)]\tLoss: 1552453.875000\n",
            "Train Epoch: 90 [4800/7471 (64%)]\tLoss: 1582668.125000\n",
            "Train Epoch: 90 [4960/7471 (66%)]\tLoss: 1563310.250000\n",
            "Train Epoch: 90 [5120/7471 (69%)]\tLoss: 1593605.500000\n",
            "Train Epoch: 90 [5280/7471 (71%)]\tLoss: 1570361.125000\n",
            "Train Epoch: 90 [5440/7471 (73%)]\tLoss: 1571690.500000\n",
            "Train Epoch: 90 [5600/7471 (75%)]\tLoss: 1615115.250000\n",
            "Train Epoch: 90 [5760/7471 (77%)]\tLoss: 1630172.625000\n",
            "Train Epoch: 90 [5920/7471 (79%)]\tLoss: 1613107.125000\n",
            "Train Epoch: 90 [6080/7471 (81%)]\tLoss: 1569674.000000\n",
            "Train Epoch: 90 [6240/7471 (84%)]\tLoss: 1599150.750000\n",
            "Train Epoch: 90 [6400/7471 (86%)]\tLoss: 1549097.375000\n",
            "Train Epoch: 90 [6560/7471 (88%)]\tLoss: 1505037.875000\n",
            "Train Epoch: 90 [6720/7471 (90%)]\tLoss: 1559921.250000\n",
            "Train Epoch: 90 [6880/7471 (92%)]\tLoss: 1583059.625000\n",
            "Train Epoch: 90 [7040/7471 (94%)]\tLoss: 1563462.375000\n",
            "Train Epoch: 90 [7200/7471 (96%)]\tLoss: 1625937.750000\n",
            "Train Epoch: 90 [7360/7471 (99%)]\tLoss: 1572733.750000\n",
            "Epoch 90 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98463.4076\n",
            "\n",
            "Train Epoch: 91 [160/7471 (2%)]\tLoss: 1607634.125000\n",
            "Train Epoch: 91 [320/7471 (4%)]\tLoss: 1565086.375000\n",
            "Train Epoch: 91 [480/7471 (6%)]\tLoss: 1603336.375000\n",
            "Train Epoch: 91 [640/7471 (9%)]\tLoss: 1484555.125000\n",
            "Train Epoch: 91 [800/7471 (11%)]\tLoss: 1617050.750000\n",
            "Train Epoch: 91 [960/7471 (13%)]\tLoss: 1581393.625000\n",
            "Train Epoch: 91 [1120/7471 (15%)]\tLoss: 1550789.625000\n",
            "Train Epoch: 91 [1280/7471 (17%)]\tLoss: 1556392.625000\n",
            "Train Epoch: 91 [1440/7471 (19%)]\tLoss: 1615352.125000\n",
            "Train Epoch: 91 [1600/7471 (21%)]\tLoss: 1619191.625000\n",
            "Train Epoch: 91 [1760/7471 (24%)]\tLoss: 1551182.000000\n",
            "Train Epoch: 91 [1920/7471 (26%)]\tLoss: 1566169.000000\n",
            "Train Epoch: 91 [2080/7471 (28%)]\tLoss: 1561045.000000\n",
            "Train Epoch: 91 [2240/7471 (30%)]\tLoss: 1545766.000000\n",
            "Train Epoch: 91 [2400/7471 (32%)]\tLoss: 1573765.750000\n",
            "Train Epoch: 91 [2560/7471 (34%)]\tLoss: 1621935.625000\n",
            "Train Epoch: 91 [2720/7471 (36%)]\tLoss: 1491550.250000\n",
            "Train Epoch: 91 [2880/7471 (39%)]\tLoss: 1578940.000000\n",
            "Train Epoch: 91 [3040/7471 (41%)]\tLoss: 1588658.125000\n",
            "Train Epoch: 91 [3200/7471 (43%)]\tLoss: 1563838.750000\n",
            "Train Epoch: 91 [3360/7471 (45%)]\tLoss: 1545138.375000\n",
            "Train Epoch: 91 [3520/7471 (47%)]\tLoss: 1519119.875000\n",
            "Train Epoch: 91 [3680/7471 (49%)]\tLoss: 1517001.250000\n",
            "Train Epoch: 91 [3840/7471 (51%)]\tLoss: 1638128.375000\n",
            "Train Epoch: 91 [4000/7471 (54%)]\tLoss: 1544661.500000\n",
            "Train Epoch: 91 [4160/7471 (56%)]\tLoss: 1627224.750000\n",
            "Train Epoch: 91 [4320/7471 (58%)]\tLoss: 1580023.625000\n",
            "Train Epoch: 91 [4480/7471 (60%)]\tLoss: 1629746.000000\n",
            "Train Epoch: 91 [4640/7471 (62%)]\tLoss: 1554256.250000\n",
            "Train Epoch: 91 [4800/7471 (64%)]\tLoss: 1607939.625000\n",
            "Train Epoch: 91 [4960/7471 (66%)]\tLoss: 1575751.875000\n",
            "Train Epoch: 91 [5120/7471 (69%)]\tLoss: 1546332.250000\n",
            "Train Epoch: 91 [5280/7471 (71%)]\tLoss: 1617320.375000\n",
            "Train Epoch: 91 [5440/7471 (73%)]\tLoss: 1599555.500000\n",
            "Train Epoch: 91 [5600/7471 (75%)]\tLoss: 1526741.875000\n",
            "Train Epoch: 91 [5760/7471 (77%)]\tLoss: 1544829.375000\n",
            "Train Epoch: 91 [5920/7471 (79%)]\tLoss: 1619966.750000\n",
            "Train Epoch: 91 [6080/7471 (81%)]\tLoss: 1507392.250000\n",
            "Train Epoch: 91 [6240/7471 (84%)]\tLoss: 1580333.750000\n",
            "Train Epoch: 91 [6400/7471 (86%)]\tLoss: 1526373.250000\n",
            "Train Epoch: 91 [6560/7471 (88%)]\tLoss: 1587410.375000\n",
            "Train Epoch: 91 [6720/7471 (90%)]\tLoss: 1627550.000000\n",
            "Train Epoch: 91 [6880/7471 (92%)]\tLoss: 1619702.250000\n",
            "Train Epoch: 91 [7040/7471 (94%)]\tLoss: 1552357.250000\n",
            "Train Epoch: 91 [7200/7471 (96%)]\tLoss: 1584636.375000\n",
            "Train Epoch: 91 [7360/7471 (99%)]\tLoss: 1608614.750000\n",
            "Epoch 91 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98473.8756\n",
            "\n",
            "Train Epoch: 92 [160/7471 (2%)]\tLoss: 1564537.125000\n",
            "Train Epoch: 92 [320/7471 (4%)]\tLoss: 1614207.125000\n",
            "Train Epoch: 92 [480/7471 (6%)]\tLoss: 1578372.625000\n",
            "Train Epoch: 92 [640/7471 (9%)]\tLoss: 1611137.625000\n",
            "Train Epoch: 92 [800/7471 (11%)]\tLoss: 1558831.125000\n",
            "Train Epoch: 92 [960/7471 (13%)]\tLoss: 1564077.625000\n",
            "Train Epoch: 92 [1120/7471 (15%)]\tLoss: 1516363.750000\n",
            "Train Epoch: 92 [1280/7471 (17%)]\tLoss: 1619175.250000\n",
            "Train Epoch: 92 [1440/7471 (19%)]\tLoss: 1595575.375000\n",
            "Train Epoch: 92 [1600/7471 (21%)]\tLoss: 1548647.750000\n",
            "Train Epoch: 92 [1760/7471 (24%)]\tLoss: 1603641.375000\n",
            "Train Epoch: 92 [1920/7471 (26%)]\tLoss: 1610762.625000\n",
            "Train Epoch: 92 [2080/7471 (28%)]\tLoss: 1510929.125000\n",
            "Train Epoch: 92 [2240/7471 (30%)]\tLoss: 1582603.000000\n",
            "Train Epoch: 92 [2400/7471 (32%)]\tLoss: 1593866.500000\n",
            "Train Epoch: 92 [2560/7471 (34%)]\tLoss: 1532696.875000\n",
            "Train Epoch: 92 [2720/7471 (36%)]\tLoss: 1607348.750000\n",
            "Train Epoch: 92 [2880/7471 (39%)]\tLoss: 1564938.250000\n",
            "Train Epoch: 92 [3040/7471 (41%)]\tLoss: 1590881.625000\n",
            "Train Epoch: 92 [3200/7471 (43%)]\tLoss: 1609967.125000\n",
            "Train Epoch: 92 [3360/7471 (45%)]\tLoss: 1620000.625000\n",
            "Train Epoch: 92 [3520/7471 (47%)]\tLoss: 1583417.375000\n",
            "Train Epoch: 92 [3680/7471 (49%)]\tLoss: 1547950.375000\n",
            "Train Epoch: 92 [3840/7471 (51%)]\tLoss: 1580545.875000\n",
            "Train Epoch: 92 [4000/7471 (54%)]\tLoss: 1494219.500000\n",
            "Train Epoch: 92 [4160/7471 (56%)]\tLoss: 1589668.625000\n",
            "Train Epoch: 92 [4320/7471 (58%)]\tLoss: 1592585.000000\n",
            "Train Epoch: 92 [4480/7471 (60%)]\tLoss: 1598769.750000\n",
            "Train Epoch: 92 [4640/7471 (62%)]\tLoss: 1619406.250000\n",
            "Train Epoch: 92 [4800/7471 (64%)]\tLoss: 1575601.750000\n",
            "Train Epoch: 92 [4960/7471 (66%)]\tLoss: 1548147.375000\n",
            "Train Epoch: 92 [5120/7471 (69%)]\tLoss: 1628643.125000\n",
            "Train Epoch: 92 [5280/7471 (71%)]\tLoss: 1585569.250000\n",
            "Train Epoch: 92 [5440/7471 (73%)]\tLoss: 1600776.500000\n",
            "Train Epoch: 92 [5600/7471 (75%)]\tLoss: 1616187.750000\n",
            "Train Epoch: 92 [5760/7471 (77%)]\tLoss: 1594676.250000\n",
            "Train Epoch: 92 [5920/7471 (79%)]\tLoss: 1603516.875000\n",
            "Train Epoch: 92 [6080/7471 (81%)]\tLoss: 1595249.375000\n",
            "Train Epoch: 92 [6240/7471 (84%)]\tLoss: 1565824.125000\n",
            "Train Epoch: 92 [6400/7471 (86%)]\tLoss: 1544123.000000\n",
            "Train Epoch: 92 [6560/7471 (88%)]\tLoss: 1614792.625000\n",
            "Train Epoch: 92 [6720/7471 (90%)]\tLoss: 1574481.375000\n",
            "Train Epoch: 92 [6880/7471 (92%)]\tLoss: 1594961.125000\n",
            "Train Epoch: 92 [7040/7471 (94%)]\tLoss: 1572018.375000\n",
            "Train Epoch: 92 [7200/7471 (96%)]\tLoss: 1600984.875000\n",
            "Train Epoch: 92 [7360/7471 (99%)]\tLoss: 1530855.500000\n",
            "Epoch 92 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98465.5270\n",
            "\n",
            "Train Epoch: 93 [160/7471 (2%)]\tLoss: 1635728.250000\n",
            "Train Epoch: 93 [320/7471 (4%)]\tLoss: 1517170.750000\n",
            "Train Epoch: 93 [480/7471 (6%)]\tLoss: 1564072.375000\n",
            "Train Epoch: 93 [640/7471 (9%)]\tLoss: 1578786.750000\n",
            "Train Epoch: 93 [800/7471 (11%)]\tLoss: 1560762.500000\n",
            "Train Epoch: 93 [960/7471 (13%)]\tLoss: 1487543.750000\n",
            "Train Epoch: 93 [1120/7471 (15%)]\tLoss: 1540907.000000\n",
            "Train Epoch: 93 [1280/7471 (17%)]\tLoss: 1599896.250000\n",
            "Train Epoch: 93 [1440/7471 (19%)]\tLoss: 1602244.000000\n",
            "Train Epoch: 93 [1600/7471 (21%)]\tLoss: 1620847.375000\n",
            "Train Epoch: 93 [1760/7471 (24%)]\tLoss: 1537863.375000\n",
            "Train Epoch: 93 [1920/7471 (26%)]\tLoss: 1591346.375000\n",
            "Train Epoch: 93 [2080/7471 (28%)]\tLoss: 1556067.875000\n",
            "Train Epoch: 93 [2240/7471 (30%)]\tLoss: 1597585.375000\n",
            "Train Epoch: 93 [2400/7471 (32%)]\tLoss: 1529660.375000\n",
            "Train Epoch: 93 [2560/7471 (34%)]\tLoss: 1589395.875000\n",
            "Train Epoch: 93 [2720/7471 (36%)]\tLoss: 1631595.750000\n",
            "Train Epoch: 93 [2880/7471 (39%)]\tLoss: 1601191.125000\n",
            "Train Epoch: 93 [3040/7471 (41%)]\tLoss: 1554682.750000\n",
            "Train Epoch: 93 [3200/7471 (43%)]\tLoss: 1575053.375000\n",
            "Train Epoch: 93 [3360/7471 (45%)]\tLoss: 1531543.375000\n",
            "Train Epoch: 93 [3520/7471 (47%)]\tLoss: 1534141.750000\n",
            "Train Epoch: 93 [3680/7471 (49%)]\tLoss: 1641479.125000\n",
            "Train Epoch: 93 [3840/7471 (51%)]\tLoss: 1600119.375000\n",
            "Train Epoch: 93 [4000/7471 (54%)]\tLoss: 1613320.125000\n",
            "Train Epoch: 93 [4160/7471 (56%)]\tLoss: 1543837.875000\n",
            "Train Epoch: 93 [4320/7471 (58%)]\tLoss: 1604082.250000\n",
            "Train Epoch: 93 [4480/7471 (60%)]\tLoss: 1572296.625000\n",
            "Train Epoch: 93 [4640/7471 (62%)]\tLoss: 1594246.250000\n",
            "Train Epoch: 93 [4800/7471 (64%)]\tLoss: 1526482.875000\n",
            "Train Epoch: 93 [4960/7471 (66%)]\tLoss: 1562529.000000\n",
            "Train Epoch: 93 [5120/7471 (69%)]\tLoss: 1567051.250000\n",
            "Train Epoch: 93 [5280/7471 (71%)]\tLoss: 1542222.875000\n",
            "Train Epoch: 93 [5440/7471 (73%)]\tLoss: 1614915.875000\n",
            "Train Epoch: 93 [5600/7471 (75%)]\tLoss: 1573429.000000\n",
            "Train Epoch: 93 [5760/7471 (77%)]\tLoss: 1538793.000000\n",
            "Train Epoch: 93 [5920/7471 (79%)]\tLoss: 1579806.875000\n",
            "Train Epoch: 93 [6080/7471 (81%)]\tLoss: 1621271.375000\n",
            "Train Epoch: 93 [6240/7471 (84%)]\tLoss: 1561193.750000\n",
            "Train Epoch: 93 [6400/7471 (86%)]\tLoss: 1601599.875000\n",
            "Train Epoch: 93 [6560/7471 (88%)]\tLoss: 1627093.625000\n",
            "Train Epoch: 93 [6720/7471 (90%)]\tLoss: 1589359.625000\n",
            "Train Epoch: 93 [6880/7471 (92%)]\tLoss: 1622143.750000\n",
            "Train Epoch: 93 [7040/7471 (94%)]\tLoss: 1575909.750000\n",
            "Train Epoch: 93 [7200/7471 (96%)]\tLoss: 1551324.500000\n",
            "Train Epoch: 93 [7360/7471 (99%)]\tLoss: 1559093.625000\n",
            "Epoch 93 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98444.7675\n",
            "\n",
            "Train Epoch: 94 [160/7471 (2%)]\tLoss: 1528345.750000\n",
            "Train Epoch: 94 [320/7471 (4%)]\tLoss: 1512573.750000\n",
            "Train Epoch: 94 [480/7471 (6%)]\tLoss: 1613015.625000\n",
            "Train Epoch: 94 [640/7471 (9%)]\tLoss: 1559995.250000\n",
            "Train Epoch: 94 [800/7471 (11%)]\tLoss: 1572908.750000\n",
            "Train Epoch: 94 [960/7471 (13%)]\tLoss: 1615278.250000\n",
            "Train Epoch: 94 [1120/7471 (15%)]\tLoss: 1511891.625000\n",
            "Train Epoch: 94 [1280/7471 (17%)]\tLoss: 1569411.875000\n",
            "Train Epoch: 94 [1440/7471 (19%)]\tLoss: 1576463.000000\n",
            "Train Epoch: 94 [1600/7471 (21%)]\tLoss: 1567419.625000\n",
            "Train Epoch: 94 [1760/7471 (24%)]\tLoss: 1569806.375000\n",
            "Train Epoch: 94 [1920/7471 (26%)]\tLoss: 1569929.250000\n",
            "Train Epoch: 94 [2080/7471 (28%)]\tLoss: 1597311.125000\n",
            "Train Epoch: 94 [2240/7471 (30%)]\tLoss: 1576433.125000\n",
            "Train Epoch: 94 [2400/7471 (32%)]\tLoss: 1563867.500000\n",
            "Train Epoch: 94 [2560/7471 (34%)]\tLoss: 1439818.125000\n",
            "Train Epoch: 94 [2720/7471 (36%)]\tLoss: 1562870.625000\n",
            "Train Epoch: 94 [2880/7471 (39%)]\tLoss: 1525697.625000\n",
            "Train Epoch: 94 [3040/7471 (41%)]\tLoss: 1532642.375000\n",
            "Train Epoch: 94 [3200/7471 (43%)]\tLoss: 1560955.000000\n",
            "Train Epoch: 94 [3360/7471 (45%)]\tLoss: 1510391.500000\n",
            "Train Epoch: 94 [3520/7471 (47%)]\tLoss: 1491343.000000\n",
            "Train Epoch: 94 [3680/7471 (49%)]\tLoss: 1530540.250000\n",
            "Train Epoch: 94 [3840/7471 (51%)]\tLoss: 1560963.125000\n",
            "Train Epoch: 94 [4000/7471 (54%)]\tLoss: 1619856.500000\n",
            "Train Epoch: 94 [4160/7471 (56%)]\tLoss: 1571285.375000\n",
            "Train Epoch: 94 [4320/7471 (58%)]\tLoss: 1570078.250000\n",
            "Train Epoch: 94 [4480/7471 (60%)]\tLoss: 1482132.625000\n",
            "Train Epoch: 94 [4640/7471 (62%)]\tLoss: 1572701.750000\n",
            "Train Epoch: 94 [4800/7471 (64%)]\tLoss: 1634081.000000\n",
            "Train Epoch: 94 [4960/7471 (66%)]\tLoss: 1512120.125000\n",
            "Train Epoch: 94 [5120/7471 (69%)]\tLoss: 1626701.000000\n",
            "Train Epoch: 94 [5280/7471 (71%)]\tLoss: 1590867.375000\n",
            "Train Epoch: 94 [5440/7471 (73%)]\tLoss: 1598024.375000\n",
            "Train Epoch: 94 [5600/7471 (75%)]\tLoss: 1549037.500000\n",
            "Train Epoch: 94 [5760/7471 (77%)]\tLoss: 1590105.500000\n",
            "Train Epoch: 94 [5920/7471 (79%)]\tLoss: 1545227.125000\n",
            "Train Epoch: 94 [6080/7471 (81%)]\tLoss: 1583747.375000\n",
            "Train Epoch: 94 [6240/7471 (84%)]\tLoss: 1622312.750000\n",
            "Train Epoch: 94 [6400/7471 (86%)]\tLoss: 1562346.750000\n",
            "Train Epoch: 94 [6560/7471 (88%)]\tLoss: 1629913.375000\n",
            "Train Epoch: 94 [6720/7471 (90%)]\tLoss: 1576526.000000\n",
            "Train Epoch: 94 [6880/7471 (92%)]\tLoss: 1511802.500000\n",
            "Train Epoch: 94 [7040/7471 (94%)]\tLoss: 1592893.375000\n",
            "Train Epoch: 94 [7200/7471 (96%)]\tLoss: 1577985.250000\n",
            "Train Epoch: 94 [7360/7471 (99%)]\tLoss: 1591320.625000\n",
            "Epoch 94 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98434.8281\n",
            "\n",
            "Train Epoch: 95 [160/7471 (2%)]\tLoss: 1597101.000000\n",
            "Train Epoch: 95 [320/7471 (4%)]\tLoss: 1557706.375000\n",
            "Train Epoch: 95 [480/7471 (6%)]\tLoss: 1613305.000000\n",
            "Train Epoch: 95 [640/7471 (9%)]\tLoss: 1607950.500000\n",
            "Train Epoch: 95 [800/7471 (11%)]\tLoss: 1625206.125000\n",
            "Train Epoch: 95 [960/7471 (13%)]\tLoss: 1539572.625000\n",
            "Train Epoch: 95 [1120/7471 (15%)]\tLoss: 1474848.000000\n",
            "Train Epoch: 95 [1280/7471 (17%)]\tLoss: 1582932.625000\n",
            "Train Epoch: 95 [1440/7471 (19%)]\tLoss: 1600367.875000\n",
            "Train Epoch: 95 [1600/7471 (21%)]\tLoss: 1603795.375000\n",
            "Train Epoch: 95 [1760/7471 (24%)]\tLoss: 1590701.375000\n",
            "Train Epoch: 95 [1920/7471 (26%)]\tLoss: 1579713.375000\n",
            "Train Epoch: 95 [2080/7471 (28%)]\tLoss: 1628841.500000\n",
            "Train Epoch: 95 [2240/7471 (30%)]\tLoss: 1541296.625000\n",
            "Train Epoch: 95 [2400/7471 (32%)]\tLoss: 1574358.125000\n",
            "Train Epoch: 95 [2560/7471 (34%)]\tLoss: 1524317.750000\n",
            "Train Epoch: 95 [2720/7471 (36%)]\tLoss: 1560030.750000\n",
            "Train Epoch: 95 [2880/7471 (39%)]\tLoss: 1521047.500000\n",
            "Train Epoch: 95 [3040/7471 (41%)]\tLoss: 1524025.875000\n",
            "Train Epoch: 95 [3200/7471 (43%)]\tLoss: 1550870.750000\n",
            "Train Epoch: 95 [3360/7471 (45%)]\tLoss: 1531009.125000\n",
            "Train Epoch: 95 [3520/7471 (47%)]\tLoss: 1572848.500000\n",
            "Train Epoch: 95 [3680/7471 (49%)]\tLoss: 1552562.375000\n",
            "Train Epoch: 95 [3840/7471 (51%)]\tLoss: 1553109.000000\n",
            "Train Epoch: 95 [4000/7471 (54%)]\tLoss: 1573882.875000\n",
            "Train Epoch: 95 [4160/7471 (56%)]\tLoss: 1613093.375000\n",
            "Train Epoch: 95 [4320/7471 (58%)]\tLoss: 1521323.625000\n",
            "Train Epoch: 95 [4480/7471 (60%)]\tLoss: 1608323.625000\n",
            "Train Epoch: 95 [4640/7471 (62%)]\tLoss: 1617182.500000\n",
            "Train Epoch: 95 [4800/7471 (64%)]\tLoss: 1612065.250000\n",
            "Train Epoch: 95 [4960/7471 (66%)]\tLoss: 1637708.000000\n",
            "Train Epoch: 95 [5120/7471 (69%)]\tLoss: 1563992.875000\n",
            "Train Epoch: 95 [5280/7471 (71%)]\tLoss: 1621675.625000\n",
            "Train Epoch: 95 [5440/7471 (73%)]\tLoss: 1564084.000000\n",
            "Train Epoch: 95 [5600/7471 (75%)]\tLoss: 1582732.250000\n",
            "Train Epoch: 95 [5760/7471 (77%)]\tLoss: 1633128.000000\n",
            "Train Epoch: 95 [5920/7471 (79%)]\tLoss: 1565364.750000\n",
            "Train Epoch: 95 [6080/7471 (81%)]\tLoss: 1603101.750000\n",
            "Train Epoch: 95 [6240/7471 (84%)]\tLoss: 1504314.875000\n",
            "Train Epoch: 95 [6400/7471 (86%)]\tLoss: 1488778.500000\n",
            "Train Epoch: 95 [6560/7471 (88%)]\tLoss: 1602722.500000\n",
            "Train Epoch: 95 [6720/7471 (90%)]\tLoss: 1589268.000000\n",
            "Train Epoch: 95 [6880/7471 (92%)]\tLoss: 1591887.250000\n",
            "Train Epoch: 95 [7040/7471 (94%)]\tLoss: 1585944.375000\n",
            "Train Epoch: 95 [7200/7471 (96%)]\tLoss: 1557664.750000\n",
            "Train Epoch: 95 [7360/7471 (99%)]\tLoss: 1572737.375000\n",
            "Epoch 95 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98435.7628\n",
            "\n",
            "Train Epoch: 96 [160/7471 (2%)]\tLoss: 1529906.375000\n",
            "Train Epoch: 96 [320/7471 (4%)]\tLoss: 1547458.625000\n",
            "Train Epoch: 96 [480/7471 (6%)]\tLoss: 1572138.875000\n",
            "Train Epoch: 96 [640/7471 (9%)]\tLoss: 1572203.625000\n",
            "Train Epoch: 96 [800/7471 (11%)]\tLoss: 1542554.375000\n",
            "Train Epoch: 96 [960/7471 (13%)]\tLoss: 1556196.625000\n",
            "Train Epoch: 96 [1120/7471 (15%)]\tLoss: 1628000.250000\n",
            "Train Epoch: 96 [1280/7471 (17%)]\tLoss: 1556829.250000\n",
            "Train Epoch: 96 [1440/7471 (19%)]\tLoss: 1585594.500000\n",
            "Train Epoch: 96 [1600/7471 (21%)]\tLoss: 1579845.500000\n",
            "Train Epoch: 96 [1760/7471 (24%)]\tLoss: 1493801.750000\n",
            "Train Epoch: 96 [1920/7471 (26%)]\tLoss: 1564001.500000\n",
            "Train Epoch: 96 [2080/7471 (28%)]\tLoss: 1592259.125000\n",
            "Train Epoch: 96 [2240/7471 (30%)]\tLoss: 1542479.000000\n",
            "Train Epoch: 96 [2400/7471 (32%)]\tLoss: 1537819.000000\n",
            "Train Epoch: 96 [2560/7471 (34%)]\tLoss: 1549188.500000\n",
            "Train Epoch: 96 [2720/7471 (36%)]\tLoss: 1520639.250000\n",
            "Train Epoch: 96 [2880/7471 (39%)]\tLoss: 1556920.375000\n",
            "Train Epoch: 96 [3040/7471 (41%)]\tLoss: 1573162.250000\n",
            "Train Epoch: 96 [3200/7471 (43%)]\tLoss: 1564828.125000\n",
            "Train Epoch: 96 [3360/7471 (45%)]\tLoss: 1523182.375000\n",
            "Train Epoch: 96 [3520/7471 (47%)]\tLoss: 1630648.000000\n",
            "Train Epoch: 96 [3680/7471 (49%)]\tLoss: 1588499.250000\n",
            "Train Epoch: 96 [3840/7471 (51%)]\tLoss: 1594118.125000\n",
            "Train Epoch: 96 [4000/7471 (54%)]\tLoss: 1605000.750000\n",
            "Train Epoch: 96 [4160/7471 (56%)]\tLoss: 1554552.000000\n",
            "Train Epoch: 96 [4320/7471 (58%)]\tLoss: 1588620.875000\n",
            "Train Epoch: 96 [4480/7471 (60%)]\tLoss: 1630138.000000\n",
            "Train Epoch: 96 [4640/7471 (62%)]\tLoss: 1511392.375000\n",
            "Train Epoch: 96 [4800/7471 (64%)]\tLoss: 1618069.375000\n",
            "Train Epoch: 96 [4960/7471 (66%)]\tLoss: 1549049.375000\n",
            "Train Epoch: 96 [5120/7471 (69%)]\tLoss: 1631757.250000\n",
            "Train Epoch: 96 [5280/7471 (71%)]\tLoss: 1609245.000000\n",
            "Train Epoch: 96 [5440/7471 (73%)]\tLoss: 1544146.500000\n",
            "Train Epoch: 96 [5600/7471 (75%)]\tLoss: 1629383.625000\n",
            "Train Epoch: 96 [5760/7471 (77%)]\tLoss: 1604044.250000\n",
            "Train Epoch: 96 [5920/7471 (79%)]\tLoss: 1620026.000000\n",
            "Train Epoch: 96 [6080/7471 (81%)]\tLoss: 1608797.875000\n",
            "Train Epoch: 96 [6240/7471 (84%)]\tLoss: 1544305.750000\n",
            "Train Epoch: 96 [6400/7471 (86%)]\tLoss: 1599473.500000\n",
            "Train Epoch: 96 [6560/7471 (88%)]\tLoss: 1539686.125000\n",
            "Train Epoch: 96 [6720/7471 (90%)]\tLoss: 1558159.125000\n",
            "Train Epoch: 96 [6880/7471 (92%)]\tLoss: 1577005.125000\n",
            "Train Epoch: 96 [7040/7471 (94%)]\tLoss: 1585502.625000\n",
            "Train Epoch: 96 [7200/7471 (96%)]\tLoss: 1610673.625000\n",
            "Train Epoch: 96 [7360/7471 (99%)]\tLoss: 1591725.625000\n",
            "Epoch 96 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98468.6730\n",
            "\n",
            "Train Epoch: 97 [160/7471 (2%)]\tLoss: 1626186.375000\n",
            "Train Epoch: 97 [320/7471 (4%)]\tLoss: 1617719.625000\n",
            "Train Epoch: 97 [480/7471 (6%)]\tLoss: 1549722.000000\n",
            "Train Epoch: 97 [640/7471 (9%)]\tLoss: 1617562.750000\n",
            "Train Epoch: 97 [800/7471 (11%)]\tLoss: 1592881.875000\n",
            "Train Epoch: 97 [960/7471 (13%)]\tLoss: 1456895.500000\n",
            "Train Epoch: 97 [1120/7471 (15%)]\tLoss: 1524315.250000\n",
            "Train Epoch: 97 [1280/7471 (17%)]\tLoss: 1578864.125000\n",
            "Train Epoch: 97 [1440/7471 (19%)]\tLoss: 1516618.000000\n",
            "Train Epoch: 97 [1600/7471 (21%)]\tLoss: 1626210.750000\n",
            "Train Epoch: 97 [1760/7471 (24%)]\tLoss: 1625874.375000\n",
            "Train Epoch: 97 [1920/7471 (26%)]\tLoss: 1547586.500000\n",
            "Train Epoch: 97 [2080/7471 (28%)]\tLoss: 1602562.500000\n",
            "Train Epoch: 97 [2240/7471 (30%)]\tLoss: 1562835.000000\n",
            "Train Epoch: 97 [2400/7471 (32%)]\tLoss: 1603000.250000\n",
            "Train Epoch: 97 [2560/7471 (34%)]\tLoss: 1553661.750000\n",
            "Train Epoch: 97 [2720/7471 (36%)]\tLoss: 1596159.875000\n",
            "Train Epoch: 97 [2880/7471 (39%)]\tLoss: 1550200.000000\n",
            "Train Epoch: 97 [3040/7471 (41%)]\tLoss: 1622686.875000\n",
            "Train Epoch: 97 [3200/7471 (43%)]\tLoss: 1563672.750000\n",
            "Train Epoch: 97 [3360/7471 (45%)]\tLoss: 1549867.875000\n",
            "Train Epoch: 97 [3520/7471 (47%)]\tLoss: 1573066.125000\n",
            "Train Epoch: 97 [3680/7471 (49%)]\tLoss: 1577150.625000\n",
            "Train Epoch: 97 [3840/7471 (51%)]\tLoss: 1583861.625000\n",
            "Train Epoch: 97 [4000/7471 (54%)]\tLoss: 1585603.250000\n",
            "Train Epoch: 97 [4160/7471 (56%)]\tLoss: 1551846.125000\n",
            "Train Epoch: 97 [4320/7471 (58%)]\tLoss: 1476634.625000\n",
            "Train Epoch: 97 [4480/7471 (60%)]\tLoss: 1601529.750000\n",
            "Train Epoch: 97 [4640/7471 (62%)]\tLoss: 1490386.375000\n",
            "Train Epoch: 97 [4800/7471 (64%)]\tLoss: 1572249.750000\n",
            "Train Epoch: 97 [4960/7471 (66%)]\tLoss: 1550135.875000\n",
            "Train Epoch: 97 [5120/7471 (69%)]\tLoss: 1514838.000000\n",
            "Train Epoch: 97 [5280/7471 (71%)]\tLoss: 1573461.000000\n",
            "Train Epoch: 97 [5440/7471 (73%)]\tLoss: 1550615.875000\n",
            "Train Epoch: 97 [5600/7471 (75%)]\tLoss: 1584710.000000\n",
            "Train Epoch: 97 [5760/7471 (77%)]\tLoss: 1513307.375000\n",
            "Train Epoch: 97 [5920/7471 (79%)]\tLoss: 1588630.875000\n",
            "Train Epoch: 97 [6080/7471 (81%)]\tLoss: 1576717.625000\n",
            "Train Epoch: 97 [6240/7471 (84%)]\tLoss: 1570419.625000\n",
            "Train Epoch: 97 [6400/7471 (86%)]\tLoss: 1592976.125000\n",
            "Train Epoch: 97 [6560/7471 (88%)]\tLoss: 1544147.875000\n",
            "Train Epoch: 97 [6720/7471 (90%)]\tLoss: 1585486.875000\n",
            "Train Epoch: 97 [6880/7471 (92%)]\tLoss: 1583296.875000\n",
            "Train Epoch: 97 [7040/7471 (94%)]\tLoss: 1461829.125000\n",
            "Train Epoch: 97 [7200/7471 (96%)]\tLoss: 1507475.000000\n",
            "Train Epoch: 97 [7360/7471 (99%)]\tLoss: 1594706.375000\n",
            "Epoch 97 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98525.7165\n",
            "\n",
            "Train Epoch: 98 [160/7471 (2%)]\tLoss: 1574534.250000\n",
            "Train Epoch: 98 [320/7471 (4%)]\tLoss: 1510457.250000\n",
            "Train Epoch: 98 [480/7471 (6%)]\tLoss: 1623423.750000\n",
            "Train Epoch: 98 [640/7471 (9%)]\tLoss: 1587943.875000\n",
            "Train Epoch: 98 [800/7471 (11%)]\tLoss: 1599030.875000\n",
            "Train Epoch: 98 [960/7471 (13%)]\tLoss: 1549109.250000\n",
            "Train Epoch: 98 [1120/7471 (15%)]\tLoss: 1630971.250000\n",
            "Train Epoch: 98 [1280/7471 (17%)]\tLoss: 1528924.625000\n",
            "Train Epoch: 98 [1440/7471 (19%)]\tLoss: 1601676.375000\n",
            "Train Epoch: 98 [1600/7471 (21%)]\tLoss: 1589057.500000\n",
            "Train Epoch: 98 [1760/7471 (24%)]\tLoss: 1566117.625000\n",
            "Train Epoch: 98 [1920/7471 (26%)]\tLoss: 1614176.625000\n",
            "Train Epoch: 98 [2080/7471 (28%)]\tLoss: 1588644.875000\n",
            "Train Epoch: 98 [2240/7471 (30%)]\tLoss: 1551461.750000\n",
            "Train Epoch: 98 [2400/7471 (32%)]\tLoss: 1584499.000000\n",
            "Train Epoch: 98 [2560/7471 (34%)]\tLoss: 1585488.500000\n",
            "Train Epoch: 98 [2720/7471 (36%)]\tLoss: 1609589.625000\n",
            "Train Epoch: 98 [2880/7471 (39%)]\tLoss: 1561305.125000\n",
            "Train Epoch: 98 [3040/7471 (41%)]\tLoss: 1613804.125000\n",
            "Train Epoch: 98 [3200/7471 (43%)]\tLoss: 1561054.000000\n",
            "Train Epoch: 98 [3360/7471 (45%)]\tLoss: 1503094.750000\n",
            "Train Epoch: 98 [3520/7471 (47%)]\tLoss: 1552439.500000\n",
            "Train Epoch: 98 [3680/7471 (49%)]\tLoss: 1594809.250000\n",
            "Train Epoch: 98 [3840/7471 (51%)]\tLoss: 1541831.750000\n",
            "Train Epoch: 98 [4000/7471 (54%)]\tLoss: 1593633.500000\n",
            "Train Epoch: 98 [4160/7471 (56%)]\tLoss: 1553759.875000\n",
            "Train Epoch: 98 [4320/7471 (58%)]\tLoss: 1593042.250000\n",
            "Train Epoch: 98 [4480/7471 (60%)]\tLoss: 1571399.250000\n",
            "Train Epoch: 98 [4640/7471 (62%)]\tLoss: 1628800.500000\n",
            "Train Epoch: 98 [4800/7471 (64%)]\tLoss: 1531672.500000\n",
            "Train Epoch: 98 [4960/7471 (66%)]\tLoss: 1501724.625000\n",
            "Train Epoch: 98 [5120/7471 (69%)]\tLoss: 1558545.125000\n",
            "Train Epoch: 98 [5280/7471 (71%)]\tLoss: 1557868.750000\n",
            "Train Epoch: 98 [5440/7471 (73%)]\tLoss: 1577409.250000\n",
            "Train Epoch: 98 [5600/7471 (75%)]\tLoss: 1561408.875000\n",
            "Train Epoch: 98 [5760/7471 (77%)]\tLoss: 1609849.875000\n",
            "Train Epoch: 98 [5920/7471 (79%)]\tLoss: 1599615.250000\n",
            "Train Epoch: 98 [6080/7471 (81%)]\tLoss: 1573246.500000\n",
            "Train Epoch: 98 [6240/7471 (84%)]\tLoss: 1524756.000000\n",
            "Train Epoch: 98 [6400/7471 (86%)]\tLoss: 1569628.750000\n",
            "Train Epoch: 98 [6560/7471 (88%)]\tLoss: 1586800.000000\n",
            "Train Epoch: 98 [6720/7471 (90%)]\tLoss: 1550272.250000\n",
            "Train Epoch: 98 [6880/7471 (92%)]\tLoss: 1552778.000000\n",
            "Train Epoch: 98 [7040/7471 (94%)]\tLoss: 1561837.000000\n",
            "Train Epoch: 98 [7200/7471 (96%)]\tLoss: 1584244.000000\n",
            "Train Epoch: 98 [7360/7471 (99%)]\tLoss: 1577333.625000\n",
            "Epoch 98 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98406.9307\n",
            "\n",
            "Train Epoch: 99 [160/7471 (2%)]\tLoss: 1586241.250000\n",
            "Train Epoch: 99 [320/7471 (4%)]\tLoss: 1544545.375000\n",
            "Train Epoch: 99 [480/7471 (6%)]\tLoss: 1615604.250000\n",
            "Train Epoch: 99 [640/7471 (9%)]\tLoss: 1619727.750000\n",
            "Train Epoch: 99 [800/7471 (11%)]\tLoss: 1577767.875000\n",
            "Train Epoch: 99 [960/7471 (13%)]\tLoss: 1560543.125000\n",
            "Train Epoch: 99 [1120/7471 (15%)]\tLoss: 1579481.500000\n",
            "Train Epoch: 99 [1280/7471 (17%)]\tLoss: 1612835.500000\n",
            "Train Epoch: 99 [1440/7471 (19%)]\tLoss: 1617744.000000\n",
            "Train Epoch: 99 [1600/7471 (21%)]\tLoss: 1624702.125000\n",
            "Train Epoch: 99 [1760/7471 (24%)]\tLoss: 1582522.125000\n",
            "Train Epoch: 99 [1920/7471 (26%)]\tLoss: 1619865.875000\n",
            "Train Epoch: 99 [2080/7471 (28%)]\tLoss: 1589063.375000\n",
            "Train Epoch: 99 [2240/7471 (30%)]\tLoss: 1505249.750000\n",
            "Train Epoch: 99 [2400/7471 (32%)]\tLoss: 1577477.125000\n",
            "Train Epoch: 99 [2560/7471 (34%)]\tLoss: 1505537.750000\n",
            "Train Epoch: 99 [2720/7471 (36%)]\tLoss: 1549556.000000\n",
            "Train Epoch: 99 [2880/7471 (39%)]\tLoss: 1611346.500000\n",
            "Train Epoch: 99 [3040/7471 (41%)]\tLoss: 1492504.375000\n",
            "Train Epoch: 99 [3200/7471 (43%)]\tLoss: 1624043.750000\n",
            "Train Epoch: 99 [3360/7471 (45%)]\tLoss: 1597789.625000\n",
            "Train Epoch: 99 [3520/7471 (47%)]\tLoss: 1559112.875000\n",
            "Train Epoch: 99 [3680/7471 (49%)]\tLoss: 1597139.625000\n",
            "Train Epoch: 99 [3840/7471 (51%)]\tLoss: 1596796.750000\n",
            "Train Epoch: 99 [4000/7471 (54%)]\tLoss: 1621146.500000\n",
            "Train Epoch: 99 [4160/7471 (56%)]\tLoss: 1573081.125000\n",
            "Train Epoch: 99 [4320/7471 (58%)]\tLoss: 1558127.375000\n",
            "Train Epoch: 99 [4480/7471 (60%)]\tLoss: 1529818.375000\n",
            "Train Epoch: 99 [4640/7471 (62%)]\tLoss: 1623619.125000\n",
            "Train Epoch: 99 [4800/7471 (64%)]\tLoss: 1532652.875000\n",
            "Train Epoch: 99 [4960/7471 (66%)]\tLoss: 1575077.625000\n",
            "Train Epoch: 99 [5120/7471 (69%)]\tLoss: 1541625.750000\n",
            "Train Epoch: 99 [5280/7471 (71%)]\tLoss: 1552932.750000\n",
            "Train Epoch: 99 [5440/7471 (73%)]\tLoss: 1585141.875000\n",
            "Train Epoch: 99 [5600/7471 (75%)]\tLoss: 1613520.375000\n",
            "Train Epoch: 99 [5760/7471 (77%)]\tLoss: 1497589.000000\n",
            "Train Epoch: 99 [5920/7471 (79%)]\tLoss: 1626544.500000\n",
            "Train Epoch: 99 [6080/7471 (81%)]\tLoss: 1622971.500000\n",
            "Train Epoch: 99 [6240/7471 (84%)]\tLoss: 1601050.375000\n",
            "Train Epoch: 99 [6400/7471 (86%)]\tLoss: 1554395.500000\n",
            "Train Epoch: 99 [6560/7471 (88%)]\tLoss: 1588460.375000\n",
            "Train Epoch: 99 [6720/7471 (90%)]\tLoss: 1639108.000000\n",
            "Train Epoch: 99 [6880/7471 (92%)]\tLoss: 1599797.875000\n",
            "Train Epoch: 99 [7040/7471 (94%)]\tLoss: 1553048.875000\n",
            "Train Epoch: 99 [7200/7471 (96%)]\tLoss: 1589609.125000\n",
            "Train Epoch: 99 [7360/7471 (99%)]\tLoss: 1607403.250000\n",
            "Epoch 99 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 99005.6195\n",
            "\n",
            "Train Epoch: 100 [160/7471 (2%)]\tLoss: 1557447.750000\n",
            "Train Epoch: 100 [320/7471 (4%)]\tLoss: 1613803.000000\n",
            "Train Epoch: 100 [480/7471 (6%)]\tLoss: 1589912.250000\n",
            "Train Epoch: 100 [640/7471 (9%)]\tLoss: 1553577.000000\n",
            "Train Epoch: 100 [800/7471 (11%)]\tLoss: 1618574.250000\n",
            "Train Epoch: 100 [960/7471 (13%)]\tLoss: 1591011.875000\n",
            "Train Epoch: 100 [1120/7471 (15%)]\tLoss: 1559544.375000\n",
            "Train Epoch: 100 [1280/7471 (17%)]\tLoss: 1513707.750000\n",
            "Train Epoch: 100 [1440/7471 (19%)]\tLoss: 1632811.875000\n",
            "Train Epoch: 100 [1600/7471 (21%)]\tLoss: 1592571.125000\n",
            "Train Epoch: 100 [1760/7471 (24%)]\tLoss: 1549451.500000\n",
            "Train Epoch: 100 [1920/7471 (26%)]\tLoss: 1607970.375000\n",
            "Train Epoch: 100 [2080/7471 (28%)]\tLoss: 1610298.875000\n",
            "Train Epoch: 100 [2240/7471 (30%)]\tLoss: 1549423.250000\n",
            "Train Epoch: 100 [2400/7471 (32%)]\tLoss: 1545236.875000\n",
            "Train Epoch: 100 [2560/7471 (34%)]\tLoss: 1524233.875000\n",
            "Train Epoch: 100 [2720/7471 (36%)]\tLoss: 1617395.125000\n",
            "Train Epoch: 100 [2880/7471 (39%)]\tLoss: 1538326.125000\n",
            "Train Epoch: 100 [3040/7471 (41%)]\tLoss: 1597264.500000\n",
            "Train Epoch: 100 [3200/7471 (43%)]\tLoss: 1574156.750000\n",
            "Train Epoch: 100 [3360/7471 (45%)]\tLoss: 1591088.125000\n",
            "Train Epoch: 100 [3520/7471 (47%)]\tLoss: 1579153.000000\n",
            "Train Epoch: 100 [3680/7471 (49%)]\tLoss: 1610317.625000\n",
            "Train Epoch: 100 [3840/7471 (51%)]\tLoss: 1555971.625000\n",
            "Train Epoch: 100 [4000/7471 (54%)]\tLoss: 1622688.250000\n",
            "Train Epoch: 100 [4160/7471 (56%)]\tLoss: 1527957.375000\n",
            "Train Epoch: 100 [4320/7471 (58%)]\tLoss: 1598225.500000\n",
            "Train Epoch: 100 [4480/7471 (60%)]\tLoss: 1575167.250000\n",
            "Train Epoch: 100 [4640/7471 (62%)]\tLoss: 1560734.375000\n",
            "Train Epoch: 100 [4800/7471 (64%)]\tLoss: 1615322.375000\n",
            "Train Epoch: 100 [4960/7471 (66%)]\tLoss: 1610757.000000\n",
            "Train Epoch: 100 [5120/7471 (69%)]\tLoss: 1592696.125000\n",
            "Train Epoch: 100 [5280/7471 (71%)]\tLoss: 1551536.875000\n",
            "Train Epoch: 100 [5440/7471 (73%)]\tLoss: 1584720.000000\n",
            "Train Epoch: 100 [5600/7471 (75%)]\tLoss: 1620469.625000\n",
            "Train Epoch: 100 [5760/7471 (77%)]\tLoss: 1526462.375000\n",
            "Train Epoch: 100 [5920/7471 (79%)]\tLoss: 1518774.875000\n",
            "Train Epoch: 100 [6080/7471 (81%)]\tLoss: 1547443.375000\n",
            "Train Epoch: 100 [6240/7471 (84%)]\tLoss: 1631281.750000\n",
            "Train Epoch: 100 [6400/7471 (86%)]\tLoss: 1593176.875000\n",
            "Train Epoch: 100 [6560/7471 (88%)]\tLoss: 1580194.375000\n",
            "Train Epoch: 100 [6720/7471 (90%)]\tLoss: 1565154.500000\n",
            "Train Epoch: 100 [6880/7471 (92%)]\tLoss: 1555931.500000\n",
            "Train Epoch: 100 [7040/7471 (94%)]\tLoss: 1542754.000000\n",
            "Train Epoch: 100 [7200/7471 (96%)]\tLoss: 1603993.875000\n",
            "Train Epoch: 100 [7360/7471 (99%)]\tLoss: 1589589.375000\n",
            "Epoch 100 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98457.4625\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZauWbfkuy9i",
        "colab_type": "text"
      },
      "source": [
        "* make Exp Module for <code>Hyperparameter opitmization</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsNLKuYziD8b",
        "colab_type": "text"
      },
      "source": [
        "Add-plot<br>\n",
        "<code>train_val plot</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7EcU6z-nVLA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c4e597c1-9359-4cc9-d336-2ff92837c263"
      },
      "source": [
        "epoch_train_losses = np.load('./results_ResNet-VAE_Exp01/ResNet_VAE_training_loss.npy')\n",
        "epoch_test_losses = np.load('./results_ResNet-VAE_Exp01/ResNet_VAE_test_loss.npy')\n",
        "\n",
        "print(epoch_train_losses.shape, epoch_test_losses.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 467) (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo9-u_XXtUJv",
        "colab_type": "text"
      },
      "source": [
        "<code>batch_size</code>를 argparser에서는 50으로 정해줬는데 결과가 16이라 의아했는데 원인을 찾았다.<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "Train_Loader에서 batch_size를 16으로 정해줬었어...!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPd3EkJSsCet",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "676093f2-27b6-400c-8103-c9604774ef08"
      },
      "source": [
        "print(epoch_train_losses.shape, epoch_test_losses.shape)\n",
        "print(epoch_train_losses[0].shape)\n",
        "\n",
        "print(len(train_loader.dataset))\n",
        "print(len(valid_loader.dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 467) (100,)\n",
            "(467,)\n",
            "7471\n",
            "1868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_t2oJR7_zBO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b7e39b6e-a679-4c65-fdac-85210090b034"
      },
      "source": [
        "epoch_train_loss = np.array(np.sum(epoch_train_losses, axis=1)/len(train_loader))\n",
        "print(type(epoch_train_loss), epoch_train_loss.shape)\n",
        "print(type(epoch_test_losses), epoch_test_losses.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (100,)\n",
            "<class 'numpy.ndarray'> (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk4l5aRhFnTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "f72bc469-297a-469e-bafc-23ec98535006"
      },
      "source": [
        "# print(epoch_train_loss)\n",
        "print(epoch_train_loss)\n",
        "print(epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1638542.16300857 1620145.3011242  1613838.34635974 1611309.79309422\n",
            " 1609294.58110278 1604131.50910064 1605361.17665953 1603169.00722698\n",
            " 1597750.57842612 1594670.69566381 1596908.4638651  1594820.83110278\n",
            " 1591787.57441113 1589724.83458244 1589253.08003212 1590662.44619914\n",
            " 1587715.87794433 1586307.64052463 1585468.14641328 1586667.18843683\n",
            " 1585635.30995717 1584972.41595289 1584345.36911135 1585177.11482869\n",
            " 1584231.52382227 1584188.90042827 1582912.37044968 1583511.24116702\n",
            " 1584135.60626338 1582163.17773019 1582088.10438972 1581855.36911135\n",
            " 1581567.88436831 1581862.14025696 1581171.07441113 1580937.06343683\n",
            " 1580699.83752677 1579828.38356531 1580349.66675589 1580600.1761242\n",
            " 1580038.31664882 1579667.83244111 1579829.20262313 1579679.86589936\n",
            " 1579711.6988758  1578713.22135974 1578928.89400428 1578330.98420771\n",
            " 1578694.30968951 1578644.82601713 1578315.04255889 1577831.36884368\n",
            " 1577992.97698073 1577751.36643469 1577560.05888651 1577430.78988223\n",
            " 1577293.02248394 1577069.23688437 1577013.47082441 1576958.40845824\n",
            " 1576406.12366167 1576562.86509636 1576422.33029979 1576229.95583512\n",
            " 1575853.71493576 1575683.0872591  1575741.07307281 1575780.82360814\n",
            " 1575882.30861884 1574954.02168094 1575597.125      1575033.44405782\n",
            " 1575318.14373662 1575119.48126338 1574604.7240364  1574612.86991435\n",
            " 1574526.29255889 1574299.13463597 1574445.46279443 1574408.07494647\n",
            " 1574237.87847966 1574076.05246253 1573984.6367773  1574016.43120985\n",
            " 1573851.62285867 1573942.85706638 1573719.92264454 1574223.66514989\n",
            " 1573480.73875803 1573358.25802998 1573214.28479657 1573489.65149893\n",
            " 1573379.83672377 1573342.13222698 1573365.32414347 1573396.05808351\n",
            " 1573183.27328694 1573067.21279443 1573034.38195931 1573201.46707709]\n",
            "[           inf            inf            inf            inf\n",
            " 2.04339794e+15            inf 1.01770823e+05 1.00493531e+05\n",
            "            inf            inf 9.95151711e+04            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            " 9.91129865e+04            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf 4.64997510e+17 9.86961006e+04 9.86702425e+04\n",
            " 9.86419925e+04 9.86430577e+04 9.88899452e+04 9.91061395e+04\n",
            " 9.86037253e+04 9.85922494e+04 9.86069897e+04 9.86739283e+04\n",
            " 9.89011277e+04 9.86097878e+04 9.86213919e+04 9.85981963e+04\n",
            " 9.85841948e+04 9.86716995e+04 9.85664475e+04 9.85906832e+04\n",
            " 9.85348911e+04 9.85212109e+04 9.85748725e+04 9.85326321e+04\n",
            " 9.85321190e+04 9.85137038e+04 9.85856745e+04 9.85935321e+04\n",
            " 9.85536405e+04 9.84884595e+04 9.84979273e+04 9.85113831e+04\n",
            " 9.85500923e+04 9.84629215e+04 9.84715464e+04 9.84851026e+04\n",
            " 9.84757658e+04 9.85063151e+04 9.84828543e+04 9.85242495e+04\n",
            " 9.84802659e+04 9.85492924e+04 9.85056253e+04 9.84501005e+04\n",
            " 9.85073227e+04 9.84660906e+04 9.84765147e+04 9.84337928e+04\n",
            " 9.84426701e+04 9.84634076e+04 9.84738756e+04 9.84655270e+04\n",
            " 9.84447675e+04 9.84348281e+04 9.84357628e+04 9.84686730e+04\n",
            " 9.85257165e+04 9.84069307e+04 9.90056195e+04 9.84574625e+04]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLuB6-kLMyum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "10387f42-425b-4b9b-8330-e7305ec70226"
      },
      "source": [
        "print(min(epoch_train_loss))\n",
        "print(max(epoch_train_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1573034.3819593147\n",
            "1638542.1630085653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VciLlUVrAy-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "0a045914-f4d1-404d-ee6f-d66fa8963778"
      },
      "source": [
        "list_epoch = np.array(range(100))\n",
        "list_epoch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
              "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6YdKnjxiI2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "1e180af9-142e-4be8-a85f-17523c4a01a7"
      },
      "source": [
        "# Reference: Stand-DL Assignment #1\n",
        "\n",
        "# ======== Loss Fluctuation ======== #\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax1.plot(list_epoch, epoch_train_loss, label='train_loss')\n",
        "ax1.plot(list_epoch, epoch_test_losses, '--', label='test_loss')\n",
        "ax1.set_xlabel('epoch')\n",
        "ax1.set_ylabel('loss')\n",
        "ax1.grid()\n",
        "ax1.legend()\n",
        "ax1.set_title('epoch vs loss')\n",
        "\n",
        "# ======== save plot ======== #\n",
        "plt.savefig('./train_val_plot.png', dpi=300)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3yJz_I9BCAP",
        "colab_type": "text"
      },
      "source": [
        "### Epoch별로 loss graph를 그려볼까??"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkKHFr1sB1lO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Backup Code\n",
        "\n",
        "backup_epoch_train_loss = epoch_train_loss\n",
        "backup_epoch_test_loss = epoch_test_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqOb7PKzNLN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "##### Back Up #####\n",
        "# Reference: Stand-DL Assignment #1\n",
        "\n",
        "# ======== Loss Fluctuation ======== #\n",
        "def plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses):\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    # set range\n",
        "    # rng = np.arange(min(epoch_train_losses), max(epoch_train_losses))\n",
        "\n",
        "    ax1.plot(list_epoch, epoch_train_losses, label='train_loss')\n",
        "    ax1.plot(list_epoch, epoch_test_losses, '--', label='test_loss')\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.set_ylabel('loss')\n",
        "    ax1.grid()\n",
        "    ax1.legend()\n",
        "    ax1.set_title('epoch vs loss')\n",
        "\n",
        "    # ======== save plot ======== #\n",
        "    plt.savefig('./results_ResNet-VAE_Exp01/loss_plot/{}train_val_plot.png'.format(list_epoch[-1]+1), dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcUoN8m7CsxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reference: Stand-DL Assignment #1\n",
        "\n",
        "# ======== Loss Fluctuation ======== #\n",
        "def plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses):\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    # set range\n",
        "    # rng = np.arange(np.amin(epoch_train_losses), np.amin(epoch_train_losses), 5000)\n",
        "\n",
        "    ax1.plot(list_epoch, epoch_train_losses, label='train_loss')\n",
        "    ax1.plot(list_epoch, epoch_test_losses+1460000, '--', label='test_loss')\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.set_ylabel('loss')\n",
        "    ax1.grid()\n",
        "    ax1.legend()\n",
        "    ax1.set_title('epoch vs loss')\n",
        "    # set limits\n",
        "    # ax1.autoscale(enable=True, axis='y')\n",
        "    \n",
        "    # Reference\n",
        "    # ax1.set_ylim(np.amin(epoch_train_losses), np.max(epoch_train_losses))\n",
        "    ax1.set_ylim(np.amin(epoch_test_losses+1460000), np.max(epoch_train_losses))\n",
        "\n",
        "    # ======== save plot ======== #\n",
        "    plt.savefig('./results_ResNet-VAE_Exp01/loss_plot/{}train_val_plot__for-test__.png'.format(list_epoch[-1]+1), dpi=300)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPf8LjDtTeAm",
        "colab_type": "text"
      },
      "source": [
        "* Min: test_losses, Max: test_losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPG-VaKgTia0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reference: Stand-DL Assignment #1\n",
        "\n",
        "# ======== Loss Fluctuation ======== #\n",
        "def plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses):\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    # set range\n",
        "    # rng = np.arange(np.amin(epoch_train_losses), np.amin(epoch_train_losses), 5000)\n",
        "\n",
        "    ax1.plot(list_epoch, epoch_train_losses, label='train_loss')\n",
        "    ax1.plot(list_epoch, epoch_test_losses, '--', label='test_loss')\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.set_ylabel('loss')\n",
        "    ax1.grid()\n",
        "    ax1.legend()\n",
        "    ax1.set_title('epoch vs loss')\n",
        "    # set limits\n",
        "    # ax1.autoscale(enable=True, axis='y')\n",
        "    \n",
        "    # Reference\n",
        "    ax1.set_ylim(np.amin(epoch_train_losses), np.amin(epoch_train_losses))\n",
        "    # ======== save plot ======== #\n",
        "    # plt.savefig('./results_ResNet-VAE_Exp01/loss_plot/{}train_val_plot.png'.format(list_epoch[-1]+1), dpi=300)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr5a66JVsuAd",
        "colab_type": "text"
      },
      "source": [
        "Test-plot<br>\n",
        "Try as many plot as possible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PISUwR3ns1Di",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "d906f0e6-6189-4853-df29-45648bac0223"
      },
      "source": [
        "# 01. 0 Epoch ~ 8 Epoch\n",
        "list_epoch = np.array(range(100))\n",
        "epoch_train_losses = backup_epoch_train_loss[:]\n",
        "epoch_test_losses = backup_epoch_test_loss[:]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvYGfea6E2v1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "108ca272-e50d-49bf-bf9f-62cd7b9b963b"
      },
      "source": [
        "# 01. 0 Epoch ~ 8 Epoch\n",
        "list_epoch = np.array(range(100))\n",
        "epoch_train_losses = backup_epoch_train_loss[:]\n",
        "epoch_test_losses = backup_epoch_test_loss[:]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Attempting to set identical bottom == top == 1573034.3819593147 results in singular transformations; automatically expanding.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoFG20bkTne1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "761fe8b1-467a-45b1-9f44-af3d2c789ac8"
      },
      "source": [
        "# 01. 0 Epoch ~ 8 Epoch\n",
        "list_epoch = np.array(range(10))\n",
        "epoch_train_losses = backup_epoch_train_loss[:]\n",
        "epoch_test_losses = backup_epoch_test_loss[:]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Attempting to set identical bottom == top == 98406.9306745182 results in singular transformations; automatically expanding.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAFNCAYAAABxInQxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXzU1b3/8deHfZVVIxAULEtZFJDVhRq1IiJXcMMdVKq3rW31/qwVe9tabW217dXWW5dqRcW648ZttaBIXGpBFkFRUBBRAiqbLAHZwuf3xzmBCUxCIMnMJPN+Ph7zyOR8tzOHL3nPOd/N3B0RERGBWumugIiISKZQKIqIiEQKRRERkUihKCIiEikURUREIoWiiIhIpFAUqWHMrIOZuZnVSeE288ysIFXbE6kqCkUREZFIoSgiIhIpFEWqmJm1NbNnzGyVmX1iZj9KmPZLM5toZk+a2UYzm2NmvRKmdzOzfDNbZ2bvm9kZCdMamtn/mNmnZrbezN40s4YJm77IzD4zs9Vm9t+l1G2gmX1hZrUTys40s3fj+wFmNsvMNpjZl2Z2ezk/c1n1HmZmH8TPu9zMfhzLW5vZ3+Mya83sDTPT3yhJKe1wIlUo/lH/P2Ae0A44GbjGzE5NmG0E8DTQEngMeN7M6ppZ3bjsFOAQ4IfAo2bWNS73B6AvcGxc9ifAzoT1Hg90jdv8hZl127N+7j4D2ASclFB8YawHwJ+AP7n7QcA3gKfK8Zn3Ve8HgP9096ZAT+DVWH4tUAAcDOQAPwV0H0pJKYWiSNXqDxzs7je7+zZ3XwLcD5yfMM9sd5/o7tuB24EGwKD4agLcGpd9Ffg7cEEM28uBq919ubsXuftb7r41Yb03ufvX7j6PEMq9SO5x4AIAM2sKDItlANuBTmbW2t0L3X16OT5zqfVOWGd3MzvI3b9y9zkJ5W2Aw919u7u/4bo5s6SYQlGkah0OtI1DguvMbB2hB5STMM+y4jfuvpPQW2obX8tiWbFPCT3O1oTw/LiMbX+R8H4zIaiSeQw4y8zqA2cBc9z90zhtLNAFWGhmM81seJmfNiir3gBnE4L3UzN7zcyOieW/BxYDU8xsiZmNK8e2RCqVQlGkai0DPnH35gmvpu4+LGGe9sVvYg8wF1gRX+33OK52GLAcWA1sIQxpVoi7f0AIrdMoOXSKuy9y9wsIw6C3ARPNrPE+VllWvXH3me4+Iq7zeeKQrLtvdPdr3f0I4Azg/5nZyRX9fCL7Q6EoUrXeBjaa2fXxxJjaZtbTzPonzNPXzM6K1xVeA2wFpgMzCD28n8RjjHnAfwBPxF7YeOD2eCJPbTM7Jvb2DsRjwNXAtwjHNwEws4vN7OC4vXWxeGeS5ROVWm8zq2dmF5lZszhcvKF4fWY23Mw6mZkB64GicmxLpFIpFEWqkLsXAcOB3sAnhB7eX4FmCbO9AJwHfAVcApwVj6ltI4TJaXG5u4HR7r4wLvdj4D1gJrCW0JM70P/TjwMnAK+6++qE8qHA+2ZWSDjp5nx3/3ofn3lf9b4EWGpmG4DvAhfF8s7AK0Ah8G/gbnefdoCfR+SAmI5ji6SPmf0S6OTuF6e7LiKinqKIiMguCkUREZFIw6ciIiKReooiIiKRQlFERCRK2fPWMl3r1q29Q4cOFVrHpk2baNx4X9c1Zx+1S3Kltsvqj8BqQatOqa9UBtD+kpzaJbkDaZfZs2evdveDk01TKEYdOnRg1qxZFVpHfn4+eXl5lVOhGkTtklyp7fLXU6BeYxj9fMrrlAm0vySndknuQNrFzD4tbZqGT0VERCKFooiISKRQFBERiXRMUSTTHH8N1NJ/zWy2fft2CgoK2LJly66yZs2asWDBgjTWKjOV1S4NGjQgNzeXunXrlnt9+p8nkmm+eXq6ayBpVlBQQNOmTenQoQPhoSGwceNGmjZtmuaaZZ7S2sXdWbNmDQUFBXTs2LHc69PwqUimWfURrF6c7lpIGm3ZsoVWrVrtCkTZf2ZGq1atSvS2y0OhKJJpXrgKXvxxumshaaZArLgDaUOFooiISKRQFBGREtatW8fdd9+938sNGzaMdevW7fdyl156KRMnTtzv5aqCQlFEREooLRR37NhR5nIvvvgizZs3r6pqpYRCUUREShg3bhwff/wxvXv3pn///gwePJgzzjiD7t27AzBy5Ej69u1Ljx49uO+++3Yt16FDB1avXs3SpUvp1q0bV1xxBT169GDIkCF8/fXX5dr21KlT6dOnD0ceeSSXX345W7du3VWn7t27c9RRR/HjH4dj7k8//TQDBw6kV69efOtb36qUz65LMkQyTd44Xacou9z0f+/zwYoNFBUVUbt27UpZZ/e2B3Hjf/Qodfqtt97K/PnzmTt3Lvn5+Zx++unMnz9/16UN48ePp2XLlnz99df079+fs88+m1atWpVYx6JFi3j88ce5//77GTVqFM888wwXX3xxmfXasmULl156KVOnTqVLly6MHj2ae+65h0suuYTnnnuOhQsXYma7hmhvvvlmnnvuObp27XpAw7bJqKcokmk6nQxHnJDuWojsMmDAgBLX+t1555306tWLQYMGsWzZMhYtWrTXMh07dqR3794A9O3bl6VLl+5zOx9++CEdO3akS5cuAIwZM4bXX3+dZs2a0aBBA8aOHcuzzz5Lo0aNADjuuOP43ve+x/33309RUVElfFL1FEUyz4q5UKs2HHpkumsiGaC4R5fOi/cTH82Un5/PK6+8wr///W8aNWpEXl5e0msB69evv+t97dq1yz18mkydOnV4++23mTp1KhMnTuTPf/4zr776Kvfeey+vvvoq+fn59O3bl9mzZ+/VY93vbVVoaRGpfC9el9WPjpL0a9q0KRs3bkw6bf369bRo0YJGjRqxcOFCpk+fXmnb7dq1K0uXLmXx4sV06tSJRx55hBNOOIHCwkI2b97MsGHDOO644zjiiCMA+Pjjj+nfvz8nnXQSL730EsuWLVMoiohI5WrVqhXHHXccPXv2pGHDhuTk5OyaNnToUO699166detG165dGTRoUKVtt0GDBjz44IOce+657Nixg/79+/Pd736XtWvXMmLECLZs2YK7c/vttwNw3XXX8eGHH2JmnHzyyfTq1avCdVAoiojIXh577LGk5fXr1+ell15KOq34uGHr1q2ZP3/+rvLis0VL89BDD+16f/LJJ/POO++UmN6mTRvefvvtvZZ79tlnK31YWSfaiIiIROopiohISlx11VX861//KlF29dVXc9lll6WpRntTKIpkmlNu1nWKUiPddddd6a7CPul/nkimOfyYdNdAJGvpmKJIpvn037BsZrprIZKVFIoimeblX8C0W9JdC5GspFAUERGJFIoiIlLCgT5PEeCPf/wjmzdvLnOe4qdpZCKFooiIlFDVoZjJdPapiEime/B0GhbtgNoJf7J7jIQBV8C2zfDouXsv0/tC6HMRbFoDT40uOe2yf5S5ucTnKZ5yyikccsghPPXUU2zdupUzzzyTm266iU2bNjFq1CgKCgooKiri5z//OV9++SUrVqzgxBNPpHXr1kybNm2fH+32229n/PjxAHznO9/hmmuuSbru8847j3HjxjFp0iTq1KnDkCFD+MMf/rDP9e8vhaJIphn2+/CUDJE0SXye4pQpU5g4cSJvv/027s4ZZ5zB66+/zqpVq2jbti3/+EcI2PXr19OsWTNuv/12pk2bRuvWrfe5ndmzZ/Pggw8yY8YM3J2BAwdywgknsGTJkr3WvWbNmqTPVKxsCkWRTNO2d7prIJnmsn/wdWn3+KzXqOyeX+NW++wZlmXKlClMmTKFPn36AFBYWMiiRYsYPHgw1157Lddffz3Dhw9n8ODB+73uN998kzPPPHPXo6nOOuss3njjDYYOHbrXunfs2LHrmYrDhw9n+PDhB/yZyqJjiiKZZvFUWPJaumshAoC7c8MNNzB37lzmzp3L4sWLGTt2LF26dGHOnDkceeSR/OxnP+Pmm2+utG0mW3fxMxXPOecc/v73vzN06NBK214ihaJIpsm/Fd68I921kCyW+DzFU089lfHjx1NYWAjA8uXLWblyJStWrKBRo0ZcfPHFXHfddcyZM2evZfdl8ODBPP/882zevJlNmzbx3HPPMXjw4KTrLiwsZP369QwbNow77riDefPmVclnr7LhUzMbDwwHVrp7z1jWEngS6AAsBUa5+1dmdhFwPWDARuB77j4vLjMU+BNQG/iru98ayzsCTwCtgNnAJe6+zczqAxOAvsAa4Dx3X1pVn1NEpKZJfJ7iaaedxoUXXsgxx4TbDzZp0oS//e1vLF68mOuuu45atWpRt25d7rnnHgCuvPJKhg4dStu2bfd5os3RRx/NpZdeyoABA4Bwok2fPn2YPHnyXuveuHFj0mcqVjp3r5IX8C3gaGB+QtnvgHHx/Tjgtvj+WKBFfH8aMCO+rw18DBwB1APmAd3jtKeA8+P7ewlBCvB94N74/nzgyfLUt2/fvl5R06ZNq/A6aiK1S3Kltsv933Z/eERK65JJtL+4f/DBB3uVbdiwIQ01yXz7apdkbQnM8lKyoMqGT939dWDtHsUjgIfj+4eBkXHet9z9q1g+HciN7wcAi919ibtvI/QMR5iZAScBE/dc1x7bmAicHOcXEREpU6rPPs1x98/j+y+AnCTzjAWKH+vcDliWMK0AGEgYMl3n7jsSytvtuYy77zCz9XH+zLx9gohIDTVw4EC2bt1aouyRRx7hyCOPTFON9i1tl2S4u5uZJ5aZ2YmEUDw+FXUwsyuBKwFycnLIz8+v0PoKCwsrvI6aSO2SXGnt0qjNGNyMr7O0zbS/QLNmzfY6WaWoqKjcJ7BkildeeSVpeWV+jn21y5YtW/Zrf0p1KH5pZm3c/XMzawOsLJ5gZkcBfwVOc/c1sXg50D5h+dxYtgZobmZ1Ym+xuDxxmQIzqwM0i/Pvxd3vA+4D6Nevn+fl5VXow+Xn51PRddREapfk1C7JqV1gwYIFNGnShMQjPxtLu04xy5XVLu5OgwYNdl1jWR6pviRjEjAmvh8DvABgZocBzxLOIP0oYf6ZQGcz62hm9QgnzkyKB0qnAefsua49tnEO8GqcX6R6WPgP+GhyumshadSgQQPWrFmD/nQdOHdnzZo1NGjQYL+Wq8pLMh4H8oDWZlYA3AjcCjxlZmOBT4FRcfZfEI773R2/Ge1w937xmOAPgMmEM1HHu/v7cZnrgSfM7NfAO8ADsfwB4BEzW0w40ef8qvqMIlXizT9CvcbQ5dR010TSJDc3l4KCAlatWrWrbMuWLfv9Bz4blNUuDRo0IDc3N+m00lRZKLr7BaVMOjnJvN8BvlPKel4EXkxSvoRwduqe5VuAJHfHFRGpHurWrUvHjh1LlOXn5+/XMGC2qOx20R1tMt226vsIFhGR6kahmMle+z38pi0UbU93TUREsoJCMZM1bgU4FK7c56wiIlJxenRUJmtyaPhZ+AU0a1f2vFJznDMeTN9XRdJBoZjJmsYb/mz8Ir31kNRq3n7f84hIldDX0UzWtE34qVDMLu9NhPefT3ctRLKSeoqZrPEhcOyP4NDMvU+gVIEZfwnXKfYYue95RaRSKRQzWe06MORX6a6FiEjW0PBpptu6ETasSHctRESygkIx0028HB7XnepERFJBoZjpmuTAxi/TXQsRkaygY4qZrumhsGkl7CyCWrXTXRtJhQse13WKImmi/3mZrumh4Dth06p9zys1Q+PW0KhlumshkpUUipmu+K42ulYxe8yZAPOeSHctRLKSQjHTtekFp/0u9BglO8x5RKEokiY6ppjpmreHgf+Z7lqIiGQF9RSrg1UfwldL010LEZEaT6FYHTw4DN78Y7prISJS4ykUq4Omh0KhrlUUEalqOqZYHTTJgY2fp7sWkiqjnwcs3bUQyUrqKVYHTdvorjbZpF5jqNco3bUQyUoKxeqgaU4YPt1ZlO6aSCpMvxdmjU93LUSykoZPq4OeZ0PbPuCe7ppIKsx/JvQW+12e7pqIZB2FYnWQ0yO8RESkSmn4tDrYthkWT4X1y9NdExGRGk2hWB1sWgV/Ows+fjXdNRERqdEUitVBk5zwUzcFFxGpUjqmWB3UbQANW0ChQjErfOfldNdAJGupp1hdNDlUPUURkSqmUKwumuYoFLPFG7fDv+9Ody1EspKGT6uLU24Gq53uWkgqfPhSuE7xmO+nuyYiWUehWF206ZXuGoiI1HgaPq0uvvo0PJF9a2G6ayIiUmMpFKuLFXNg0g/0sGERkSqkUKwumrYJP3VZRs1Xt2F4iUjK6ZhidbHrAn49QqrGGzMp3TUQyVoKxerioHZw4dPQ5qh010REpMZSKFYXdepBlyHproWkwrTfQJ36MPjadNdEJOvomKJIpvl4GnzyRrprIZKVFIoiIiKRQlFERCRSKIqIiEQ60UYk0zRqpesURdJEoSiSaS58It01EMlaGj4VERGJFIoimWbKz+DVX6e7FiJZqcpC0czGm9lKM5ufUNbSzF42s0XxZ4tYbmZ2p5ktNrN3zezohGXGxPkXmdmYhPK+ZvZeXOZOM7OytiFSbXw2AwpmpbsWIlmpKnuKDwFD9ygbB0x1987A1Pg7wGlA5/i6ErgHQsABNwIDgQHAjQkhdw9wRcJyQ/exDRERkTJVWSi6++vA2j2KRwAPx/cPAyMTyid4MB1obmZtgFOBl919rbt/BbwMDI3TDnL36e7uwIQ91pVsGyIiImVK9THFHHf/PL7/AoiPfqAdsCxhvoJYVlZ5QZLysrYhIiJSprRdkuHubmaezm2Y2ZWE4VpycnLIz8+v0PYKCwsrvI6aSO2SXGnt0n1rXYp2GB9maZtpf0lO7ZJcZbdLqkPxSzNr4+6fxyHQlbF8OdA+Yb7cWLYcyNujPD+W5yaZv6xt7MXd7wPuA+jXr5/n5eWVNmu55OfnU9F11ERql+RKbZdY1ialtckc2l+SU7skV9ntkurh00lA8RmkY4AXEspHx7NQBwHr4xDoZGCImbWIJ9gMASbHaRvMbFA863T0HutKtg0REZEyVVlP0cweJ/TyWptZAeEs0luBp8xsLPApMCrO/iIwDFgMbAYuA3D3tWb2K2BmnO9mdy8+eef7hDNcGwIvxRdlbEOkevjHtVCnAZx6S7prIpJ1qiwU3f2CUiadnGReB64qZT3jgfFJymcBPZOUr0m2DZFq4/N3oV7jdNdCJCvpjjYiIiKRQlFERCRSKIqIiER6dJRIpmndWc9TFEkThaJIphl5d7prIJK1NHwqIiISKRRFMs3z3w/XKopIymn4VCTTrF6k6xRF0kQ9RRERkUihKCIiEikURUREIh1TFMk0bY4KNwQXkZRTKIpkmtP/J901EMlaGj4VERGJFIoimebpS8O1iiKScho+Fck065frOkWRNFFPUUREJFIoioiIRApFkZpq89p010Ck2lEoimSawwZCbr+KrWPeE/D7b8DKBZVTJ5EsoRNtRDLNkF9XbPkt62HKz8F3wuKpcEi3yqmXSBZQT1GkptlZBJ1PgYYtYOkb6a6NSMUseQ12bEvZ5hSKIpnmsfPh6csOfPlGLWHk3XDU+VBLg0FSja1eBBPOgKk3pWyT+h8jkmk2rzmw6xTd4ZVfQo8zoW1vOO3WSq+aSEq16gSND4F1n6Vsk+opitQUH/0T/vVH+PStkuU7dx74OhdPhc/nVaxeIgfKDA4/Fla8k7JNKhRFaoKiHTDlZ9C6Kwy4Ynf5xLHw5MUHts51n8HjF8CzV4ZeqEiqPXMFfPk+rF8W7vSUAgpFkZrg3SdgzWL49o1Qu+7u8gYHwSevh9DcX1NvhqKtsGohLJlWeXUVKY9tm2D+RGjRIfy+bHpKNqtQFMk03zgROg4u//w7tsFrt0HbPtB1WMlpHY6HbRvhi/0cAt2+BdZ+Asf+EHL7w/av9295kYr6fF64rKjvpTD2Zfjm8JRsVifaiGSaE3+6f/N7EfQZHS74Nys57fDjw89P3oB2fcu/zroNwh+inTugTr39q49IZVg+O/xsPwCaHJKyzaqnKFLd1W0IJ1wXeph7apoTjjMufbP861sxFwpXQa1auwNx+xYomFU59RUpj+VzoNlhIRBXLoDJ/w1bN1b5ZhWKIpnm4TPgl83CiTPznw3DmKWd6PLexDBPWSfCDPoefPP08m17xzaYeBk8cUHJ8sk/hQkjwt1y9pc7bFqz/8tJdmvcGroODe83LId//zklX8w0fCqSaXpdANsKYcZfoCjeyePoMXDGnSFg3n823LVmZxG8ciO06hyuTSxNv3LcCMAdFv4DXv0VrF0CQ28rOf3o0TDrAXjnb3DMVeX/LBs+hxeugo9fhVNugmN/tPcQr0gyw36/+31uf8Bg2YzkIyKVqFyhaGZXAw8CG4G/An2Ace4+pQrrJpKdel8QXju2waoF4dhKi45h2vplMPHyhJkNRk3Yd9Bs/DLc8q1Nb2hyMNRrEoaiGjaHbZvDXUMKZoaLpUc9Al2GlFy+bW847Bj4912wviAEY7PccP3YR5MhpyfkdIeDcncPuW5ZD/ceF9bf4Xh4+RewciGM+DPUql1pzbXf3BXMmW5nUcl9pEEzyOkBn1X9Gajl7Sle7u5/MrNTgRbAJcAjgEJRpKrUqQdteoVXsUat4Qezwx91qxXufFOekxAePQe+eLdk2aFHwXffgHqN4NAjQ2+w14VQu5Q/C8f/Fzx2HsyZEHqmzXJDYOffCiQM3zY+BP7zNTioLZz8i3CyT6tvhDNkt6wPf+yWzYR5j8MX78Hm1XDUedBvbAjsZLYWhjMRGxy078+aaOUCmPYbaD8Qjv1B2P7t3eEbJ0G3/4DOQ8IXA8ksr90G7z4JP5i1+xKj9gPh3af2DsxKVt5QLP5aNQx4xN3fN9NXLZGUq9cIWnfa/+UueS6EYuEq2LQStmyAlh13Tx9+x77X0eVU+PnqkqHZ/zthuHflgvDasCIc/2nYMkzve+nuefPG7T72uXYJvPd0COPmh0H+b+GtP8O1C8P01YtC4G/bBDMfgLmPhqHXE64L0167DTqdEk4yMgPinU8axe1uWgP5v4FZD0L9JnDECbH9mkLPs0LvdsGkcG/YHmfCST/bfT1cWdZ8DB++GG5S3eE4OOaHpX+JqIidO8PQed0GJcu3Fobe+ufzoPeF0C01lylUua/Xlfxysnw21G1c8prb9gPhg+fDSEWLw6usKuX915xtZlOAjsANZtYUqMC9o0QkpRq3Dr2jikoWAPUah8tByvMMyOLv0j3PgiPPDWe4Qgi6gpkhwACevhS+nB/e16oLPUZCp5PD72s+Drefe+/pkuv+3lshFOc9AZN+FC4n6T8W8m7YHZa1asEZ/xtCZ/nscHx29sNhOLhFh3AMtPCL8IDmr7+CjV9A7XowMN7V56HhsHEFNGsPi1+GhS/CWX+BlkeUvw13FoUe8qdvhdDrF4fDd2wLdfrghRDYhSvhnAeg+wgo2kGbFf+EO68IX2qatglD2t2GQ9H20B6dT0neg/poSpg3hZc1lIs7fPoveOt/wy0K+10Ow/4QRkCWz977usSeZ8NRo6p86Lu8oTgW6A0scffNZtYSqMBt/EUkqyX2AABadw6vYkN+BV8tDUHRYyQ0PXT3tK5D4ccfhSD1ovDH1XeGE44A6jaCo84NvbhDvpl8+7VqQfv+4ZU3LhyzApj0wxB2JerWNYSiWQjA5oeH3u17E8NZuds2h/ne+VvoLReH6YYVYb6LngrTn7wk9HJWLwo3VAA45gfh545t8NvccAeh2vXDF4Dmh+++tvSdCXT96J5wXPf8R8OJJ8UnYS38e/gS0aJjONO4dZdwa7RjfxCOGz95UQjOw48Ny21ZB91HhhNWiraHEYS2RycPmw0rwqURXy0Nw9ybVofPd8b/hi8an7wRPvPBXeDgb0KTnLJDyz0s36glbN0Aj44KXwy6ng6zxocRjBOuD/PseV1tVfTIkyjvVo4B5rr7JjO7GDga+FPVVUtEstq+erW164YTe5LpfkZ4lVdxIAIcf004W7dhy/CHu0lOyekdv7X7/VHnhp5a3Ybh9/nPwGczwjBgk0PCcdRDj9w9/0HtQkgdNSoE1GHHQLN2YVrR1nDMtnXnMExdv2nJOtY/iPd6/pQjz/7J7tCpUz/8/OZwOOdBmH43vPSTUNbk0NDzqtcErsyHDyaFHuhbMcza9A7zLXkNHj07BPBhg8Lw7NdfwZn3hJ7zu0+FM5whDDU3ahXapnjbH/0zXCqR2JYHtYPv/it88Zg1PrSJ74QdW8JoQIPmcNX0MO8lz4Zj5nUbwhu3h4Auvvl3spGHLz+A7ZvLNypxgMobivcAvcysF3At4QzUCcAJVVUxEZGU63D8/s1fHIgAF00s+wSQsh7lVb8pnHhD6dOPPIc1a/KT98Jq1w3D0T3PCtfxrfs03O6vuG45PcLrxBv2PvM2tx+MuCtc67r0XyHQG7YIPci4XToMDsefG7bYe/tDfh1uBbjqw3CP3NUfhWBNHBb/7K0wJFqrTvgy0PGE3fU4bNDudQ3+f2FYe8Uc6H8FHNxt789a2hehSlTeUNzh7m5mI4A/u/sDZja2KismIlKtpPMyk2L7Ora7Z6g1bA59Lg6vZJrlhldZ62t6aHgdkaSPNPS34VVetWqV//h0FSlvKG40sxsIl2IMNrNaQN19LCMiIlKtlPc2b+cBWwnXK34B5AK/L3sRERGR6qVcoRiD8FGgmZkNB7a4+4QqrZmIiEiKlSsUzWwU8DZwLjAKmGFm51RlxURERFKtvMcU/xvo7+4rAczsYOAVYGJVVUxERCTVyntMsVZxIEZr9mNZERGRaqG8wfZPM5tsZpea2aXAP4AXD3SjZna1mc03s/fN7JpY1tvMppvZXDObZWYDYrmZ2Z1mttjM3jWzoxPWM8bMFsXXmITyvmb2XlzmTt2nVUREyqO8J9pcB9wHHBVf97n79QeyQTPrCVwBDAB6AcPNrBPwO+Amd+8N/CL+DnAa0Dm+riTcSIB4q7kbgYFxXTeaWYu4zD1xG8XLDT2QuoqISHYp983k3P0Z4JlK2GY3YIa7bwYws9eAswjPnil+LkwzYEV8PwKY4O4OTDez5mbWBsgDXnb3tXE9L8Fh/a4AAA3iSURBVANDzSwfOMjdp8fyCcBI4KVKqLuIiNRgZYaimW2kxIPSdk8C3N338+FmAMwHbjGzVsDXhMdRzQKuASab2R8IPdhj4/ztgGUJyxfEsrLKC5KUi4iIlKnMUHT3pmVNPxDuvsDMbiM8oHgTMBcoAr4H/Je7PxMvAXkA+HZlbz+RmV1JGJIlJyeH/Pz8Cq2vsLCwwuuoidQuyaldklO7JKd2Sa6y2yU1z+LYg7s/QAg9zOw3hN7cb4Gr4yxPE246DrAcaJ+weG4sW04YQk0sz4/luUnmT1aP+wjHSunXr5/n5eUlm63c8vPzqeg6aiK1S3Jql+TULsmpXZKr7HZJy2UVZnZI/HkY4XjiY4RjiMV3lD0JWBTfTwJGx7NQBwHr3f1zYDIwxMxaxBNshgCT47QNZjYonnU6GnghVZ9NRESqr7T0FIFn4jHF7cBV7r7OzK4A/mRmdYAtxGFNwqUfw4DFwGbiw43dfa2Z/QqYGee7ufikG+D7wENAQ8IJNjrJRkRE9ildw6eDk5S9CfRNUu7AVaWsZzwwPkn5LKBnxWsqIiLZRHelERERiRSKIiIikUJRREQkUiiKiIhECkUREZFIoSgiIhIpFEVERCKFooiISKRQFBERiRSKIiIikUJRREQkUiiKiIhECkUREZFIoSgiIhIpFEVERCKFooiISKRQFBERiRSKIiIikUJRREQkUiiKiIhECkUREZFIoSgiIhIpFEVERCKFooiISKRQFBERiRSKIiIikUJRREQkUiiKiIhECkUREZFIoSgiIhIpFEVERCKFooiISKRQFBERiRSKIiIikUJRREQkUiiKiIhECkUREZFIoSgiIhIpFEVERCKFooiISKRQFBERiRSKIiIikUJRREQkUiiKiIhECkUREZFIoSgiIhIpFEVERKK0hKKZXW1m883sfTO7JqH8h2a2MJb/LqH8BjNbbGYfmtmpCeVDY9liMxuXUN7RzGbE8ifNrF7qPp2IiFRXKQ9FM+sJXAEMAHoBw82sk5mdCIwAerl7D+APcf7uwPlAD2AocLeZ1Taz2sBdwGlAd+CCOC/AbcAd7t4J+AoYm7IPKCIi1VY6eordgBnuvtnddwCvAWcB3wNudfetAO6+Ms4/AnjC3be6+yfAYkKgDgAWu/sSd98GPAGMMDMDTgImxuUfBkam6LOJiEg1VicN25wP3GJmrYCvgWHALKALMNjMbgG2AD9295lAO2B6wvIFsQxg2R7lA4FWwLoYuHvOX4KZXQlcCZCTk0N+fn6FPlhhYWGF11ETqV2SU7skp3ZJTu2SXGW3S8pD0d0XmNltwBRgEzAXKIp1aQkMAvoDT5nZEVVcl/uA+wD69evneXl5FVpffn4+FV1HTaR2SU7tkpzaJTm1S3KV3S5pOdHG3R9w977u/i3CMb+PCD26Zz14G9gJtAaWA+0TFs+NZaWVrwGam1mdPcpFRETKlK6zTw+JPw8jHE98DHgeODGWdwHqAauBScD5ZlbfzDoCnYG3gZlA53imaT3CyTiT3N2BacA5cXNjgBdS9dlERKT6SscxRYBn4jHF7cBV7r7OzMYD481sPrANGBMD7n0zewr4ANgR5y8CMLMfAJOB2sB4d38/rv964Akz+zXwDvBAKj+ciIhUT2kJRXcfnKRsG3BxKfPfAtySpPxF4MUk5UsIZ6eKiIiUm+5oIyIiEikURUREIoWiiIhIpFAUERGJFIoiIiKRQlFERCRSKIqIiEQKRRERkUihKCIiEikURUREIoWiiIhIpFAUERGJFIoiIiKRQlFERCRSKIqIiEQKRRERkUihKCIiEikURUREIoWiiIhIpFAUERGJFIoiIiKRQlFERCRSKIqIiEQKRRERkUihKCIiEikURUREIoWiiIhIpFAUERGJFIoiIiKRQlFERCRSKIqIiEQKRRERkUihKCIiEikURUREIoWiiIhIpFAUERGJFIoiIiKRQlFERCRSKIqIiEQKRRERkUihKCIiEikURUREIoWiiIhIpFAUERGJFIoiIiKRQlFERCRKSyia2dVmNt/M3jeza/aYdq2ZuZm1jr+bmd1pZovN7F0zOzph3jFmtii+xiSU9zWz9+Iyd5qZpe7TiYhIdZXyUDSznsAVwACgFzDczDrFae2BIcBnCYucBnSOryuBe+K8LYEbgYFxXTeaWYu4zD1xG8XLDa3aTyUiIjVBOnqK3YAZ7r7Z3XcArwFnxWl3AD8BPGH+EcAED6YDzc2sDXAq8LK7r3X3r4CXgaFx2kHuPt3dHZgAjEzNRxMRkeosHaE4HxhsZq3MrBEwDGhvZiOA5e4+b4/52wHLEn4viGVllRckKRcRESlTnVRv0N0XmNltwBRgEzAXqA/8lDB0mjJmdiVhSJacnBzy8/MrtL7CwsIKr6MmUrskp3ZJTu2SnNolucpul5SHIoC7PwA8AGBmvwG+JAxxzovnxOQCc8xsALAcaJ+weG4sWw7k7VGeH8tzk8yfrB73AfcB9OvXz/Py8pLNVm75+flUdB01kdolObVLcmqX5NQuyVV2u6Tr7NND4s/DCMcTH3b3Q9y9g7t3IAx5Hu3uXwCTgNHxLNRBwHp3/xyYDAwxsxbxBJshwOQ4bYOZDYpnnY4GXkj5hxQRkWonLT1F4BkzawVsB65y93VlzPsi4bjjYmAzcBmAu681s18BM+N8N7v72vj++8BDQEPgpfgSEREpU7qGTwfvY3qHhPcOXFXKfOOB8UnKZwE9K1ZLERHJNrqjjYiISKRQFBERiRSKIiIikUJRREQkUiiKiIhECkUREZFIoSgiIhIpFEVERCKFooiISGThhjFiZquATyu4mtbA6kqoTk2jdklO7ZKc2iU5tUtyB9Iuh7v7wckmKBQrkZnNcvd+6a5HplG7JKd2SU7tkpzaJbnKbhcNn4qIiEQKRRERkUihWLnuS3cFMpTaJTm1S3Jql+TULslVarvomKKIiEiknqKIiEikUKwkZjbUzD40s8VmNi7d9UkXM2tvZtPM7AMze9/Mro7lLc3sZTNbFH+2SHddU83MapvZO2b29/h7RzObEfeZJ82sXrrrmGpm1tzMJprZQjNbYGbHaF8BM/uv+P9nvpk9bmYNsnF/MbPxZrbSzOYnlCXdPyy4M7bPu2Z29IFsU6FYCcysNnAXcBrQHbjAzLqnt1ZpswO41t27A4OAq2JbjAOmuntnYGr8PdtcDSxI+P024A537wR8BYxNS63S60/AP939m0AvQvtk9b5iZu2AHwH93L0nUBs4n+zcXx4Chu5RVtr+cRrQOb6uBO45kA0qFCvHAGCxuy9x923AE8CINNcpLdz9c3efE99vJPyRa0doj4fjbA8DI9NTw/Qws1zgdOCv8XcDTgImxlmysU2aAd8CHgBw923uvo4s31eiOkBDM6sDNAI+Jwv3F3d/HVi7R3Fp+8cIYIIH04HmZtZmf7epUKwc7YBlCb8XxLKsZmYdgD7ADCDH3T+Pk74ActJUrXT5I/ATYGf8vRWwzt13xN+zcZ/pCKwCHozDyn81s8Zk+b7i7suBPwCfEcJwPTAb7S/FSts/KuXvsEJRqoSZNQGeAa5x9w2J0zyc8pw1pz2b2XBgpbvPTnddMkwd4GjgHnfvA2xij6HSbNtXAOIxshGELw1tgcbsPYQoVM3+oVCsHMuB9gm/58ayrGRmdQmB+Ki7PxuLvyweyog/V6arfmlwHHCGmS0lDK2fRDiW1jwOj0F27jMFQIG7z4i/TySEZDbvKwDfBj5x91Xuvh14lrAPZfv+Uqy0/aNS/g4rFCvHTKBzPDusHuGg+KQ01ykt4rGyB4AF7n57wqRJwJj4fgzwQqrrli7ufoO757p7B8K+8aq7XwRMA86Js2VVmwC4+xfAMjPrGotOBj4gi/eV6DNgkJk1iv+fitslq/eXBKXtH5OA0fEs1EHA+oRh1nLTxfuVxMyGEY4b1QbGu/staa5SWpjZ8cAbwHvsPn72U8JxxaeAwwhPIxnl7nseQK/xzCwP+LG7DzezIwg9x5bAO8DF7r41nfVLNTPrTTj5qB6wBLiM8GU9q/cVM7sJOI9wNvc7wHcIx8eyan8xs8eBPMKTML4EbgSeJ8n+Eb9A/Jkw1LwZuMzdZ+33NhWKIiIigYZPRUREIoWiiIhIpFAUERGJFIoiIiKRQlFERCRSKIpImcwsr/jJHiI1nUJRREQkUiiK1BBmdrGZvW1mc83sL/H5jYVmdkd8Nt9UMzs4ztvbzKbH5849l/BMuk5m9oqZzTOzOWb2jbj6JgnPPXw0XigtUuMoFEVqADPrRrgDynHu3hsoAi4i3Ex6lrv3AF4j3BEEYAJwvbsfRbj7UHH5o8Bd7t4LOJbwlAYITzu5hvC80CMI9+IUqXHq7HsWEakGTgb6AjNjJ64h4UbJO4En4zx/A56NzzFs7u6vxfKHgafNrCnQzt2fA3D3LQBxfW+7e0H8fS7QAXiz6j+WSGopFEVqBgMedvcbShSa/XyP+Q70vo6J99gsQn87pIbS8KlIzTAVOMfMDgEws5Zmdjjh/3jxkxUuBN509/XAV2Y2OJZfArzm7huBAjMbGddR38wapfRTiKSZvu2J1ADu/oGZ/QyYYma1gO3AVYQH9w6I01YSjjtCeOTOvTH0ip9OASEg/2JmN8d1nJvCjyGSdnpKhkgNZmaF7t4k3fUQqS40fCoiIhKppygiIhKppygiIhIpFEVERCKFooiISKRQFBERiRSKIiIikUJRREQk+v+vey3YtblpzAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hQJ5yAisBIp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "024b1f36-7548-4a50-9d7b-4a7a12351bd8"
      },
      "source": [
        "# 01. 0 Epoch ~ 8 Epoch\n",
        "list_epoch = np.array(range(100))\n",
        "epoch_train_losses = backup_epoch_train_loss[:]\n",
        "epoch_test_losses = backup_epoch_test_loss[:]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Attempting to set identical bottom == top == 98406.9306745182 results in singular transformations; automatically expanding.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "045VPwO7UP5X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "c336fd0b-1e17-4888-922c-a135e7318c11"
      },
      "source": [
        "# 03. 0 Epoch ~ 100 Epoch\n",
        "list_epoch = np.array(range(100))\n",
        "epoch_train_losses = backup_epoch_train_loss[:]\n",
        "epoch_test_losses = backup_epoch_test_loss[:]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmUH4stqR8pr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "fca93ba5-cb6b-4d76-be38-f337f277be70"
      },
      "source": [
        "# 01. 0 Epoch ~ 8 Epoch\n",
        "list_epoch = np.array(range(50))\n",
        "epoch_train_losses = backup_epoch_train_loss[:50]\n",
        "epoch_test_losses = backup_epoch_test_loss[:50]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Attempting to set identical bottom == top == 1578330.9842077089 results in singular transformations; automatically expanding.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4kpKVxCEpBs",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 10::</code>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6tXLL2rErou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "f0ad1420-d017-471d-c2f7-c28a4d8cf79f"
      },
      "source": [
        "# 01. 0 Epoch ~ 10 Epoch\n",
        "list_epoch = np.array(range(45))\n",
        "epoch_train_losses = backup_epoch_train_loss[:45]\n",
        "epoch_test_losses = backup_epoch_test_loss[:45]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Attempting to set identical bottom == top == 1579667.8324411134 results in singular transformations; automatically expanding.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WkyuNtrCLJJ",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 20::</code>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXv9m-SxBIfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "d3ecba5d-f14c-4807-cf02-b9c9cdfed1ef"
      },
      "source": [
        "# 01. 0 Epoch ~ 20 Epoch\n",
        "list_epoch = np.array(range(20))\n",
        "epoch_train_losses = backup_epoch_train_loss[:20]\n",
        "epoch_test_losses = backup_epoch_test_loss[:20]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Ukvua8BBkb",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 40::</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQGuW3TtC_5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "2d0f989b-8e7a-4563-a772-cf73e0c4d890"
      },
      "source": [
        "# 02. 0 Epoch ~ 40 Epoch\n",
        "list_epoch = np.array(range(45))\n",
        "epoch_train_losses = backup_epoch_train_loss[:45]\n",
        "epoch_test_losses = backup_epoch_test_loss[:45]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXa8FCwPQpzM",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 60::</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C7EuJKTDNiA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "4fa53f7a-23ff-4665-fec6-348b9b7b5cca"
      },
      "source": [
        "# 03. 0 Epoch ~ 60 Epoch\n",
        "list_epoch = np.array(range(60))\n",
        "epoch_train_losses = backup_epoch_train_loss[:60]\n",
        "epoch_test_losses = backup_epoch_test_loss[:60]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiFH0axWQtXo",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 80::</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yehSO166D7Ih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "7518f4bf-9b1a-4c95-9c19-1396e87984e1"
      },
      "source": [
        "# 03. 0 Epoch ~ 80 Epoch\n",
        "list_epoch = np.array(range(80))\n",
        "epoch_train_losses = backup_epoch_train_loss[:80]\n",
        "epoch_test_losses = backup_epoch_test_loss[:80]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAFNCAYAAABL8stCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwV9b3/8dfnJCEBwiKLEUQFVFAEAcPiUipoVUTrVtdWW6otP++vtXrdqrftbbV36f3VUtur1W5oa6u24nKttdWqoWpvBQGxAkIBxRJUQJQlQAIkn98fM2BCFmLIOfM9mffz8ZgHOfM9Z+adw0k++c7Md77m7oiIiIQik3QAERGR+lSYREQkKCpMIiISFBUmEREJigqTiIgERYVJRESCosIk0s7MbKCZuZkV5nCfE82sMlf7E8kmFSYREQmKCpOIiARFhUk6PDPrb2YPm9k6M3vTzL5Sr+1bZjbTzH5jZpvNbL6ZjazXfqSZzTKzDWa2yMzOqtfW2cy+Z2ZvmdlGM3vRzDrX2/VnzOwfZvaemX2tmWzjzexdMyuot+5cM/tb/PU4M5trZpvMbI2ZTW/l99xS7ilmtjj+fleb2fXx+j5m9kT8mvfN7AUz0+8IyTl96KRDi3+x/g54FTgQOBm4xsxOq/e0s4GHgF7A/cBjZlZkZkXxa58G9geuAn5tZkPj190GlAPHx6+9Eairt92PAUPjff6rmR25Zz53nw1sAU6qt/rTcQ6AHwA/cPfuwKHAb1vxPe8t98+B/+Pu3YDhwHPx+uuASqAvUAb8C6B7lknOBVeYzGyGma01s4WteO7H479wd5rZ+fXWTzKzBfWWajM7J7vJJVBjgb7ufqu7b3f3N4CfAhfXe848d5/p7juA6UAJcGy8lALfiV/7HPAEcElc8C4Hrnb31e5e6+7/6+419bZ7i7tvc/dXiQrjSJr2AHAJgJl1A6bE6wB2AIeZWR93r3L3l1rxPTebu942h5lZd3f/wN3n11vfDzjE3Xe4+wuum2lKAoIrTMC9wORWPvcfwFQ+/OsSAHevcPdR7j6K6C/RrUR/PUr6HAL0jw9PbTCzDUQ9gbJ6z1m16wt3ryPqNfSPl1Xxul3eIup59SEqYCta2Pe79b7eSlQsmnI/cJ6ZFQPnAfPd/a247QpgCLDEzF42szNb/G4jLeUG+BRR8XvLzP5sZsfF678LLAeeNrM3zOymVuxLpN0FV5jc/Xng/frrzOxQM/ujmc2Lj3sfET93pbv/jYaHT/Z0PvAHd9+avdQSsFXAm+7es97Szd2n1HvOQbu+iHtCA4C34+WgPc6zHAysBt4DqokOr+0Td19MVDhOp+FhPNx9mbtfQnRI7r+AmWbWdS+bbCk37v6yu58db/Mx4sOD7r7Z3a9z98HAWcC1Znbyvn5/Ih9VcIWpGT8BrnL3cuB64Ecf4bUX8+FhEUmfOcBmM/tqfLFCgZkNN7Ox9Z5TbmbnxeOOrgFqgJeA2UQ9nRvjc04TgU8CD8a9kRnA9PjiigIzOy7u9bTF/cDVwMeJzncBYGaXmlnfeH8b4tUt/SFGS7nNrJOZfcbMesSHLjft2p6ZnWlmh5mZARuB2lbsS6TdBV+YzKyU6OTyQ2a2APgx0XHw1ry2HzACeCp7CSVk7l4LnAmMAt4k6un8DOhR72n/A1wEfABcBpwXn2PZTvQL/fT4dT8CPuvuS+LXXQ+8BrxM1Mv/L9r+M/UAcCLwnLu/V2/9ZGCRmVURXQhxsbtv28v3vLfclwErzWwTcCXwmXj94cAzQBXwV+BH7l7Rxu9HpM0sxHObZjYQeMLdh5tZd2CpuzdbjMzs3vj5M/dYfzVwlLtPy2JcyWNm9i3gMHe/NOksIhIJvsfk7puAN83sAgCLNHd1054uQYfxRETySnCFycweIDqMMNTMKs3sCqJDDVeY2avAIqJxJ5jZWIvuD3YB8GMzW1RvOwOJTmr/ObffgYiI7IsgD+WJiEh6BddjEhGRdFNhEhGRoORsvpjW6NOnjw8cOHCftrFlyxa6dt3b+MNw5FPefMoKypttyps9+ZQV2pZ33rx577l73yYb3T2Ypby83PdVRUXFPm8jl/Ipbz5ldVfebFPe7MmnrO5tywvM9WZqgQ7liYhIUFSYREQkKCpMIiISlKAufhARCcGOHTuorKykuro6kf336NGD119/PZF9t0VLeUtKShgwYABFRUWt3p4Kk4jIHiorK+nWrRsDBw4kutl6bm3evJlu3brlfL9t1Vxed2f9+vVUVlYyaNCgVm9Ph/JERPZQXV1N7969EylKHYmZ0bt374/c81RhEhFpgopS+2jL+6jCJCIiQVFhEhEJzIYNG/jRjz7KRN2RKVOmsGHDhr0/cQ9Tp05l5syZe39ijqgwiYgEZuPGjU0Wpp07d7b4uieffJKePXtmK1bOqDCJhGjlX2Dm5VC1LukkkoBvfvObrFixglGjRjF27FgmTJjAWWedxbBhwwA455xzKC8v56ijjuInP/nJ7tcNHDiQ9957j5UrV3LkkUfyxS9+kaOOOopTTz2Vbdu2tWrfzz77LKNHj2bEiBFcfvnl1NTUAHDTTTcxbNgwjj76aK6//noAHnroIYYPH87xxx/Pxz/+8Xb7/nW5uEiINq6ChQ/DpK9BadP3uZTcuOV3i1j89qZ23eaw/t355iePan6ft9zC0qVLWbBgAbNmzeKMM85g4cKFuy+5njFjBr169WLbtm2MHTuWT33qU/Tu3bvBNpYtW8YDDzzAT3/6Uy688EIefvhhLr300hZzVVdXM3XqVJ599lmGDBnCZz/7We666y4uu+wyHn30UZYsWYKZ7T5ceOutt/LUU0/RvXt3amtr9/Fd+ZB6TCIhysR/M9a13w+75K9x48Y1GAf0wx/+kJEjR3LssceyatUqli1b1ug1gwYNYtSoUQCUl5ezcuXKve5n6dKlDBo0iCFDhgDwuc99jueff54ePXpQUlLCFVdcwSOPPEKXLl0AOOGEE5g6dSr33ntvuxYm9ZhEQrS7MO1INoe02LPJlfpTSsyaNYtnnnmGv/71r3Tp0oWJEyc2OU6ouLh499cFBQWtPpTXlMLCQubMmcOzzz7LzJkzueOOO3juuee4++67mT17No888gjl5eXMmzevUc+tTfvb5y2ISPsriG/fUtfyyW7pmEpLS9m8eXOTbRs3bmS//fajS5cuLFmyhJdeeqnd9jt06FBWrlzJ8uXLOeyww7jvvvs48cQTqaqqYuvWrUyZMoUTTjiBwYMHA7BixQrGjx/PsGHDeO6551i1apUKk0iHVdQZSssADfJMo969e3PCCScwfPhwOnfuTFlZ2e62yZMnc/fdd3PkkUcydOhQjj322Hbbb0lJCffccw8XXHABO3fuZOzYsVx55ZW8//77nH322VRXV+PuTJ8+HYAbbriBZcuWUVtbyymnnMLIkSPbJYcKk0iIDj0Jrv970ikkQffff3+T64uLi/nDH/7QZNuu80h9+vRh4cKFu9fvuoquOffee+/ur08++WReeeWVBu39+vVjzpw5jV73yCOPAO1/bz9d/CAiIkFRYRIJ0bqlcP9F8M6rSSeRDuRLX/oSo0aNarDcc889ScdqRIfyREJUvQn+/kcY+8Wkk0gHcueddyYdoVXUYxIJUaYg+leXi0sKqTCJhEiXi0uKqTCJhGjXANta9ZgkfVSYREJU1Bl6DYaiLkknEck5FSaREO03EL7yCgydnHQSSUBb52MCuP3229m6dWuLz9l1F/JQqTCJiASmufmYWqM1hSl0ulxcJERb34ffXAbH/hMceWbSaeSeMxqvO+ocGPdF2L4Vfn1B4/ZRn4bRn4Et6+G3n23Y9vnft7i7+vMxnXLKKey///789re/paamhnPPPZdbbrmFLVu2cOGFF1JZWUltbS3f+MY3WLNmDW+//TaTJk2iT58+VFRU7PVbmz59OjNmzADgC1/4Atdcc02T277ooou46aabePzxxyksLOTUU0/ltttu2+v220KFSSRE7vDWizDs7KSTSALqz8f09NNPM3PmTObMmYO7c9ZZZ/H888+zbt06+vfvz+9/HxW5jRs30qNHD6ZPn05FRQV9+vTZ637mzZvHPffcw+zZs3F3xo8fz4knnsgbb7zRaNvr169vck6mbFBhEgmRxjGFpaUeTqcuLbd37b3XHlJLnn76aZ5++mlGjx4NQFVVFcuWLWPChAlcd911fPWrX+XMM89kwoQJH3nbL774Iueee+7uaTXOO+88XnjhBSZPntxo2zt37tw9J9OZZ57JmWdmryevc0wiIdI4Jom5OzfffDMLFixgwYIFLF++nCuuuIIhQ4Ywf/58RowYwde//nVuvfXWdttnU9veNSfT+eefzxNPPMHkydm7MEeFSSREGseUavXnYzrttNOYMWMGVVVVAKxevZq1a9fy9ttv06VLFy699FJuuOEG5s+fD0C3bt2anctpTxMmTOCxxx5j69atbNmyhUcffZQJEyY0ue2qqio2btzIlClT+P73v8+rr2bvPo46lCcSokwRHDACuvZNOokkoP58TKeffjqf/vSnOe6444CoaP3qV79i+fLl3HDDDWQyGYqKirjrrrsAmDZtGpMnT6Z///57vfjhmGOOYerUqYwbNw6ILn4YPXo0Tz31VKNtb968uck5mbJBhUkkRJkMXPli0ikkQXvOx3T11Vc3eHzooYdy2mmnNXrdVVddxVVXXdXitnfN2wRw7bXXcu211zZoP+2005rcdlNzMmWDDuWJiEhQ1GMSCdWM0+GIM+D4LyedRPLU+PHjqampabDuvvvuY8SIEQklah0VJpFQrVkE/Y5OOoXksdmzZycdoU10KE8kVAWFulw8Qe6edIQOoS3vowqTSKgyhbpcPCElJSWsX79exWkfuTvr16+npKTkI71Oh/JEQpUpgrrapFOk0oABA6isrGTdunWJ7L+6uvoj/zJPUkt5S0pKGDBgwEfangqTSKgOGge9ByedIpWKiooYNGhQYvufNWvW7lsQ5YP2zqvCJBKqC+5JOoFIIrJ+jsnMCszsFTN7Itv7EhGR/JeLix+uBl7PwX5EOpYHPg2/uybpFCI5l9XCZGYDgDOAn2VzPyId0qbV0SKSMtnuMd0O3AjUZXk/Ih1PQZHGMUkqWbau0zezM4Ep7v5/zWwicL27N5pZysymAdMAysrKyh988MF92m9VVRWlpaX7tI1cyqe8+ZQV8j/vqFduxq2AV0f9W4Kpmpfv72/I8ikrtC3vpEmT5rn7mCYb3T0rC/CfQCWwEngX2Ar8qqXXlJeX+76qqKjY523kUj7lzaes7h0g7z1nuP98ciJZWiPv39+A5VNW97blBeZ6M7Uga4fy3P1mdx/g7gOBi4Hn3P3SbO1PpMM5+Fg4aGzSKURyTuOYREJ10teTTiCSiJwUJnefBczKxb5ERCS/6SauIqF6/CvRnEwiKaPCJBKqms2wZW3SKURyToVJJFQFRZr2QlJJhUkkVJr2QlJKhUkkVJkCqFOPSdJHl4uLhGrA2Kg4iaSMCpNIqI65LFpEUkaH8kREJCgqTCKhqvhP+M7BSacQyTkVJpFQeW00lkkkZVSYREKVKQSvgzpNZybposIkEqpMfG2SJguUlFFhEgnV7sKksUySLipMIqHqdzSMuRxMP6aSLhrHJBKqQ0+KFpGU0Z9iIiGrqwP3pFOI5JQKk0io5v8Sbt0PNq1OOolITqkwiYRq18UPmvpCUkaFSSRUu6/K09QXki4qTCKh0uXiklIqTCKh0gBbSSkVJpFQ9Tkcjr8KuvROOolITmkck0io9j8STv23pFOI5Jx6TCKhqt0J1Rt1VZ6kjgqTSKhWvRTNx/SPvyadRCSnVJhEQqVxTJJSKkwiocoURf9qHJOkjAqTSKgyBdG/GsckKaPCJBKqgl09Jo1jknRRYRIJVWkZTLwZ+gxNOolITmkck0iouvaBiTclnUIk59RjEglVXS1sXA01VUknEckpFSaRUFWtge8Pg9ceSjqJSE6pMImESjdxlZRSYRIJlQqTpJQKk0ioVJgkpVSYREK1axyTbkkkKaPLxUVCVdAJTv13OPi4pJOI5JQKk0ioMgVw/JeTTiGSczqUJxKy95ZB1dqkU4jklAqTSMju/hj8738nnUIkp1SYREKWKdK0F5I6KkwiIcsUaNoLSR0VJpGQFRRpHJOkTtYKk5mVmNkcM3vVzBaZ2S3Z2pdIh5Up1DgmSZ1sXi5eA5zk7lVmVgS8aGZ/cPeXsrhPkY7llG9DjwOTTiGSU1krTO7uwK779RfFi2drfyId0tEXJJ1AJOeyeo7JzArMbAGwFviTu8/O5v5EOpw1i+G95UmnEMkpizo2Wd6JWU/gUeAqd1+4R9s0YBpAWVlZ+YMPPrhP+6qqqqK0tHSftpFL+ZQ3n7JCx8g75uWvsK1zPxYNvzmhVM3rCO9vqPIpK7Qt76RJk+a5+5gmG909Jwvwr8D1LT2nvLzc91VFRcU+byOX8ilvPmV17yB57/qY+68vzHmW1ugQ72+g8imre9vyAnO9mVqQzavy+sY9JcysM3AKsCRb+xPpkDKFulxcUiebV+X1A35hZgVE57J+6+5PZHF/Ih1PQZEuF5fUyeZVeX8DRmdr+yKpkCnULYkkdTTthUjITvxq0glEck6FSSRkg09MOoFIzuleeSIhW7MIVr2cdAqRnFJhEglZxX/A765OOoVITqkwiYQsU6hpLyR1VJhEQqZpLySFVJhEQpYphFoVJkkXFSaRkOnOD5JCulxcJGTjr4QRmvpC0kWFSSRkBwxPOoFIzulQnkjI1r4OS55MOoVITqkwiYRswf0w8/KkU4jklAqTSMg0jklSSIVJJGS7xjHlYKZpkVCoMImELBNfn6SpLyRFVJhEQra7MGksk6SHLhcXCdnRF8Ihx0eH9ERSQoVJJGQ9BkSLSIroUJ5IyN5bDgsegB3bkk4ikjMqTCIhW/kCPHYlbPsg6SQiOaPCJBKyXeeWdPGDpIgKk0jIMnFhqtUgW0kPFSaRkGUKon81jklSRIVJJGS7D+WpxyTp0arCZGZXm1l3i/zczOab2anZDieSeoNOhCtfhF6Dk04ikjOt7TFd7u6bgFOB/YDLgO9kLZWIRDr3hANGQFHnpJOI5ExrC5PF/04B7nP3RfXWiUi2bFwNc34Km95JOolIzrS2MM0zs6eJCtNTZtYNqMteLBEBYP1yePJ6eH9F0klEcqa1tyS6AhgFvOHuW82sF/D57MUSEeDDix90ubikSGt7TMcBS919g5ldCnwd2Ji9WCICfDiOSQNsJUVaW5juAraa2UjgOmAF8MuspRKRyO5xTCpMkh6tLUw73d2Bs4E73P1OoFv2YokIoEN5kkqtPce02cxuJrpMfIKZZQBNECOSbX2GwFdegdKypJOI5Exre0wXATVE45neBQYA381aKhGJFBZHg2s7dU06iUjOtKowxcXo10APMzsTqHZ3nWMSybbqjfDCdHh3YdJJRHKmtbckuhCYA1wAXAjMNrPzsxlMRICazfDsLfD2/KSTiORMa88xfQ0Y6+5rAcysL/AMMDNbwUQEyMQ/orr4QVKkteeYMruKUmz9R3itiLSVxjFJCrW2x/RHM3sKeCB+fBHwZHYiichuBfGPqAqTpEirCpO732BmnwJOiFf9xN0fzV4sEQF0KE9SqbU9Jtz9YeDhLGYRkT0VdYHrl0NxadJJRHKmxcJkZpsBb6oJcHfvnpVUIhIxg9K+SacQyakWC5O767ZDIkmr+A84aDwcdnLSSURyQlfWiYTuxdvhzeeTTiGSM1krTGZ2kJlVmNliM1tkZldna18iHVqmUFflSaq0+uKHNtgJXOfu8+MZb+eZ2Z/cfXEW9ynS8RSoMEm6ZK3H5O7vuPv8+OvNwOvAgdnan0iHlSnU5eKSKjk5x2RmA4HRwOxc7E+kQ8kUqcckqWLR/H9Z3IFZKfBn4N/d/ZEm2qcB0wDKysrKH3zwwX3aX1VVFaWl+TPmI5/y5lNW6Dh5M7U1uBXiu2azDURHeX9DlE9ZoW15J02aNM/dxzTZ6O5ZW4gmE3wKuLY1zy8vL/d9VVFRsc/byKV8yptPWd2VN9uUN3vyKat72/ICc72ZWpDNq/IM+DnwurtPz9Z+RDq8F74H8+9LOoVIzmTzHNMJRFOxn2RmC+JlShb3J9Ix/e0hWPZU0ilEciZrl4u7+4tEty4SkX1RUAh1tUmnEMkZ3flBJHS6XFxSRoVJJHS6XFxSRoVJJHQFnZJOIJJT2bwlkYi0h8//PukEIjmlHpOIiARFhUkkdC/dHc3JJJISKkwioXvzeViiw3mSHipMIqHTtBeSMipMIqHTOCZJGRUmkdBpHJOkjAqTSOiKu0Gn/JkCQWRfaRyTSOjOuC3pBCI5pR6TiIgERYVJJHQL7oeHv5h0CpGcUWESCd2aRbDkiaRTiOSMCpNI6AqKdLm4pIoKk0joMhpgK+miwiQSukwR4JrFVlJDhUkkdJ33gx4Hq9ckqaFxTCKhGz8tWkRSQj0mEREJigqTSOiW/hF+eQ5s25B0EpGcUGESCd3mt+GNCthZnXQSkZxQYRIJXSY+FayxTJISKkwiocsURf/qqjxJCRUmkdDt6jGpMElKqDCJhK7zfrD/MMgUJJ1EJCc0jkkkdId/IlpEUkI9JhERCYoKk0joKufBz06BdxcmnUQkJ1SYREK3fTNUzoHqjUknEckJFSaR0OmqPEkZFSaR0O0ex6QBtpIOKkwiodvdY9J8TJIOKkwioSvpDgPGQnG3pJOI5ITGMYmErs/h8IVnkk4hkjPqMYmISFBUmERCt7ESfnQcLHky6SQiOaHCJBI6r4O1i2Hr+qSTiOSECpNI6DTthaSMCpNI6DTAVlJGhUkkdAUqTJIuKkwioSsohsGToHv/pJOI5ITGMYmErlMX+OxjSacQyRn1mEREJChZK0xmNsPM1pqZJpER2Ve3j4C//DDpFCI5kc0e073A5CxuXyQ9Nr0N2z5IOoVITmStMLn788D72dq+SKpkijTthaSGuXv2Nm42EHjC3Ye38JxpwDSAsrKy8gcffHCf9llVVUVpaek+bSOX8ilvPmWFjpX3Yy9czDv9PsGKw76Q41TN60jvb2jyKSu0Le+kSZPmufuYJhvdPWsLMBBY2Nrnl5eX+76qqKjY523kUj7lzaes7h0s73cOcX/iulxFaZUO9f4GJp+yurctLzDXm6kFuipPJB8MPQMOaPbAg0iHonFMIvngnDuTTiCSM9m8XPwB4K/AUDOrNLMrsrUvERHpOLLWY3L3S7K1bZHU+ckkKDsKzr4j6SQiWadzTCL5YPsWqNmcdAqRnFBhEskHmULdXVxSQ4VJJB8UqDBJeqgwieSDTCHU6s4Pkg66XFwkHww5HQqLk04hkhMqTCL54MQbkk4gkjM6lCciIkFRYRLJB/dfDD89OekUIjmhwiSSL2q3J51AJCdUmETyQaZAl4tLaqgwieSDgiIVJkkNFSaRfKBxTJIiulxcJB8cejL0PizpFCI5ocIkkg9G6Wb9kh46lCeSD2p3wo5tSacQyQkVJpF88NS/wPeOSDqFSE6oMInkA017ISmiwiSSDzTthaSICpNIPtDl4pIiKkwi+SBTBF4L7kknEck6FSaRfHDI8TDhehUmSQWNYxLJB4NPjBaRFFCPSSQf7NgGVeugri7pJCJZp8Ikkg/m/QJuOwyqNySdRCTrVJhE8kGmIPpXl4xLCqgwieSDgqLoXxUmSQEVJpF8kImvU9JYJkkBFSaRfJBRj0nSQ4VJJB/0Oxo+8S3ovF/SSUSyTuOYRPLB/kdGi0gKqMckkg+2b4X334Ad1UknEck6FSaRfLDyRfjhaFizMOkkIlmnwiSSDwrio+66+EFSQIVJJB/ocnFJERUmkXygy8UlRVSYRPJBRofyJD1UmETywX6HwJTboO/QpJOIZJ3GMYnkg9L9YdwXk04hkhPqMYnkg5018O5rsPX9pJOIZJ0Kk0g+2LAK7v4YLH8m6SQiWafCJJIPNI5JUkSFSSQfaByTpIgKk0g+0DgmSREVJpF8oHFMkiJZLUxmNtnMlprZcjO7KZv7EunQikvhnLth8KSkk4hkXdbGMZlZAXAncApQCbxsZo+7++Js7VOkwyoshlGXJJ1CJCfM3bOzYbPjgG+5+2nx45sB3P0/m3vNmDFjfO7cuW3e5y2/W8T/Lv4HPXv2bPM2WmtEzXw6ec3ux+8WHMjqooM/8nY2bNiQk7ztIZ+yQgfL686I7a9w4M5VrCvYf/fq9QV9WVl0GADHVL+E0fDneW3BAawqGoR5LcfUzGm02V2f20LfzsiaeY3aVxcezLuFB1JcV83w7a80aNuyZQvrewxnXeEBdK7bwrDtf2v0+pVFh7G+oC9d6zZzxPbGU3asKBrChoLedK/dwOE7Xm/UvqzoSDYV9GS/2vUM3vH3Ru1LOg1nS6YbvWvXMnDHikbtizsdzbZMV/bf+Q69Ni6ia9euDdpfKx7Ndiuh385K+u9c1ej1C4rHUGtFHLjjLQ6ofbtR+/zi8bhlOHjHG/StXdOgzckwv2Q8AIN2LKNX7XsN2ndSxKslYwA4dPtSetZ9OEZty5YtFJX24rXiYwA4fPtiutdtbPD6bdaFxcUjAThi+0K61m1u0L4l040lnYYDcFTNq5T41gbtmzI9WNZpGND49xnAhkwvVnSK7jQyqvplCogOI2/O9KBw4LF885NH7X7urFmzmDhxYqP3pyVmNs/dxzTVls07PxwI1P+frgTG7/kkM5sGTAMoKytj1qxZbd5hZWUNtbW1bNiwoc3baK3Lq2+nv6/d/fj+wnNZVHTRR95OrvK2h3zKCh0rr3kd/1z9bbqyrcH6pwomMr3TlQBct+3bFFLboP3Rgsnc3WkqRb6dG6tvabTdBwrPYVHRxfTwTU22/7zwEpYUnc0BdWu4saZx+x0bPs+ywtPoVfdWk+3fLfq/rCj8OP1rl3Lj9sbtt3S6lpUF4zisdgE3bv9Oo/abO/0L/yg4mhG1L3Pj9tsbtV9d/G1WZw5n3M6/cN2OHzdqn1b8Xd7JHMTEnRX8045fwvaG7ZcW38GGTB9O3/Enpu78TaPXf8kRC1IAAAlDSURBVKrkZ1RZKeft+D0X7Xy8UfuUkl9Ra4V8ZvtjnFX7dIO27RTxyc73AfCJ7b/lE7UvNGjfQDcu6vxTAM6o+RUn1L3coP2dD/ZnaskPATiv5h6OqXutQfsbdjD/VPL/ALi4+m6O9OUN2hdlhnBt8a0AfK76vznEKxu0z82M5GvFN0fvU/X32N/XN2h/PjOefy/+ZwC+vO07dGMLAK9khvOjwgHMmrVu93Orqqr26Xf3nrLZYzofmOzuX4gfXwaMd/cvN/eafe0xQdsqd5usWQy19T7lpWXQvd9H3kzO8raDfMoKHTDv5nejpb4uvaBn3FN/e0Hj13TtAz0GQF0dvNu4R7P7c1u7A9YsatzerR90K4vuPLG2YY9m7rx5jDnp7Ggf27fCe417NPQ8OMpYUwXrlzdu328gdO4J1Rvh/Tcbt/caDCXdYdsH8MFbjdv7HA6dusKW9bCxcY+HvkOhqDNUrWNuxeOMKS9v2L7/MCjs1PR7C1A2PBpDtultqFrbuL3fSDCLBkBvbfiLHbOoHaLs2z5o2J4phAOiHg3vvwHVm3Y3zZ03jzHjjoOyqEfD+hVQ07BHRFHnD++duO7vsKNhj4hOXaP3B2DtEti5x+zHxd2g96HR12sWNR6KUNIDeg2Kvn73Nairbfy6WD71mFYDB9V7PCBe1zHs+sCI5Eq3A6KlOf1HNd+WybTcXlDUcnthcaP2qr9viIoSQKcuLb++uLTl9pIeLbd33i9amtO1d7Q0p7QvVd0ObX4fe3tvu/ePlub0PChamrPfIdHSnF6DGzys+vuGhr9j9igEjfQd0nL7/ke03F52VMvtB4xoub2dZfOqvJeBw81skJl1Ai4GGveFRURE6slaj8ndd5rZl4GngAJghrs3caxARETkQ1md9sLdnwSezOY+RESkY9GdH0REJCgqTCIiEhQVJhERCYoKk4iIBEWFSUREgqLCJCIiQVFhEhGRoGTtXnltYWbrgCZuiPWR9AHe2+uzwpFPefMpKyhvtilv9uRTVmhb3kPcvW9TDUEVpvZgZnObuzFgiPIpbz5lBeXNNuXNnnzKCu2fV4fyREQkKCpMIiISlI5YmH6SdICPKJ/y5lNWUN5sU97syaes0M55O9w5JhERyW8dscckIiJ5rMMUJjObbGZLzWy5md2UdJ49mdkMM1trZgvrretlZn8ys2Xxvy1M0ZlbZnaQmVWY2WIzW2RmV8frg8xsZiVmNsfMXo3z3hKvH2Rms+PPxW/iSSuDYGYFZvaKmT0RPw4560oze83MFpjZ3HhdkJ8FADPraWYzzWyJmb1uZseFmtfMhsbv665lk5ldE3Def45/xhaa2QPxz167fnY7RGEyswLgTuB0YBhwiZmFNvf5vcDkPdbdBDzr7ocDz8aPQ7ETuM7dhwHHAl+K39NQM9cAJ7n7SGAUMNnMjgX+C/i+ux8GfABckWDGPV0NvF7vcchZASa5+6h6lwWH+lkA+AHwR3c/AhhJ9D4Hmdfdl8bv6yigHNgKPEqAec3sQOArwBh3H040CezFtPdn193zfgGOA56q9/hm4OakczWRcyCwsN7jpUC/+Ot+wNKkM7aQ/X+AU/IhM9AFmA+MJxr0V9jU5yThjAOIftmcBDwBWKhZ4zwrgT57rAvyswD0AN4kPoceet49Mp4K/CXUvMCBwCqgF9FEs08Ap7X3Z7dD9Jj48M3apTJeF7oyd38n/vpdoCzJMM0xs4HAaGA2AWeOD40tANYCfwJWABvcfWf8lJA+F7cDNwJ18ePehJsVwIGnzWyemU2L14X6WRgErAPuiQ+V/szMuhJu3vouBh6Ivw4ur7uvBm4D/gG8A2wE5tHOn92OUpjynkd/agR3iaSZlQIPA9e4+6b6baFldvdajw6HDADGAUckHKlJZnYmsNbd5yWd5SP4mLsfQ3S4/Etm9vH6jYF9FgqBY4C73H00sIU9DoMFlheA+LzMWcBDe7aFkjc+z3U2UfHvD3Sl8SmKfdZRCtNq4KB6jwfE60K3xsz6AcT/rk04TwNmVkRUlH7t7o/Eq4PODODuG4AKokMKPc2sMG4K5XNxAnCWma0EHiQ6nPcDwswK7P5LGXdfS3T+YxzhfhYqgUp3nx0/nklUqELNu8vpwHx3XxM/DjHvJ4A33X2du+8AHiH6PLfrZ7ejFKaXgcPjK0M6EXWHH084U2s8Dnwu/vpzROdxgmBmBvwceN3dp9drCjKzmfU1s57x152Jzoe9TlSgzo+fFkRed7/Z3Qe4+0Ciz+pz7v4ZAswKYGZdzazbrq+JzoMsJNDPgru/C6wys6HxqpOBxQSat55L+PAwHoSZ9x/AsWbWJf4dseu9bd/PbtIn09rxpNwU4O9E5xW+lnSeJvI9QHRMdgfRX3RXEJ1XeBZYBjwD9Eo6Z728HyM6dPA3YEG8TAk1M3A08EqcdyHwr/H6wcAcYDnRIZLipLPukXsi8ETIWeNcr8bLol0/X6F+FuJso4C58efhMWC/wPN2BdYDPeqtCzIvcAuwJP45uw8obu/Pru78ICIiQekoh/JERKSDUGESEZGgqDCJiEhQVJhERCQoKkwiIhIUFSaRwJnZxF13IBdJAxUmEREJigqTSDsxs0vjOaEWmNmP45vKVpnZ9+P5a541s77xc0eZ2Utm9jcze3TXXDtmdpiZPRPPKzXfzA6NN19ab36hX8ej7kU6JBUmkXZgZkcCFwEneHQj2VrgM0Qj+ue6+1HAn4Fvxi/5JfBVdz8aeK3e+l8Dd3o0r9TxRHcLgeju7tcQzTc2mOj+ZCIdUuHenyIirXAy0SRvL8edmc5EN92sA34TP+dXwCNm1gPo6e5/jtf/Angovh/dge7+KIC7VwPE25vj7pXx4wVEc3u9mP1vSyT3VJhE2ocBv3D3mxusNPvGHs9r6z3Aaup9XYt+dqUD06E8kfbxLHC+me0PYGa9zOwQop+xXXdd/jTwortvBD4wswnx+suAP7v7ZqDSzM6Jt1FsZl1y+l2IBEB/dYm0A3dfbGZfJ5rlNUN0F/kvEU1SNy5uW0t0HgqiqQHujgvPG8Dn4/WXAT82s1vjbVyQw29DJAi6u7hIFplZlbuXJp1DJJ/oUJ6IiARFPSYREQmKekwiIhIUFSYREQmKCpOIiARFhUlERIKiwiQiIkFRYRIRkaD8f8mWc5V3HUopAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KmmwyW4QwdC",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 100::</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f152o_NwD-5z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "08b95dd7-6a35-45bd-957f-968670560e5d"
      },
      "source": [
        "# 03. 0 Epoch ~ 100 Epoch\n",
        "list_epoch = np.array(range(100))\n",
        "epoch_train_losses = backup_epoch_train_loss[:]\n",
        "epoch_test_losses = backup_epoch_test_loss[:]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Attempting to set identical bottom == top == 1573034.3819593147 results in singular transformations; automatically expanding.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cofyXuQJuI3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2d2f3a21-36cf-4d02-8a18-3b1c68a39ed8"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL8ixXdrdLpi",
        "colab_type": "text"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uKB5_XmdMzz",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation\n",
        "* train, val, test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0uHo2uDdUYr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "ff953b65-6912-4613-f006-1a562a58e87c"
      },
      "source": [
        "transforms = transforms.Compose([\n",
        "                                transforms.Resize((224, 224)),                # Change size of Image to (224, 224)\n",
        "                                transforms.Grayscale(num_output_channels=1),  # Makes it 1-dimension channel\n",
        "                                transforms.ToTensor(),                        # Convert a PIL Image or numpy.ndarray to tensor.\n",
        "                                                                              # Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8\n",
        "                                                                              # In the other cases, tensors are returned without scaling.\n",
        "                                # transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "                                \n",
        "                                ])\n",
        "\n",
        "# make custom dataset \n",
        "# use torchvision.datasets.ImageFolder()\n",
        "trainset = torchvision.datasets.ImageFolder(root='../../../../InformationSecurity_Summer/malimg',\n",
        "                                            transform=transforms)  # make custom dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-11eb43b621a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transforms = transforms.Compose([\n\u001b[0m\u001b[1;32m      2\u001b[0m                                 \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0;31m# Change size of Image to (224, 224)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGrayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_output_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Makes it 1-dimension channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0;31m# Convert a PIL Image or numpy.ndarray to tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                               \u001b[0;31m# Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Compose' object has no attribute 'Compose'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5wfy8WBdWch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_dataset = trainset\n",
        "\n",
        "# maek train, val, test: 8:1:1\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = int(0.1 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "print(train_size, val_size, test_size)\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBJn4oeOdaBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make custom data_loader\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                         batch_size=16,\n",
        "                         shuffle=True,\n",
        "                         pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=16,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True)\n",
        " \n",
        "test_loader = DataLoader(test_dataset,\n",
        "                        batch_size=16,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True)  # Instead, we recommend using automatic memory pinning (i.e., setting pin_memory=True)\n",
        "                                          #  which enables fast data transfer to CUDA-enabled GPUs\n",
        "\n",
        "# First, insert all test dataset\n",
        "# z_loader: for latent vector extraction\n",
        "z_loader = DataLoader(full_dataset,\n",
        "                        batch_size=9339,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}