{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Again] Re-Start_Training_ResNet-VAE .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM9t+pdyb6CNoJM248SZ7c2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5d09d521994e4158af198e34db272d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0940dfe53d74400ab24532a8ecba876c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ace390a4137c4eee8984d3c5e7940b92",
              "IPY_MODEL_cd5cca8bcd5744f49f508b10eb035c5f"
            ]
          }
        },
        "0940dfe53d74400ab24532a8ecba876c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ace390a4137c4eee8984d3c5e7940b92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b3ab21dc231d4922b73f1daeb2dce84e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a4b82b23cb4344c49fe215349d2c1c52"
          }
        },
        "cd5cca8bcd5744f49f508b10eb035c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_78f81e1a6a864937b3a74a4f9944b933",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [00:01&lt;00:00, 130MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aededc4b050a425da95ca025c6509c52"
          }
        },
        "b3ab21dc231d4922b73f1daeb2dce84e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a4b82b23cb4344c49fe215349d2c1c52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78f81e1a6a864937b3a74a4f9944b933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aededc4b050a425da95ca025c6509c52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Steve-YJ/Colab_Exercise/blob/master/%5BAgain%5D_Re_Start_Training_ResNet_VAE_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyu1S97RFUTe",
        "colab_type": "text"
      },
      "source": [
        "# README.ME\n",
        "* Again, Re-Start it!\n",
        "* Training ResNet-VAE Again!!!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHP0jjvZFhfQ",
        "colab_type": "text"
      },
      "source": [
        "✅ Check Point<br>\n",
        "> 1. Train ResNet-VAE\n",
        "> 2. Hyperparameter Optimization\n",
        "> 3. Save Model's Parameters\n",
        "> 4. Plot & Save values\n",
        "\n",
        "<br>\n",
        "<code>::start:: 20.09.08.Tue pm1:00 ~ </code><br>\n",
        "<code>::Continue:: 20.09.08.Tue pm 3:10 ~</code><br>\n",
        "<code>::Add: Plot 20.09.09.Wed pm 1:00</code><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7_BuKPEYXfH",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "< Questions ><br>\n",
        "Q1. ResNet-VAE를 학습시킬때는 Validation Set을 따로 두지 않아도 될까?<br>\n",
        "-> Training은 Train, Testset만 구성한다?! -20.09.08.Tue. pm3:30-\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5L94y80VeP1",
        "colab_type": "text"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGODey7EFOAI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "55017b6c-4b29-48ee-cdda-a6552bc3cf72"
      },
      "source": [
        "# Mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/3wGQD4n0PoL0Vqvktc1rnDGCpkFsZj3Js8ha8b8ZldI3kq3xubXECZw\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZttVYB7WlCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuAGvGTZWwIK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "6769e9ca-1807-4af7-88e2-285484099e40"
      },
      "source": [
        "%cd drive/My\\ Drive/Post_InfoSec_Exps/ResNet-VAE/ResNetVAE-master.zip (Unzipped Files)/ResNetVAE-master\n",
        "! ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Post_InfoSec_Exps/ResNet-VAE/ResNetVAE-master.zip (Unzipped Files)/ResNetVAE-master\n",
            "'01.Tutorial-ResNet-VAE.ipynb의 사본'\n",
            " 01.Tutorial-ResNet-VAE-Recon.ipynb\n",
            " 02.Tutorial-ResNet-VAE-Tunning.ipynb\n",
            " 03.Tutorial-ResNet-VAE-Tunning.ipynb\n",
            "'03.Tutorial-ResNet-VAE-Tunning.ipynb의 사본'\n",
            "'04.Post-01.Tutorial-ResNet-VAE.ipynb사본의 사본'\n",
            "'05.Post-01.Tutorial-ResNet-VAE_Train_Again.ipynb의 사본'\n",
            " Again_ResNet-VAE_Exp01\n",
            "'[Again] Re-Start_Training_ResNet-VAE .ipynb'\n",
            " fig\n",
            " generated_Malimg.png\n",
            " modules.py\n",
            " plot_latent.ipynb\n",
            " plot_latent_vector\n",
            " plot_train_test_loss\n",
            "'[Post_Exp]04-2.ResNet-VAE_Train_Again.ipynb'\n",
            "'[Post_Exp]04-2.ResNet-VAE_Train_Again.ipynb의 사본'\n",
            "'[Post_Exp]04-3.ResNet-VAE_Train_Again.ipynb'\n",
            "'[Post_Exp]04-3.ResNet-VAE_Train_Again.ipynb의 사본'\n",
            "'[Post_Exp]05-2.ResNet-VAE_Reduce_lr.ipynb'\n",
            " __pycache__\n",
            " README.md\n",
            " recon_sampling\n",
            " reconstruction_Malimg.png\n",
            " ResNetVAE_cifar10.py\n",
            " ResNetVAE_FACE.py\n",
            " ResNetVAE_MNIST.py\n",
            " ResNetVAE_reconstruction.ipynb\n",
            " results_Malimg\n",
            " results_Malimg_Exp3\n",
            " results_Malimg_Exp4\n",
            " results_Malimg_Exp4_3\n",
            " results_Malimg_Exp5\n",
            " results_Malimg_Exp5_2\n",
            " results_ResNet-VAE_Exp01\n",
            " train_test_loss_plot.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YmzTtwbW7nl",
        "colab_type": "text"
      },
      "source": [
        " our working directory results should be saved in 'Again_ResNet-VAE_Exp01'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObxrqpqMVg-x",
        "colab_type": "text"
      },
      "source": [
        "## 01. Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LO1BCfgVh9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "c8394b5e-5e5f-4cee-9fb8-b11f30a34985"
      },
      "source": [
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# save single numpy array\n",
        "from tempfile import TemporaryFile\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import torch\n",
        "import torch.utils.data  # torch.utils.data\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torchvision \n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# load modules\n",
        "from torchvision import models\n",
        "from modules import *"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG2NjnRqVi7s",
        "colab_type": "text"
      },
      "source": [
        "## 02. Data Preparation\n",
        "\n",
        "* Make Custom Dataset\n",
        "* Make Custom DataLoader\n",
        "* Train_Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtOGbYZ9ZcaG",
        "colab_type": "text"
      },
      "source": [
        "< Notice ><br>\n",
        "ResNet-VAE 모델을 훈련시킬 때는 데이터셋을 train, test만 준비한다.(Validation set 없이!)<br>\n",
        "이는 일반적인 VAE Model Tutorial의 학습방식을 따른다. <code>-20.09.08.Tue. pm3:00-</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAIvvzgqVlnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor()])  # Composes several transforms together.\n",
        "\n",
        "# make custom dataset\n",
        "trainset = torchvision.datasets.ImageFolder(root='../../../../InformationSecurity_Summer/malimg',\n",
        "                                            transform=transforms)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O6Dfsmfbeoa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "0707d16b-70b9-48db-9a39-d6b40fae2c86"
      },
      "source": [
        "classes = trainset.classes\n",
        "classes"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Adialer.C',\n",
              " 'Agent.FYI',\n",
              " 'Allaple.A',\n",
              " 'Allaple.L',\n",
              " 'Alueron.gen!J',\n",
              " 'Autorun.K',\n",
              " 'C2LOP.P',\n",
              " 'C2LOP.gen!g',\n",
              " 'Dialplatform.B',\n",
              " 'Dontovo.A',\n",
              " 'Fakerean',\n",
              " 'Instantaccess',\n",
              " 'Lolyda.AA1',\n",
              " 'Lolyda.AA2',\n",
              " 'Lolyda.AA3',\n",
              " 'Lolyda.AT',\n",
              " 'Malex.gen!J',\n",
              " 'Obfuscator.AD',\n",
              " 'Rbot!gen',\n",
              " 'Skintrim.N',\n",
              " 'Swizzor.gen!E',\n",
              " 'Swizzor.gen!I',\n",
              " 'VB.AT',\n",
              " 'Wintrim.BX',\n",
              " 'Yuner.A']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPMemVTjbkPX",
        "colab_type": "text"
      },
      "source": [
        "Split Train Data to Train, Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z84JNpgWboc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bc4fd2e5-1d8f-4ec7-d89d-eac4b93ac6a9"
      },
      "source": [
        "full_dataset = trainset\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "print('print train_size, test_size: ', train_size, test_size)\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])  # Randomly split a dataset into non-overlapping new datasets of given lengths. Optionally fix the generator for reproducible results, e.g.:"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "print train_size, test_size:  7471 1868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bmFNAWlbjpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=16,\n",
        "                                           shuffle=True,\n",
        "                                           pin_memory=True)\n",
        "valid_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                           batch_size=16,\n",
        "                                           shuffle=True,\n",
        "                                           pin_memory=True)  # (i.e., setting pin_memory=True)\n",
        "                                                             #  which enables fast data transfer to CUDA-enabled GPUs\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9eC3rSSdeu_",
        "colab_type": "text"
      },
      "source": [
        "3-channel Image 출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjS2jwTLdgds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    np_img = img.numpy()\n",
        "\n",
        "    plt.imshow(np.transpose(np_img, (1, 2, 0)))  # Convert (C, W, H) to (W, H, C)\n",
        "\n",
        "    print(np_img.shape)  # np_img shape\n",
        "    print((np.transpose(np_img, (1, 2, 0))).shape)  # transposed shape "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh144ur6dx9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a10d6e6c-4c87-479d-e2c0-77abd7539528"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "print(labels)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 2,  2, 15,  2,  2, 24, 24,  2, 24, 22, 12,  2,  4,  2, 11, 16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-McJWBIRdyO8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "4c882ea6-9b48-43e3-84d6-2309197cb837"
      },
      "source": [
        "print(images.shape)\n",
        "imshow(torchvision.utils.make_grid(images, nrow=4))\n",
        "print(images.shape)\n",
        "print((torchvision.utils.make_grid(images)).shape)\n",
        "print(\"\".join(\"%5s \"%classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 3, 224, 224])\n",
            "(3, 906, 906)\n",
            "(906, 906, 3)\n",
            "torch.Size([16, 3, 224, 224])\n",
            "torch.Size([3, 454, 1810])\n",
            "Allaple.A Allaple.A Lolyda.AT Allaple.A \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXtaJBsMVnki",
        "colab_type": "text"
      },
      "source": [
        "## 03. Model Architecture\n",
        "\n",
        "* argparser\n",
        "* save model parameters\n",
        "* loss function\n",
        "* define hyperthesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gupryt18d2Pk",
        "colab_type": "text"
      },
      "source": [
        "✅ Have to-do <br>\n",
        "> 1. Train ResNet-VAE\n",
        "> 2. 📌 Hyperparameter Optimization >>> argparser...!!\n",
        "> 3. 📌 Save Model's Parameters\n",
        "> 4. 📌 Plot & Save values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmCALY3ogDOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save model\n",
        "save_model_path = './results_ResNet-VAE_Exp01'  # save_model parameter"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57uBwQXKgUsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_mkdir(dir_name):\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.mkdir(dir_name)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzUcKJYGgivD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    # MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    MSE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return MSE + KLD"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzJXvtlnmvUb",
        "colab_type": "text"
      },
      "source": [
        "model from this repository: https://github.com/hsinyilin19/ResNetVAE/blob/master/modules.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diJLO0clmrL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet_VAE(nn.Module):\n",
        "    def __init__(self, fc_hidden1=1024, fc_hidden2=768, drop_p=0.3, CNN_embed_dim=256):\n",
        "        super(ResNet_VAE, self).__init__()\n",
        "\n",
        "        self.fc_hidden1, self.fc_hidden2, self.CNN_embed_dim = fc_hidden1, fc_hidden2, CNN_embed_dim\n",
        "\n",
        "        # CNN architechtures\n",
        "        self.ch1, self.ch2, self.ch3, self.ch4 = 16, 32, 64, 128\n",
        "        self.k1, self.k2, self.k3, self.k4 = (5, 5), (3, 3), (3, 3), (3, 3)      # 2d kernal size\n",
        "        self.s1, self.s2, self.s3, self.s4 = (2, 2), (2, 2), (2, 2), (2, 2)      # 2d strides\n",
        "        self.pd1, self.pd2, self.pd3, self.pd4 = (0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\n",
        "\n",
        "        # encoding components\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.fc1 = nn.Linear(resnet.fc.in_features, self.fc_hidden1)\n",
        "        self.bn1 = nn.BatchNorm1d(self.fc_hidden1, momentum=0.01)\n",
        "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
        "        self.bn2 = nn.BatchNorm1d(self.fc_hidden2, momentum=0.01)\n",
        "        # Latent vectors mu and sigma\n",
        "        self.fc3_mu = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)      # output = CNN embedding latent variables\n",
        "        self.fc3_logvar = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)  # output = CNN embedding latent variables\n",
        "\n",
        "        # Sampling vector\n",
        "        self.fc4 = nn.Linear(self.CNN_embed_dim, self.fc_hidden2)\n",
        "        self.fc_bn4 = nn.BatchNorm1d(self.fc_hidden2)\n",
        "        self.fc5 = nn.Linear(self.fc_hidden2, 64 * 4 * 4)\n",
        "        self.fc_bn5 = nn.BatchNorm1d(64 * 4 * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Decoder\n",
        "        self.convTrans6 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=self.k4, stride=self.s4,\n",
        "                               padding=self.pd4),\n",
        "            nn.BatchNorm2d(32, momentum=0.01),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.convTrans7 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=32, out_channels=8, kernel_size=self.k3, stride=self.s3,\n",
        "                               padding=self.pd3),\n",
        "            nn.BatchNorm2d(8, momentum=0.01),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.convTrans8 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=self.k2, stride=self.s2,\n",
        "                               padding=self.pd2),\n",
        "            nn.BatchNorm2d(3, momentum=0.01),\n",
        "            nn.Sigmoid()    # y = (y1, y2, y3) \\in [0 ,1]^3\n",
        "        )\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.resnet(x)  # ResNet\n",
        "        x = x.view(x.size(0), -1)  # flatten output of conv\n",
        "\n",
        "        # FC layers\n",
        "        x = self.bn1(self.fc1(x))\n",
        "        x = self.relu(x)\n",
        "        x = self.bn2(self.fc2(x))\n",
        "        x = self.relu(x)\n",
        "        # x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "        mu, logvar = self.fc3_mu(x), self.fc3_logvar(x)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = Variable(std.data.new(std.size()).normal_())\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def decode(self, z):\n",
        "        x = self.relu(self.fc_bn4(self.fc4(z)))\n",
        "        x = self.relu(self.fc_bn5(self.fc5(x))).view(-1, 64, 4, 4)\n",
        "        x = self.convTrans6(x)\n",
        "        x = self.convTrans7(x)\n",
        "        x = self.convTrans8(x)\n",
        "        x = F.interpolate(x, size=(224, 224), mode='bilinear')\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_reconst = self.decode(z)\n",
        "\n",
        "        return x_reconst, z, mu, logvar"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuGbzAY_Vp7g",
        "colab_type": "text"
      },
      "source": [
        "## 04. Define Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dfNGY24giRq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "14e43753-5fee-4f9f-de63-2d67b8f03dd1"
      },
      "source": [
        "# Detect devices\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mDXrJ4bgfpM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "5d09d521994e4158af198e34db272d3a",
            "0940dfe53d74400ab24532a8ecba876c",
            "ace390a4137c4eee8984d3c5e7940b92",
            "cd5cca8bcd5744f49f508b10eb035c5f",
            "b3ab21dc231d4922b73f1daeb2dce84e",
            "a4b82b23cb4344c49fe215349d2c1c52",
            "78f81e1a6a864937b3a74a4f9944b933",
            "aededc4b050a425da95ca025c6509c52"
          ]
        },
        "outputId": "630fea7a-bc6b-48b6-8792-28ac7cb3d988"
      },
      "source": [
        "'''\n",
        "### If you want to use pre-trained model ####\n",
        "pre_saved_model_path = './results_Malimg'\n",
        "epoch=20\n",
        "'''\n",
        "\n",
        "# Create model\n",
        "resnet_vae = ResNet_VAE(fc_hidden1=args.CNN_fc_hidden1, fc_hidden2=args.CNN_fc_hidden2, drop_p=args.dropout_p, CNN_embed_dim=args.CNN_embed_dim).to(device)\n",
        "'''\n",
        "### If you want to use pre-trained model ####\n",
        "resnet_vae.load_state_dict(torch.load(os.path.join(pre_saved_model_path, 'model_epoch{}.pth'.format(epoch))))\n",
        "'''\n",
        "print('Number of {} parameters'.format(sum(p.numel() for p in resnet_vae.parameters() if p.requires_grad)))\n",
        "print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n",
        "model_params = list(resnet_vae.parameters())\n",
        "optimizer = torch.optim.Adam(model_params, lr=args.learning_rate)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d09d521994e4158af198e34db272d3a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of 63158425 parameters\n",
            "Using 1 GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh1Ve1PdgjAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
        "    # set model as training mode\n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "    all_y, all_z, all_mu, all_logvar = [], [], [], []\n",
        "    N_count = 0   # counting total trained sample in one epoch\n",
        "    for batch_idx, (X, y) in enumerate(train_loader):\n",
        "        # distribute data to device\n",
        "        X, y = X.to(device), y.to(device).view(-1, )\n",
        "        N_count += X.size(0)  # count batch_size sample\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        X_reconst, z, mu, logvar = model(X)  # VAE\n",
        "        loss = loss_function(X_reconst, X, mu, logvar)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        all_y.extend(y.data.cpu().numpy())\n",
        "        all_z.extend(z.data.cpu().numpy())\n",
        "        all_mu.extend(mu.data.cpu().numpy())\n",
        "        all_logvar.extend(logvar.data.cpu().numpy())\n",
        "\n",
        "        # show information\n",
        "        if (batch_idx + 1) % log_interval == 0:  # if batch_size = 16 => 160\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "    # calculate train_loss\n",
        "    losses /= len(train_loader.dataset)\n",
        "    all_y = np.stack(all_y, axis=0)\n",
        "    all_z = np.stack(all_z, axis=0)\n",
        "    all_mu = np.stack(all_mu, axis=0)\n",
        "    all_logvar = np.stack(all_logvar, axis=0)\n",
        "\n",
        "    # save Pytorch models of best record\n",
        "    torch.save(model.state_dict(), os.path.join(save_model_path, 'model_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
        "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
        "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
        "\n",
        "    return X.data.cpu().numpy(), all_y, all_z, all_mu, all_logvar, losses\n",
        "\n",
        "\n",
        "def validation(model, device, optimizer, test_loader):\n",
        "    # set model as testing mode\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    all_y, all_z, all_mu, all_logvar = [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y) in enumerate(test_loader):\n",
        "            # distribute data to device\n",
        "            X, y = X.to(device), y.to(device).view(-1, )\n",
        "            X_reconst, z, mu, logvar = model(X)\n",
        "\n",
        "            loss = loss_function(X_reconst, X, mu, logvar)\n",
        "            test_loss += loss.item()  # sum up batch loss\n",
        "\n",
        "            all_y.extend(y.data.cpu().numpy())\n",
        "            all_z.extend(z.data.cpu().numpy())\n",
        "            all_mu.extend(mu.data.cpu().numpy())\n",
        "            all_logvar.extend(logvar.data.cpu().numpy())\n",
        "            \n",
        "        ## Save_Recon_Malimg\n",
        "            if i == 0:\n",
        "                n = min(X.size(0), 8)\n",
        "                comparison = torch.cat([X[:n],\n",
        "                                    X_reconst.view(16, 3, 224, 224)[:n]])  # Recon 4 Images\n",
        "                save_image(comparison.cpu(),\n",
        "                        './results_ResNet-VAE_Exp01/recon_sampling/reconstruction_' + str(epoch + 1) + '.png', nrow=n)\n",
        "                # save_image(comparison.cpu(),\n",
        "                #         os.path.join(save_model_path, '/recon_sampling/reconstruction{}.png'.format(epoch + 1)))\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    all_y = np.stack(all_y, axis=0)\n",
        "    all_z = np.stack(all_z, axis=0)\n",
        "    all_mu = np.stack(all_mu, axis=0)\n",
        "    all_logvar = np.stack(all_logvar, axis=0)\n",
        "\n",
        "    # show information\n",
        "    print('\\nTest set ({:d} samples): Average loss: {:.4f}\\n'.format(len(test_loader.dataset), test_loss))\n",
        "    return X.data.cpu().numpy(), all_y, all_z, all_mu, all_logvar, test_loss\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWrOAYCHw-Dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(resnet_vae)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGZkel39Vo0L",
        "colab_type": "text"
      },
      "source": [
        "### Argparse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o89ggU79exTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "db2e915d-94f6-483b-bb5b-20328ec181ef"
      },
      "source": [
        "import argparse\n",
        "\n",
        "seed = 123\n",
        "np.random.seed(seed)  # Reseed a legacy MT19937 BitGenerator\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "args.CNN_fc_hidden1 = 1024\n",
        "args.CNN_fc_hidden2 = 1024\n",
        "args.CNN_embed_dim = 256  # latent dim extracted by 2D CNN\n",
        "args.res_size = 224       # ResNet Image size\n",
        "args.dropout_p = 0.2           # dropout probability\n",
        "\n",
        "# training parameters\n",
        "args.epochs = 20\n",
        "args.batch_size = 50\n",
        "args.learning_rate = 1e-3\n",
        "args.log_interval = 10  # interval for displaying training info\n",
        "\n",
        "print(args)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(CNN_embed_dim=256, CNN_fc_hidden1=1024, CNN_fc_hidden2=1024, batch_size=50, dropout_p=0.2, epochs=20, learning_rate=0.001, log_interval=10, res_size=224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFuyo3QTrfJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "10bbf9e7-0578-4e2c-f8a8-5276ef6e8c57"
      },
      "source": [
        "print(args)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(CNN_embed_dim=256, CNN_fc_hidden1=1024, CNN_fc_hidden2=1024, batch_size=50, dropout_p=0.2, epochs=20, learning_rate=0.001, log_interval=10, res_size=224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMvQL9RPtkPM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "4430fbad-b7b1-4aa6-f7c6-a4b4319b7742"
      },
      "source": [
        "args.epochs = 100\n",
        "print(args)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(CNN_embed_dim=256, CNN_fc_hidden1=1024, CNN_fc_hidden2=1024, batch_size=50, dropout_p=0.2, epochs=100, learning_rate=0.001, log_interval=10, res_size=224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2-9ZnrDVuoI",
        "colab_type": "text"
      },
      "source": [
        "## 05. Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdrpoBUQVwz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dad3eb7f-992f-4b19-988e-162f9ab52081"
      },
      "source": [
        "# start training\n",
        "\n",
        "list_epoch=[]\n",
        "epoch_train_losses = []\n",
        "epoch_test_losses = []\n",
        "list_acc = []\n",
        "list_acc_epoch =[]\n",
        "check_mkdir(save_model_path)\n",
        "\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "\n",
        "    # train, test model\n",
        "    X_train, y_train, z_train, mu_train, logvar_train, epoch_train_loss = train(args.log_interval, resnet_vae, device, train_loader, optimizer, epoch)\n",
        "    X_test, y_test, z_test, mu_test, logvar_test, epoch_test_loss = validation(resnet_vae, device, optimizer, valid_loader)\n",
        "\n",
        "    # save results\n",
        "    list_epoch.append(epoch)\n",
        "    epoch_train_losses.append(epoch_train_loss)\n",
        "    epoch_test_losses.append(epoch_test_loss)\n",
        "\n",
        "    \n",
        "    # save all train test results\n",
        "    # A = np.array(all_train_losses)\n",
        "    B = np.array(epoch_train_losses)\n",
        "    C = np.array(epoch_test_losses)\n",
        "    \n",
        "    # np.save(os.path.join(save_model_path, 'ResNet_VAE_training_loss_all.npy'), A)\n",
        "    np.save(os.path.join(save_model_path, 'ResNet_VAE_training_loss.npy'), B)\n",
        "    np.save(os.path.join(save_model_path, 'ResNet_VAE_test_loss.npy'), C)\n",
        "\n",
        "    np.save(os.path.join(save_model_path, 'X_Malimg_train_epoch{}.npy'.format(epoch + 1)), X_train) #save last batch\n",
        "    np.save(os.path.join(save_model_path, 'y_Malimg_train_epoch{}.npy'.format(epoch + 1)), y_train)\n",
        "    np.save(os.path.join(save_model_path, 'z_Malimg_train_epoch{}.npy'.format(epoch + 1)), z_train)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Train Epoch: 1 [320/7471 (4%)]\tLoss: 1663872.000000\n",
            "Train Epoch: 1 [480/7471 (6%)]\tLoss: 1673245.250000\n",
            "Train Epoch: 1 [640/7471 (9%)]\tLoss: 1666329.125000\n",
            "Train Epoch: 1 [800/7471 (11%)]\tLoss: 1655970.125000\n",
            "Train Epoch: 1 [960/7471 (13%)]\tLoss: 1652011.250000\n",
            "Train Epoch: 1 [1120/7471 (15%)]\tLoss: 1653168.625000\n",
            "Train Epoch: 1 [1280/7471 (17%)]\tLoss: 1664516.125000\n",
            "Train Epoch: 1 [1440/7471 (19%)]\tLoss: 1650637.125000\n",
            "Train Epoch: 1 [1600/7471 (21%)]\tLoss: 1638597.250000\n",
            "Train Epoch: 1 [1760/7471 (24%)]\tLoss: 1656080.250000\n",
            "Train Epoch: 1 [1920/7471 (26%)]\tLoss: 1664741.750000\n",
            "Train Epoch: 1 [2080/7471 (28%)]\tLoss: 1627625.750000\n",
            "Train Epoch: 1 [2240/7471 (30%)]\tLoss: 1633431.375000\n",
            "Train Epoch: 1 [2400/7471 (32%)]\tLoss: 1643345.000000\n",
            "Train Epoch: 1 [2560/7471 (34%)]\tLoss: 1627399.750000\n",
            "Train Epoch: 1 [2720/7471 (36%)]\tLoss: 1617293.500000\n",
            "Train Epoch: 1 [2880/7471 (39%)]\tLoss: 1625719.125000\n",
            "Train Epoch: 1 [3040/7471 (41%)]\tLoss: 1655760.625000\n",
            "Train Epoch: 1 [3200/7471 (43%)]\tLoss: 1654996.000000\n",
            "Train Epoch: 1 [3360/7471 (45%)]\tLoss: 1645986.000000\n",
            "Train Epoch: 1 [3520/7471 (47%)]\tLoss: 1671683.250000\n",
            "Train Epoch: 1 [3680/7471 (49%)]\tLoss: 1647754.625000\n",
            "Train Epoch: 1 [3840/7471 (51%)]\tLoss: 1618821.125000\n",
            "Train Epoch: 1 [4000/7471 (54%)]\tLoss: 1630413.125000\n",
            "Train Epoch: 1 [4160/7471 (56%)]\tLoss: 1627947.875000\n",
            "Train Epoch: 1 [4320/7471 (58%)]\tLoss: 1643958.500000\n",
            "Train Epoch: 1 [4480/7471 (60%)]\tLoss: 1651149.375000\n",
            "Train Epoch: 1 [4640/7471 (62%)]\tLoss: 1650797.375000\n",
            "Train Epoch: 1 [4800/7471 (64%)]\tLoss: 1614956.750000\n",
            "Train Epoch: 1 [4960/7471 (66%)]\tLoss: 1621760.125000\n",
            "Train Epoch: 1 [5120/7471 (69%)]\tLoss: 1614780.500000\n",
            "Train Epoch: 1 [5280/7471 (71%)]\tLoss: 1595116.250000\n",
            "Train Epoch: 1 [5440/7471 (73%)]\tLoss: 1627965.375000\n",
            "Train Epoch: 1 [5600/7471 (75%)]\tLoss: 1627943.625000\n",
            "Train Epoch: 1 [5760/7471 (77%)]\tLoss: 1606273.250000\n",
            "Train Epoch: 1 [5920/7471 (79%)]\tLoss: 1610396.875000\n",
            "Train Epoch: 1 [6080/7471 (81%)]\tLoss: 1643688.000000\n",
            "Train Epoch: 1 [6240/7471 (84%)]\tLoss: 1628070.000000\n",
            "Train Epoch: 1 [6400/7471 (86%)]\tLoss: 1648356.250000\n",
            "Train Epoch: 1 [6560/7471 (88%)]\tLoss: 1565806.625000\n",
            "Train Epoch: 1 [6720/7471 (90%)]\tLoss: 1600846.125000\n",
            "Train Epoch: 1 [6880/7471 (92%)]\tLoss: 1642413.625000\n",
            "Train Epoch: 1 [7040/7471 (94%)]\tLoss: 1615052.500000\n",
            "Train Epoch: 1 [7200/7471 (96%)]\tLoss: 1624421.500000\n",
            "Train Epoch: 1 [7360/7471 (99%)]\tLoss: 1595021.000000\n",
            "Epoch 1 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 2 [160/7471 (2%)]\tLoss: 1638124.750000\n",
            "Train Epoch: 2 [320/7471 (4%)]\tLoss: 1630460.625000\n",
            "Train Epoch: 2 [480/7471 (6%)]\tLoss: 1615888.125000\n",
            "Train Epoch: 2 [640/7471 (9%)]\tLoss: 1600745.625000\n",
            "Train Epoch: 2 [800/7471 (11%)]\tLoss: 1608715.375000\n",
            "Train Epoch: 2 [960/7471 (13%)]\tLoss: 1652586.500000\n",
            "Train Epoch: 2 [1120/7471 (15%)]\tLoss: 1606889.500000\n",
            "Train Epoch: 2 [1280/7471 (17%)]\tLoss: 1625643.000000\n",
            "Train Epoch: 2 [1440/7471 (19%)]\tLoss: 1599501.875000\n",
            "Train Epoch: 2 [1600/7471 (21%)]\tLoss: 1660855.125000\n",
            "Train Epoch: 2 [1760/7471 (24%)]\tLoss: 1617220.000000\n",
            "Train Epoch: 2 [1920/7471 (26%)]\tLoss: 1623305.125000\n",
            "Train Epoch: 2 [2080/7471 (28%)]\tLoss: 1650226.750000\n",
            "Train Epoch: 2 [2240/7471 (30%)]\tLoss: 1642594.375000\n",
            "Train Epoch: 2 [2400/7471 (32%)]\tLoss: 1618370.375000\n",
            "Train Epoch: 2 [2560/7471 (34%)]\tLoss: 1617079.500000\n",
            "Train Epoch: 2 [2720/7471 (36%)]\tLoss: 1633970.875000\n",
            "Train Epoch: 2 [2880/7471 (39%)]\tLoss: 1638984.375000\n",
            "Train Epoch: 2 [3040/7471 (41%)]\tLoss: 1640539.375000\n",
            "Train Epoch: 2 [3200/7471 (43%)]\tLoss: 1595387.500000\n",
            "Train Epoch: 2 [3360/7471 (45%)]\tLoss: 1612598.250000\n",
            "Train Epoch: 2 [3520/7471 (47%)]\tLoss: 1587785.125000\n",
            "Train Epoch: 2 [3680/7471 (49%)]\tLoss: 1618284.250000\n",
            "Train Epoch: 2 [3840/7471 (51%)]\tLoss: 1670260.125000\n",
            "Train Epoch: 2 [4000/7471 (54%)]\tLoss: 1639710.750000\n",
            "Train Epoch: 2 [4160/7471 (56%)]\tLoss: 1585709.875000\n",
            "Train Epoch: 2 [4320/7471 (58%)]\tLoss: 1619067.625000\n",
            "Train Epoch: 2 [4480/7471 (60%)]\tLoss: 1600471.875000\n",
            "Train Epoch: 2 [4640/7471 (62%)]\tLoss: 1581970.000000\n",
            "Train Epoch: 2 [4800/7471 (64%)]\tLoss: 1568222.500000\n",
            "Train Epoch: 2 [4960/7471 (66%)]\tLoss: 1625650.250000\n",
            "Train Epoch: 2 [5120/7471 (69%)]\tLoss: 1637322.250000\n",
            "Train Epoch: 2 [5280/7471 (71%)]\tLoss: 1579114.375000\n",
            "Train Epoch: 2 [5440/7471 (73%)]\tLoss: 1614714.875000\n",
            "Train Epoch: 2 [5600/7471 (75%)]\tLoss: 1600310.375000\n",
            "Train Epoch: 2 [5760/7471 (77%)]\tLoss: 1613841.375000\n",
            "Train Epoch: 2 [5920/7471 (79%)]\tLoss: 1589390.250000\n",
            "Train Epoch: 2 [6080/7471 (81%)]\tLoss: 1622623.750000\n",
            "Train Epoch: 2 [6240/7471 (84%)]\tLoss: 1533361.375000\n",
            "Train Epoch: 2 [6400/7471 (86%)]\tLoss: 1606695.125000\n",
            "Train Epoch: 2 [6560/7471 (88%)]\tLoss: 1623617.625000\n",
            "Train Epoch: 2 [6720/7471 (90%)]\tLoss: 1636675.000000\n",
            "Train Epoch: 2 [6880/7471 (92%)]\tLoss: 1632004.625000\n",
            "Train Epoch: 2 [7040/7471 (94%)]\tLoss: 1614112.875000\n",
            "Train Epoch: 2 [7200/7471 (96%)]\tLoss: 1614774.625000\n",
            "Train Epoch: 2 [7360/7471 (99%)]\tLoss: 1649852.000000\n",
            "Epoch 2 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 3 [160/7471 (2%)]\tLoss: 1646686.750000\n",
            "Train Epoch: 3 [320/7471 (4%)]\tLoss: 1577615.375000\n",
            "Train Epoch: 3 [480/7471 (6%)]\tLoss: 1621973.125000\n",
            "Train Epoch: 3 [640/7471 (9%)]\tLoss: 1622402.375000\n",
            "Train Epoch: 3 [800/7471 (11%)]\tLoss: 1615995.000000\n",
            "Train Epoch: 3 [960/7471 (13%)]\tLoss: 1646218.750000\n",
            "Train Epoch: 3 [1120/7471 (15%)]\tLoss: 1610477.250000\n",
            "Train Epoch: 3 [1280/7471 (17%)]\tLoss: 1621476.875000\n",
            "Train Epoch: 3 [1440/7471 (19%)]\tLoss: 1632998.500000\n",
            "Train Epoch: 3 [1600/7471 (21%)]\tLoss: 1621830.625000\n",
            "Train Epoch: 3 [1760/7471 (24%)]\tLoss: 1574590.375000\n",
            "Train Epoch: 3 [1920/7471 (26%)]\tLoss: 1622255.000000\n",
            "Train Epoch: 3 [2080/7471 (28%)]\tLoss: 1627478.750000\n",
            "Train Epoch: 3 [2240/7471 (30%)]\tLoss: 1588174.750000\n",
            "Train Epoch: 3 [2400/7471 (32%)]\tLoss: 1647936.500000\n",
            "Train Epoch: 3 [2560/7471 (34%)]\tLoss: 1600886.875000\n",
            "Train Epoch: 3 [2720/7471 (36%)]\tLoss: 1608709.000000\n",
            "Train Epoch: 3 [2880/7471 (39%)]\tLoss: 1609930.250000\n",
            "Train Epoch: 3 [3040/7471 (41%)]\tLoss: 1589301.250000\n",
            "Train Epoch: 3 [3200/7471 (43%)]\tLoss: 1601460.375000\n",
            "Train Epoch: 3 [3360/7471 (45%)]\tLoss: 1615997.375000\n",
            "Train Epoch: 3 [3520/7471 (47%)]\tLoss: 1613724.750000\n",
            "Train Epoch: 3 [3680/7471 (49%)]\tLoss: 1559322.000000\n",
            "Train Epoch: 3 [3840/7471 (51%)]\tLoss: 1565618.250000\n",
            "Train Epoch: 3 [4000/7471 (54%)]\tLoss: 1623344.750000\n",
            "Train Epoch: 3 [4160/7471 (56%)]\tLoss: 1550898.500000\n",
            "Train Epoch: 3 [4320/7471 (58%)]\tLoss: 1611786.000000\n",
            "Train Epoch: 3 [4480/7471 (60%)]\tLoss: 1613765.250000\n",
            "Train Epoch: 3 [4640/7471 (62%)]\tLoss: 1593745.875000\n",
            "Train Epoch: 3 [4800/7471 (64%)]\tLoss: 1566163.625000\n",
            "Train Epoch: 3 [4960/7471 (66%)]\tLoss: 1576930.750000\n",
            "Train Epoch: 3 [5120/7471 (69%)]\tLoss: 1643413.000000\n",
            "Train Epoch: 3 [5280/7471 (71%)]\tLoss: 1674588.750000\n",
            "Train Epoch: 3 [5440/7471 (73%)]\tLoss: 1603321.500000\n",
            "Train Epoch: 3 [5600/7471 (75%)]\tLoss: 1615882.875000\n",
            "Train Epoch: 3 [5760/7471 (77%)]\tLoss: 1574501.250000\n",
            "Train Epoch: 3 [5920/7471 (79%)]\tLoss: 1629088.375000\n",
            "Train Epoch: 3 [6080/7471 (81%)]\tLoss: 1629943.125000\n",
            "Train Epoch: 3 [6240/7471 (84%)]\tLoss: 1585108.125000\n",
            "Train Epoch: 3 [6400/7471 (86%)]\tLoss: 1592669.875000\n",
            "Train Epoch: 3 [6560/7471 (88%)]\tLoss: 1621371.875000\n",
            "Train Epoch: 3 [6720/7471 (90%)]\tLoss: 1655093.875000\n",
            "Train Epoch: 3 [6880/7471 (92%)]\tLoss: 1583736.375000\n",
            "Train Epoch: 3 [7040/7471 (94%)]\tLoss: 1615828.625000\n",
            "Train Epoch: 3 [7200/7471 (96%)]\tLoss: 1631256.625000\n",
            "Train Epoch: 3 [7360/7471 (99%)]\tLoss: 1578683.250000\n",
            "Epoch 3 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 4 [160/7471 (2%)]\tLoss: 1577907.625000\n",
            "Train Epoch: 4 [320/7471 (4%)]\tLoss: 1617395.625000\n",
            "Train Epoch: 4 [480/7471 (6%)]\tLoss: 1601049.125000\n",
            "Train Epoch: 4 [640/7471 (9%)]\tLoss: 1608837.000000\n",
            "Train Epoch: 4 [800/7471 (11%)]\tLoss: 1601950.125000\n",
            "Train Epoch: 4 [960/7471 (13%)]\tLoss: 1585947.375000\n",
            "Train Epoch: 4 [1120/7471 (15%)]\tLoss: 1618168.250000\n",
            "Train Epoch: 4 [1280/7471 (17%)]\tLoss: 1652667.625000\n",
            "Train Epoch: 4 [1440/7471 (19%)]\tLoss: 1620717.875000\n",
            "Train Epoch: 4 [1600/7471 (21%)]\tLoss: 1611458.250000\n",
            "Train Epoch: 4 [1760/7471 (24%)]\tLoss: 1633973.500000\n",
            "Train Epoch: 4 [1920/7471 (26%)]\tLoss: 1623466.375000\n",
            "Train Epoch: 4 [2080/7471 (28%)]\tLoss: 1628139.625000\n",
            "Train Epoch: 4 [2240/7471 (30%)]\tLoss: 1622696.375000\n",
            "Train Epoch: 4 [2400/7471 (32%)]\tLoss: 1609970.375000\n",
            "Train Epoch: 4 [2560/7471 (34%)]\tLoss: 1602367.875000\n",
            "Train Epoch: 4 [2720/7471 (36%)]\tLoss: 1623455.500000\n",
            "Train Epoch: 4 [2880/7471 (39%)]\tLoss: 1652228.000000\n",
            "Train Epoch: 4 [3040/7471 (41%)]\tLoss: 1612147.125000\n",
            "Train Epoch: 4 [3200/7471 (43%)]\tLoss: 1620009.375000\n",
            "Train Epoch: 4 [3360/7471 (45%)]\tLoss: 1582172.250000\n",
            "Train Epoch: 4 [3520/7471 (47%)]\tLoss: 1576030.125000\n",
            "Train Epoch: 4 [3680/7471 (49%)]\tLoss: 1594783.500000\n",
            "Train Epoch: 4 [3840/7471 (51%)]\tLoss: 1520140.500000\n",
            "Train Epoch: 4 [4000/7471 (54%)]\tLoss: 1591723.375000\n",
            "Train Epoch: 4 [4160/7471 (56%)]\tLoss: 1590871.250000\n",
            "Train Epoch: 4 [4320/7471 (58%)]\tLoss: 1648433.875000\n",
            "Train Epoch: 4 [4480/7471 (60%)]\tLoss: 1643392.000000\n",
            "Train Epoch: 4 [4640/7471 (62%)]\tLoss: 1545399.250000\n",
            "Train Epoch: 4 [4800/7471 (64%)]\tLoss: 1607695.000000\n",
            "Train Epoch: 4 [4960/7471 (66%)]\tLoss: 1596125.625000\n",
            "Train Epoch: 4 [5120/7471 (69%)]\tLoss: 1630229.500000\n",
            "Train Epoch: 4 [5280/7471 (71%)]\tLoss: 1618706.625000\n",
            "Train Epoch: 4 [5440/7471 (73%)]\tLoss: 1677364.125000\n",
            "Train Epoch: 4 [5600/7471 (75%)]\tLoss: 1647619.000000\n",
            "Train Epoch: 4 [5760/7471 (77%)]\tLoss: 1609320.500000\n",
            "Train Epoch: 4 [5920/7471 (79%)]\tLoss: 1620627.750000\n",
            "Train Epoch: 4 [6080/7471 (81%)]\tLoss: 1642723.625000\n",
            "Train Epoch: 4 [6240/7471 (84%)]\tLoss: 1590775.500000\n",
            "Train Epoch: 4 [6400/7471 (86%)]\tLoss: 1604084.750000\n",
            "Train Epoch: 4 [6560/7471 (88%)]\tLoss: 1620020.375000\n",
            "Train Epoch: 4 [6720/7471 (90%)]\tLoss: 1617686.000000\n",
            "Train Epoch: 4 [6880/7471 (92%)]\tLoss: 1579645.125000\n",
            "Train Epoch: 4 [7040/7471 (94%)]\tLoss: 1627866.875000\n",
            "Train Epoch: 4 [7200/7471 (96%)]\tLoss: 1575168.625000\n",
            "Train Epoch: 4 [7360/7471 (99%)]\tLoss: 1554660.875000\n",
            "Epoch 4 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 5 [160/7471 (2%)]\tLoss: 1636329.875000\n",
            "Train Epoch: 5 [320/7471 (4%)]\tLoss: 1620082.625000\n",
            "Train Epoch: 5 [480/7471 (6%)]\tLoss: 1629063.375000\n",
            "Train Epoch: 5 [640/7471 (9%)]\tLoss: 1657729.000000\n",
            "Train Epoch: 5 [800/7471 (11%)]\tLoss: 1624493.500000\n",
            "Train Epoch: 5 [960/7471 (13%)]\tLoss: 1631253.125000\n",
            "Train Epoch: 5 [1120/7471 (15%)]\tLoss: 1599759.875000\n",
            "Train Epoch: 5 [1280/7471 (17%)]\tLoss: 1583858.625000\n",
            "Train Epoch: 5 [1440/7471 (19%)]\tLoss: 1564886.250000\n",
            "Train Epoch: 5 [1600/7471 (21%)]\tLoss: 1639682.125000\n",
            "Train Epoch: 5 [1760/7471 (24%)]\tLoss: 1600521.875000\n",
            "Train Epoch: 5 [1920/7471 (26%)]\tLoss: 1596378.750000\n",
            "Train Epoch: 5 [2080/7471 (28%)]\tLoss: 1595632.125000\n",
            "Train Epoch: 5 [2240/7471 (30%)]\tLoss: 1627861.375000\n",
            "Train Epoch: 5 [2400/7471 (32%)]\tLoss: 1610138.500000\n",
            "Train Epoch: 5 [2560/7471 (34%)]\tLoss: 1633927.250000\n",
            "Train Epoch: 5 [2720/7471 (36%)]\tLoss: 1587774.000000\n",
            "Train Epoch: 5 [2880/7471 (39%)]\tLoss: 1564040.875000\n",
            "Train Epoch: 5 [3040/7471 (41%)]\tLoss: 1603307.750000\n",
            "Train Epoch: 5 [3200/7471 (43%)]\tLoss: 1569016.750000\n",
            "Train Epoch: 5 [3360/7471 (45%)]\tLoss: 1614844.875000\n",
            "Train Epoch: 5 [3520/7471 (47%)]\tLoss: 1607187.250000\n",
            "Train Epoch: 5 [3680/7471 (49%)]\tLoss: 1566551.375000\n",
            "Train Epoch: 5 [3840/7471 (51%)]\tLoss: 1618628.125000\n",
            "Train Epoch: 5 [4000/7471 (54%)]\tLoss: 1602401.125000\n",
            "Train Epoch: 5 [4160/7471 (56%)]\tLoss: 1624442.625000\n",
            "Train Epoch: 5 [4320/7471 (58%)]\tLoss: 1612315.875000\n",
            "Train Epoch: 5 [4480/7471 (60%)]\tLoss: 1563085.500000\n",
            "Train Epoch: 5 [4640/7471 (62%)]\tLoss: 1599928.375000\n",
            "Train Epoch: 5 [4800/7471 (64%)]\tLoss: 1636901.875000\n",
            "Train Epoch: 5 [4960/7471 (66%)]\tLoss: 1653554.375000\n",
            "Train Epoch: 5 [5120/7471 (69%)]\tLoss: 1592159.375000\n",
            "Train Epoch: 5 [5280/7471 (71%)]\tLoss: 1601088.375000\n",
            "Train Epoch: 5 [5440/7471 (73%)]\tLoss: 1664219.375000\n",
            "Train Epoch: 5 [5600/7471 (75%)]\tLoss: 1658508.625000\n",
            "Train Epoch: 5 [5760/7471 (77%)]\tLoss: 1620997.000000\n",
            "Train Epoch: 5 [5920/7471 (79%)]\tLoss: 1627283.750000\n",
            "Train Epoch: 5 [6080/7471 (81%)]\tLoss: 1582777.500000\n",
            "Train Epoch: 5 [6240/7471 (84%)]\tLoss: 1603502.625000\n",
            "Train Epoch: 5 [6400/7471 (86%)]\tLoss: 1601128.625000\n",
            "Train Epoch: 5 [6560/7471 (88%)]\tLoss: 1590542.500000\n",
            "Train Epoch: 5 [6720/7471 (90%)]\tLoss: 1649821.625000\n",
            "Train Epoch: 5 [6880/7471 (92%)]\tLoss: 1533051.500000\n",
            "Train Epoch: 5 [7040/7471 (94%)]\tLoss: 1621246.875000\n",
            "Train Epoch: 5 [7200/7471 (96%)]\tLoss: 1565614.500000\n",
            "Train Epoch: 5 [7360/7471 (99%)]\tLoss: 1586562.375000\n",
            "Epoch 5 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 2043397937569089.7500\n",
            "\n",
            "Train Epoch: 6 [160/7471 (2%)]\tLoss: 1646461.750000\n",
            "Train Epoch: 6 [320/7471 (4%)]\tLoss: 1590788.625000\n",
            "Train Epoch: 6 [480/7471 (6%)]\tLoss: 1610837.750000\n",
            "Train Epoch: 6 [640/7471 (9%)]\tLoss: 1647338.500000\n",
            "Train Epoch: 6 [800/7471 (11%)]\tLoss: 1610491.500000\n",
            "Train Epoch: 6 [960/7471 (13%)]\tLoss: 1555603.375000\n",
            "Train Epoch: 6 [1120/7471 (15%)]\tLoss: 1588647.500000\n",
            "Train Epoch: 6 [1280/7471 (17%)]\tLoss: 1630242.000000\n",
            "Train Epoch: 6 [1440/7471 (19%)]\tLoss: 1598875.250000\n",
            "Train Epoch: 6 [1600/7471 (21%)]\tLoss: 1641983.000000\n",
            "Train Epoch: 6 [1760/7471 (24%)]\tLoss: 1622674.750000\n",
            "Train Epoch: 6 [1920/7471 (26%)]\tLoss: 1584188.375000\n",
            "Train Epoch: 6 [2080/7471 (28%)]\tLoss: 1562322.500000\n",
            "Train Epoch: 6 [2240/7471 (30%)]\tLoss: 1632849.250000\n",
            "Train Epoch: 6 [2400/7471 (32%)]\tLoss: 1611517.500000\n",
            "Train Epoch: 6 [2560/7471 (34%)]\tLoss: 1632644.000000\n",
            "Train Epoch: 6 [2720/7471 (36%)]\tLoss: 1620246.250000\n",
            "Train Epoch: 6 [2880/7471 (39%)]\tLoss: 1567492.000000\n",
            "Train Epoch: 6 [3040/7471 (41%)]\tLoss: 1627843.375000\n",
            "Train Epoch: 6 [3200/7471 (43%)]\tLoss: 1569358.750000\n",
            "Train Epoch: 6 [3360/7471 (45%)]\tLoss: 1623988.125000\n",
            "Train Epoch: 6 [3520/7471 (47%)]\tLoss: 1543995.750000\n",
            "Train Epoch: 6 [3680/7471 (49%)]\tLoss: 1589607.625000\n",
            "Train Epoch: 6 [3840/7471 (51%)]\tLoss: 1608273.625000\n",
            "Train Epoch: 6 [4000/7471 (54%)]\tLoss: 1624285.250000\n",
            "Train Epoch: 6 [4160/7471 (56%)]\tLoss: 1596753.750000\n",
            "Train Epoch: 6 [4320/7471 (58%)]\tLoss: 1627423.375000\n",
            "Train Epoch: 6 [4480/7471 (60%)]\tLoss: 1585216.375000\n",
            "Train Epoch: 6 [4640/7471 (62%)]\tLoss: 1590045.250000\n",
            "Train Epoch: 6 [4800/7471 (64%)]\tLoss: 1632040.125000\n",
            "Train Epoch: 6 [4960/7471 (66%)]\tLoss: 1627175.500000\n",
            "Train Epoch: 6 [5120/7471 (69%)]\tLoss: 1581858.250000\n",
            "Train Epoch: 6 [5280/7471 (71%)]\tLoss: 1615847.625000\n",
            "Train Epoch: 6 [5440/7471 (73%)]\tLoss: 1664251.500000\n",
            "Train Epoch: 6 [5600/7471 (75%)]\tLoss: 1655505.375000\n",
            "Train Epoch: 6 [5760/7471 (77%)]\tLoss: 1579515.625000\n",
            "Train Epoch: 6 [5920/7471 (79%)]\tLoss: 1585560.875000\n",
            "Train Epoch: 6 [6080/7471 (81%)]\tLoss: 1573832.500000\n",
            "Train Epoch: 6 [6240/7471 (84%)]\tLoss: 1645162.750000\n",
            "Train Epoch: 6 [6400/7471 (86%)]\tLoss: 1597319.500000\n",
            "Train Epoch: 6 [6560/7471 (88%)]\tLoss: 1577627.500000\n",
            "Train Epoch: 6 [6720/7471 (90%)]\tLoss: 1579047.500000\n",
            "Train Epoch: 6 [6880/7471 (92%)]\tLoss: 1626357.750000\n",
            "Train Epoch: 6 [7040/7471 (94%)]\tLoss: 1644365.500000\n",
            "Train Epoch: 6 [7200/7471 (96%)]\tLoss: 1656482.375000\n",
            "Train Epoch: 6 [7360/7471 (99%)]\tLoss: 1639596.375000\n",
            "Epoch 6 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 7 [160/7471 (2%)]\tLoss: 1617760.125000\n",
            "Train Epoch: 7 [320/7471 (4%)]\tLoss: 1580578.625000\n",
            "Train Epoch: 7 [480/7471 (6%)]\tLoss: 1636048.125000\n",
            "Train Epoch: 7 [640/7471 (9%)]\tLoss: 1605419.250000\n",
            "Train Epoch: 7 [800/7471 (11%)]\tLoss: 1633288.000000\n",
            "Train Epoch: 7 [960/7471 (13%)]\tLoss: 1581317.000000\n",
            "Train Epoch: 7 [1120/7471 (15%)]\tLoss: 1637222.875000\n",
            "Train Epoch: 7 [1280/7471 (17%)]\tLoss: 1605729.500000\n",
            "Train Epoch: 7 [1440/7471 (19%)]\tLoss: 1617294.375000\n",
            "Train Epoch: 7 [1600/7471 (21%)]\tLoss: 1563806.250000\n",
            "Train Epoch: 7 [1760/7471 (24%)]\tLoss: 1598177.750000\n",
            "Train Epoch: 7 [1920/7471 (26%)]\tLoss: 1582031.750000\n",
            "Train Epoch: 7 [2080/7471 (28%)]\tLoss: 1604806.000000\n",
            "Train Epoch: 7 [2240/7471 (30%)]\tLoss: 1610064.375000\n",
            "Train Epoch: 7 [2400/7471 (32%)]\tLoss: 1578898.125000\n",
            "Train Epoch: 7 [2560/7471 (34%)]\tLoss: 1616955.875000\n",
            "Train Epoch: 7 [2720/7471 (36%)]\tLoss: 1524218.250000\n",
            "Train Epoch: 7 [2880/7471 (39%)]\tLoss: 1632576.750000\n",
            "Train Epoch: 7 [3040/7471 (41%)]\tLoss: 1637747.500000\n",
            "Train Epoch: 7 [3200/7471 (43%)]\tLoss: 1589013.875000\n",
            "Train Epoch: 7 [3360/7471 (45%)]\tLoss: 1596517.250000\n",
            "Train Epoch: 7 [3520/7471 (47%)]\tLoss: 1629036.125000\n",
            "Train Epoch: 7 [3680/7471 (49%)]\tLoss: 1621452.125000\n",
            "Train Epoch: 7 [3840/7471 (51%)]\tLoss: 1611401.125000\n",
            "Train Epoch: 7 [4000/7471 (54%)]\tLoss: 1612265.000000\n",
            "Train Epoch: 7 [4160/7471 (56%)]\tLoss: 1611193.625000\n",
            "Train Epoch: 7 [4320/7471 (58%)]\tLoss: 1663525.625000\n",
            "Train Epoch: 7 [4480/7471 (60%)]\tLoss: 1537043.500000\n",
            "Train Epoch: 7 [4640/7471 (62%)]\tLoss: 1615439.375000\n",
            "Train Epoch: 7 [4800/7471 (64%)]\tLoss: 1634931.875000\n",
            "Train Epoch: 7 [4960/7471 (66%)]\tLoss: 1634702.625000\n",
            "Train Epoch: 7 [5120/7471 (69%)]\tLoss: 1639354.000000\n",
            "Train Epoch: 7 [5280/7471 (71%)]\tLoss: 1613216.875000\n",
            "Train Epoch: 7 [5440/7471 (73%)]\tLoss: 1623273.375000\n",
            "Train Epoch: 7 [5600/7471 (75%)]\tLoss: 1568277.500000\n",
            "Train Epoch: 7 [5760/7471 (77%)]\tLoss: 1575440.125000\n",
            "Train Epoch: 7 [5920/7471 (79%)]\tLoss: 1632654.625000\n",
            "Train Epoch: 7 [6080/7471 (81%)]\tLoss: 1637149.250000\n",
            "Train Epoch: 7 [6240/7471 (84%)]\tLoss: 1645262.000000\n",
            "Train Epoch: 7 [6400/7471 (86%)]\tLoss: 1601657.125000\n",
            "Train Epoch: 7 [6560/7471 (88%)]\tLoss: 1619254.750000\n",
            "Train Epoch: 7 [6720/7471 (90%)]\tLoss: 1572113.250000\n",
            "Train Epoch: 7 [6880/7471 (92%)]\tLoss: 1614074.000000\n",
            "Train Epoch: 7 [7040/7471 (94%)]\tLoss: 1620417.750000\n",
            "Train Epoch: 7 [7200/7471 (96%)]\tLoss: 1560626.750000\n",
            "Train Epoch: 7 [7360/7471 (99%)]\tLoss: 1593482.000000\n",
            "Epoch 7 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 101770.8230\n",
            "\n",
            "Train Epoch: 8 [160/7471 (2%)]\tLoss: 1627113.625000\n",
            "Train Epoch: 8 [320/7471 (4%)]\tLoss: 1606368.875000\n",
            "Train Epoch: 8 [480/7471 (6%)]\tLoss: 1578488.500000\n",
            "Train Epoch: 8 [640/7471 (9%)]\tLoss: 1596773.875000\n",
            "Train Epoch: 8 [800/7471 (11%)]\tLoss: 1564150.750000\n",
            "Train Epoch: 8 [960/7471 (13%)]\tLoss: 1607177.125000\n",
            "Train Epoch: 8 [1120/7471 (15%)]\tLoss: 1634424.750000\n",
            "Train Epoch: 8 [1280/7471 (17%)]\tLoss: 1593597.250000\n",
            "Train Epoch: 8 [1440/7471 (19%)]\tLoss: 1564774.000000\n",
            "Train Epoch: 8 [1600/7471 (21%)]\tLoss: 1609698.750000\n",
            "Train Epoch: 8 [1760/7471 (24%)]\tLoss: 1606271.875000\n",
            "Train Epoch: 8 [1920/7471 (26%)]\tLoss: 1627314.375000\n",
            "Train Epoch: 8 [2080/7471 (28%)]\tLoss: 1591004.375000\n",
            "Train Epoch: 8 [2240/7471 (30%)]\tLoss: 1668009.250000\n",
            "Train Epoch: 8 [2400/7471 (32%)]\tLoss: 1597317.875000\n",
            "Train Epoch: 8 [2560/7471 (34%)]\tLoss: 1603816.750000\n",
            "Train Epoch: 8 [2720/7471 (36%)]\tLoss: 1616842.125000\n",
            "Train Epoch: 8 [2880/7471 (39%)]\tLoss: 1597892.875000\n",
            "Train Epoch: 8 [3040/7471 (41%)]\tLoss: 1553766.125000\n",
            "Train Epoch: 8 [3200/7471 (43%)]\tLoss: 1601841.125000\n",
            "Train Epoch: 8 [3360/7471 (45%)]\tLoss: 1636914.000000\n",
            "Train Epoch: 8 [3520/7471 (47%)]\tLoss: 1633996.000000\n",
            "Train Epoch: 8 [3680/7471 (49%)]\tLoss: 1579913.875000\n",
            "Train Epoch: 8 [3840/7471 (51%)]\tLoss: 1625016.625000\n",
            "Train Epoch: 8 [4000/7471 (54%)]\tLoss: 1637213.250000\n",
            "Train Epoch: 8 [4160/7471 (56%)]\tLoss: 1589665.125000\n",
            "Train Epoch: 8 [4320/7471 (58%)]\tLoss: 1598753.250000\n",
            "Train Epoch: 8 [4480/7471 (60%)]\tLoss: 1563494.625000\n",
            "Train Epoch: 8 [4640/7471 (62%)]\tLoss: 1649750.250000\n",
            "Train Epoch: 8 [4800/7471 (64%)]\tLoss: 1597230.875000\n",
            "Train Epoch: 8 [4960/7471 (66%)]\tLoss: 1578616.875000\n",
            "Train Epoch: 8 [5120/7471 (69%)]\tLoss: 1552191.250000\n",
            "Train Epoch: 8 [5280/7471 (71%)]\tLoss: 1627956.250000\n",
            "Train Epoch: 8 [5440/7471 (73%)]\tLoss: 1595706.250000\n",
            "Train Epoch: 8 [5600/7471 (75%)]\tLoss: 1608619.875000\n",
            "Train Epoch: 8 [5760/7471 (77%)]\tLoss: 1627565.250000\n",
            "Train Epoch: 8 [5920/7471 (79%)]\tLoss: 1644788.875000\n",
            "Train Epoch: 8 [6080/7471 (81%)]\tLoss: 1646253.000000\n",
            "Train Epoch: 8 [6240/7471 (84%)]\tLoss: 1636913.375000\n",
            "Train Epoch: 8 [6400/7471 (86%)]\tLoss: 1604534.000000\n",
            "Train Epoch: 8 [6560/7471 (88%)]\tLoss: 1535052.250000\n",
            "Train Epoch: 8 [6720/7471 (90%)]\tLoss: 1583749.500000\n",
            "Train Epoch: 8 [6880/7471 (92%)]\tLoss: 1496856.125000\n",
            "Train Epoch: 8 [7040/7471 (94%)]\tLoss: 1599821.125000\n",
            "Train Epoch: 8 [7200/7471 (96%)]\tLoss: 1612866.750000\n",
            "Train Epoch: 8 [7360/7471 (99%)]\tLoss: 1643911.000000\n",
            "Epoch 8 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 100493.5306\n",
            "\n",
            "Train Epoch: 9 [160/7471 (2%)]\tLoss: 1591276.750000\n",
            "Train Epoch: 9 [320/7471 (4%)]\tLoss: 1574161.125000\n",
            "Train Epoch: 9 [480/7471 (6%)]\tLoss: 1587974.250000\n",
            "Train Epoch: 9 [640/7471 (9%)]\tLoss: 1571128.000000\n",
            "Train Epoch: 9 [800/7471 (11%)]\tLoss: 1579048.000000\n",
            "Train Epoch: 9 [960/7471 (13%)]\tLoss: 1597728.875000\n",
            "Train Epoch: 9 [1120/7471 (15%)]\tLoss: 1583069.250000\n",
            "Train Epoch: 9 [1280/7471 (17%)]\tLoss: 1572368.875000\n",
            "Train Epoch: 9 [1440/7471 (19%)]\tLoss: 1638794.500000\n",
            "Train Epoch: 9 [1600/7471 (21%)]\tLoss: 1625061.125000\n",
            "Train Epoch: 9 [1760/7471 (24%)]\tLoss: 1611392.500000\n",
            "Train Epoch: 9 [1920/7471 (26%)]\tLoss: 1633471.625000\n",
            "Train Epoch: 9 [2080/7471 (28%)]\tLoss: 1583158.875000\n",
            "Train Epoch: 9 [2240/7471 (30%)]\tLoss: 1628403.125000\n",
            "Train Epoch: 9 [2400/7471 (32%)]\tLoss: 1611244.250000\n",
            "Train Epoch: 9 [2560/7471 (34%)]\tLoss: 1616632.375000\n",
            "Train Epoch: 9 [2720/7471 (36%)]\tLoss: 1583725.250000\n",
            "Train Epoch: 9 [2880/7471 (39%)]\tLoss: 1590799.625000\n",
            "Train Epoch: 9 [3040/7471 (41%)]\tLoss: 1616859.125000\n",
            "Train Epoch: 9 [3200/7471 (43%)]\tLoss: 1623124.750000\n",
            "Train Epoch: 9 [3360/7471 (45%)]\tLoss: 1525932.375000\n",
            "Train Epoch: 9 [3520/7471 (47%)]\tLoss: 1590273.875000\n",
            "Train Epoch: 9 [3680/7471 (49%)]\tLoss: 1591966.000000\n",
            "Train Epoch: 9 [3840/7471 (51%)]\tLoss: 1584940.125000\n",
            "Train Epoch: 9 [4000/7471 (54%)]\tLoss: 1569780.750000\n",
            "Train Epoch: 9 [4160/7471 (56%)]\tLoss: 1547131.625000\n",
            "Train Epoch: 9 [4320/7471 (58%)]\tLoss: 1588752.250000\n",
            "Train Epoch: 9 [4480/7471 (60%)]\tLoss: 1560256.000000\n",
            "Train Epoch: 9 [4640/7471 (62%)]\tLoss: 1535226.625000\n",
            "Train Epoch: 9 [4800/7471 (64%)]\tLoss: 1617287.625000\n",
            "Train Epoch: 9 [4960/7471 (66%)]\tLoss: 1579919.750000\n",
            "Train Epoch: 9 [5120/7471 (69%)]\tLoss: 1598268.375000\n",
            "Train Epoch: 9 [5280/7471 (71%)]\tLoss: 1576143.875000\n",
            "Train Epoch: 9 [5440/7471 (73%)]\tLoss: 1568704.875000\n",
            "Train Epoch: 9 [5600/7471 (75%)]\tLoss: 1490757.250000\n",
            "Train Epoch: 9 [5760/7471 (77%)]\tLoss: 1635608.125000\n",
            "Train Epoch: 9 [5920/7471 (79%)]\tLoss: 1547085.875000\n",
            "Train Epoch: 9 [6080/7471 (81%)]\tLoss: 1547617.125000\n",
            "Train Epoch: 9 [6240/7471 (84%)]\tLoss: 1621658.875000\n",
            "Train Epoch: 9 [6400/7471 (86%)]\tLoss: 1620033.625000\n",
            "Train Epoch: 9 [6560/7471 (88%)]\tLoss: 1599629.500000\n",
            "Train Epoch: 9 [6720/7471 (90%)]\tLoss: 1629686.000000\n",
            "Train Epoch: 9 [6880/7471 (92%)]\tLoss: 1628076.125000\n",
            "Train Epoch: 9 [7040/7471 (94%)]\tLoss: 1627341.875000\n",
            "Train Epoch: 9 [7200/7471 (96%)]\tLoss: 1611895.625000\n",
            "Train Epoch: 9 [7360/7471 (99%)]\tLoss: 1581065.250000\n",
            "Epoch 9 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 10 [160/7471 (2%)]\tLoss: 1617140.625000\n",
            "Train Epoch: 10 [320/7471 (4%)]\tLoss: 1550745.875000\n",
            "Train Epoch: 10 [480/7471 (6%)]\tLoss: 1566244.375000\n",
            "Train Epoch: 10 [640/7471 (9%)]\tLoss: 1597871.375000\n",
            "Train Epoch: 10 [800/7471 (11%)]\tLoss: 1525755.625000\n",
            "Train Epoch: 10 [960/7471 (13%)]\tLoss: 1621986.875000\n",
            "Train Epoch: 10 [1120/7471 (15%)]\tLoss: 1594872.000000\n",
            "Train Epoch: 10 [1280/7471 (17%)]\tLoss: 1624306.500000\n",
            "Train Epoch: 10 [1440/7471 (19%)]\tLoss: 1580050.250000\n",
            "Train Epoch: 10 [1600/7471 (21%)]\tLoss: 1617848.125000\n",
            "Train Epoch: 10 [1760/7471 (24%)]\tLoss: 1557918.625000\n",
            "Train Epoch: 10 [1920/7471 (26%)]\tLoss: 1640531.500000\n",
            "Train Epoch: 10 [2080/7471 (28%)]\tLoss: 1570439.875000\n",
            "Train Epoch: 10 [2240/7471 (30%)]\tLoss: 1660403.875000\n",
            "Train Epoch: 10 [2400/7471 (32%)]\tLoss: 1562102.375000\n",
            "Train Epoch: 10 [2560/7471 (34%)]\tLoss: 1607533.875000\n",
            "Train Epoch: 10 [2720/7471 (36%)]\tLoss: 1543178.125000\n",
            "Train Epoch: 10 [2880/7471 (39%)]\tLoss: 1551943.875000\n",
            "Train Epoch: 10 [3040/7471 (41%)]\tLoss: 1636552.375000\n",
            "Train Epoch: 10 [3200/7471 (43%)]\tLoss: 1632708.625000\n",
            "Train Epoch: 10 [3360/7471 (45%)]\tLoss: 1622049.000000\n",
            "Train Epoch: 10 [3520/7471 (47%)]\tLoss: 1563728.500000\n",
            "Train Epoch: 10 [3680/7471 (49%)]\tLoss: 1587369.250000\n",
            "Train Epoch: 10 [3840/7471 (51%)]\tLoss: 1591968.250000\n",
            "Train Epoch: 10 [4000/7471 (54%)]\tLoss: 1606621.000000\n",
            "Train Epoch: 10 [4160/7471 (56%)]\tLoss: 1619672.125000\n",
            "Train Epoch: 10 [4320/7471 (58%)]\tLoss: 1621953.875000\n",
            "Train Epoch: 10 [4480/7471 (60%)]\tLoss: 1557957.000000\n",
            "Train Epoch: 10 [4640/7471 (62%)]\tLoss: 1568729.750000\n",
            "Train Epoch: 10 [4800/7471 (64%)]\tLoss: 1484590.375000\n",
            "Train Epoch: 10 [4960/7471 (66%)]\tLoss: 1605514.375000\n",
            "Train Epoch: 10 [5120/7471 (69%)]\tLoss: 1576060.000000\n",
            "Train Epoch: 10 [5280/7471 (71%)]\tLoss: 1648159.625000\n",
            "Train Epoch: 10 [5440/7471 (73%)]\tLoss: 1574701.625000\n",
            "Train Epoch: 10 [5600/7471 (75%)]\tLoss: 1587709.375000\n",
            "Train Epoch: 10 [5760/7471 (77%)]\tLoss: 1637279.375000\n",
            "Train Epoch: 10 [5920/7471 (79%)]\tLoss: 1613054.000000\n",
            "Train Epoch: 10 [6080/7471 (81%)]\tLoss: 1601194.250000\n",
            "Train Epoch: 10 [6240/7471 (84%)]\tLoss: 1581760.500000\n",
            "Train Epoch: 10 [6400/7471 (86%)]\tLoss: 1575330.000000\n",
            "Train Epoch: 10 [6560/7471 (88%)]\tLoss: 1570170.625000\n",
            "Train Epoch: 10 [6720/7471 (90%)]\tLoss: 1621502.500000\n",
            "Train Epoch: 10 [6880/7471 (92%)]\tLoss: 1585563.875000\n",
            "Train Epoch: 10 [7040/7471 (94%)]\tLoss: 1601013.500000\n",
            "Train Epoch: 10 [7200/7471 (96%)]\tLoss: 1599401.875000\n",
            "Train Epoch: 10 [7360/7471 (99%)]\tLoss: 1652462.500000\n",
            "Epoch 10 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 11 [160/7471 (2%)]\tLoss: 1526891.250000\n",
            "Train Epoch: 11 [320/7471 (4%)]\tLoss: 1598266.000000\n",
            "Train Epoch: 11 [480/7471 (6%)]\tLoss: 1581917.000000\n",
            "Train Epoch: 11 [640/7471 (9%)]\tLoss: 1644553.250000\n",
            "Train Epoch: 11 [800/7471 (11%)]\tLoss: 1603105.250000\n",
            "Train Epoch: 11 [960/7471 (13%)]\tLoss: 1620001.875000\n",
            "Train Epoch: 11 [1120/7471 (15%)]\tLoss: 1608315.500000\n",
            "Train Epoch: 11 [1280/7471 (17%)]\tLoss: 1625671.875000\n",
            "Train Epoch: 11 [1440/7471 (19%)]\tLoss: 1632931.375000\n",
            "Train Epoch: 11 [1600/7471 (21%)]\tLoss: 1577659.250000\n",
            "Train Epoch: 11 [1760/7471 (24%)]\tLoss: 1606280.250000\n",
            "Train Epoch: 11 [1920/7471 (26%)]\tLoss: 1600284.750000\n",
            "Train Epoch: 11 [2080/7471 (28%)]\tLoss: 1577747.250000\n",
            "Train Epoch: 11 [2240/7471 (30%)]\tLoss: 1593128.500000\n",
            "Train Epoch: 11 [2400/7471 (32%)]\tLoss: 1609523.000000\n",
            "Train Epoch: 11 [2560/7471 (34%)]\tLoss: 1623756.875000\n",
            "Train Epoch: 11 [2720/7471 (36%)]\tLoss: 1591125.000000\n",
            "Train Epoch: 11 [2880/7471 (39%)]\tLoss: 1609310.125000\n",
            "Train Epoch: 11 [3040/7471 (41%)]\tLoss: 1622996.125000\n",
            "Train Epoch: 11 [3200/7471 (43%)]\tLoss: 1633673.250000\n",
            "Train Epoch: 11 [3360/7471 (45%)]\tLoss: 1623186.000000\n",
            "Train Epoch: 11 [3520/7471 (47%)]\tLoss: 1603090.500000\n",
            "Train Epoch: 11 [3680/7471 (49%)]\tLoss: 1619301.500000\n",
            "Train Epoch: 11 [3840/7471 (51%)]\tLoss: 1639240.500000\n",
            "Train Epoch: 11 [4000/7471 (54%)]\tLoss: 1561738.875000\n",
            "Train Epoch: 11 [4160/7471 (56%)]\tLoss: 1585055.500000\n",
            "Train Epoch: 11 [4320/7471 (58%)]\tLoss: 1567187.625000\n",
            "Train Epoch: 11 [4480/7471 (60%)]\tLoss: 1571996.125000\n",
            "Train Epoch: 11 [4640/7471 (62%)]\tLoss: 1612093.625000\n",
            "Train Epoch: 11 [4800/7471 (64%)]\tLoss: 1572187.375000\n",
            "Train Epoch: 11 [4960/7471 (66%)]\tLoss: 1570021.125000\n",
            "Train Epoch: 11 [5120/7471 (69%)]\tLoss: 1618451.500000\n",
            "Train Epoch: 11 [5280/7471 (71%)]\tLoss: 1539592.750000\n",
            "Train Epoch: 11 [5440/7471 (73%)]\tLoss: 1547196.625000\n",
            "Train Epoch: 11 [5600/7471 (75%)]\tLoss: 1560873.500000\n",
            "Train Epoch: 11 [5760/7471 (77%)]\tLoss: 1595088.375000\n",
            "Train Epoch: 11 [5920/7471 (79%)]\tLoss: 1636530.125000\n",
            "Train Epoch: 11 [6080/7471 (81%)]\tLoss: 1604775.875000\n",
            "Train Epoch: 11 [6240/7471 (84%)]\tLoss: 1573878.375000\n",
            "Train Epoch: 11 [6400/7471 (86%)]\tLoss: 1614799.750000\n",
            "Train Epoch: 11 [6560/7471 (88%)]\tLoss: 1567784.625000\n",
            "Train Epoch: 11 [6720/7471 (90%)]\tLoss: 1610238.375000\n",
            "Train Epoch: 11 [6880/7471 (92%)]\tLoss: 1575593.125000\n",
            "Train Epoch: 11 [7040/7471 (94%)]\tLoss: 1623169.625000\n",
            "Train Epoch: 11 [7200/7471 (96%)]\tLoss: 1655590.625000\n",
            "Train Epoch: 11 [7360/7471 (99%)]\tLoss: 1627673.250000\n",
            "Epoch 11 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 99515.1711\n",
            "\n",
            "Train Epoch: 12 [160/7471 (2%)]\tLoss: 1591741.250000\n",
            "Train Epoch: 12 [320/7471 (4%)]\tLoss: 1623391.625000\n",
            "Train Epoch: 12 [480/7471 (6%)]\tLoss: 1558865.375000\n",
            "Train Epoch: 12 [640/7471 (9%)]\tLoss: 1584075.375000\n",
            "Train Epoch: 12 [800/7471 (11%)]\tLoss: 1597124.625000\n",
            "Train Epoch: 12 [960/7471 (13%)]\tLoss: 1622183.250000\n",
            "Train Epoch: 12 [1120/7471 (15%)]\tLoss: 1602825.750000\n",
            "Train Epoch: 12 [1280/7471 (17%)]\tLoss: 1539896.250000\n",
            "Train Epoch: 12 [1440/7471 (19%)]\tLoss: 1565424.375000\n",
            "Train Epoch: 12 [1600/7471 (21%)]\tLoss: 1620013.625000\n",
            "Train Epoch: 12 [1760/7471 (24%)]\tLoss: 1566347.125000\n",
            "Train Epoch: 12 [1920/7471 (26%)]\tLoss: 1616167.750000\n",
            "Train Epoch: 12 [2080/7471 (28%)]\tLoss: 1562404.000000\n",
            "Train Epoch: 12 [2240/7471 (30%)]\tLoss: 1634954.375000\n",
            "Train Epoch: 12 [2400/7471 (32%)]\tLoss: 1619961.125000\n",
            "Train Epoch: 12 [2560/7471 (34%)]\tLoss: 1608059.750000\n",
            "Train Epoch: 12 [2720/7471 (36%)]\tLoss: 1530705.875000\n",
            "Train Epoch: 12 [2880/7471 (39%)]\tLoss: 1600595.500000\n",
            "Train Epoch: 12 [3040/7471 (41%)]\tLoss: 1584016.875000\n",
            "Train Epoch: 12 [3200/7471 (43%)]\tLoss: 1586017.750000\n",
            "Train Epoch: 12 [3360/7471 (45%)]\tLoss: 1604691.500000\n",
            "Train Epoch: 12 [3520/7471 (47%)]\tLoss: 1613466.125000\n",
            "Train Epoch: 12 [3680/7471 (49%)]\tLoss: 1602365.000000\n",
            "Train Epoch: 12 [3840/7471 (51%)]\tLoss: 1600627.000000\n",
            "Train Epoch: 12 [4000/7471 (54%)]\tLoss: 1621291.625000\n",
            "Train Epoch: 12 [4160/7471 (56%)]\tLoss: 1576439.000000\n",
            "Train Epoch: 12 [4320/7471 (58%)]\tLoss: 1583792.500000\n",
            "Train Epoch: 12 [4480/7471 (60%)]\tLoss: 1539381.625000\n",
            "Train Epoch: 12 [4640/7471 (62%)]\tLoss: 1622748.125000\n",
            "Train Epoch: 12 [4800/7471 (64%)]\tLoss: 1589426.500000\n",
            "Train Epoch: 12 [4960/7471 (66%)]\tLoss: 1585651.625000\n",
            "Train Epoch: 12 [5120/7471 (69%)]\tLoss: 1577569.500000\n",
            "Train Epoch: 12 [5280/7471 (71%)]\tLoss: 1616269.250000\n",
            "Train Epoch: 12 [5440/7471 (73%)]\tLoss: 1597055.250000\n",
            "Train Epoch: 12 [5600/7471 (75%)]\tLoss: 1609540.125000\n",
            "Train Epoch: 12 [5760/7471 (77%)]\tLoss: 1589482.000000\n",
            "Train Epoch: 12 [5920/7471 (79%)]\tLoss: 1584022.875000\n",
            "Train Epoch: 12 [6080/7471 (81%)]\tLoss: 1584818.750000\n",
            "Train Epoch: 12 [6240/7471 (84%)]\tLoss: 1652080.500000\n",
            "Train Epoch: 12 [6400/7471 (86%)]\tLoss: 1593448.500000\n",
            "Train Epoch: 12 [6560/7471 (88%)]\tLoss: 1618563.375000\n",
            "Train Epoch: 12 [6720/7471 (90%)]\tLoss: 1625835.625000\n",
            "Train Epoch: 12 [6880/7471 (92%)]\tLoss: 1663843.375000\n",
            "Train Epoch: 12 [7040/7471 (94%)]\tLoss: 1582280.875000\n",
            "Train Epoch: 12 [7200/7471 (96%)]\tLoss: 1565758.125000\n",
            "Train Epoch: 12 [7360/7471 (99%)]\tLoss: 1627608.500000\n",
            "Epoch 12 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 13 [160/7471 (2%)]\tLoss: 1544594.250000\n",
            "Train Epoch: 13 [320/7471 (4%)]\tLoss: 1620878.000000\n",
            "Train Epoch: 13 [480/7471 (6%)]\tLoss: 1581865.500000\n",
            "Train Epoch: 13 [640/7471 (9%)]\tLoss: 1578402.000000\n",
            "Train Epoch: 13 [800/7471 (11%)]\tLoss: 1603594.375000\n",
            "Train Epoch: 13 [960/7471 (13%)]\tLoss: 1596538.375000\n",
            "Train Epoch: 13 [1120/7471 (15%)]\tLoss: 1615045.000000\n",
            "Train Epoch: 13 [1280/7471 (17%)]\tLoss: 1644850.125000\n",
            "Train Epoch: 13 [1440/7471 (19%)]\tLoss: 1621869.875000\n",
            "Train Epoch: 13 [1600/7471 (21%)]\tLoss: 1581102.625000\n",
            "Train Epoch: 13 [1760/7471 (24%)]\tLoss: 1632906.250000\n",
            "Train Epoch: 13 [1920/7471 (26%)]\tLoss: 1564247.625000\n",
            "Train Epoch: 13 [2080/7471 (28%)]\tLoss: 1612766.250000\n",
            "Train Epoch: 13 [2240/7471 (30%)]\tLoss: 1586361.500000\n",
            "Train Epoch: 13 [2400/7471 (32%)]\tLoss: 1572988.750000\n",
            "Train Epoch: 13 [2560/7471 (34%)]\tLoss: 1599654.625000\n",
            "Train Epoch: 13 [2720/7471 (36%)]\tLoss: 1632457.000000\n",
            "Train Epoch: 13 [2880/7471 (39%)]\tLoss: 1608829.125000\n",
            "Train Epoch: 13 [3040/7471 (41%)]\tLoss: 1646266.625000\n",
            "Train Epoch: 13 [3200/7471 (43%)]\tLoss: 1637689.375000\n",
            "Train Epoch: 13 [3360/7471 (45%)]\tLoss: 1612534.875000\n",
            "Train Epoch: 13 [3520/7471 (47%)]\tLoss: 1640371.250000\n",
            "Train Epoch: 13 [3680/7471 (49%)]\tLoss: 1586847.375000\n",
            "Train Epoch: 13 [3840/7471 (51%)]\tLoss: 1601380.500000\n",
            "Train Epoch: 13 [4000/7471 (54%)]\tLoss: 1620791.125000\n",
            "Train Epoch: 13 [4160/7471 (56%)]\tLoss: 1607049.750000\n",
            "Train Epoch: 13 [4320/7471 (58%)]\tLoss: 1620527.625000\n",
            "Train Epoch: 13 [4480/7471 (60%)]\tLoss: 1632870.250000\n",
            "Train Epoch: 13 [4640/7471 (62%)]\tLoss: 1626277.875000\n",
            "Train Epoch: 13 [4800/7471 (64%)]\tLoss: 1631748.500000\n",
            "Train Epoch: 13 [4960/7471 (66%)]\tLoss: 1577885.375000\n",
            "Train Epoch: 13 [5120/7471 (69%)]\tLoss: 1626241.625000\n",
            "Train Epoch: 13 [5280/7471 (71%)]\tLoss: 1573533.250000\n",
            "Train Epoch: 13 [5440/7471 (73%)]\tLoss: 1583372.375000\n",
            "Train Epoch: 13 [5600/7471 (75%)]\tLoss: 1616025.750000\n",
            "Train Epoch: 13 [5760/7471 (77%)]\tLoss: 1590710.500000\n",
            "Train Epoch: 13 [5920/7471 (79%)]\tLoss: 1597758.125000\n",
            "Train Epoch: 13 [6080/7471 (81%)]\tLoss: 1548348.125000\n",
            "Train Epoch: 13 [6240/7471 (84%)]\tLoss: 1561913.500000\n",
            "Train Epoch: 13 [6400/7471 (86%)]\tLoss: 1607364.625000\n",
            "Train Epoch: 13 [6560/7471 (88%)]\tLoss: 1598832.750000\n",
            "Train Epoch: 13 [6720/7471 (90%)]\tLoss: 1599590.250000\n",
            "Train Epoch: 13 [6880/7471 (92%)]\tLoss: 1542201.875000\n",
            "Train Epoch: 13 [7040/7471 (94%)]\tLoss: 1627410.375000\n",
            "Train Epoch: 13 [7200/7471 (96%)]\tLoss: 1616421.500000\n",
            "Train Epoch: 13 [7360/7471 (99%)]\tLoss: 1580130.625000\n",
            "Epoch 13 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 14 [160/7471 (2%)]\tLoss: 1573064.625000\n",
            "Train Epoch: 14 [320/7471 (4%)]\tLoss: 1570829.250000\n",
            "Train Epoch: 14 [480/7471 (6%)]\tLoss: 1579697.875000\n",
            "Train Epoch: 14 [640/7471 (9%)]\tLoss: 1573308.375000\n",
            "Train Epoch: 14 [800/7471 (11%)]\tLoss: 1642841.625000\n",
            "Train Epoch: 14 [960/7471 (13%)]\tLoss: 1626822.375000\n",
            "Train Epoch: 14 [1120/7471 (15%)]\tLoss: 1516003.250000\n",
            "Train Epoch: 14 [1280/7471 (17%)]\tLoss: 1616090.875000\n",
            "Train Epoch: 14 [1440/7471 (19%)]\tLoss: 1564093.750000\n",
            "Train Epoch: 14 [1600/7471 (21%)]\tLoss: 1593028.250000\n",
            "Train Epoch: 14 [1760/7471 (24%)]\tLoss: 1588522.750000\n",
            "Train Epoch: 14 [1920/7471 (26%)]\tLoss: 1589749.375000\n",
            "Train Epoch: 14 [2080/7471 (28%)]\tLoss: 1607672.000000\n",
            "Train Epoch: 14 [2240/7471 (30%)]\tLoss: 1596477.375000\n",
            "Train Epoch: 14 [2400/7471 (32%)]\tLoss: 1591850.000000\n",
            "Train Epoch: 14 [2560/7471 (34%)]\tLoss: 1586639.000000\n",
            "Train Epoch: 14 [2720/7471 (36%)]\tLoss: 1555976.875000\n",
            "Train Epoch: 14 [2880/7471 (39%)]\tLoss: 1498918.750000\n",
            "Train Epoch: 14 [3040/7471 (41%)]\tLoss: 1615206.250000\n",
            "Train Epoch: 14 [3200/7471 (43%)]\tLoss: 1595183.125000\n",
            "Train Epoch: 14 [3360/7471 (45%)]\tLoss: 1637917.625000\n",
            "Train Epoch: 14 [3520/7471 (47%)]\tLoss: 1629636.625000\n",
            "Train Epoch: 14 [3680/7471 (49%)]\tLoss: 1600554.500000\n",
            "Train Epoch: 14 [3840/7471 (51%)]\tLoss: 1500670.625000\n",
            "Train Epoch: 14 [4000/7471 (54%)]\tLoss: 1515715.000000\n",
            "Train Epoch: 14 [4160/7471 (56%)]\tLoss: 1599331.500000\n",
            "Train Epoch: 14 [4320/7471 (58%)]\tLoss: 1547939.375000\n",
            "Train Epoch: 14 [4480/7471 (60%)]\tLoss: 1631396.500000\n",
            "Train Epoch: 14 [4640/7471 (62%)]\tLoss: 1575332.875000\n",
            "Train Epoch: 14 [4800/7471 (64%)]\tLoss: 1608873.125000\n",
            "Train Epoch: 14 [4960/7471 (66%)]\tLoss: 1519020.750000\n",
            "Train Epoch: 14 [5120/7471 (69%)]\tLoss: 1585971.500000\n",
            "Train Epoch: 14 [5280/7471 (71%)]\tLoss: 1592318.000000\n",
            "Train Epoch: 14 [5440/7471 (73%)]\tLoss: 1602054.875000\n",
            "Train Epoch: 14 [5600/7471 (75%)]\tLoss: 1585735.500000\n",
            "Train Epoch: 14 [5760/7471 (77%)]\tLoss: 1525147.250000\n",
            "Train Epoch: 14 [5920/7471 (79%)]\tLoss: 1587630.625000\n",
            "Train Epoch: 14 [6080/7471 (81%)]\tLoss: 1602884.875000\n",
            "Train Epoch: 14 [6240/7471 (84%)]\tLoss: 1585550.500000\n",
            "Train Epoch: 14 [6400/7471 (86%)]\tLoss: 1653593.250000\n",
            "Train Epoch: 14 [6560/7471 (88%)]\tLoss: 1627971.750000\n",
            "Train Epoch: 14 [6720/7471 (90%)]\tLoss: 1586406.125000\n",
            "Train Epoch: 14 [6880/7471 (92%)]\tLoss: 1631766.875000\n",
            "Train Epoch: 14 [7040/7471 (94%)]\tLoss: 1617407.875000\n",
            "Train Epoch: 14 [7200/7471 (96%)]\tLoss: 1569229.750000\n",
            "Train Epoch: 14 [7360/7471 (99%)]\tLoss: 1526912.500000\n",
            "Epoch 14 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 15 [160/7471 (2%)]\tLoss: 1528888.375000\n",
            "Train Epoch: 15 [320/7471 (4%)]\tLoss: 1553042.250000\n",
            "Train Epoch: 15 [480/7471 (6%)]\tLoss: 1613986.875000\n",
            "Train Epoch: 15 [640/7471 (9%)]\tLoss: 1575144.875000\n",
            "Train Epoch: 15 [800/7471 (11%)]\tLoss: 1598630.375000\n",
            "Train Epoch: 15 [960/7471 (13%)]\tLoss: 1542250.875000\n",
            "Train Epoch: 15 [1120/7471 (15%)]\tLoss: 1581870.375000\n",
            "Train Epoch: 15 [1280/7471 (17%)]\tLoss: 1605282.875000\n",
            "Train Epoch: 15 [1440/7471 (19%)]\tLoss: 1585246.500000\n",
            "Train Epoch: 15 [1600/7471 (21%)]\tLoss: 1562331.375000\n",
            "Train Epoch: 15 [1760/7471 (24%)]\tLoss: 1604539.750000\n",
            "Train Epoch: 15 [1920/7471 (26%)]\tLoss: 1652528.375000\n",
            "Train Epoch: 15 [2080/7471 (28%)]\tLoss: 1620499.750000\n",
            "Train Epoch: 15 [2240/7471 (30%)]\tLoss: 1613464.000000\n",
            "Train Epoch: 15 [2400/7471 (32%)]\tLoss: 1520155.875000\n",
            "Train Epoch: 15 [2560/7471 (34%)]\tLoss: 1626181.875000\n",
            "Train Epoch: 15 [2720/7471 (36%)]\tLoss: 1592427.625000\n",
            "Train Epoch: 15 [2880/7471 (39%)]\tLoss: 1610230.125000\n",
            "Train Epoch: 15 [3040/7471 (41%)]\tLoss: 1624797.500000\n",
            "Train Epoch: 15 [3200/7471 (43%)]\tLoss: 1563354.625000\n",
            "Train Epoch: 15 [3360/7471 (45%)]\tLoss: 1588234.750000\n",
            "Train Epoch: 15 [3520/7471 (47%)]\tLoss: 1519803.375000\n",
            "Train Epoch: 15 [3680/7471 (49%)]\tLoss: 1513910.125000\n",
            "Train Epoch: 15 [3840/7471 (51%)]\tLoss: 1559364.750000\n",
            "Train Epoch: 15 [4000/7471 (54%)]\tLoss: 1564931.500000\n",
            "Train Epoch: 15 [4160/7471 (56%)]\tLoss: 1536549.875000\n",
            "Train Epoch: 15 [4320/7471 (58%)]\tLoss: 1597941.875000\n",
            "Train Epoch: 15 [4480/7471 (60%)]\tLoss: 1591725.375000\n",
            "Train Epoch: 15 [4640/7471 (62%)]\tLoss: 1612613.125000\n",
            "Train Epoch: 15 [4800/7471 (64%)]\tLoss: 1597326.250000\n",
            "Train Epoch: 15 [4960/7471 (66%)]\tLoss: 1635943.750000\n",
            "Train Epoch: 15 [5120/7471 (69%)]\tLoss: 1562647.875000\n",
            "Train Epoch: 15 [5280/7471 (71%)]\tLoss: 1565306.625000\n",
            "Train Epoch: 15 [5440/7471 (73%)]\tLoss: 1582150.250000\n",
            "Train Epoch: 15 [5600/7471 (75%)]\tLoss: 1586887.625000\n",
            "Train Epoch: 15 [5760/7471 (77%)]\tLoss: 1588524.875000\n",
            "Train Epoch: 15 [5920/7471 (79%)]\tLoss: 1567206.625000\n",
            "Train Epoch: 15 [6080/7471 (81%)]\tLoss: 1520679.750000\n",
            "Train Epoch: 15 [6240/7471 (84%)]\tLoss: 1589724.750000\n",
            "Train Epoch: 15 [6400/7471 (86%)]\tLoss: 1588518.625000\n",
            "Train Epoch: 15 [6560/7471 (88%)]\tLoss: 1574492.250000\n",
            "Train Epoch: 15 [6720/7471 (90%)]\tLoss: 1584410.500000\n",
            "Train Epoch: 15 [6880/7471 (92%)]\tLoss: 1601717.625000\n",
            "Train Epoch: 15 [7040/7471 (94%)]\tLoss: 1593180.625000\n",
            "Train Epoch: 15 [7200/7471 (96%)]\tLoss: 1556955.500000\n",
            "Train Epoch: 15 [7360/7471 (99%)]\tLoss: 1592908.875000\n",
            "Epoch 15 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 16 [160/7471 (2%)]\tLoss: 1599833.250000\n",
            "Train Epoch: 16 [320/7471 (4%)]\tLoss: 1544388.000000\n",
            "Train Epoch: 16 [480/7471 (6%)]\tLoss: 1579525.125000\n",
            "Train Epoch: 16 [640/7471 (9%)]\tLoss: 1516537.250000\n",
            "Train Epoch: 16 [800/7471 (11%)]\tLoss: 1646375.125000\n",
            "Train Epoch: 16 [960/7471 (13%)]\tLoss: 1635067.875000\n",
            "Train Epoch: 16 [1120/7471 (15%)]\tLoss: 1596071.000000\n",
            "Train Epoch: 16 [1280/7471 (17%)]\tLoss: 1572007.250000\n",
            "Train Epoch: 16 [1440/7471 (19%)]\tLoss: 1619227.000000\n",
            "Train Epoch: 16 [1600/7471 (21%)]\tLoss: 1509570.125000\n",
            "Train Epoch: 16 [1760/7471 (24%)]\tLoss: 1631570.750000\n",
            "Train Epoch: 16 [1920/7471 (26%)]\tLoss: 1513674.875000\n",
            "Train Epoch: 16 [2080/7471 (28%)]\tLoss: 1569701.625000\n",
            "Train Epoch: 16 [2240/7471 (30%)]\tLoss: 1593284.250000\n",
            "Train Epoch: 16 [2400/7471 (32%)]\tLoss: 1631089.750000\n",
            "Train Epoch: 16 [2560/7471 (34%)]\tLoss: 1532320.000000\n",
            "Train Epoch: 16 [2720/7471 (36%)]\tLoss: 1568948.500000\n",
            "Train Epoch: 16 [2880/7471 (39%)]\tLoss: 1627271.750000\n",
            "Train Epoch: 16 [3040/7471 (41%)]\tLoss: 1594059.875000\n",
            "Train Epoch: 16 [3200/7471 (43%)]\tLoss: 1618603.250000\n",
            "Train Epoch: 16 [3360/7471 (45%)]\tLoss: 1566061.375000\n",
            "Train Epoch: 16 [3520/7471 (47%)]\tLoss: 1599313.125000\n",
            "Train Epoch: 16 [3680/7471 (49%)]\tLoss: 1599686.625000\n",
            "Train Epoch: 16 [3840/7471 (51%)]\tLoss: 1632684.125000\n",
            "Train Epoch: 16 [4000/7471 (54%)]\tLoss: 1614397.125000\n",
            "Train Epoch: 16 [4160/7471 (56%)]\tLoss: 1578766.625000\n",
            "Train Epoch: 16 [4320/7471 (58%)]\tLoss: 1579415.750000\n",
            "Train Epoch: 16 [4480/7471 (60%)]\tLoss: 1615901.375000\n",
            "Train Epoch: 16 [4640/7471 (62%)]\tLoss: 1626552.000000\n",
            "Train Epoch: 16 [4800/7471 (64%)]\tLoss: 1621608.125000\n",
            "Train Epoch: 16 [4960/7471 (66%)]\tLoss: 1506755.875000\n",
            "Train Epoch: 16 [5120/7471 (69%)]\tLoss: 1559007.875000\n",
            "Train Epoch: 16 [5280/7471 (71%)]\tLoss: 1589881.000000\n",
            "Train Epoch: 16 [5440/7471 (73%)]\tLoss: 1554719.125000\n",
            "Train Epoch: 16 [5600/7471 (75%)]\tLoss: 1582615.875000\n",
            "Train Epoch: 16 [5760/7471 (77%)]\tLoss: 1611325.250000\n",
            "Train Epoch: 16 [5920/7471 (79%)]\tLoss: 1596269.250000\n",
            "Train Epoch: 16 [6080/7471 (81%)]\tLoss: 1623965.375000\n",
            "Train Epoch: 16 [6240/7471 (84%)]\tLoss: 1595731.000000\n",
            "Train Epoch: 16 [6400/7471 (86%)]\tLoss: 1580244.250000\n",
            "Train Epoch: 16 [6560/7471 (88%)]\tLoss: 1600511.875000\n",
            "Train Epoch: 16 [6720/7471 (90%)]\tLoss: 1529677.625000\n",
            "Train Epoch: 16 [6880/7471 (92%)]\tLoss: 1617535.250000\n",
            "Train Epoch: 16 [7040/7471 (94%)]\tLoss: 1617337.500000\n",
            "Train Epoch: 16 [7200/7471 (96%)]\tLoss: 1576537.000000\n",
            "Train Epoch: 16 [7360/7471 (99%)]\tLoss: 1598889.125000\n",
            "Epoch 16 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 17 [160/7471 (2%)]\tLoss: 1590434.500000\n",
            "Train Epoch: 17 [320/7471 (4%)]\tLoss: 1535442.625000\n",
            "Train Epoch: 17 [480/7471 (6%)]\tLoss: 1634152.125000\n",
            "Train Epoch: 17 [640/7471 (9%)]\tLoss: 1612819.875000\n",
            "Train Epoch: 17 [800/7471 (11%)]\tLoss: 1562790.125000\n",
            "Train Epoch: 17 [960/7471 (13%)]\tLoss: 1611723.500000\n",
            "Train Epoch: 17 [1120/7471 (15%)]\tLoss: 1525289.250000\n",
            "Train Epoch: 17 [1280/7471 (17%)]\tLoss: 1618976.625000\n",
            "Train Epoch: 17 [1440/7471 (19%)]\tLoss: 1554516.125000\n",
            "Train Epoch: 17 [1600/7471 (21%)]\tLoss: 1608997.000000\n",
            "Train Epoch: 17 [1760/7471 (24%)]\tLoss: 1604459.625000\n",
            "Train Epoch: 17 [1920/7471 (26%)]\tLoss: 1599694.250000\n",
            "Train Epoch: 17 [2080/7471 (28%)]\tLoss: 1581349.875000\n",
            "Train Epoch: 17 [2240/7471 (30%)]\tLoss: 1611130.250000\n",
            "Train Epoch: 17 [2400/7471 (32%)]\tLoss: 1638294.375000\n",
            "Train Epoch: 17 [2560/7471 (34%)]\tLoss: 1594746.375000\n",
            "Train Epoch: 17 [2720/7471 (36%)]\tLoss: 1553897.875000\n",
            "Train Epoch: 17 [2880/7471 (39%)]\tLoss: 1536002.625000\n",
            "Train Epoch: 17 [3040/7471 (41%)]\tLoss: 1519994.875000\n",
            "Train Epoch: 17 [3200/7471 (43%)]\tLoss: 1576896.000000\n",
            "Train Epoch: 17 [3360/7471 (45%)]\tLoss: 1561177.250000\n",
            "Train Epoch: 17 [3520/7471 (47%)]\tLoss: 1622485.875000\n",
            "Train Epoch: 17 [3680/7471 (49%)]\tLoss: 1587866.125000\n",
            "Train Epoch: 17 [3840/7471 (51%)]\tLoss: 1569649.250000\n",
            "Train Epoch: 17 [4000/7471 (54%)]\tLoss: 1572786.875000\n",
            "Train Epoch: 17 [4160/7471 (56%)]\tLoss: 1648237.875000\n",
            "Train Epoch: 17 [4320/7471 (58%)]\tLoss: 1500256.750000\n",
            "Train Epoch: 17 [4480/7471 (60%)]\tLoss: 1557519.625000\n",
            "Train Epoch: 17 [4640/7471 (62%)]\tLoss: 1628349.750000\n",
            "Train Epoch: 17 [4800/7471 (64%)]\tLoss: 1572941.375000\n",
            "Train Epoch: 17 [4960/7471 (66%)]\tLoss: 1637931.250000\n",
            "Train Epoch: 17 [5120/7471 (69%)]\tLoss: 1607256.625000\n",
            "Train Epoch: 17 [5280/7471 (71%)]\tLoss: 1530232.250000\n",
            "Train Epoch: 17 [5440/7471 (73%)]\tLoss: 1648159.000000\n",
            "Train Epoch: 17 [5600/7471 (75%)]\tLoss: 1592714.750000\n",
            "Train Epoch: 17 [5760/7471 (77%)]\tLoss: 1580215.750000\n",
            "Train Epoch: 17 [5920/7471 (79%)]\tLoss: 1603960.250000\n",
            "Train Epoch: 17 [6080/7471 (81%)]\tLoss: 1615801.000000\n",
            "Train Epoch: 17 [6240/7471 (84%)]\tLoss: 1505148.500000\n",
            "Train Epoch: 17 [6400/7471 (86%)]\tLoss: 1599358.750000\n",
            "Train Epoch: 17 [6560/7471 (88%)]\tLoss: 1628091.750000\n",
            "Train Epoch: 17 [6720/7471 (90%)]\tLoss: 1517679.625000\n",
            "Train Epoch: 17 [6880/7471 (92%)]\tLoss: 1578753.500000\n",
            "Train Epoch: 17 [7040/7471 (94%)]\tLoss: 1533023.875000\n",
            "Train Epoch: 17 [7200/7471 (96%)]\tLoss: 1607119.125000\n",
            "Train Epoch: 17 [7360/7471 (99%)]\tLoss: 1584719.750000\n",
            "Epoch 17 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 18 [160/7471 (2%)]\tLoss: 1564283.000000\n",
            "Train Epoch: 18 [320/7471 (4%)]\tLoss: 1639349.250000\n",
            "Train Epoch: 18 [480/7471 (6%)]\tLoss: 1515995.625000\n",
            "Train Epoch: 18 [640/7471 (9%)]\tLoss: 1615937.125000\n",
            "Train Epoch: 18 [800/7471 (11%)]\tLoss: 1562218.500000\n",
            "Train Epoch: 18 [960/7471 (13%)]\tLoss: 1586647.750000\n",
            "Train Epoch: 18 [1120/7471 (15%)]\tLoss: 1592447.875000\n",
            "Train Epoch: 18 [1280/7471 (17%)]\tLoss: 1561535.875000\n",
            "Train Epoch: 18 [1440/7471 (19%)]\tLoss: 1618942.375000\n",
            "Train Epoch: 18 [1600/7471 (21%)]\tLoss: 1622005.500000\n",
            "Train Epoch: 18 [1760/7471 (24%)]\tLoss: 1510490.375000\n",
            "Train Epoch: 18 [1920/7471 (26%)]\tLoss: 1570144.875000\n",
            "Train Epoch: 18 [2080/7471 (28%)]\tLoss: 1601686.625000\n",
            "Train Epoch: 18 [2240/7471 (30%)]\tLoss: 1618635.875000\n",
            "Train Epoch: 18 [2400/7471 (32%)]\tLoss: 1615793.750000\n",
            "Train Epoch: 18 [2560/7471 (34%)]\tLoss: 1544174.750000\n",
            "Train Epoch: 18 [2720/7471 (36%)]\tLoss: 1597196.750000\n",
            "Train Epoch: 18 [2880/7471 (39%)]\tLoss: 1629967.875000\n",
            "Train Epoch: 18 [3040/7471 (41%)]\tLoss: 1616295.125000\n",
            "Train Epoch: 18 [3200/7471 (43%)]\tLoss: 1591234.500000\n",
            "Train Epoch: 18 [3360/7471 (45%)]\tLoss: 1599927.875000\n",
            "Train Epoch: 18 [3520/7471 (47%)]\tLoss: 1605646.625000\n",
            "Train Epoch: 18 [3680/7471 (49%)]\tLoss: 1634355.750000\n",
            "Train Epoch: 18 [3840/7471 (51%)]\tLoss: 1548907.875000\n",
            "Train Epoch: 18 [4000/7471 (54%)]\tLoss: 1584318.750000\n",
            "Train Epoch: 18 [4160/7471 (56%)]\tLoss: 1654431.125000\n",
            "Train Epoch: 18 [4320/7471 (58%)]\tLoss: 1575183.125000\n",
            "Train Epoch: 18 [4480/7471 (60%)]\tLoss: 1603570.125000\n",
            "Train Epoch: 18 [4640/7471 (62%)]\tLoss: 1582381.500000\n",
            "Train Epoch: 18 [4800/7471 (64%)]\tLoss: 1568323.250000\n",
            "Train Epoch: 18 [4960/7471 (66%)]\tLoss: 1565656.125000\n",
            "Train Epoch: 18 [5120/7471 (69%)]\tLoss: 1632784.375000\n",
            "Train Epoch: 18 [5280/7471 (71%)]\tLoss: 1595303.375000\n",
            "Train Epoch: 18 [5440/7471 (73%)]\tLoss: 1538241.500000\n",
            "Train Epoch: 18 [5600/7471 (75%)]\tLoss: 1603323.500000\n",
            "Train Epoch: 18 [5760/7471 (77%)]\tLoss: 1565687.000000\n",
            "Train Epoch: 18 [5920/7471 (79%)]\tLoss: 1563219.875000\n",
            "Train Epoch: 18 [6080/7471 (81%)]\tLoss: 1605352.250000\n",
            "Train Epoch: 18 [6240/7471 (84%)]\tLoss: 1585501.125000\n",
            "Train Epoch: 18 [6400/7471 (86%)]\tLoss: 1571841.000000\n",
            "Train Epoch: 18 [6560/7471 (88%)]\tLoss: 1573402.750000\n",
            "Train Epoch: 18 [6720/7471 (90%)]\tLoss: 1505004.500000\n",
            "Train Epoch: 18 [6880/7471 (92%)]\tLoss: 1586942.250000\n",
            "Train Epoch: 18 [7040/7471 (94%)]\tLoss: 1525106.500000\n",
            "Train Epoch: 18 [7200/7471 (96%)]\tLoss: 1642645.000000\n",
            "Train Epoch: 18 [7360/7471 (99%)]\tLoss: 1596565.125000\n",
            "Epoch 18 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 19 [160/7471 (2%)]\tLoss: 1633419.875000\n",
            "Train Epoch: 19 [320/7471 (4%)]\tLoss: 1583055.500000\n",
            "Train Epoch: 19 [480/7471 (6%)]\tLoss: 1610883.500000\n",
            "Train Epoch: 19 [640/7471 (9%)]\tLoss: 1627058.625000\n",
            "Train Epoch: 19 [800/7471 (11%)]\tLoss: 1533657.625000\n",
            "Train Epoch: 19 [960/7471 (13%)]\tLoss: 1559804.250000\n",
            "Train Epoch: 19 [1120/7471 (15%)]\tLoss: 1629110.625000\n",
            "Train Epoch: 19 [1280/7471 (17%)]\tLoss: 1626034.250000\n",
            "Train Epoch: 19 [1440/7471 (19%)]\tLoss: 1587477.000000\n",
            "Train Epoch: 19 [1600/7471 (21%)]\tLoss: 1554465.750000\n",
            "Train Epoch: 19 [1760/7471 (24%)]\tLoss: 1593343.250000\n",
            "Train Epoch: 19 [1920/7471 (26%)]\tLoss: 1599093.875000\n",
            "Train Epoch: 19 [2080/7471 (28%)]\tLoss: 1573266.875000\n",
            "Train Epoch: 19 [2240/7471 (30%)]\tLoss: 1575141.500000\n",
            "Train Epoch: 19 [2400/7471 (32%)]\tLoss: 1539422.000000\n",
            "Train Epoch: 19 [2560/7471 (34%)]\tLoss: 1592825.125000\n",
            "Train Epoch: 19 [2720/7471 (36%)]\tLoss: 1608234.250000\n",
            "Train Epoch: 19 [2880/7471 (39%)]\tLoss: 1590011.625000\n",
            "Train Epoch: 19 [3040/7471 (41%)]\tLoss: 1560321.250000\n",
            "Train Epoch: 19 [3200/7471 (43%)]\tLoss: 1592191.375000\n",
            "Train Epoch: 19 [3360/7471 (45%)]\tLoss: 1501503.750000\n",
            "Train Epoch: 19 [3520/7471 (47%)]\tLoss: 1558999.875000\n",
            "Train Epoch: 19 [3680/7471 (49%)]\tLoss: 1595751.375000\n",
            "Train Epoch: 19 [3840/7471 (51%)]\tLoss: 1622972.750000\n",
            "Train Epoch: 19 [4000/7471 (54%)]\tLoss: 1571406.125000\n",
            "Train Epoch: 19 [4160/7471 (56%)]\tLoss: 1558556.875000\n",
            "Train Epoch: 19 [4320/7471 (58%)]\tLoss: 1598188.250000\n",
            "Train Epoch: 19 [4480/7471 (60%)]\tLoss: 1562318.625000\n",
            "Train Epoch: 19 [4640/7471 (62%)]\tLoss: 1589510.875000\n",
            "Train Epoch: 19 [4800/7471 (64%)]\tLoss: 1619693.000000\n",
            "Train Epoch: 19 [4960/7471 (66%)]\tLoss: 1543296.625000\n",
            "Train Epoch: 19 [5120/7471 (69%)]\tLoss: 1583020.750000\n",
            "Train Epoch: 19 [5280/7471 (71%)]\tLoss: 1612315.875000\n",
            "Train Epoch: 19 [5440/7471 (73%)]\tLoss: 1607390.875000\n",
            "Train Epoch: 19 [5600/7471 (75%)]\tLoss: 1625227.875000\n",
            "Train Epoch: 19 [5760/7471 (77%)]\tLoss: 1529790.125000\n",
            "Train Epoch: 19 [5920/7471 (79%)]\tLoss: 1588839.125000\n",
            "Train Epoch: 19 [6080/7471 (81%)]\tLoss: 1573262.750000\n",
            "Train Epoch: 19 [6240/7471 (84%)]\tLoss: 1563539.250000\n",
            "Train Epoch: 19 [6400/7471 (86%)]\tLoss: 1588065.000000\n",
            "Train Epoch: 19 [6560/7471 (88%)]\tLoss: 1587790.750000\n",
            "Train Epoch: 19 [6720/7471 (90%)]\tLoss: 1560032.500000\n",
            "Train Epoch: 19 [6880/7471 (92%)]\tLoss: 1550940.875000\n",
            "Train Epoch: 19 [7040/7471 (94%)]\tLoss: 1519064.250000\n",
            "Train Epoch: 19 [7200/7471 (96%)]\tLoss: 1539587.250000\n",
            "Train Epoch: 19 [7360/7471 (99%)]\tLoss: 1621540.375000\n",
            "Epoch 19 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 20 [160/7471 (2%)]\tLoss: 1577982.875000\n",
            "Train Epoch: 20 [320/7471 (4%)]\tLoss: 1622913.125000\n",
            "Train Epoch: 20 [480/7471 (6%)]\tLoss: 1575464.875000\n",
            "Train Epoch: 20 [640/7471 (9%)]\tLoss: 1581490.375000\n",
            "Train Epoch: 20 [800/7471 (11%)]\tLoss: 1558964.375000\n",
            "Train Epoch: 20 [960/7471 (13%)]\tLoss: 1627836.250000\n",
            "Train Epoch: 20 [1120/7471 (15%)]\tLoss: 1624319.500000\n",
            "Train Epoch: 20 [1280/7471 (17%)]\tLoss: 1607160.250000\n",
            "Train Epoch: 20 [1440/7471 (19%)]\tLoss: 1571864.500000\n",
            "Train Epoch: 20 [1600/7471 (21%)]\tLoss: 1610226.000000\n",
            "Train Epoch: 20 [1760/7471 (24%)]\tLoss: 1624510.250000\n",
            "Train Epoch: 20 [1920/7471 (26%)]\tLoss: 1600905.875000\n",
            "Train Epoch: 20 [2080/7471 (28%)]\tLoss: 1592094.625000\n",
            "Train Epoch: 20 [2240/7471 (30%)]\tLoss: 1624593.000000\n",
            "Train Epoch: 20 [2400/7471 (32%)]\tLoss: 1569831.000000\n",
            "Train Epoch: 20 [2560/7471 (34%)]\tLoss: 1574768.750000\n",
            "Train Epoch: 20 [2720/7471 (36%)]\tLoss: 1558606.500000\n",
            "Train Epoch: 20 [2880/7471 (39%)]\tLoss: 1569061.375000\n",
            "Train Epoch: 20 [3040/7471 (41%)]\tLoss: 1517823.500000\n",
            "Train Epoch: 20 [3200/7471 (43%)]\tLoss: 1587861.250000\n",
            "Train Epoch: 20 [3360/7471 (45%)]\tLoss: 1600460.875000\n",
            "Train Epoch: 20 [3520/7471 (47%)]\tLoss: 1582791.375000\n",
            "Train Epoch: 20 [3680/7471 (49%)]\tLoss: 1580026.875000\n",
            "Train Epoch: 20 [3840/7471 (51%)]\tLoss: 1580632.875000\n",
            "Train Epoch: 20 [4000/7471 (54%)]\tLoss: 1554281.625000\n",
            "Train Epoch: 20 [4160/7471 (56%)]\tLoss: 1623739.375000\n",
            "Train Epoch: 20 [4320/7471 (58%)]\tLoss: 1572662.125000\n",
            "Train Epoch: 20 [4480/7471 (60%)]\tLoss: 1602064.250000\n",
            "Train Epoch: 20 [4640/7471 (62%)]\tLoss: 1605487.500000\n",
            "Train Epoch: 20 [4800/7471 (64%)]\tLoss: 1618845.500000\n",
            "Train Epoch: 20 [4960/7471 (66%)]\tLoss: 1517509.250000\n",
            "Train Epoch: 20 [5120/7471 (69%)]\tLoss: 1607085.625000\n",
            "Train Epoch: 20 [5280/7471 (71%)]\tLoss: 1616318.750000\n",
            "Train Epoch: 20 [5440/7471 (73%)]\tLoss: 1559470.750000\n",
            "Train Epoch: 20 [5600/7471 (75%)]\tLoss: 1596057.875000\n",
            "Train Epoch: 20 [5760/7471 (77%)]\tLoss: 1583369.500000\n",
            "Train Epoch: 20 [5920/7471 (79%)]\tLoss: 1570722.750000\n",
            "Train Epoch: 20 [6080/7471 (81%)]\tLoss: 1634027.625000\n",
            "Train Epoch: 20 [6240/7471 (84%)]\tLoss: 1583339.375000\n",
            "Train Epoch: 20 [6400/7471 (86%)]\tLoss: 1615657.875000\n",
            "Train Epoch: 20 [6560/7471 (88%)]\tLoss: 1541800.250000\n",
            "Train Epoch: 20 [6720/7471 (90%)]\tLoss: 1582159.875000\n",
            "Train Epoch: 20 [6880/7471 (92%)]\tLoss: 1557304.875000\n",
            "Train Epoch: 20 [7040/7471 (94%)]\tLoss: 1623610.250000\n",
            "Train Epoch: 20 [7200/7471 (96%)]\tLoss: 1659801.500000\n",
            "Train Epoch: 20 [7360/7471 (99%)]\tLoss: 1625108.375000\n",
            "Epoch 20 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 21 [160/7471 (2%)]\tLoss: 1600198.875000\n",
            "Train Epoch: 21 [320/7471 (4%)]\tLoss: 1559775.875000\n",
            "Train Epoch: 21 [480/7471 (6%)]\tLoss: 1590422.125000\n",
            "Train Epoch: 21 [640/7471 (9%)]\tLoss: 1536476.250000\n",
            "Train Epoch: 21 [800/7471 (11%)]\tLoss: 1568477.625000\n",
            "Train Epoch: 21 [960/7471 (13%)]\tLoss: 1628093.125000\n",
            "Train Epoch: 21 [1120/7471 (15%)]\tLoss: 1543172.250000\n",
            "Train Epoch: 21 [1280/7471 (17%)]\tLoss: 1540384.750000\n",
            "Train Epoch: 21 [1440/7471 (19%)]\tLoss: 1599039.875000\n",
            "Train Epoch: 21 [1600/7471 (21%)]\tLoss: 1579281.375000\n",
            "Train Epoch: 21 [1760/7471 (24%)]\tLoss: 1589906.250000\n",
            "Train Epoch: 21 [1920/7471 (26%)]\tLoss: 1586280.625000\n",
            "Train Epoch: 21 [2080/7471 (28%)]\tLoss: 1556365.250000\n",
            "Train Epoch: 21 [2240/7471 (30%)]\tLoss: 1553857.375000\n",
            "Train Epoch: 21 [2400/7471 (32%)]\tLoss: 1549776.250000\n",
            "Train Epoch: 21 [2560/7471 (34%)]\tLoss: 1522210.375000\n",
            "Train Epoch: 21 [2720/7471 (36%)]\tLoss: 1545779.125000\n",
            "Train Epoch: 21 [2880/7471 (39%)]\tLoss: 1560503.000000\n",
            "Train Epoch: 21 [3040/7471 (41%)]\tLoss: 1589248.500000\n",
            "Train Epoch: 21 [3200/7471 (43%)]\tLoss: 1522510.500000\n",
            "Train Epoch: 21 [3360/7471 (45%)]\tLoss: 1569947.750000\n",
            "Train Epoch: 21 [3520/7471 (47%)]\tLoss: 1592872.625000\n",
            "Train Epoch: 21 [3680/7471 (49%)]\tLoss: 1620902.875000\n",
            "Train Epoch: 21 [3840/7471 (51%)]\tLoss: 1607369.250000\n",
            "Train Epoch: 21 [4000/7471 (54%)]\tLoss: 1578272.875000\n",
            "Train Epoch: 21 [4160/7471 (56%)]\tLoss: 1622052.375000\n",
            "Train Epoch: 21 [4320/7471 (58%)]\tLoss: 1582494.250000\n",
            "Train Epoch: 21 [4480/7471 (60%)]\tLoss: 1583718.875000\n",
            "Train Epoch: 21 [4640/7471 (62%)]\tLoss: 1617800.625000\n",
            "Train Epoch: 21 [4800/7471 (64%)]\tLoss: 1649110.625000\n",
            "Train Epoch: 21 [4960/7471 (66%)]\tLoss: 1642672.250000\n",
            "Train Epoch: 21 [5120/7471 (69%)]\tLoss: 1605321.500000\n",
            "Train Epoch: 21 [5280/7471 (71%)]\tLoss: 1600239.750000\n",
            "Train Epoch: 21 [5440/7471 (73%)]\tLoss: 1533126.250000\n",
            "Train Epoch: 21 [5600/7471 (75%)]\tLoss: 1605064.125000\n",
            "Train Epoch: 21 [5760/7471 (77%)]\tLoss: 1616413.750000\n",
            "Train Epoch: 21 [5920/7471 (79%)]\tLoss: 1602956.500000\n",
            "Train Epoch: 21 [6080/7471 (81%)]\tLoss: 1521510.875000\n",
            "Train Epoch: 21 [6240/7471 (84%)]\tLoss: 1595190.625000\n",
            "Train Epoch: 21 [6400/7471 (86%)]\tLoss: 1613442.125000\n",
            "Train Epoch: 21 [6560/7471 (88%)]\tLoss: 1582714.625000\n",
            "Train Epoch: 21 [6720/7471 (90%)]\tLoss: 1596008.500000\n",
            "Train Epoch: 21 [6880/7471 (92%)]\tLoss: 1631695.250000\n",
            "Train Epoch: 21 [7040/7471 (94%)]\tLoss: 1561249.750000\n",
            "Train Epoch: 21 [7200/7471 (96%)]\tLoss: 1571953.125000\n",
            "Train Epoch: 21 [7360/7471 (99%)]\tLoss: 1611108.875000\n",
            "Epoch 21 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 22 [160/7471 (2%)]\tLoss: 1620010.500000\n",
            "Train Epoch: 22 [320/7471 (4%)]\tLoss: 1524320.000000\n",
            "Train Epoch: 22 [480/7471 (6%)]\tLoss: 1600991.750000\n",
            "Train Epoch: 22 [640/7471 (9%)]\tLoss: 1559866.000000\n",
            "Train Epoch: 22 [800/7471 (11%)]\tLoss: 1563888.875000\n",
            "Train Epoch: 22 [960/7471 (13%)]\tLoss: 1590811.000000\n",
            "Train Epoch: 22 [1120/7471 (15%)]\tLoss: 1620457.125000\n",
            "Train Epoch: 22 [1280/7471 (17%)]\tLoss: 1548393.000000\n",
            "Train Epoch: 22 [1440/7471 (19%)]\tLoss: 1614052.125000\n",
            "Train Epoch: 22 [1600/7471 (21%)]\tLoss: 1614091.625000\n",
            "Train Epoch: 22 [1760/7471 (24%)]\tLoss: 1628209.500000\n",
            "Train Epoch: 22 [1920/7471 (26%)]\tLoss: 1601094.750000\n",
            "Train Epoch: 22 [2080/7471 (28%)]\tLoss: 1564787.000000\n",
            "Train Epoch: 22 [2240/7471 (30%)]\tLoss: 1585848.750000\n",
            "Train Epoch: 22 [2400/7471 (32%)]\tLoss: 1540245.250000\n",
            "Train Epoch: 22 [2560/7471 (34%)]\tLoss: 1618077.000000\n",
            "Train Epoch: 22 [2720/7471 (36%)]\tLoss: 1507231.250000\n",
            "Train Epoch: 22 [2880/7471 (39%)]\tLoss: 1555737.500000\n",
            "Train Epoch: 22 [3040/7471 (41%)]\tLoss: 1571502.250000\n",
            "Train Epoch: 22 [3200/7471 (43%)]\tLoss: 1608562.500000\n",
            "Train Epoch: 22 [3360/7471 (45%)]\tLoss: 1537348.750000\n",
            "Train Epoch: 22 [3520/7471 (47%)]\tLoss: 1592820.125000\n",
            "Train Epoch: 22 [3680/7471 (49%)]\tLoss: 1621128.125000\n",
            "Train Epoch: 22 [3840/7471 (51%)]\tLoss: 1589111.875000\n",
            "Train Epoch: 22 [4000/7471 (54%)]\tLoss: 1617310.125000\n",
            "Train Epoch: 22 [4160/7471 (56%)]\tLoss: 1593601.250000\n",
            "Train Epoch: 22 [4320/7471 (58%)]\tLoss: 1594830.500000\n",
            "Train Epoch: 22 [4480/7471 (60%)]\tLoss: 1581277.875000\n",
            "Train Epoch: 22 [4640/7471 (62%)]\tLoss: 1605333.875000\n",
            "Train Epoch: 22 [4800/7471 (64%)]\tLoss: 1548155.250000\n",
            "Train Epoch: 22 [4960/7471 (66%)]\tLoss: 1573432.750000\n",
            "Train Epoch: 22 [5120/7471 (69%)]\tLoss: 1527966.000000\n",
            "Train Epoch: 22 [5280/7471 (71%)]\tLoss: 1610581.000000\n",
            "Train Epoch: 22 [5440/7471 (73%)]\tLoss: 1532848.000000\n",
            "Train Epoch: 22 [5600/7471 (75%)]\tLoss: 1605505.625000\n",
            "Train Epoch: 22 [5760/7471 (77%)]\tLoss: 1617943.000000\n",
            "Train Epoch: 22 [5920/7471 (79%)]\tLoss: 1556951.125000\n",
            "Train Epoch: 22 [6080/7471 (81%)]\tLoss: 1586122.500000\n",
            "Train Epoch: 22 [6240/7471 (84%)]\tLoss: 1589201.625000\n",
            "Train Epoch: 22 [6400/7471 (86%)]\tLoss: 1565871.125000\n",
            "Train Epoch: 22 [6560/7471 (88%)]\tLoss: 1555125.375000\n",
            "Train Epoch: 22 [6720/7471 (90%)]\tLoss: 1625029.625000\n",
            "Train Epoch: 22 [6880/7471 (92%)]\tLoss: 1545955.125000\n",
            "Train Epoch: 22 [7040/7471 (94%)]\tLoss: 1553168.750000\n",
            "Train Epoch: 22 [7200/7471 (96%)]\tLoss: 1631073.125000\n",
            "Train Epoch: 22 [7360/7471 (99%)]\tLoss: 1579036.500000\n",
            "Epoch 22 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 23 [160/7471 (2%)]\tLoss: 1583660.375000\n",
            "Train Epoch: 23 [320/7471 (4%)]\tLoss: 1609126.250000\n",
            "Train Epoch: 23 [480/7471 (6%)]\tLoss: 1544185.250000\n",
            "Train Epoch: 23 [640/7471 (9%)]\tLoss: 1585593.625000\n",
            "Train Epoch: 23 [800/7471 (11%)]\tLoss: 1551313.375000\n",
            "Train Epoch: 23 [960/7471 (13%)]\tLoss: 1624818.625000\n",
            "Train Epoch: 23 [1120/7471 (15%)]\tLoss: 1581563.250000\n",
            "Train Epoch: 23 [1280/7471 (17%)]\tLoss: 1635987.000000\n",
            "Train Epoch: 23 [1440/7471 (19%)]\tLoss: 1538021.500000\n",
            "Train Epoch: 23 [1600/7471 (21%)]\tLoss: 1572194.875000\n",
            "Train Epoch: 23 [1760/7471 (24%)]\tLoss: 1526350.375000\n",
            "Train Epoch: 23 [1920/7471 (26%)]\tLoss: 1599671.500000\n",
            "Train Epoch: 23 [2080/7471 (28%)]\tLoss: 1567018.500000\n",
            "Train Epoch: 23 [2240/7471 (30%)]\tLoss: 1555378.375000\n",
            "Train Epoch: 23 [2400/7471 (32%)]\tLoss: 1500394.250000\n",
            "Train Epoch: 23 [2560/7471 (34%)]\tLoss: 1574995.750000\n",
            "Train Epoch: 23 [2720/7471 (36%)]\tLoss: 1615068.625000\n",
            "Train Epoch: 23 [2880/7471 (39%)]\tLoss: 1543530.125000\n",
            "Train Epoch: 23 [3040/7471 (41%)]\tLoss: 1586244.000000\n",
            "Train Epoch: 23 [3200/7471 (43%)]\tLoss: 1601403.875000\n",
            "Train Epoch: 23 [3360/7471 (45%)]\tLoss: 1594387.500000\n",
            "Train Epoch: 23 [3520/7471 (47%)]\tLoss: 1590558.750000\n",
            "Train Epoch: 23 [3680/7471 (49%)]\tLoss: 1593239.500000\n",
            "Train Epoch: 23 [3840/7471 (51%)]\tLoss: 1591594.000000\n",
            "Train Epoch: 23 [4000/7471 (54%)]\tLoss: 1627522.875000\n",
            "Train Epoch: 23 [4160/7471 (56%)]\tLoss: 1637407.250000\n",
            "Train Epoch: 23 [4320/7471 (58%)]\tLoss: 1615813.500000\n",
            "Train Epoch: 23 [4480/7471 (60%)]\tLoss: 1616437.750000\n",
            "Train Epoch: 23 [4640/7471 (62%)]\tLoss: 1536719.500000\n",
            "Train Epoch: 23 [4800/7471 (64%)]\tLoss: 1614723.750000\n",
            "Train Epoch: 23 [4960/7471 (66%)]\tLoss: 1540469.500000\n",
            "Train Epoch: 23 [5120/7471 (69%)]\tLoss: 1634024.250000\n",
            "Train Epoch: 23 [5280/7471 (71%)]\tLoss: 1589573.250000\n",
            "Train Epoch: 23 [5440/7471 (73%)]\tLoss: 1617629.375000\n",
            "Train Epoch: 23 [5600/7471 (75%)]\tLoss: 1593918.875000\n",
            "Train Epoch: 23 [5760/7471 (77%)]\tLoss: 1640370.375000\n",
            "Train Epoch: 23 [5920/7471 (79%)]\tLoss: 1613511.250000\n",
            "Train Epoch: 23 [6080/7471 (81%)]\tLoss: 1568180.125000\n",
            "Train Epoch: 23 [6240/7471 (84%)]\tLoss: 1569082.875000\n",
            "Train Epoch: 23 [6400/7471 (86%)]\tLoss: 1596247.125000\n",
            "Train Epoch: 23 [6560/7471 (88%)]\tLoss: 1642165.000000\n",
            "Train Epoch: 23 [6720/7471 (90%)]\tLoss: 1585645.750000\n",
            "Train Epoch: 23 [6880/7471 (92%)]\tLoss: 1581405.000000\n",
            "Train Epoch: 23 [7040/7471 (94%)]\tLoss: 1632807.750000\n",
            "Train Epoch: 23 [7200/7471 (96%)]\tLoss: 1591224.250000\n",
            "Train Epoch: 23 [7360/7471 (99%)]\tLoss: 1523659.250000\n",
            "Epoch 23 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 24 [160/7471 (2%)]\tLoss: 1590561.125000\n",
            "Train Epoch: 24 [320/7471 (4%)]\tLoss: 1515764.625000\n",
            "Train Epoch: 24 [480/7471 (6%)]\tLoss: 1553047.000000\n",
            "Train Epoch: 24 [640/7471 (9%)]\tLoss: 1576052.125000\n",
            "Train Epoch: 24 [800/7471 (11%)]\tLoss: 1556732.625000\n",
            "Train Epoch: 24 [960/7471 (13%)]\tLoss: 1631955.125000\n",
            "Train Epoch: 24 [1120/7471 (15%)]\tLoss: 1580229.750000\n",
            "Train Epoch: 24 [1280/7471 (17%)]\tLoss: 1494567.750000\n",
            "Train Epoch: 24 [1440/7471 (19%)]\tLoss: 1622526.250000\n",
            "Train Epoch: 24 [1600/7471 (21%)]\tLoss: 1626084.125000\n",
            "Train Epoch: 24 [1760/7471 (24%)]\tLoss: 1625940.375000\n",
            "Train Epoch: 24 [1920/7471 (26%)]\tLoss: 1611241.250000\n",
            "Train Epoch: 24 [2080/7471 (28%)]\tLoss: 1624619.625000\n",
            "Train Epoch: 24 [2240/7471 (30%)]\tLoss: 1558172.125000\n",
            "Train Epoch: 24 [2400/7471 (32%)]\tLoss: 1615635.250000\n",
            "Train Epoch: 24 [2560/7471 (34%)]\tLoss: 1540068.625000\n",
            "Train Epoch: 24 [2720/7471 (36%)]\tLoss: 1561818.125000\n",
            "Train Epoch: 24 [2880/7471 (39%)]\tLoss: 1579490.250000\n",
            "Train Epoch: 24 [3040/7471 (41%)]\tLoss: 1572991.000000\n",
            "Train Epoch: 24 [3200/7471 (43%)]\tLoss: 1559473.125000\n",
            "Train Epoch: 24 [3360/7471 (45%)]\tLoss: 1629740.875000\n",
            "Train Epoch: 24 [3520/7471 (47%)]\tLoss: 1626872.000000\n",
            "Train Epoch: 24 [3680/7471 (49%)]\tLoss: 1557303.750000\n",
            "Train Epoch: 24 [3840/7471 (51%)]\tLoss: 1570463.500000\n",
            "Train Epoch: 24 [4000/7471 (54%)]\tLoss: 1617146.625000\n",
            "Train Epoch: 24 [4160/7471 (56%)]\tLoss: 1621117.125000\n",
            "Train Epoch: 24 [4320/7471 (58%)]\tLoss: 1537317.000000\n",
            "Train Epoch: 24 [4480/7471 (60%)]\tLoss: 1589717.875000\n",
            "Train Epoch: 24 [4640/7471 (62%)]\tLoss: 1517229.250000\n",
            "Train Epoch: 24 [4800/7471 (64%)]\tLoss: 1603392.250000\n",
            "Train Epoch: 24 [4960/7471 (66%)]\tLoss: 1610095.875000\n",
            "Train Epoch: 24 [5120/7471 (69%)]\tLoss: 1573447.875000\n",
            "Train Epoch: 24 [5280/7471 (71%)]\tLoss: 1557226.750000\n",
            "Train Epoch: 24 [5440/7471 (73%)]\tLoss: 1504964.125000\n",
            "Train Epoch: 24 [5600/7471 (75%)]\tLoss: 1581156.500000\n",
            "Train Epoch: 24 [5760/7471 (77%)]\tLoss: 1609508.250000\n",
            "Train Epoch: 24 [5920/7471 (79%)]\tLoss: 1576976.750000\n",
            "Train Epoch: 24 [6080/7471 (81%)]\tLoss: 1626739.500000\n",
            "Train Epoch: 24 [6240/7471 (84%)]\tLoss: 1591411.500000\n",
            "Train Epoch: 24 [6400/7471 (86%)]\tLoss: 1598469.000000\n",
            "Train Epoch: 24 [6560/7471 (88%)]\tLoss: 1586589.125000\n",
            "Train Epoch: 24 [6720/7471 (90%)]\tLoss: 1621330.750000\n",
            "Train Epoch: 24 [6880/7471 (92%)]\tLoss: 1601105.375000\n",
            "Train Epoch: 24 [7040/7471 (94%)]\tLoss: 1627032.750000\n",
            "Train Epoch: 24 [7200/7471 (96%)]\tLoss: 1599168.250000\n",
            "Train Epoch: 24 [7360/7471 (99%)]\tLoss: 1567331.625000\n",
            "Epoch 24 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 25 [160/7471 (2%)]\tLoss: 1616227.625000\n",
            "Train Epoch: 25 [320/7471 (4%)]\tLoss: 1610847.875000\n",
            "Train Epoch: 25 [480/7471 (6%)]\tLoss: 1639008.250000\n",
            "Train Epoch: 25 [640/7471 (9%)]\tLoss: 1576231.250000\n",
            "Train Epoch: 25 [800/7471 (11%)]\tLoss: 1543584.125000\n",
            "Train Epoch: 25 [960/7471 (13%)]\tLoss: 1570320.000000\n",
            "Train Epoch: 25 [1120/7471 (15%)]\tLoss: 1591498.375000\n",
            "Train Epoch: 25 [1280/7471 (17%)]\tLoss: 1641868.750000\n",
            "Train Epoch: 25 [1440/7471 (19%)]\tLoss: 1630250.375000\n",
            "Train Epoch: 25 [1600/7471 (21%)]\tLoss: 1553346.750000\n",
            "Train Epoch: 25 [1760/7471 (24%)]\tLoss: 1596944.500000\n",
            "Train Epoch: 25 [1920/7471 (26%)]\tLoss: 1625777.875000\n",
            "Train Epoch: 25 [2080/7471 (28%)]\tLoss: 1610605.625000\n",
            "Train Epoch: 25 [2240/7471 (30%)]\tLoss: 1597316.250000\n",
            "Train Epoch: 25 [2400/7471 (32%)]\tLoss: 1604403.875000\n",
            "Train Epoch: 25 [2560/7471 (34%)]\tLoss: 1591594.625000\n",
            "Train Epoch: 25 [2720/7471 (36%)]\tLoss: 1601729.125000\n",
            "Train Epoch: 25 [2880/7471 (39%)]\tLoss: 1625471.625000\n",
            "Train Epoch: 25 [3040/7471 (41%)]\tLoss: 1539560.625000\n",
            "Train Epoch: 25 [3200/7471 (43%)]\tLoss: 1577094.625000\n",
            "Train Epoch: 25 [3360/7471 (45%)]\tLoss: 1598244.625000\n",
            "Train Epoch: 25 [3520/7471 (47%)]\tLoss: 1584225.250000\n",
            "Train Epoch: 25 [3680/7471 (49%)]\tLoss: 1593768.750000\n",
            "Train Epoch: 25 [3840/7471 (51%)]\tLoss: 1603505.875000\n",
            "Train Epoch: 25 [4000/7471 (54%)]\tLoss: 1552096.250000\n",
            "Train Epoch: 25 [4160/7471 (56%)]\tLoss: 1550736.250000\n",
            "Train Epoch: 25 [4320/7471 (58%)]\tLoss: 1593719.750000\n",
            "Train Epoch: 25 [4480/7471 (60%)]\tLoss: 1634656.875000\n",
            "Train Epoch: 25 [4640/7471 (62%)]\tLoss: 1525160.750000\n",
            "Train Epoch: 25 [4800/7471 (64%)]\tLoss: 1582801.000000\n",
            "Train Epoch: 25 [4960/7471 (66%)]\tLoss: 1611222.625000\n",
            "Train Epoch: 25 [5120/7471 (69%)]\tLoss: 1615703.500000\n",
            "Train Epoch: 25 [5280/7471 (71%)]\tLoss: 1555102.125000\n",
            "Train Epoch: 25 [5440/7471 (73%)]\tLoss: 1566505.375000\n",
            "Train Epoch: 25 [5600/7471 (75%)]\tLoss: 1518353.750000\n",
            "Train Epoch: 25 [5760/7471 (77%)]\tLoss: 1526427.000000\n",
            "Train Epoch: 25 [5920/7471 (79%)]\tLoss: 1626920.750000\n",
            "Train Epoch: 25 [6080/7471 (81%)]\tLoss: 1611972.375000\n",
            "Train Epoch: 25 [6240/7471 (84%)]\tLoss: 1518000.125000\n",
            "Train Epoch: 25 [6400/7471 (86%)]\tLoss: 1625701.500000\n",
            "Train Epoch: 25 [6560/7471 (88%)]\tLoss: 1564003.250000\n",
            "Train Epoch: 25 [6720/7471 (90%)]\tLoss: 1566932.500000\n",
            "Train Epoch: 25 [6880/7471 (92%)]\tLoss: 1585277.500000\n",
            "Train Epoch: 25 [7040/7471 (94%)]\tLoss: 1575378.500000\n",
            "Train Epoch: 25 [7200/7471 (96%)]\tLoss: 1647869.750000\n",
            "Train Epoch: 25 [7360/7471 (99%)]\tLoss: 1566393.375000\n",
            "Epoch 25 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 99112.9865\n",
            "\n",
            "Train Epoch: 26 [160/7471 (2%)]\tLoss: 1585473.625000\n",
            "Train Epoch: 26 [320/7471 (4%)]\tLoss: 1558632.875000\n",
            "Train Epoch: 26 [480/7471 (6%)]\tLoss: 1604970.750000\n",
            "Train Epoch: 26 [640/7471 (9%)]\tLoss: 1606024.625000\n",
            "Train Epoch: 26 [800/7471 (11%)]\tLoss: 1619187.375000\n",
            "Train Epoch: 26 [960/7471 (13%)]\tLoss: 1617979.875000\n",
            "Train Epoch: 26 [1120/7471 (15%)]\tLoss: 1544862.625000\n",
            "Train Epoch: 26 [1280/7471 (17%)]\tLoss: 1595689.250000\n",
            "Train Epoch: 26 [1440/7471 (19%)]\tLoss: 1545747.500000\n",
            "Train Epoch: 26 [1600/7471 (21%)]\tLoss: 1610574.500000\n",
            "Train Epoch: 26 [1760/7471 (24%)]\tLoss: 1535779.500000\n",
            "Train Epoch: 26 [1920/7471 (26%)]\tLoss: 1633128.125000\n",
            "Train Epoch: 26 [2080/7471 (28%)]\tLoss: 1631964.125000\n",
            "Train Epoch: 26 [2240/7471 (30%)]\tLoss: 1563158.625000\n",
            "Train Epoch: 26 [2400/7471 (32%)]\tLoss: 1591300.375000\n",
            "Train Epoch: 26 [2560/7471 (34%)]\tLoss: 1613517.375000\n",
            "Train Epoch: 26 [2720/7471 (36%)]\tLoss: 1595263.750000\n",
            "Train Epoch: 26 [2880/7471 (39%)]\tLoss: 1507680.750000\n",
            "Train Epoch: 26 [3040/7471 (41%)]\tLoss: 1588715.500000\n",
            "Train Epoch: 26 [3200/7471 (43%)]\tLoss: 1593561.625000\n",
            "Train Epoch: 26 [3360/7471 (45%)]\tLoss: 1592788.875000\n",
            "Train Epoch: 26 [3520/7471 (47%)]\tLoss: 1599587.375000\n",
            "Train Epoch: 26 [3680/7471 (49%)]\tLoss: 1549175.125000\n",
            "Train Epoch: 26 [3840/7471 (51%)]\tLoss: 1600797.875000\n",
            "Train Epoch: 26 [4000/7471 (54%)]\tLoss: 1587960.500000\n",
            "Train Epoch: 26 [4160/7471 (56%)]\tLoss: 1582829.250000\n",
            "Train Epoch: 26 [4320/7471 (58%)]\tLoss: 1596240.625000\n",
            "Train Epoch: 26 [4480/7471 (60%)]\tLoss: 1599618.125000\n",
            "Train Epoch: 26 [4640/7471 (62%)]\tLoss: 1604410.750000\n",
            "Train Epoch: 26 [4800/7471 (64%)]\tLoss: 1619898.625000\n",
            "Train Epoch: 26 [4960/7471 (66%)]\tLoss: 1634936.750000\n",
            "Train Epoch: 26 [5120/7471 (69%)]\tLoss: 1633808.000000\n",
            "Train Epoch: 26 [5280/7471 (71%)]\tLoss: 1527734.750000\n",
            "Train Epoch: 26 [5440/7471 (73%)]\tLoss: 1613764.500000\n",
            "Train Epoch: 26 [5600/7471 (75%)]\tLoss: 1590489.000000\n",
            "Train Epoch: 26 [5760/7471 (77%)]\tLoss: 1570831.250000\n",
            "Train Epoch: 26 [5920/7471 (79%)]\tLoss: 1580298.250000\n",
            "Train Epoch: 26 [6080/7471 (81%)]\tLoss: 1592188.000000\n",
            "Train Epoch: 26 [6240/7471 (84%)]\tLoss: 1639271.125000\n",
            "Train Epoch: 26 [6400/7471 (86%)]\tLoss: 1582390.625000\n",
            "Train Epoch: 26 [6560/7471 (88%)]\tLoss: 1567006.500000\n",
            "Train Epoch: 26 [6720/7471 (90%)]\tLoss: 1619691.375000\n",
            "Train Epoch: 26 [6880/7471 (92%)]\tLoss: 1600030.375000\n",
            "Train Epoch: 26 [7040/7471 (94%)]\tLoss: 1620915.375000\n",
            "Train Epoch: 26 [7200/7471 (96%)]\tLoss: 1566296.875000\n",
            "Train Epoch: 26 [7360/7471 (99%)]\tLoss: 1584645.625000\n",
            "Epoch 26 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 27 [160/7471 (2%)]\tLoss: 1548505.500000\n",
            "Train Epoch: 27 [320/7471 (4%)]\tLoss: 1638204.500000\n",
            "Train Epoch: 27 [480/7471 (6%)]\tLoss: 1638069.375000\n",
            "Train Epoch: 27 [640/7471 (9%)]\tLoss: 1577613.875000\n",
            "Train Epoch: 27 [800/7471 (11%)]\tLoss: 1614765.375000\n",
            "Train Epoch: 27 [960/7471 (13%)]\tLoss: 1640597.000000\n",
            "Train Epoch: 27 [1120/7471 (15%)]\tLoss: 1562803.250000\n",
            "Train Epoch: 27 [1280/7471 (17%)]\tLoss: 1624816.750000\n",
            "Train Epoch: 27 [1440/7471 (19%)]\tLoss: 1594742.750000\n",
            "Train Epoch: 27 [1600/7471 (21%)]\tLoss: 1603284.375000\n",
            "Train Epoch: 27 [1760/7471 (24%)]\tLoss: 1647574.125000\n",
            "Train Epoch: 27 [1920/7471 (26%)]\tLoss: 1622895.000000\n",
            "Train Epoch: 27 [2080/7471 (28%)]\tLoss: 1651681.375000\n",
            "Train Epoch: 27 [2240/7471 (30%)]\tLoss: 1578373.875000\n",
            "Train Epoch: 27 [2400/7471 (32%)]\tLoss: 1603808.500000\n",
            "Train Epoch: 27 [2560/7471 (34%)]\tLoss: 1620005.125000\n",
            "Train Epoch: 27 [2720/7471 (36%)]\tLoss: 1579744.375000\n",
            "Train Epoch: 27 [2880/7471 (39%)]\tLoss: 1591931.625000\n",
            "Train Epoch: 27 [3040/7471 (41%)]\tLoss: 1605574.000000\n",
            "Train Epoch: 27 [3200/7471 (43%)]\tLoss: 1567395.375000\n",
            "Train Epoch: 27 [3360/7471 (45%)]\tLoss: 1612714.750000\n",
            "Train Epoch: 27 [3520/7471 (47%)]\tLoss: 1572816.500000\n",
            "Train Epoch: 27 [3680/7471 (49%)]\tLoss: 1536642.750000\n",
            "Train Epoch: 27 [3840/7471 (51%)]\tLoss: 1471127.500000\n",
            "Train Epoch: 27 [4000/7471 (54%)]\tLoss: 1515883.375000\n",
            "Train Epoch: 27 [4160/7471 (56%)]\tLoss: 1562123.625000\n",
            "Train Epoch: 27 [4320/7471 (58%)]\tLoss: 1616099.250000\n",
            "Train Epoch: 27 [4480/7471 (60%)]\tLoss: 1547074.000000\n",
            "Train Epoch: 27 [4640/7471 (62%)]\tLoss: 1546451.500000\n",
            "Train Epoch: 27 [4800/7471 (64%)]\tLoss: 1603909.750000\n",
            "Train Epoch: 27 [4960/7471 (66%)]\tLoss: 1613655.250000\n",
            "Train Epoch: 27 [5120/7471 (69%)]\tLoss: 1534848.125000\n",
            "Train Epoch: 27 [5280/7471 (71%)]\tLoss: 1560816.250000\n",
            "Train Epoch: 27 [5440/7471 (73%)]\tLoss: 1563315.375000\n",
            "Train Epoch: 27 [5600/7471 (75%)]\tLoss: 1586976.500000\n",
            "Train Epoch: 27 [5760/7471 (77%)]\tLoss: 1590745.875000\n",
            "Train Epoch: 27 [5920/7471 (79%)]\tLoss: 1580414.125000\n",
            "Train Epoch: 27 [6080/7471 (81%)]\tLoss: 1589181.625000\n",
            "Train Epoch: 27 [6240/7471 (84%)]\tLoss: 1615162.750000\n",
            "Train Epoch: 27 [6400/7471 (86%)]\tLoss: 1558023.750000\n",
            "Train Epoch: 27 [6560/7471 (88%)]\tLoss: 1572974.250000\n",
            "Train Epoch: 27 [6720/7471 (90%)]\tLoss: 1562101.000000\n",
            "Train Epoch: 27 [6880/7471 (92%)]\tLoss: 1588747.750000\n",
            "Train Epoch: 27 [7040/7471 (94%)]\tLoss: 1582402.750000\n",
            "Train Epoch: 27 [7200/7471 (96%)]\tLoss: 1643842.625000\n",
            "Train Epoch: 27 [7360/7471 (99%)]\tLoss: 1574922.250000\n",
            "Epoch 27 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 28 [160/7471 (2%)]\tLoss: 1552265.875000\n",
            "Train Epoch: 28 [320/7471 (4%)]\tLoss: 1597789.500000\n",
            "Train Epoch: 28 [480/7471 (6%)]\tLoss: 1631642.375000\n",
            "Train Epoch: 28 [640/7471 (9%)]\tLoss: 1616043.500000\n",
            "Train Epoch: 28 [800/7471 (11%)]\tLoss: 1589985.750000\n",
            "Train Epoch: 28 [960/7471 (13%)]\tLoss: 1567141.375000\n",
            "Train Epoch: 28 [1120/7471 (15%)]\tLoss: 1607521.750000\n",
            "Train Epoch: 28 [1280/7471 (17%)]\tLoss: 1498058.875000\n",
            "Train Epoch: 28 [1440/7471 (19%)]\tLoss: 1492814.625000\n",
            "Train Epoch: 28 [1600/7471 (21%)]\tLoss: 1573812.125000\n",
            "Train Epoch: 28 [1760/7471 (24%)]\tLoss: 1508839.625000\n",
            "Train Epoch: 28 [1920/7471 (26%)]\tLoss: 1597981.000000\n",
            "Train Epoch: 28 [2080/7471 (28%)]\tLoss: 1556848.125000\n",
            "Train Epoch: 28 [2240/7471 (30%)]\tLoss: 1566678.750000\n",
            "Train Epoch: 28 [2400/7471 (32%)]\tLoss: 1514197.250000\n",
            "Train Epoch: 28 [2560/7471 (34%)]\tLoss: 1532758.875000\n",
            "Train Epoch: 28 [2720/7471 (36%)]\tLoss: 1580755.875000\n",
            "Train Epoch: 28 [2880/7471 (39%)]\tLoss: 1622000.875000\n",
            "Train Epoch: 28 [3040/7471 (41%)]\tLoss: 1566369.125000\n",
            "Train Epoch: 28 [3200/7471 (43%)]\tLoss: 1563621.000000\n",
            "Train Epoch: 28 [3360/7471 (45%)]\tLoss: 1543423.500000\n",
            "Train Epoch: 28 [3520/7471 (47%)]\tLoss: 1588647.625000\n",
            "Train Epoch: 28 [3680/7471 (49%)]\tLoss: 1587070.625000\n",
            "Train Epoch: 28 [3840/7471 (51%)]\tLoss: 1610230.125000\n",
            "Train Epoch: 28 [4000/7471 (54%)]\tLoss: 1628174.375000\n",
            "Train Epoch: 28 [4160/7471 (56%)]\tLoss: 1586573.750000\n",
            "Train Epoch: 28 [4320/7471 (58%)]\tLoss: 1653734.875000\n",
            "Train Epoch: 28 [4480/7471 (60%)]\tLoss: 1557531.250000\n",
            "Train Epoch: 28 [4640/7471 (62%)]\tLoss: 1568882.500000\n",
            "Train Epoch: 28 [4800/7471 (64%)]\tLoss: 1613395.875000\n",
            "Train Epoch: 28 [4960/7471 (66%)]\tLoss: 1566433.625000\n",
            "Train Epoch: 28 [5120/7471 (69%)]\tLoss: 1546214.625000\n",
            "Train Epoch: 28 [5280/7471 (71%)]\tLoss: 1558105.250000\n",
            "Train Epoch: 28 [5440/7471 (73%)]\tLoss: 1502504.500000\n",
            "Train Epoch: 28 [5600/7471 (75%)]\tLoss: 1627041.500000\n",
            "Train Epoch: 28 [5760/7471 (77%)]\tLoss: 1589691.375000\n",
            "Train Epoch: 28 [5920/7471 (79%)]\tLoss: 1619381.250000\n",
            "Train Epoch: 28 [6080/7471 (81%)]\tLoss: 1562801.000000\n",
            "Train Epoch: 28 [6240/7471 (84%)]\tLoss: 1612377.750000\n",
            "Train Epoch: 28 [6400/7471 (86%)]\tLoss: 1566282.500000\n",
            "Train Epoch: 28 [6560/7471 (88%)]\tLoss: 1524566.000000\n",
            "Train Epoch: 28 [6720/7471 (90%)]\tLoss: 1536252.250000\n",
            "Train Epoch: 28 [6880/7471 (92%)]\tLoss: 1550928.625000\n",
            "Train Epoch: 28 [7040/7471 (94%)]\tLoss: 1535884.375000\n",
            "Train Epoch: 28 [7200/7471 (96%)]\tLoss: 1557496.375000\n",
            "Train Epoch: 28 [7360/7471 (99%)]\tLoss: 1593716.750000\n",
            "Epoch 28 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 29 [160/7471 (2%)]\tLoss: 1631484.250000\n",
            "Train Epoch: 29 [320/7471 (4%)]\tLoss: 1612445.750000\n",
            "Train Epoch: 29 [480/7471 (6%)]\tLoss: 1563539.500000\n",
            "Train Epoch: 29 [640/7471 (9%)]\tLoss: 1552536.250000\n",
            "Train Epoch: 29 [800/7471 (11%)]\tLoss: 1537642.125000\n",
            "Train Epoch: 29 [960/7471 (13%)]\tLoss: 1644382.250000\n",
            "Train Epoch: 29 [1120/7471 (15%)]\tLoss: 1638512.125000\n",
            "Train Epoch: 29 [1280/7471 (17%)]\tLoss: 1552298.500000\n",
            "Train Epoch: 29 [1440/7471 (19%)]\tLoss: 1559952.500000\n",
            "Train Epoch: 29 [1600/7471 (21%)]\tLoss: 1581292.500000\n",
            "Train Epoch: 29 [1760/7471 (24%)]\tLoss: 1552098.500000\n",
            "Train Epoch: 29 [1920/7471 (26%)]\tLoss: 1542109.375000\n",
            "Train Epoch: 29 [2080/7471 (28%)]\tLoss: 1601541.500000\n",
            "Train Epoch: 29 [2240/7471 (30%)]\tLoss: 1610233.750000\n",
            "Train Epoch: 29 [2400/7471 (32%)]\tLoss: 1572339.625000\n",
            "Train Epoch: 29 [2560/7471 (34%)]\tLoss: 1564902.500000\n",
            "Train Epoch: 29 [2720/7471 (36%)]\tLoss: 1612492.625000\n",
            "Train Epoch: 29 [2880/7471 (39%)]\tLoss: 1534305.625000\n",
            "Train Epoch: 29 [3040/7471 (41%)]\tLoss: 1575177.125000\n",
            "Train Epoch: 29 [3200/7471 (43%)]\tLoss: 1623259.125000\n",
            "Train Epoch: 29 [3360/7471 (45%)]\tLoss: 1573879.250000\n",
            "Train Epoch: 29 [3520/7471 (47%)]\tLoss: 1594026.000000\n",
            "Train Epoch: 29 [3680/7471 (49%)]\tLoss: 1604019.250000\n",
            "Train Epoch: 29 [3840/7471 (51%)]\tLoss: 1599413.250000\n",
            "Train Epoch: 29 [4000/7471 (54%)]\tLoss: 1551970.500000\n",
            "Train Epoch: 29 [4160/7471 (56%)]\tLoss: 1626112.125000\n",
            "Train Epoch: 29 [4320/7471 (58%)]\tLoss: 1552315.250000\n",
            "Train Epoch: 29 [4480/7471 (60%)]\tLoss: 1593775.250000\n",
            "Train Epoch: 29 [4640/7471 (62%)]\tLoss: 1617416.125000\n",
            "Train Epoch: 29 [4800/7471 (64%)]\tLoss: 1601348.375000\n",
            "Train Epoch: 29 [4960/7471 (66%)]\tLoss: 1532113.875000\n",
            "Train Epoch: 29 [5120/7471 (69%)]\tLoss: 1538123.375000\n",
            "Train Epoch: 29 [5280/7471 (71%)]\tLoss: 1615191.000000\n",
            "Train Epoch: 29 [5440/7471 (73%)]\tLoss: 1569751.250000\n",
            "Train Epoch: 29 [5600/7471 (75%)]\tLoss: 1635807.750000\n",
            "Train Epoch: 29 [5760/7471 (77%)]\tLoss: 1568726.750000\n",
            "Train Epoch: 29 [5920/7471 (79%)]\tLoss: 1598137.250000\n",
            "Train Epoch: 29 [6080/7471 (81%)]\tLoss: 1615723.250000\n",
            "Train Epoch: 29 [6240/7471 (84%)]\tLoss: 1554051.000000\n",
            "Train Epoch: 29 [6400/7471 (86%)]\tLoss: 1491685.750000\n",
            "Train Epoch: 29 [6560/7471 (88%)]\tLoss: 1635252.500000\n",
            "Train Epoch: 29 [6720/7471 (90%)]\tLoss: 1623542.250000\n",
            "Train Epoch: 29 [6880/7471 (92%)]\tLoss: 1589533.875000\n",
            "Train Epoch: 29 [7040/7471 (94%)]\tLoss: 1602172.250000\n",
            "Train Epoch: 29 [7200/7471 (96%)]\tLoss: 1558970.750000\n",
            "Train Epoch: 29 [7360/7471 (99%)]\tLoss: 1597274.750000\n",
            "Epoch 29 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 30 [160/7471 (2%)]\tLoss: 1578465.750000\n",
            "Train Epoch: 30 [320/7471 (4%)]\tLoss: 1618643.000000\n",
            "Train Epoch: 30 [480/7471 (6%)]\tLoss: 1631933.125000\n",
            "Train Epoch: 30 [640/7471 (9%)]\tLoss: 1548556.500000\n",
            "Train Epoch: 30 [800/7471 (11%)]\tLoss: 1568497.500000\n",
            "Train Epoch: 30 [960/7471 (13%)]\tLoss: 1532670.750000\n",
            "Train Epoch: 30 [1120/7471 (15%)]\tLoss: 1541585.375000\n",
            "Train Epoch: 30 [1280/7471 (17%)]\tLoss: 1574575.125000\n",
            "Train Epoch: 30 [1440/7471 (19%)]\tLoss: 1539603.375000\n",
            "Train Epoch: 30 [1600/7471 (21%)]\tLoss: 1592164.875000\n",
            "Train Epoch: 30 [1760/7471 (24%)]\tLoss: 1591782.625000\n",
            "Train Epoch: 30 [1920/7471 (26%)]\tLoss: 1624378.250000\n",
            "Train Epoch: 30 [2080/7471 (28%)]\tLoss: 1566449.500000\n",
            "Train Epoch: 30 [2240/7471 (30%)]\tLoss: 1550551.750000\n",
            "Train Epoch: 30 [2400/7471 (32%)]\tLoss: 1505334.875000\n",
            "Train Epoch: 30 [2560/7471 (34%)]\tLoss: 1574010.375000\n",
            "Train Epoch: 30 [2720/7471 (36%)]\tLoss: 1608979.625000\n",
            "Train Epoch: 30 [2880/7471 (39%)]\tLoss: 1561140.125000\n",
            "Train Epoch: 30 [3040/7471 (41%)]\tLoss: 1603429.375000\n",
            "Train Epoch: 30 [3200/7471 (43%)]\tLoss: 1570827.125000\n",
            "Train Epoch: 30 [3360/7471 (45%)]\tLoss: 1544406.375000\n",
            "Train Epoch: 30 [3520/7471 (47%)]\tLoss: 1596345.125000\n",
            "Train Epoch: 30 [3680/7471 (49%)]\tLoss: 1563632.250000\n",
            "Train Epoch: 30 [3840/7471 (51%)]\tLoss: 1629888.750000\n",
            "Train Epoch: 30 [4000/7471 (54%)]\tLoss: 1587790.500000\n",
            "Train Epoch: 30 [4160/7471 (56%)]\tLoss: 1571991.000000\n",
            "Train Epoch: 30 [4320/7471 (58%)]\tLoss: 1513766.750000\n",
            "Train Epoch: 30 [4480/7471 (60%)]\tLoss: 1604342.125000\n",
            "Train Epoch: 30 [4640/7471 (62%)]\tLoss: 1568022.625000\n",
            "Train Epoch: 30 [4800/7471 (64%)]\tLoss: 1601851.500000\n",
            "Train Epoch: 30 [4960/7471 (66%)]\tLoss: 1647777.875000\n",
            "Train Epoch: 30 [5120/7471 (69%)]\tLoss: 1556865.875000\n",
            "Train Epoch: 30 [5280/7471 (71%)]\tLoss: 1527313.750000\n",
            "Train Epoch: 30 [5440/7471 (73%)]\tLoss: 1570404.750000\n",
            "Train Epoch: 30 [5600/7471 (75%)]\tLoss: 1575758.875000\n",
            "Train Epoch: 30 [5760/7471 (77%)]\tLoss: 1590149.000000\n",
            "Train Epoch: 30 [5920/7471 (79%)]\tLoss: 1612281.500000\n",
            "Train Epoch: 30 [6080/7471 (81%)]\tLoss: 1529396.750000\n",
            "Train Epoch: 30 [6240/7471 (84%)]\tLoss: 1612901.875000\n",
            "Train Epoch: 30 [6400/7471 (86%)]\tLoss: 1551288.125000\n",
            "Train Epoch: 30 [6560/7471 (88%)]\tLoss: 1644063.750000\n",
            "Train Epoch: 30 [6720/7471 (90%)]\tLoss: 1521367.500000\n",
            "Train Epoch: 30 [6880/7471 (92%)]\tLoss: 1614062.375000\n",
            "Train Epoch: 30 [7040/7471 (94%)]\tLoss: 1604467.500000\n",
            "Train Epoch: 30 [7200/7471 (96%)]\tLoss: 1628192.500000\n",
            "Train Epoch: 30 [7360/7471 (99%)]\tLoss: 1553088.875000\n",
            "Epoch 30 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 31 [160/7471 (2%)]\tLoss: 1638215.000000\n",
            "Train Epoch: 31 [320/7471 (4%)]\tLoss: 1625368.125000\n",
            "Train Epoch: 31 [480/7471 (6%)]\tLoss: 1595051.875000\n",
            "Train Epoch: 31 [640/7471 (9%)]\tLoss: 1626051.875000\n",
            "Train Epoch: 31 [800/7471 (11%)]\tLoss: 1545758.500000\n",
            "Train Epoch: 31 [960/7471 (13%)]\tLoss: 1561526.875000\n",
            "Train Epoch: 31 [1120/7471 (15%)]\tLoss: 1560966.250000\n",
            "Train Epoch: 31 [1280/7471 (17%)]\tLoss: 1557192.000000\n",
            "Train Epoch: 31 [1440/7471 (19%)]\tLoss: 1571599.250000\n",
            "Train Epoch: 31 [1600/7471 (21%)]\tLoss: 1533409.375000\n",
            "Train Epoch: 31 [1760/7471 (24%)]\tLoss: 1601587.000000\n",
            "Train Epoch: 31 [1920/7471 (26%)]\tLoss: 1579533.000000\n",
            "Train Epoch: 31 [2080/7471 (28%)]\tLoss: 1557205.125000\n",
            "Train Epoch: 31 [2240/7471 (30%)]\tLoss: 1594633.125000\n",
            "Train Epoch: 31 [2400/7471 (32%)]\tLoss: 1557207.875000\n",
            "Train Epoch: 31 [2560/7471 (34%)]\tLoss: 1586236.625000\n",
            "Train Epoch: 31 [2720/7471 (36%)]\tLoss: 1634816.000000\n",
            "Train Epoch: 31 [2880/7471 (39%)]\tLoss: 1611049.250000\n",
            "Train Epoch: 31 [3040/7471 (41%)]\tLoss: 1572779.000000\n",
            "Train Epoch: 31 [3200/7471 (43%)]\tLoss: 1571362.875000\n",
            "Train Epoch: 31 [3360/7471 (45%)]\tLoss: 1553106.875000\n",
            "Train Epoch: 31 [3520/7471 (47%)]\tLoss: 1562040.750000\n",
            "Train Epoch: 31 [3680/7471 (49%)]\tLoss: 1627640.625000\n",
            "Train Epoch: 31 [3840/7471 (51%)]\tLoss: 1563142.875000\n",
            "Train Epoch: 31 [4000/7471 (54%)]\tLoss: 1574080.250000\n",
            "Train Epoch: 31 [4160/7471 (56%)]\tLoss: 1544412.000000\n",
            "Train Epoch: 31 [4320/7471 (58%)]\tLoss: 1523018.875000\n",
            "Train Epoch: 31 [4480/7471 (60%)]\tLoss: 1597814.000000\n",
            "Train Epoch: 31 [4640/7471 (62%)]\tLoss: 1605341.875000\n",
            "Train Epoch: 31 [4800/7471 (64%)]\tLoss: 1594135.250000\n",
            "Train Epoch: 31 [4960/7471 (66%)]\tLoss: 1553531.875000\n",
            "Train Epoch: 31 [5120/7471 (69%)]\tLoss: 1556373.250000\n",
            "Train Epoch: 31 [5280/7471 (71%)]\tLoss: 1635660.500000\n",
            "Train Epoch: 31 [5440/7471 (73%)]\tLoss: 1599184.375000\n",
            "Train Epoch: 31 [5600/7471 (75%)]\tLoss: 1611203.000000\n",
            "Train Epoch: 31 [5760/7471 (77%)]\tLoss: 1587424.875000\n",
            "Train Epoch: 31 [5920/7471 (79%)]\tLoss: 1599029.750000\n",
            "Train Epoch: 31 [6080/7471 (81%)]\tLoss: 1590155.500000\n",
            "Train Epoch: 31 [6240/7471 (84%)]\tLoss: 1612477.625000\n",
            "Train Epoch: 31 [6400/7471 (86%)]\tLoss: 1609666.625000\n",
            "Train Epoch: 31 [6560/7471 (88%)]\tLoss: 1555545.875000\n",
            "Train Epoch: 31 [6720/7471 (90%)]\tLoss: 1546606.375000\n",
            "Train Epoch: 31 [6880/7471 (92%)]\tLoss: 1594124.125000\n",
            "Train Epoch: 31 [7040/7471 (94%)]\tLoss: 1580784.875000\n",
            "Train Epoch: 31 [7200/7471 (96%)]\tLoss: 1610844.000000\n",
            "Train Epoch: 31 [7360/7471 (99%)]\tLoss: 1599699.500000\n",
            "Epoch 31 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 32 [160/7471 (2%)]\tLoss: 1583772.000000\n",
            "Train Epoch: 32 [320/7471 (4%)]\tLoss: 1542797.375000\n",
            "Train Epoch: 32 [480/7471 (6%)]\tLoss: 1559996.250000\n",
            "Train Epoch: 32 [640/7471 (9%)]\tLoss: 1512281.875000\n",
            "Train Epoch: 32 [800/7471 (11%)]\tLoss: 1598265.875000\n",
            "Train Epoch: 32 [960/7471 (13%)]\tLoss: 1620449.375000\n",
            "Train Epoch: 32 [1120/7471 (15%)]\tLoss: 1566640.875000\n",
            "Train Epoch: 32 [1280/7471 (17%)]\tLoss: 1614038.250000\n",
            "Train Epoch: 32 [1440/7471 (19%)]\tLoss: 1626531.750000\n",
            "Train Epoch: 32 [1600/7471 (21%)]\tLoss: 1562486.250000\n",
            "Train Epoch: 32 [1760/7471 (24%)]\tLoss: 1531558.000000\n",
            "Train Epoch: 32 [1920/7471 (26%)]\tLoss: 1574842.875000\n",
            "Train Epoch: 32 [2080/7471 (28%)]\tLoss: 1576484.750000\n",
            "Train Epoch: 32 [2240/7471 (30%)]\tLoss: 1592002.750000\n",
            "Train Epoch: 32 [2400/7471 (32%)]\tLoss: 1605065.750000\n",
            "Train Epoch: 32 [2560/7471 (34%)]\tLoss: 1593841.250000\n",
            "Train Epoch: 32 [2720/7471 (36%)]\tLoss: 1621075.250000\n",
            "Train Epoch: 32 [2880/7471 (39%)]\tLoss: 1583533.375000\n",
            "Train Epoch: 32 [3040/7471 (41%)]\tLoss: 1600093.500000\n",
            "Train Epoch: 32 [3200/7471 (43%)]\tLoss: 1619037.375000\n",
            "Train Epoch: 32 [3360/7471 (45%)]\tLoss: 1621077.750000\n",
            "Train Epoch: 32 [3520/7471 (47%)]\tLoss: 1608164.000000\n",
            "Train Epoch: 32 [3680/7471 (49%)]\tLoss: 1631745.875000\n",
            "Train Epoch: 32 [3840/7471 (51%)]\tLoss: 1602874.750000\n",
            "Train Epoch: 32 [4000/7471 (54%)]\tLoss: 1497706.375000\n",
            "Train Epoch: 32 [4160/7471 (56%)]\tLoss: 1608600.250000\n",
            "Train Epoch: 32 [4320/7471 (58%)]\tLoss: 1604146.375000\n",
            "Train Epoch: 32 [4480/7471 (60%)]\tLoss: 1626866.500000\n",
            "Train Epoch: 32 [4640/7471 (62%)]\tLoss: 1624606.375000\n",
            "Train Epoch: 32 [4800/7471 (64%)]\tLoss: 1569519.000000\n",
            "Train Epoch: 32 [4960/7471 (66%)]\tLoss: 1577886.500000\n",
            "Train Epoch: 32 [5120/7471 (69%)]\tLoss: 1616102.250000\n",
            "Train Epoch: 32 [5280/7471 (71%)]\tLoss: 1640917.875000\n",
            "Train Epoch: 32 [5440/7471 (73%)]\tLoss: 1584856.750000\n",
            "Train Epoch: 32 [5600/7471 (75%)]\tLoss: 1613092.625000\n",
            "Train Epoch: 32 [5760/7471 (77%)]\tLoss: 1576291.250000\n",
            "Train Epoch: 32 [5920/7471 (79%)]\tLoss: 1539006.375000\n",
            "Train Epoch: 32 [6080/7471 (81%)]\tLoss: 1595043.625000\n",
            "Train Epoch: 32 [6240/7471 (84%)]\tLoss: 1589357.125000\n",
            "Train Epoch: 32 [6400/7471 (86%)]\tLoss: 1557485.250000\n",
            "Train Epoch: 32 [6560/7471 (88%)]\tLoss: 1600017.000000\n",
            "Train Epoch: 32 [6720/7471 (90%)]\tLoss: 1594668.500000\n",
            "Train Epoch: 32 [6880/7471 (92%)]\tLoss: 1597166.375000\n",
            "Train Epoch: 32 [7040/7471 (94%)]\tLoss: 1587653.750000\n",
            "Train Epoch: 32 [7200/7471 (96%)]\tLoss: 1571494.125000\n",
            "Train Epoch: 32 [7360/7471 (99%)]\tLoss: 1612121.250000\n",
            "Epoch 32 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 33 [160/7471 (2%)]\tLoss: 1614972.125000\n",
            "Train Epoch: 33 [320/7471 (4%)]\tLoss: 1591135.375000\n",
            "Train Epoch: 33 [480/7471 (6%)]\tLoss: 1601891.750000\n",
            "Train Epoch: 33 [640/7471 (9%)]\tLoss: 1575779.375000\n",
            "Train Epoch: 33 [800/7471 (11%)]\tLoss: 1570269.250000\n",
            "Train Epoch: 33 [960/7471 (13%)]\tLoss: 1510083.000000\n",
            "Train Epoch: 33 [1120/7471 (15%)]\tLoss: 1551091.625000\n",
            "Train Epoch: 33 [1280/7471 (17%)]\tLoss: 1490609.250000\n",
            "Train Epoch: 33 [1440/7471 (19%)]\tLoss: 1620044.250000\n",
            "Train Epoch: 33 [1600/7471 (21%)]\tLoss: 1478997.875000\n",
            "Train Epoch: 33 [1760/7471 (24%)]\tLoss: 1585512.750000\n",
            "Train Epoch: 33 [1920/7471 (26%)]\tLoss: 1641792.625000\n",
            "Train Epoch: 33 [2080/7471 (28%)]\tLoss: 1552723.250000\n",
            "Train Epoch: 33 [2240/7471 (30%)]\tLoss: 1618048.375000\n",
            "Train Epoch: 33 [2400/7471 (32%)]\tLoss: 1598132.875000\n",
            "Train Epoch: 33 [2560/7471 (34%)]\tLoss: 1568924.625000\n",
            "Train Epoch: 33 [2720/7471 (36%)]\tLoss: 1569725.875000\n",
            "Train Epoch: 33 [2880/7471 (39%)]\tLoss: 1559426.500000\n",
            "Train Epoch: 33 [3040/7471 (41%)]\tLoss: 1534306.875000\n",
            "Train Epoch: 33 [3200/7471 (43%)]\tLoss: 1616234.625000\n",
            "Train Epoch: 33 [3360/7471 (45%)]\tLoss: 1578816.750000\n",
            "Train Epoch: 33 [3520/7471 (47%)]\tLoss: 1550154.250000\n",
            "Train Epoch: 33 [3680/7471 (49%)]\tLoss: 1641933.375000\n",
            "Train Epoch: 33 [3840/7471 (51%)]\tLoss: 1573523.375000\n",
            "Train Epoch: 33 [4000/7471 (54%)]\tLoss: 1597623.500000\n",
            "Train Epoch: 33 [4160/7471 (56%)]\tLoss: 1553280.875000\n",
            "Train Epoch: 33 [4320/7471 (58%)]\tLoss: 1622427.375000\n",
            "Train Epoch: 33 [4480/7471 (60%)]\tLoss: 1521105.375000\n",
            "Train Epoch: 33 [4640/7471 (62%)]\tLoss: 1623031.875000\n",
            "Train Epoch: 33 [4800/7471 (64%)]\tLoss: 1627304.750000\n",
            "Train Epoch: 33 [4960/7471 (66%)]\tLoss: 1564096.750000\n",
            "Train Epoch: 33 [5120/7471 (69%)]\tLoss: 1562700.250000\n",
            "Train Epoch: 33 [5280/7471 (71%)]\tLoss: 1572657.750000\n",
            "Train Epoch: 33 [5440/7471 (73%)]\tLoss: 1563743.375000\n",
            "Train Epoch: 33 [5600/7471 (75%)]\tLoss: 1620823.500000\n",
            "Train Epoch: 33 [5760/7471 (77%)]\tLoss: 1590950.875000\n",
            "Train Epoch: 33 [5920/7471 (79%)]\tLoss: 1620925.750000\n",
            "Train Epoch: 33 [6080/7471 (81%)]\tLoss: 1599591.000000\n",
            "Train Epoch: 33 [6240/7471 (84%)]\tLoss: 1611338.125000\n",
            "Train Epoch: 33 [6400/7471 (86%)]\tLoss: 1538228.500000\n",
            "Train Epoch: 33 [6560/7471 (88%)]\tLoss: 1613924.000000\n",
            "Train Epoch: 33 [6720/7471 (90%)]\tLoss: 1591221.875000\n",
            "Train Epoch: 33 [6880/7471 (92%)]\tLoss: 1578477.500000\n",
            "Train Epoch: 33 [7040/7471 (94%)]\tLoss: 1560292.375000\n",
            "Train Epoch: 33 [7200/7471 (96%)]\tLoss: 1617221.250000\n",
            "Train Epoch: 33 [7360/7471 (99%)]\tLoss: 1592354.250000\n",
            "Epoch 33 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 34 [160/7471 (2%)]\tLoss: 1610135.250000\n",
            "Train Epoch: 34 [320/7471 (4%)]\tLoss: 1587111.250000\n",
            "Train Epoch: 34 [480/7471 (6%)]\tLoss: 1583014.000000\n",
            "Train Epoch: 34 [640/7471 (9%)]\tLoss: 1561543.750000\n",
            "Train Epoch: 34 [800/7471 (11%)]\tLoss: 1587553.750000\n",
            "Train Epoch: 34 [960/7471 (13%)]\tLoss: 1625151.625000\n",
            "Train Epoch: 34 [1120/7471 (15%)]\tLoss: 1570262.625000\n",
            "Train Epoch: 34 [1280/7471 (17%)]\tLoss: 1584233.875000\n",
            "Train Epoch: 34 [1440/7471 (19%)]\tLoss: 1588959.125000\n",
            "Train Epoch: 34 [1600/7471 (21%)]\tLoss: 1590838.875000\n",
            "Train Epoch: 34 [1760/7471 (24%)]\tLoss: 1564437.875000\n",
            "Train Epoch: 34 [1920/7471 (26%)]\tLoss: 1620543.625000\n",
            "Train Epoch: 34 [2080/7471 (28%)]\tLoss: 1582874.875000\n",
            "Train Epoch: 34 [2240/7471 (30%)]\tLoss: 1603756.125000\n",
            "Train Epoch: 34 [2400/7471 (32%)]\tLoss: 1633785.875000\n",
            "Train Epoch: 34 [2560/7471 (34%)]\tLoss: 1635469.125000\n",
            "Train Epoch: 34 [2720/7471 (36%)]\tLoss: 1578507.750000\n",
            "Train Epoch: 34 [2880/7471 (39%)]\tLoss: 1527988.000000\n",
            "Train Epoch: 34 [3040/7471 (41%)]\tLoss: 1633098.375000\n",
            "Train Epoch: 34 [3200/7471 (43%)]\tLoss: 1609212.375000\n",
            "Train Epoch: 34 [3360/7471 (45%)]\tLoss: 1583686.000000\n",
            "Train Epoch: 34 [3520/7471 (47%)]\tLoss: 1552323.500000\n",
            "Train Epoch: 34 [3680/7471 (49%)]\tLoss: 1567115.500000\n",
            "Train Epoch: 34 [3840/7471 (51%)]\tLoss: 1619882.875000\n",
            "Train Epoch: 34 [4000/7471 (54%)]\tLoss: 1544029.250000\n",
            "Train Epoch: 34 [4160/7471 (56%)]\tLoss: 1628577.125000\n",
            "Train Epoch: 34 [4320/7471 (58%)]\tLoss: 1589134.375000\n",
            "Train Epoch: 34 [4480/7471 (60%)]\tLoss: 1622194.125000\n",
            "Train Epoch: 34 [4640/7471 (62%)]\tLoss: 1564861.750000\n",
            "Train Epoch: 34 [4800/7471 (64%)]\tLoss: 1554570.375000\n",
            "Train Epoch: 34 [4960/7471 (66%)]\tLoss: 1533003.875000\n",
            "Train Epoch: 34 [5120/7471 (69%)]\tLoss: 1577700.625000\n",
            "Train Epoch: 34 [5280/7471 (71%)]\tLoss: 1466727.000000\n",
            "Train Epoch: 34 [5440/7471 (73%)]\tLoss: 1564732.625000\n",
            "Train Epoch: 34 [5600/7471 (75%)]\tLoss: 1578875.375000\n",
            "Train Epoch: 34 [5760/7471 (77%)]\tLoss: 1599308.750000\n",
            "Train Epoch: 34 [5920/7471 (79%)]\tLoss: 1595599.625000\n",
            "Train Epoch: 34 [6080/7471 (81%)]\tLoss: 1603304.250000\n",
            "Train Epoch: 34 [6240/7471 (84%)]\tLoss: 1548152.625000\n",
            "Train Epoch: 34 [6400/7471 (86%)]\tLoss: 1614805.750000\n",
            "Train Epoch: 34 [6560/7471 (88%)]\tLoss: 1566336.250000\n",
            "Train Epoch: 34 [6720/7471 (90%)]\tLoss: 1536119.750000\n",
            "Train Epoch: 34 [6880/7471 (92%)]\tLoss: 1553986.500000\n",
            "Train Epoch: 34 [7040/7471 (94%)]\tLoss: 1591920.750000\n",
            "Train Epoch: 34 [7200/7471 (96%)]\tLoss: 1618896.500000\n",
            "Train Epoch: 34 [7360/7471 (99%)]\tLoss: 1595491.000000\n",
            "Epoch 34 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 35 [160/7471 (2%)]\tLoss: 1602654.000000\n",
            "Train Epoch: 35 [320/7471 (4%)]\tLoss: 1593845.875000\n",
            "Train Epoch: 35 [480/7471 (6%)]\tLoss: 1566622.875000\n",
            "Train Epoch: 35 [640/7471 (9%)]\tLoss: 1567717.000000\n",
            "Train Epoch: 35 [800/7471 (11%)]\tLoss: 1567530.875000\n",
            "Train Epoch: 35 [960/7471 (13%)]\tLoss: 1599161.625000\n",
            "Train Epoch: 35 [1120/7471 (15%)]\tLoss: 1542673.875000\n",
            "Train Epoch: 35 [1280/7471 (17%)]\tLoss: 1575246.875000\n",
            "Train Epoch: 35 [1440/7471 (19%)]\tLoss: 1572496.375000\n",
            "Train Epoch: 35 [1600/7471 (21%)]\tLoss: 1614162.000000\n",
            "Train Epoch: 35 [1760/7471 (24%)]\tLoss: 1567744.750000\n",
            "Train Epoch: 35 [1920/7471 (26%)]\tLoss: 1569052.250000\n",
            "Train Epoch: 35 [2080/7471 (28%)]\tLoss: 1572815.000000\n",
            "Train Epoch: 35 [2240/7471 (30%)]\tLoss: 1603309.000000\n",
            "Train Epoch: 35 [2400/7471 (32%)]\tLoss: 1550178.250000\n",
            "Train Epoch: 35 [2560/7471 (34%)]\tLoss: 1574753.250000\n",
            "Train Epoch: 35 [2720/7471 (36%)]\tLoss: 1589648.500000\n",
            "Train Epoch: 35 [2880/7471 (39%)]\tLoss: 1584663.500000\n",
            "Train Epoch: 35 [3040/7471 (41%)]\tLoss: 1621052.625000\n",
            "Train Epoch: 35 [3200/7471 (43%)]\tLoss: 1599831.750000\n",
            "Train Epoch: 35 [3360/7471 (45%)]\tLoss: 1612055.750000\n",
            "Train Epoch: 35 [3520/7471 (47%)]\tLoss: 1633655.375000\n",
            "Train Epoch: 35 [3680/7471 (49%)]\tLoss: 1559772.875000\n",
            "Train Epoch: 35 [3840/7471 (51%)]\tLoss: 1569829.125000\n",
            "Train Epoch: 35 [4000/7471 (54%)]\tLoss: 1554366.500000\n",
            "Train Epoch: 35 [4160/7471 (56%)]\tLoss: 1643785.250000\n",
            "Train Epoch: 35 [4320/7471 (58%)]\tLoss: 1549367.750000\n",
            "Train Epoch: 35 [4480/7471 (60%)]\tLoss: 1553232.875000\n",
            "Train Epoch: 35 [4640/7471 (62%)]\tLoss: 1572254.500000\n",
            "Train Epoch: 35 [4800/7471 (64%)]\tLoss: 1639961.625000\n",
            "Train Epoch: 35 [4960/7471 (66%)]\tLoss: 1561066.250000\n",
            "Train Epoch: 35 [5120/7471 (69%)]\tLoss: 1584760.875000\n",
            "Train Epoch: 35 [5280/7471 (71%)]\tLoss: 1500592.500000\n",
            "Train Epoch: 35 [5440/7471 (73%)]\tLoss: 1563905.750000\n",
            "Train Epoch: 35 [5600/7471 (75%)]\tLoss: 1563564.250000\n",
            "Train Epoch: 35 [5760/7471 (77%)]\tLoss: 1532179.125000\n",
            "Train Epoch: 35 [5920/7471 (79%)]\tLoss: 1586198.625000\n",
            "Train Epoch: 35 [6080/7471 (81%)]\tLoss: 1495896.750000\n",
            "Train Epoch: 35 [6240/7471 (84%)]\tLoss: 1536057.875000\n",
            "Train Epoch: 35 [6400/7471 (86%)]\tLoss: 1574138.000000\n",
            "Train Epoch: 35 [6560/7471 (88%)]\tLoss: 1542358.375000\n",
            "Train Epoch: 35 [6720/7471 (90%)]\tLoss: 1627076.375000\n",
            "Train Epoch: 35 [6880/7471 (92%)]\tLoss: 1558191.375000\n",
            "Train Epoch: 35 [7040/7471 (94%)]\tLoss: 1541981.500000\n",
            "Train Epoch: 35 [7200/7471 (96%)]\tLoss: 1606336.500000\n",
            "Train Epoch: 35 [7360/7471 (99%)]\tLoss: 1574970.000000\n",
            "Epoch 35 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 36 [160/7471 (2%)]\tLoss: 1591544.125000\n",
            "Train Epoch: 36 [320/7471 (4%)]\tLoss: 1573415.125000\n",
            "Train Epoch: 36 [480/7471 (6%)]\tLoss: 1608086.750000\n",
            "Train Epoch: 36 [640/7471 (9%)]\tLoss: 1579212.875000\n",
            "Train Epoch: 36 [800/7471 (11%)]\tLoss: 1568528.250000\n",
            "Train Epoch: 36 [960/7471 (13%)]\tLoss: 1607742.250000\n",
            "Train Epoch: 36 [1120/7471 (15%)]\tLoss: 1570543.375000\n",
            "Train Epoch: 36 [1280/7471 (17%)]\tLoss: 1530377.375000\n",
            "Train Epoch: 36 [1440/7471 (19%)]\tLoss: 1528669.750000\n",
            "Train Epoch: 36 [1600/7471 (21%)]\tLoss: 1595823.750000\n",
            "Train Epoch: 36 [1760/7471 (24%)]\tLoss: 1573791.750000\n",
            "Train Epoch: 36 [1920/7471 (26%)]\tLoss: 1570393.375000\n",
            "Train Epoch: 36 [2080/7471 (28%)]\tLoss: 1541139.250000\n",
            "Train Epoch: 36 [2240/7471 (30%)]\tLoss: 1573780.375000\n",
            "Train Epoch: 36 [2400/7471 (32%)]\tLoss: 1585793.625000\n",
            "Train Epoch: 36 [2560/7471 (34%)]\tLoss: 1529957.625000\n",
            "Train Epoch: 36 [2720/7471 (36%)]\tLoss: 1586210.875000\n",
            "Train Epoch: 36 [2880/7471 (39%)]\tLoss: 1529923.750000\n",
            "Train Epoch: 36 [3040/7471 (41%)]\tLoss: 1618334.500000\n",
            "Train Epoch: 36 [3200/7471 (43%)]\tLoss: 1561322.125000\n",
            "Train Epoch: 36 [3360/7471 (45%)]\tLoss: 1609900.625000\n",
            "Train Epoch: 36 [3520/7471 (47%)]\tLoss: 1586762.875000\n",
            "Train Epoch: 36 [3680/7471 (49%)]\tLoss: 1554119.125000\n",
            "Train Epoch: 36 [3840/7471 (51%)]\tLoss: 1531047.875000\n",
            "Train Epoch: 36 [4000/7471 (54%)]\tLoss: 1558265.500000\n",
            "Train Epoch: 36 [4160/7471 (56%)]\tLoss: 1645918.750000\n",
            "Train Epoch: 36 [4320/7471 (58%)]\tLoss: 1581476.750000\n",
            "Train Epoch: 36 [4480/7471 (60%)]\tLoss: 1536492.375000\n",
            "Train Epoch: 36 [4640/7471 (62%)]\tLoss: 1573340.625000\n",
            "Train Epoch: 36 [4800/7471 (64%)]\tLoss: 1622582.625000\n",
            "Train Epoch: 36 [4960/7471 (66%)]\tLoss: 1577932.750000\n",
            "Train Epoch: 36 [5120/7471 (69%)]\tLoss: 1617450.625000\n",
            "Train Epoch: 36 [5280/7471 (71%)]\tLoss: 1593010.375000\n",
            "Train Epoch: 36 [5440/7471 (73%)]\tLoss: 1572910.375000\n",
            "Train Epoch: 36 [5600/7471 (75%)]\tLoss: 1575023.250000\n",
            "Train Epoch: 36 [5760/7471 (77%)]\tLoss: 1561354.500000\n",
            "Train Epoch: 36 [5920/7471 (79%)]\tLoss: 1568918.500000\n",
            "Train Epoch: 36 [6080/7471 (81%)]\tLoss: 1595858.500000\n",
            "Train Epoch: 36 [6240/7471 (84%)]\tLoss: 1588432.750000\n",
            "Train Epoch: 36 [6400/7471 (86%)]\tLoss: 1584851.625000\n",
            "Train Epoch: 36 [6560/7471 (88%)]\tLoss: 1605520.250000\n",
            "Train Epoch: 36 [6720/7471 (90%)]\tLoss: 1563424.500000\n",
            "Train Epoch: 36 [6880/7471 (92%)]\tLoss: 1591328.875000\n",
            "Train Epoch: 36 [7040/7471 (94%)]\tLoss: 1587063.750000\n",
            "Train Epoch: 36 [7200/7471 (96%)]\tLoss: 1570373.000000\n",
            "Train Epoch: 36 [7360/7471 (99%)]\tLoss: 1588442.750000\n",
            "Epoch 36 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 37 [160/7471 (2%)]\tLoss: 1577080.250000\n",
            "Train Epoch: 37 [320/7471 (4%)]\tLoss: 1635886.500000\n",
            "Train Epoch: 37 [480/7471 (6%)]\tLoss: 1597608.875000\n",
            "Train Epoch: 37 [640/7471 (9%)]\tLoss: 1567954.375000\n",
            "Train Epoch: 37 [800/7471 (11%)]\tLoss: 1612215.125000\n",
            "Train Epoch: 37 [960/7471 (13%)]\tLoss: 1556079.625000\n",
            "Train Epoch: 37 [1120/7471 (15%)]\tLoss: 1634680.875000\n",
            "Train Epoch: 37 [1280/7471 (17%)]\tLoss: 1544069.750000\n",
            "Train Epoch: 37 [1440/7471 (19%)]\tLoss: 1612438.250000\n",
            "Train Epoch: 37 [1600/7471 (21%)]\tLoss: 1588642.875000\n",
            "Train Epoch: 37 [1760/7471 (24%)]\tLoss: 1558381.125000\n",
            "Train Epoch: 37 [1920/7471 (26%)]\tLoss: 1513681.125000\n",
            "Train Epoch: 37 [2080/7471 (28%)]\tLoss: 1593600.125000\n",
            "Train Epoch: 37 [2240/7471 (30%)]\tLoss: 1538439.000000\n",
            "Train Epoch: 37 [2400/7471 (32%)]\tLoss: 1600685.750000\n",
            "Train Epoch: 37 [2560/7471 (34%)]\tLoss: 1631997.875000\n",
            "Train Epoch: 37 [2720/7471 (36%)]\tLoss: 1541540.625000\n",
            "Train Epoch: 37 [2880/7471 (39%)]\tLoss: 1584719.875000\n",
            "Train Epoch: 37 [3040/7471 (41%)]\tLoss: 1621221.375000\n",
            "Train Epoch: 37 [3200/7471 (43%)]\tLoss: 1564343.000000\n",
            "Train Epoch: 37 [3360/7471 (45%)]\tLoss: 1611506.000000\n",
            "Train Epoch: 37 [3520/7471 (47%)]\tLoss: 1553181.875000\n",
            "Train Epoch: 37 [3680/7471 (49%)]\tLoss: 1636249.625000\n",
            "Train Epoch: 37 [3840/7471 (51%)]\tLoss: 1510271.125000\n",
            "Train Epoch: 37 [4000/7471 (54%)]\tLoss: 1621209.375000\n",
            "Train Epoch: 37 [4160/7471 (56%)]\tLoss: 1595234.375000\n",
            "Train Epoch: 37 [4320/7471 (58%)]\tLoss: 1546042.750000\n",
            "Train Epoch: 37 [4480/7471 (60%)]\tLoss: 1494812.375000\n",
            "Train Epoch: 37 [4640/7471 (62%)]\tLoss: 1632224.000000\n",
            "Train Epoch: 37 [4800/7471 (64%)]\tLoss: 1554654.000000\n",
            "Train Epoch: 37 [4960/7471 (66%)]\tLoss: 1568586.250000\n",
            "Train Epoch: 37 [5120/7471 (69%)]\tLoss: 1649292.000000\n",
            "Train Epoch: 37 [5280/7471 (71%)]\tLoss: 1629887.125000\n",
            "Train Epoch: 37 [5440/7471 (73%)]\tLoss: 1585930.250000\n",
            "Train Epoch: 37 [5600/7471 (75%)]\tLoss: 1541343.000000\n",
            "Train Epoch: 37 [5760/7471 (77%)]\tLoss: 1596631.500000\n",
            "Train Epoch: 37 [5920/7471 (79%)]\tLoss: 1571838.625000\n",
            "Train Epoch: 37 [6080/7471 (81%)]\tLoss: 1599777.750000\n",
            "Train Epoch: 37 [6240/7471 (84%)]\tLoss: 1565314.375000\n",
            "Train Epoch: 37 [6400/7471 (86%)]\tLoss: 1572580.375000\n",
            "Train Epoch: 37 [6560/7471 (88%)]\tLoss: 1576414.750000\n",
            "Train Epoch: 37 [6720/7471 (90%)]\tLoss: 1604391.625000\n",
            "Train Epoch: 37 [6880/7471 (92%)]\tLoss: 1577205.000000\n",
            "Train Epoch: 37 [7040/7471 (94%)]\tLoss: 1575501.000000\n",
            "Train Epoch: 37 [7200/7471 (96%)]\tLoss: 1587665.375000\n",
            "Train Epoch: 37 [7360/7471 (99%)]\tLoss: 1581613.250000\n",
            "Epoch 37 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 38 [160/7471 (2%)]\tLoss: 1487404.500000\n",
            "Train Epoch: 38 [320/7471 (4%)]\tLoss: 1607705.625000\n",
            "Train Epoch: 38 [480/7471 (6%)]\tLoss: 1619911.375000\n",
            "Train Epoch: 38 [640/7471 (9%)]\tLoss: 1623461.500000\n",
            "Train Epoch: 38 [800/7471 (11%)]\tLoss: 1559177.250000\n",
            "Train Epoch: 38 [960/7471 (13%)]\tLoss: 1574582.000000\n",
            "Train Epoch: 38 [1120/7471 (15%)]\tLoss: 1557700.500000\n",
            "Train Epoch: 38 [1280/7471 (17%)]\tLoss: 1621687.750000\n",
            "Train Epoch: 38 [1440/7471 (19%)]\tLoss: 1568685.500000\n",
            "Train Epoch: 38 [1600/7471 (21%)]\tLoss: 1537031.000000\n",
            "Train Epoch: 38 [1760/7471 (24%)]\tLoss: 1552193.250000\n",
            "Train Epoch: 38 [1920/7471 (26%)]\tLoss: 1516815.750000\n",
            "Train Epoch: 38 [2080/7471 (28%)]\tLoss: 1503304.750000\n",
            "Train Epoch: 38 [2240/7471 (30%)]\tLoss: 1540356.625000\n",
            "Train Epoch: 38 [2400/7471 (32%)]\tLoss: 1588452.875000\n",
            "Train Epoch: 38 [2560/7471 (34%)]\tLoss: 1600320.750000\n",
            "Train Epoch: 38 [2720/7471 (36%)]\tLoss: 1492796.250000\n",
            "Train Epoch: 38 [2880/7471 (39%)]\tLoss: 1594607.500000\n",
            "Train Epoch: 38 [3040/7471 (41%)]\tLoss: 1623568.375000\n",
            "Train Epoch: 38 [3200/7471 (43%)]\tLoss: 1550621.625000\n",
            "Train Epoch: 38 [3360/7471 (45%)]\tLoss: 1468897.500000\n",
            "Train Epoch: 38 [3520/7471 (47%)]\tLoss: 1556236.500000\n",
            "Train Epoch: 38 [3680/7471 (49%)]\tLoss: 1591226.375000\n",
            "Train Epoch: 38 [3840/7471 (51%)]\tLoss: 1592912.375000\n",
            "Train Epoch: 38 [4000/7471 (54%)]\tLoss: 1545655.750000\n",
            "Train Epoch: 38 [4160/7471 (56%)]\tLoss: 1536377.750000\n",
            "Train Epoch: 38 [4320/7471 (58%)]\tLoss: 1612977.125000\n",
            "Train Epoch: 38 [4480/7471 (60%)]\tLoss: 1616597.875000\n",
            "Train Epoch: 38 [4640/7471 (62%)]\tLoss: 1544913.750000\n",
            "Train Epoch: 38 [4800/7471 (64%)]\tLoss: 1602027.000000\n",
            "Train Epoch: 38 [4960/7471 (66%)]\tLoss: 1609984.625000\n",
            "Train Epoch: 38 [5120/7471 (69%)]\tLoss: 1627886.875000\n",
            "Train Epoch: 38 [5280/7471 (71%)]\tLoss: 1534940.500000\n",
            "Train Epoch: 38 [5440/7471 (73%)]\tLoss: 1589668.875000\n",
            "Train Epoch: 38 [5600/7471 (75%)]\tLoss: 1532972.000000\n",
            "Train Epoch: 38 [5760/7471 (77%)]\tLoss: 1603078.000000\n",
            "Train Epoch: 38 [5920/7471 (79%)]\tLoss: 1615971.875000\n",
            "Train Epoch: 38 [6080/7471 (81%)]\tLoss: 1542239.750000\n",
            "Train Epoch: 38 [6240/7471 (84%)]\tLoss: 1555319.000000\n",
            "Train Epoch: 38 [6400/7471 (86%)]\tLoss: 1577536.500000\n",
            "Train Epoch: 38 [6560/7471 (88%)]\tLoss: 1604092.625000\n",
            "Train Epoch: 38 [6720/7471 (90%)]\tLoss: 1551200.750000\n",
            "Train Epoch: 38 [6880/7471 (92%)]\tLoss: 1624101.125000\n",
            "Train Epoch: 38 [7040/7471 (94%)]\tLoss: 1620891.750000\n",
            "Train Epoch: 38 [7200/7471 (96%)]\tLoss: 1647330.625000\n",
            "Train Epoch: 38 [7360/7471 (99%)]\tLoss: 1643371.500000\n",
            "Epoch 38 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 39 [160/7471 (2%)]\tLoss: 1584199.875000\n",
            "Train Epoch: 39 [320/7471 (4%)]\tLoss: 1632724.625000\n",
            "Train Epoch: 39 [480/7471 (6%)]\tLoss: 1575262.875000\n",
            "Train Epoch: 39 [640/7471 (9%)]\tLoss: 1615697.875000\n",
            "Train Epoch: 39 [800/7471 (11%)]\tLoss: 1584572.750000\n",
            "Train Epoch: 39 [960/7471 (13%)]\tLoss: 1606917.500000\n",
            "Train Epoch: 39 [1120/7471 (15%)]\tLoss: 1600058.000000\n",
            "Train Epoch: 39 [1280/7471 (17%)]\tLoss: 1595191.375000\n",
            "Train Epoch: 39 [1440/7471 (19%)]\tLoss: 1598148.875000\n",
            "Train Epoch: 39 [1600/7471 (21%)]\tLoss: 1581320.500000\n",
            "Train Epoch: 39 [1760/7471 (24%)]\tLoss: 1642636.000000\n",
            "Train Epoch: 39 [1920/7471 (26%)]\tLoss: 1541178.000000\n",
            "Train Epoch: 39 [2080/7471 (28%)]\tLoss: 1614974.000000\n",
            "Train Epoch: 39 [2240/7471 (30%)]\tLoss: 1570415.625000\n",
            "Train Epoch: 39 [2400/7471 (32%)]\tLoss: 1626857.250000\n",
            "Train Epoch: 39 [2560/7471 (34%)]\tLoss: 1635724.625000\n",
            "Train Epoch: 39 [2720/7471 (36%)]\tLoss: 1609058.750000\n",
            "Train Epoch: 39 [2880/7471 (39%)]\tLoss: 1552864.875000\n",
            "Train Epoch: 39 [3040/7471 (41%)]\tLoss: 1620386.250000\n",
            "Train Epoch: 39 [3200/7471 (43%)]\tLoss: 1547463.000000\n",
            "Train Epoch: 39 [3360/7471 (45%)]\tLoss: 1517724.000000\n",
            "Train Epoch: 39 [3520/7471 (47%)]\tLoss: 1577359.250000\n",
            "Train Epoch: 39 [3680/7471 (49%)]\tLoss: 1556665.000000\n",
            "Train Epoch: 39 [3840/7471 (51%)]\tLoss: 1627121.500000\n",
            "Train Epoch: 39 [4000/7471 (54%)]\tLoss: 1620301.125000\n",
            "Train Epoch: 39 [4160/7471 (56%)]\tLoss: 1525976.625000\n",
            "Train Epoch: 39 [4320/7471 (58%)]\tLoss: 1577686.500000\n",
            "Train Epoch: 39 [4480/7471 (60%)]\tLoss: 1590472.750000\n",
            "Train Epoch: 39 [4640/7471 (62%)]\tLoss: 1571694.125000\n",
            "Train Epoch: 39 [4800/7471 (64%)]\tLoss: 1618903.500000\n",
            "Train Epoch: 39 [4960/7471 (66%)]\tLoss: 1594700.875000\n",
            "Train Epoch: 39 [5120/7471 (69%)]\tLoss: 1555693.625000\n",
            "Train Epoch: 39 [5280/7471 (71%)]\tLoss: 1570264.500000\n",
            "Train Epoch: 39 [5440/7471 (73%)]\tLoss: 1594777.750000\n",
            "Train Epoch: 39 [5600/7471 (75%)]\tLoss: 1574520.375000\n",
            "Train Epoch: 39 [5760/7471 (77%)]\tLoss: 1617313.125000\n",
            "Train Epoch: 39 [5920/7471 (79%)]\tLoss: 1599566.000000\n",
            "Train Epoch: 39 [6080/7471 (81%)]\tLoss: 1538314.875000\n",
            "Train Epoch: 39 [6240/7471 (84%)]\tLoss: 1571964.000000\n",
            "Train Epoch: 39 [6400/7471 (86%)]\tLoss: 1549481.375000\n",
            "Train Epoch: 39 [6560/7471 (88%)]\tLoss: 1618550.750000\n",
            "Train Epoch: 39 [6720/7471 (90%)]\tLoss: 1522901.875000\n",
            "Train Epoch: 39 [6880/7471 (92%)]\tLoss: 1580120.875000\n",
            "Train Epoch: 39 [7040/7471 (94%)]\tLoss: 1578921.375000\n",
            "Train Epoch: 39 [7200/7471 (96%)]\tLoss: 1575726.125000\n",
            "Train Epoch: 39 [7360/7471 (99%)]\tLoss: 1547848.250000\n",
            "Epoch 39 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 40 [160/7471 (2%)]\tLoss: 1515913.000000\n",
            "Train Epoch: 40 [320/7471 (4%)]\tLoss: 1609100.500000\n",
            "Train Epoch: 40 [480/7471 (6%)]\tLoss: 1527935.875000\n",
            "Train Epoch: 40 [640/7471 (9%)]\tLoss: 1591983.750000\n",
            "Train Epoch: 40 [800/7471 (11%)]\tLoss: 1574587.500000\n",
            "Train Epoch: 40 [960/7471 (13%)]\tLoss: 1535299.500000\n",
            "Train Epoch: 40 [1120/7471 (15%)]\tLoss: 1594549.375000\n",
            "Train Epoch: 40 [1280/7471 (17%)]\tLoss: 1647330.750000\n",
            "Train Epoch: 40 [1440/7471 (19%)]\tLoss: 1584410.125000\n",
            "Train Epoch: 40 [1600/7471 (21%)]\tLoss: 1615363.500000\n",
            "Train Epoch: 40 [1760/7471 (24%)]\tLoss: 1514393.625000\n",
            "Train Epoch: 40 [1920/7471 (26%)]\tLoss: 1515106.625000\n",
            "Train Epoch: 40 [2080/7471 (28%)]\tLoss: 1577202.375000\n",
            "Train Epoch: 40 [2240/7471 (30%)]\tLoss: 1551203.750000\n",
            "Train Epoch: 40 [2400/7471 (32%)]\tLoss: 1581491.250000\n",
            "Train Epoch: 40 [2560/7471 (34%)]\tLoss: 1621069.625000\n",
            "Train Epoch: 40 [2720/7471 (36%)]\tLoss: 1633496.000000\n",
            "Train Epoch: 40 [2880/7471 (39%)]\tLoss: 1576860.375000\n",
            "Train Epoch: 40 [3040/7471 (41%)]\tLoss: 1591822.000000\n",
            "Train Epoch: 40 [3200/7471 (43%)]\tLoss: 1522671.875000\n",
            "Train Epoch: 40 [3360/7471 (45%)]\tLoss: 1525350.250000\n",
            "Train Epoch: 40 [3520/7471 (47%)]\tLoss: 1531830.000000\n",
            "Train Epoch: 40 [3680/7471 (49%)]\tLoss: 1519172.750000\n",
            "Train Epoch: 40 [3840/7471 (51%)]\tLoss: 1568838.125000\n",
            "Train Epoch: 40 [4000/7471 (54%)]\tLoss: 1557292.750000\n",
            "Train Epoch: 40 [4160/7471 (56%)]\tLoss: 1622466.375000\n",
            "Train Epoch: 40 [4320/7471 (58%)]\tLoss: 1539611.500000\n",
            "Train Epoch: 40 [4480/7471 (60%)]\tLoss: 1602999.125000\n",
            "Train Epoch: 40 [4640/7471 (62%)]\tLoss: 1562748.250000\n",
            "Train Epoch: 40 [4800/7471 (64%)]\tLoss: 1597124.000000\n",
            "Train Epoch: 40 [4960/7471 (66%)]\tLoss: 1583933.375000\n",
            "Train Epoch: 40 [5120/7471 (69%)]\tLoss: 1584622.625000\n",
            "Train Epoch: 40 [5280/7471 (71%)]\tLoss: 1571642.000000\n",
            "Train Epoch: 40 [5440/7471 (73%)]\tLoss: 1580969.250000\n",
            "Train Epoch: 40 [5600/7471 (75%)]\tLoss: 1591375.375000\n",
            "Train Epoch: 40 [5760/7471 (77%)]\tLoss: 1587092.875000\n",
            "Train Epoch: 40 [5920/7471 (79%)]\tLoss: 1612994.375000\n",
            "Train Epoch: 40 [6080/7471 (81%)]\tLoss: 1598726.125000\n",
            "Train Epoch: 40 [6240/7471 (84%)]\tLoss: 1584525.000000\n",
            "Train Epoch: 40 [6400/7471 (86%)]\tLoss: 1572903.875000\n",
            "Train Epoch: 40 [6560/7471 (88%)]\tLoss: 1586291.375000\n",
            "Train Epoch: 40 [6720/7471 (90%)]\tLoss: 1586268.750000\n",
            "Train Epoch: 40 [6880/7471 (92%)]\tLoss: 1594089.125000\n",
            "Train Epoch: 40 [7040/7471 (94%)]\tLoss: 1605068.875000\n",
            "Train Epoch: 40 [7200/7471 (96%)]\tLoss: 1600151.750000\n",
            "Train Epoch: 40 [7360/7471 (99%)]\tLoss: 1592719.250000\n",
            "Epoch 40 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 41 [160/7471 (2%)]\tLoss: 1596354.000000\n",
            "Train Epoch: 41 [320/7471 (4%)]\tLoss: 1573093.625000\n",
            "Train Epoch: 41 [480/7471 (6%)]\tLoss: 1550367.750000\n",
            "Train Epoch: 41 [640/7471 (9%)]\tLoss: 1603727.750000\n",
            "Train Epoch: 41 [800/7471 (11%)]\tLoss: 1597140.000000\n",
            "Train Epoch: 41 [960/7471 (13%)]\tLoss: 1561171.875000\n",
            "Train Epoch: 41 [1120/7471 (15%)]\tLoss: 1617321.375000\n",
            "Train Epoch: 41 [1280/7471 (17%)]\tLoss: 1557603.250000\n",
            "Train Epoch: 41 [1440/7471 (19%)]\tLoss: 1536359.875000\n",
            "Train Epoch: 41 [1600/7471 (21%)]\tLoss: 1535952.875000\n",
            "Train Epoch: 41 [1760/7471 (24%)]\tLoss: 1607643.125000\n",
            "Train Epoch: 41 [1920/7471 (26%)]\tLoss: 1628559.750000\n",
            "Train Epoch: 41 [2080/7471 (28%)]\tLoss: 1625188.625000\n",
            "Train Epoch: 41 [2240/7471 (30%)]\tLoss: 1510601.875000\n",
            "Train Epoch: 41 [2400/7471 (32%)]\tLoss: 1577260.750000\n",
            "Train Epoch: 41 [2560/7471 (34%)]\tLoss: 1616540.375000\n",
            "Train Epoch: 41 [2720/7471 (36%)]\tLoss: 1527543.125000\n",
            "Train Epoch: 41 [2880/7471 (39%)]\tLoss: 1557345.875000\n",
            "Train Epoch: 41 [3040/7471 (41%)]\tLoss: 1586392.375000\n",
            "Train Epoch: 41 [3200/7471 (43%)]\tLoss: 1514268.000000\n",
            "Train Epoch: 41 [3360/7471 (45%)]\tLoss: 1571158.875000\n",
            "Train Epoch: 41 [3520/7471 (47%)]\tLoss: 1585434.250000\n",
            "Train Epoch: 41 [3680/7471 (49%)]\tLoss: 1616875.750000\n",
            "Train Epoch: 41 [3840/7471 (51%)]\tLoss: 1556308.375000\n",
            "Train Epoch: 41 [4000/7471 (54%)]\tLoss: 1487462.875000\n",
            "Train Epoch: 41 [4160/7471 (56%)]\tLoss: 1627140.250000\n",
            "Train Epoch: 41 [4320/7471 (58%)]\tLoss: 1602108.875000\n",
            "Train Epoch: 41 [4480/7471 (60%)]\tLoss: 1597927.250000\n",
            "Train Epoch: 41 [4640/7471 (62%)]\tLoss: 1545253.625000\n",
            "Train Epoch: 41 [4800/7471 (64%)]\tLoss: 1545019.875000\n",
            "Train Epoch: 41 [4960/7471 (66%)]\tLoss: 1555417.000000\n",
            "Train Epoch: 41 [5120/7471 (69%)]\tLoss: 1578382.000000\n",
            "Train Epoch: 41 [5280/7471 (71%)]\tLoss: 1633052.500000\n",
            "Train Epoch: 41 [5440/7471 (73%)]\tLoss: 1594752.125000\n",
            "Train Epoch: 41 [5600/7471 (75%)]\tLoss: 1579062.375000\n",
            "Train Epoch: 41 [5760/7471 (77%)]\tLoss: 1538280.500000\n",
            "Train Epoch: 41 [5920/7471 (79%)]\tLoss: 1622441.500000\n",
            "Train Epoch: 41 [6080/7471 (81%)]\tLoss: 1637004.125000\n",
            "Train Epoch: 41 [6240/7471 (84%)]\tLoss: 1579051.500000\n",
            "Train Epoch: 41 [6400/7471 (86%)]\tLoss: 1581515.500000\n",
            "Train Epoch: 41 [6560/7471 (88%)]\tLoss: 1599097.000000\n",
            "Train Epoch: 41 [6720/7471 (90%)]\tLoss: 1592709.125000\n",
            "Train Epoch: 41 [6880/7471 (92%)]\tLoss: 1506493.875000\n",
            "Train Epoch: 41 [7040/7471 (94%)]\tLoss: 1601089.000000\n",
            "Train Epoch: 41 [7200/7471 (96%)]\tLoss: 1575978.500000\n",
            "Train Epoch: 41 [7360/7471 (99%)]\tLoss: 1576498.125000\n",
            "Epoch 41 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: inf\n",
            "\n",
            "Train Epoch: 42 [160/7471 (2%)]\tLoss: 1579228.500000\n",
            "Train Epoch: 42 [320/7471 (4%)]\tLoss: 1536213.250000\n",
            "Train Epoch: 42 [480/7471 (6%)]\tLoss: 1575887.125000\n",
            "Train Epoch: 42 [640/7471 (9%)]\tLoss: 1630641.625000\n",
            "Train Epoch: 42 [800/7471 (11%)]\tLoss: 1621255.000000\n",
            "Train Epoch: 42 [960/7471 (13%)]\tLoss: 1534528.750000\n",
            "Train Epoch: 42 [1120/7471 (15%)]\tLoss: 1558416.500000\n",
            "Train Epoch: 42 [1280/7471 (17%)]\tLoss: 1579809.625000\n",
            "Train Epoch: 42 [1440/7471 (19%)]\tLoss: 1622578.250000\n",
            "Train Epoch: 42 [1600/7471 (21%)]\tLoss: 1566249.000000\n",
            "Train Epoch: 42 [1760/7471 (24%)]\tLoss: 1552914.375000\n",
            "Train Epoch: 42 [1920/7471 (26%)]\tLoss: 1606757.875000\n",
            "Train Epoch: 42 [2080/7471 (28%)]\tLoss: 1548058.125000\n",
            "Train Epoch: 42 [2240/7471 (30%)]\tLoss: 1611313.250000\n",
            "Train Epoch: 42 [2400/7471 (32%)]\tLoss: 1620138.250000\n",
            "Train Epoch: 42 [2560/7471 (34%)]\tLoss: 1610334.000000\n",
            "Train Epoch: 42 [2720/7471 (36%)]\tLoss: 1563979.250000\n",
            "Train Epoch: 42 [2880/7471 (39%)]\tLoss: 1587410.750000\n",
            "Train Epoch: 42 [3040/7471 (41%)]\tLoss: 1619577.375000\n",
            "Train Epoch: 42 [3200/7471 (43%)]\tLoss: 1588123.625000\n",
            "Train Epoch: 42 [3360/7471 (45%)]\tLoss: 1597382.250000\n",
            "Train Epoch: 42 [3520/7471 (47%)]\tLoss: 1612066.250000\n",
            "Train Epoch: 42 [3680/7471 (49%)]\tLoss: 1629859.125000\n",
            "Train Epoch: 42 [3840/7471 (51%)]\tLoss: 1514374.875000\n",
            "Train Epoch: 42 [4000/7471 (54%)]\tLoss: 1629496.375000\n",
            "Train Epoch: 42 [4160/7471 (56%)]\tLoss: 1555819.000000\n",
            "Train Epoch: 42 [4320/7471 (58%)]\tLoss: 1578704.250000\n",
            "Train Epoch: 42 [4480/7471 (60%)]\tLoss: 1521884.750000\n",
            "Train Epoch: 42 [4640/7471 (62%)]\tLoss: 1582593.500000\n",
            "Train Epoch: 42 [4800/7471 (64%)]\tLoss: 1560536.500000\n",
            "Train Epoch: 42 [4960/7471 (66%)]\tLoss: 1621925.125000\n",
            "Train Epoch: 42 [5120/7471 (69%)]\tLoss: 1599476.250000\n",
            "Train Epoch: 42 [5280/7471 (71%)]\tLoss: 1585783.875000\n",
            "Train Epoch: 42 [5440/7471 (73%)]\tLoss: 1619947.250000\n",
            "Train Epoch: 42 [5600/7471 (75%)]\tLoss: 1585415.125000\n",
            "Train Epoch: 42 [5760/7471 (77%)]\tLoss: 1561478.000000\n",
            "Train Epoch: 42 [5920/7471 (79%)]\tLoss: 1592449.500000\n",
            "Train Epoch: 42 [6080/7471 (81%)]\tLoss: 1516540.250000\n",
            "Train Epoch: 42 [6240/7471 (84%)]\tLoss: 1655135.000000\n",
            "Train Epoch: 42 [6400/7471 (86%)]\tLoss: 1559312.250000\n",
            "Train Epoch: 42 [6560/7471 (88%)]\tLoss: 1602361.125000\n",
            "Train Epoch: 42 [6720/7471 (90%)]\tLoss: 1611253.875000\n",
            "Train Epoch: 42 [6880/7471 (92%)]\tLoss: 1582957.250000\n",
            "Train Epoch: 42 [7040/7471 (94%)]\tLoss: 1625370.125000\n",
            "Train Epoch: 42 [7200/7471 (96%)]\tLoss: 1545555.250000\n",
            "Train Epoch: 42 [7360/7471 (99%)]\tLoss: 1493300.500000\n",
            "Epoch 42 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 464997510447131136.0000\n",
            "\n",
            "Train Epoch: 43 [160/7471 (2%)]\tLoss: 1582178.000000\n",
            "Train Epoch: 43 [320/7471 (4%)]\tLoss: 1565687.625000\n",
            "Train Epoch: 43 [480/7471 (6%)]\tLoss: 1547876.875000\n",
            "Train Epoch: 43 [640/7471 (9%)]\tLoss: 1498752.500000\n",
            "Train Epoch: 43 [800/7471 (11%)]\tLoss: 1606051.500000\n",
            "Train Epoch: 43 [960/7471 (13%)]\tLoss: 1608957.500000\n",
            "Train Epoch: 43 [1120/7471 (15%)]\tLoss: 1581211.625000\n",
            "Train Epoch: 43 [1280/7471 (17%)]\tLoss: 1588525.375000\n",
            "Train Epoch: 43 [1440/7471 (19%)]\tLoss: 1631727.375000\n",
            "Train Epoch: 43 [1600/7471 (21%)]\tLoss: 1598245.750000\n",
            "Train Epoch: 43 [1760/7471 (24%)]\tLoss: 1588858.875000\n",
            "Train Epoch: 43 [1920/7471 (26%)]\tLoss: 1608645.625000\n",
            "Train Epoch: 43 [2080/7471 (28%)]\tLoss: 1627062.375000\n",
            "Train Epoch: 43 [2240/7471 (30%)]\tLoss: 1608986.625000\n",
            "Train Epoch: 43 [2400/7471 (32%)]\tLoss: 1585409.125000\n",
            "Train Epoch: 43 [2560/7471 (34%)]\tLoss: 1639731.625000\n",
            "Train Epoch: 43 [2720/7471 (36%)]\tLoss: 1536528.250000\n",
            "Train Epoch: 43 [2880/7471 (39%)]\tLoss: 1481327.000000\n",
            "Train Epoch: 43 [3040/7471 (41%)]\tLoss: 1581743.250000\n",
            "Train Epoch: 43 [3200/7471 (43%)]\tLoss: 1543671.750000\n",
            "Train Epoch: 43 [3360/7471 (45%)]\tLoss: 1565802.500000\n",
            "Train Epoch: 43 [3520/7471 (47%)]\tLoss: 1625828.750000\n",
            "Train Epoch: 43 [3680/7471 (49%)]\tLoss: 1554370.250000\n",
            "Train Epoch: 43 [3840/7471 (51%)]\tLoss: 1584598.250000\n",
            "Train Epoch: 43 [4000/7471 (54%)]\tLoss: 1613277.125000\n",
            "Train Epoch: 43 [4160/7471 (56%)]\tLoss: 1560223.375000\n",
            "Train Epoch: 43 [4320/7471 (58%)]\tLoss: 1630150.625000\n",
            "Train Epoch: 43 [4480/7471 (60%)]\tLoss: 1523328.000000\n",
            "Train Epoch: 43 [4640/7471 (62%)]\tLoss: 1517238.750000\n",
            "Train Epoch: 43 [4800/7471 (64%)]\tLoss: 1543541.000000\n",
            "Train Epoch: 43 [4960/7471 (66%)]\tLoss: 1620248.250000\n",
            "Train Epoch: 43 [5120/7471 (69%)]\tLoss: 1626207.500000\n",
            "Train Epoch: 43 [5280/7471 (71%)]\tLoss: 1519952.750000\n",
            "Train Epoch: 43 [5440/7471 (73%)]\tLoss: 1553866.625000\n",
            "Train Epoch: 43 [5600/7471 (75%)]\tLoss: 1568376.750000\n",
            "Train Epoch: 43 [5760/7471 (77%)]\tLoss: 1629952.625000\n",
            "Train Epoch: 43 [5920/7471 (79%)]\tLoss: 1598584.250000\n",
            "Train Epoch: 43 [6080/7471 (81%)]\tLoss: 1569554.250000\n",
            "Train Epoch: 43 [6240/7471 (84%)]\tLoss: 1476076.625000\n",
            "Train Epoch: 43 [6400/7471 (86%)]\tLoss: 1608547.625000\n",
            "Train Epoch: 43 [6560/7471 (88%)]\tLoss: 1567386.000000\n",
            "Train Epoch: 43 [6720/7471 (90%)]\tLoss: 1593423.625000\n",
            "Train Epoch: 43 [6880/7471 (92%)]\tLoss: 1605983.125000\n",
            "Train Epoch: 43 [7040/7471 (94%)]\tLoss: 1575765.750000\n",
            "Train Epoch: 43 [7200/7471 (96%)]\tLoss: 1552765.750000\n",
            "Train Epoch: 43 [7360/7471 (99%)]\tLoss: 1600835.500000\n",
            "Epoch 43 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98696.1006\n",
            "\n",
            "Train Epoch: 44 [160/7471 (2%)]\tLoss: 1641140.875000\n",
            "Train Epoch: 44 [320/7471 (4%)]\tLoss: 1601313.750000\n",
            "Train Epoch: 44 [480/7471 (6%)]\tLoss: 1580851.375000\n",
            "Train Epoch: 44 [640/7471 (9%)]\tLoss: 1568941.875000\n",
            "Train Epoch: 44 [800/7471 (11%)]\tLoss: 1626766.625000\n",
            "Train Epoch: 44 [960/7471 (13%)]\tLoss: 1599350.750000\n",
            "Train Epoch: 44 [1120/7471 (15%)]\tLoss: 1612609.875000\n",
            "Train Epoch: 44 [1280/7471 (17%)]\tLoss: 1615502.375000\n",
            "Train Epoch: 44 [1440/7471 (19%)]\tLoss: 1633796.750000\n",
            "Train Epoch: 44 [1600/7471 (21%)]\tLoss: 1612037.250000\n",
            "Train Epoch: 44 [1760/7471 (24%)]\tLoss: 1554842.875000\n",
            "Train Epoch: 44 [1920/7471 (26%)]\tLoss: 1600403.250000\n",
            "Train Epoch: 44 [2080/7471 (28%)]\tLoss: 1573406.875000\n",
            "Train Epoch: 44 [2240/7471 (30%)]\tLoss: 1570535.500000\n",
            "Train Epoch: 44 [2400/7471 (32%)]\tLoss: 1580743.625000\n",
            "Train Epoch: 44 [2560/7471 (34%)]\tLoss: 1563004.250000\n",
            "Train Epoch: 44 [2720/7471 (36%)]\tLoss: 1551479.750000\n",
            "Train Epoch: 44 [2880/7471 (39%)]\tLoss: 1528837.125000\n",
            "Train Epoch: 44 [3040/7471 (41%)]\tLoss: 1564517.875000\n",
            "Train Epoch: 44 [3200/7471 (43%)]\tLoss: 1518001.125000\n",
            "Train Epoch: 44 [3360/7471 (45%)]\tLoss: 1559354.875000\n",
            "Train Epoch: 44 [3520/7471 (47%)]\tLoss: 1617936.500000\n",
            "Train Epoch: 44 [3680/7471 (49%)]\tLoss: 1616289.625000\n",
            "Train Epoch: 44 [3840/7471 (51%)]\tLoss: 1554000.375000\n",
            "Train Epoch: 44 [4000/7471 (54%)]\tLoss: 1577969.625000\n",
            "Train Epoch: 44 [4160/7471 (56%)]\tLoss: 1584533.000000\n",
            "Train Epoch: 44 [4320/7471 (58%)]\tLoss: 1616898.250000\n",
            "Train Epoch: 44 [4480/7471 (60%)]\tLoss: 1498230.000000\n",
            "Train Epoch: 44 [4640/7471 (62%)]\tLoss: 1569943.750000\n",
            "Train Epoch: 44 [4800/7471 (64%)]\tLoss: 1583052.875000\n",
            "Train Epoch: 44 [4960/7471 (66%)]\tLoss: 1532858.625000\n",
            "Train Epoch: 44 [5120/7471 (69%)]\tLoss: 1606628.375000\n",
            "Train Epoch: 44 [5280/7471 (71%)]\tLoss: 1611994.000000\n",
            "Train Epoch: 44 [5440/7471 (73%)]\tLoss: 1535562.500000\n",
            "Train Epoch: 44 [5600/7471 (75%)]\tLoss: 1488003.250000\n",
            "Train Epoch: 44 [5760/7471 (77%)]\tLoss: 1586057.375000\n",
            "Train Epoch: 44 [5920/7471 (79%)]\tLoss: 1616736.500000\n",
            "Train Epoch: 44 [6080/7471 (81%)]\tLoss: 1599318.875000\n",
            "Train Epoch: 44 [6240/7471 (84%)]\tLoss: 1577753.375000\n",
            "Train Epoch: 44 [6400/7471 (86%)]\tLoss: 1585333.875000\n",
            "Train Epoch: 44 [6560/7471 (88%)]\tLoss: 1563551.500000\n",
            "Train Epoch: 44 [6720/7471 (90%)]\tLoss: 1621390.500000\n",
            "Train Epoch: 44 [6880/7471 (92%)]\tLoss: 1607486.250000\n",
            "Train Epoch: 44 [7040/7471 (94%)]\tLoss: 1585815.750000\n",
            "Train Epoch: 44 [7200/7471 (96%)]\tLoss: 1593638.375000\n",
            "Train Epoch: 44 [7360/7471 (99%)]\tLoss: 1608059.125000\n",
            "Epoch 44 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98670.2425\n",
            "\n",
            "Train Epoch: 45 [160/7471 (2%)]\tLoss: 1590721.500000\n",
            "Train Epoch: 45 [320/7471 (4%)]\tLoss: 1527302.125000\n",
            "Train Epoch: 45 [480/7471 (6%)]\tLoss: 1617389.375000\n",
            "Train Epoch: 45 [640/7471 (9%)]\tLoss: 1558698.875000\n",
            "Train Epoch: 45 [800/7471 (11%)]\tLoss: 1650449.625000\n",
            "Train Epoch: 45 [960/7471 (13%)]\tLoss: 1555403.500000\n",
            "Train Epoch: 45 [1120/7471 (15%)]\tLoss: 1614083.500000\n",
            "Train Epoch: 45 [1280/7471 (17%)]\tLoss: 1611478.125000\n",
            "Train Epoch: 45 [1440/7471 (19%)]\tLoss: 1506693.875000\n",
            "Train Epoch: 45 [1600/7471 (21%)]\tLoss: 1607278.375000\n",
            "Train Epoch: 45 [1760/7471 (24%)]\tLoss: 1563738.625000\n",
            "Train Epoch: 45 [1920/7471 (26%)]\tLoss: 1607334.250000\n",
            "Train Epoch: 45 [2080/7471 (28%)]\tLoss: 1591564.750000\n",
            "Train Epoch: 45 [2240/7471 (30%)]\tLoss: 1598045.250000\n",
            "Train Epoch: 45 [2400/7471 (32%)]\tLoss: 1595200.500000\n",
            "Train Epoch: 45 [2560/7471 (34%)]\tLoss: 1584240.750000\n",
            "Train Epoch: 45 [2720/7471 (36%)]\tLoss: 1605934.750000\n",
            "Train Epoch: 45 [2880/7471 (39%)]\tLoss: 1550187.000000\n",
            "Train Epoch: 45 [3040/7471 (41%)]\tLoss: 1593446.750000\n",
            "Train Epoch: 45 [3200/7471 (43%)]\tLoss: 1615400.625000\n",
            "Train Epoch: 45 [3360/7471 (45%)]\tLoss: 1572350.375000\n",
            "Train Epoch: 45 [3520/7471 (47%)]\tLoss: 1567616.625000\n",
            "Train Epoch: 45 [3680/7471 (49%)]\tLoss: 1528384.125000\n",
            "Train Epoch: 45 [3840/7471 (51%)]\tLoss: 1575123.625000\n",
            "Train Epoch: 45 [4000/7471 (54%)]\tLoss: 1593890.375000\n",
            "Train Epoch: 45 [4160/7471 (56%)]\tLoss: 1630023.125000\n",
            "Train Epoch: 45 [4320/7471 (58%)]\tLoss: 1475841.750000\n",
            "Train Epoch: 45 [4480/7471 (60%)]\tLoss: 1596393.125000\n",
            "Train Epoch: 45 [4640/7471 (62%)]\tLoss: 1625030.250000\n",
            "Train Epoch: 45 [4800/7471 (64%)]\tLoss: 1550602.875000\n",
            "Train Epoch: 45 [4960/7471 (66%)]\tLoss: 1559076.750000\n",
            "Train Epoch: 45 [5120/7471 (69%)]\tLoss: 1616080.250000\n",
            "Train Epoch: 45 [5280/7471 (71%)]\tLoss: 1569953.000000\n",
            "Train Epoch: 45 [5440/7471 (73%)]\tLoss: 1593911.500000\n",
            "Train Epoch: 45 [5600/7471 (75%)]\tLoss: 1502086.000000\n",
            "Train Epoch: 45 [5760/7471 (77%)]\tLoss: 1603405.375000\n",
            "Train Epoch: 45 [5920/7471 (79%)]\tLoss: 1526464.375000\n",
            "Train Epoch: 45 [6080/7471 (81%)]\tLoss: 1610375.500000\n",
            "Train Epoch: 45 [6240/7471 (84%)]\tLoss: 1613847.625000\n",
            "Train Epoch: 45 [6400/7471 (86%)]\tLoss: 1593108.250000\n",
            "Train Epoch: 45 [6560/7471 (88%)]\tLoss: 1624981.250000\n",
            "Train Epoch: 45 [6720/7471 (90%)]\tLoss: 1556312.625000\n",
            "Train Epoch: 45 [6880/7471 (92%)]\tLoss: 1587251.000000\n",
            "Train Epoch: 45 [7040/7471 (94%)]\tLoss: 1596697.750000\n",
            "Train Epoch: 45 [7200/7471 (96%)]\tLoss: 1592813.250000\n",
            "Train Epoch: 45 [7360/7471 (99%)]\tLoss: 1544927.000000\n",
            "Epoch 45 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98641.9925\n",
            "\n",
            "Train Epoch: 46 [160/7471 (2%)]\tLoss: 1576996.000000\n",
            "Train Epoch: 46 [320/7471 (4%)]\tLoss: 1600741.250000\n",
            "Train Epoch: 46 [480/7471 (6%)]\tLoss: 1648447.625000\n",
            "Train Epoch: 46 [640/7471 (9%)]\tLoss: 1620327.625000\n",
            "Train Epoch: 46 [800/7471 (11%)]\tLoss: 1592771.125000\n",
            "Train Epoch: 46 [960/7471 (13%)]\tLoss: 1541522.250000\n",
            "Train Epoch: 46 [1120/7471 (15%)]\tLoss: 1555896.000000\n",
            "Train Epoch: 46 [1280/7471 (17%)]\tLoss: 1596968.125000\n",
            "Train Epoch: 46 [1440/7471 (19%)]\tLoss: 1604418.375000\n",
            "Train Epoch: 46 [1600/7471 (21%)]\tLoss: 1551365.375000\n",
            "Train Epoch: 46 [1760/7471 (24%)]\tLoss: 1570521.875000\n",
            "Train Epoch: 46 [1920/7471 (26%)]\tLoss: 1567248.375000\n",
            "Train Epoch: 46 [2080/7471 (28%)]\tLoss: 1578973.000000\n",
            "Train Epoch: 46 [2240/7471 (30%)]\tLoss: 1612973.375000\n",
            "Train Epoch: 46 [2400/7471 (32%)]\tLoss: 1587004.375000\n",
            "Train Epoch: 46 [2560/7471 (34%)]\tLoss: 1563945.000000\n",
            "Train Epoch: 46 [2720/7471 (36%)]\tLoss: 1582721.500000\n",
            "Train Epoch: 46 [2880/7471 (39%)]\tLoss: 1587414.625000\n",
            "Train Epoch: 46 [3040/7471 (41%)]\tLoss: 1625098.750000\n",
            "Train Epoch: 46 [3200/7471 (43%)]\tLoss: 1536686.500000\n",
            "Train Epoch: 46 [3360/7471 (45%)]\tLoss: 1629588.875000\n",
            "Train Epoch: 46 [3520/7471 (47%)]\tLoss: 1563563.500000\n",
            "Train Epoch: 46 [3680/7471 (49%)]\tLoss: 1597204.375000\n",
            "Train Epoch: 46 [3840/7471 (51%)]\tLoss: 1490971.625000\n",
            "Train Epoch: 46 [4000/7471 (54%)]\tLoss: 1552796.500000\n",
            "Train Epoch: 46 [4160/7471 (56%)]\tLoss: 1589037.750000\n",
            "Train Epoch: 46 [4320/7471 (58%)]\tLoss: 1605481.000000\n",
            "Train Epoch: 46 [4480/7471 (60%)]\tLoss: 1614937.375000\n",
            "Train Epoch: 46 [4640/7471 (62%)]\tLoss: 1580260.625000\n",
            "Train Epoch: 46 [4800/7471 (64%)]\tLoss: 1615962.125000\n",
            "Train Epoch: 46 [4960/7471 (66%)]\tLoss: 1607879.500000\n",
            "Train Epoch: 46 [5120/7471 (69%)]\tLoss: 1578212.375000\n",
            "Train Epoch: 46 [5280/7471 (71%)]\tLoss: 1622106.125000\n",
            "Train Epoch: 46 [5440/7471 (73%)]\tLoss: 1552165.125000\n",
            "Train Epoch: 46 [5600/7471 (75%)]\tLoss: 1498178.875000\n",
            "Train Epoch: 46 [5760/7471 (77%)]\tLoss: 1560205.500000\n",
            "Train Epoch: 46 [5920/7471 (79%)]\tLoss: 1550162.250000\n",
            "Train Epoch: 46 [6080/7471 (81%)]\tLoss: 1624521.250000\n",
            "Train Epoch: 46 [6240/7471 (84%)]\tLoss: 1637525.250000\n",
            "Train Epoch: 46 [6400/7471 (86%)]\tLoss: 1530318.125000\n",
            "Train Epoch: 46 [6560/7471 (88%)]\tLoss: 1607932.625000\n",
            "Train Epoch: 46 [6720/7471 (90%)]\tLoss: 1587088.375000\n",
            "Train Epoch: 46 [6880/7471 (92%)]\tLoss: 1609609.750000\n",
            "Train Epoch: 46 [7040/7471 (94%)]\tLoss: 1534373.375000\n",
            "Train Epoch: 46 [7200/7471 (96%)]\tLoss: 1578822.375000\n",
            "Train Epoch: 46 [7360/7471 (99%)]\tLoss: 1626248.875000\n",
            "Epoch 46 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98643.0577\n",
            "\n",
            "Train Epoch: 47 [160/7471 (2%)]\tLoss: 1544179.125000\n",
            "Train Epoch: 47 [320/7471 (4%)]\tLoss: 1571171.625000\n",
            "Train Epoch: 47 [480/7471 (6%)]\tLoss: 1487019.875000\n",
            "Train Epoch: 47 [640/7471 (9%)]\tLoss: 1613742.375000\n",
            "Train Epoch: 47 [800/7471 (11%)]\tLoss: 1533853.250000\n",
            "Train Epoch: 47 [960/7471 (13%)]\tLoss: 1545417.500000\n",
            "Train Epoch: 47 [1120/7471 (15%)]\tLoss: 1606167.250000\n",
            "Train Epoch: 47 [1280/7471 (17%)]\tLoss: 1600137.000000\n",
            "Train Epoch: 47 [1440/7471 (19%)]\tLoss: 1608841.625000\n",
            "Train Epoch: 47 [1600/7471 (21%)]\tLoss: 1532955.750000\n",
            "Train Epoch: 47 [1760/7471 (24%)]\tLoss: 1619625.000000\n",
            "Train Epoch: 47 [1920/7471 (26%)]\tLoss: 1608920.000000\n",
            "Train Epoch: 47 [2080/7471 (28%)]\tLoss: 1556452.625000\n",
            "Train Epoch: 47 [2240/7471 (30%)]\tLoss: 1463787.000000\n",
            "Train Epoch: 47 [2400/7471 (32%)]\tLoss: 1605644.000000\n",
            "Train Epoch: 47 [2560/7471 (34%)]\tLoss: 1605019.000000\n",
            "Train Epoch: 47 [2720/7471 (36%)]\tLoss: 1601971.250000\n",
            "Train Epoch: 47 [2880/7471 (39%)]\tLoss: 1563786.750000\n",
            "Train Epoch: 47 [3040/7471 (41%)]\tLoss: 1571373.125000\n",
            "Train Epoch: 47 [3200/7471 (43%)]\tLoss: 1584784.250000\n",
            "Train Epoch: 47 [3360/7471 (45%)]\tLoss: 1605798.125000\n",
            "Train Epoch: 47 [3520/7471 (47%)]\tLoss: 1592580.125000\n",
            "Train Epoch: 47 [3680/7471 (49%)]\tLoss: 1641048.500000\n",
            "Train Epoch: 47 [3840/7471 (51%)]\tLoss: 1607973.750000\n",
            "Train Epoch: 47 [4000/7471 (54%)]\tLoss: 1611942.500000\n",
            "Train Epoch: 47 [4160/7471 (56%)]\tLoss: 1592058.625000\n",
            "Train Epoch: 47 [4320/7471 (58%)]\tLoss: 1544278.500000\n",
            "Train Epoch: 47 [4480/7471 (60%)]\tLoss: 1602897.875000\n",
            "Train Epoch: 47 [4640/7471 (62%)]\tLoss: 1635447.625000\n",
            "Train Epoch: 47 [4800/7471 (64%)]\tLoss: 1602877.375000\n",
            "Train Epoch: 47 [4960/7471 (66%)]\tLoss: 1599890.625000\n",
            "Train Epoch: 47 [5120/7471 (69%)]\tLoss: 1612028.000000\n",
            "Train Epoch: 47 [5280/7471 (71%)]\tLoss: 1525908.000000\n",
            "Train Epoch: 47 [5440/7471 (73%)]\tLoss: 1597098.625000\n",
            "Train Epoch: 47 [5600/7471 (75%)]\tLoss: 1557575.375000\n",
            "Train Epoch: 47 [5760/7471 (77%)]\tLoss: 1542935.375000\n",
            "Train Epoch: 47 [5920/7471 (79%)]\tLoss: 1520707.875000\n",
            "Train Epoch: 47 [6080/7471 (81%)]\tLoss: 1532944.125000\n",
            "Train Epoch: 47 [6240/7471 (84%)]\tLoss: 1604263.125000\n",
            "Train Epoch: 47 [6400/7471 (86%)]\tLoss: 1559815.125000\n",
            "Train Epoch: 47 [6560/7471 (88%)]\tLoss: 1630736.625000\n",
            "Train Epoch: 47 [6720/7471 (90%)]\tLoss: 1643466.875000\n",
            "Train Epoch: 47 [6880/7471 (92%)]\tLoss: 1502669.250000\n",
            "Train Epoch: 47 [7040/7471 (94%)]\tLoss: 1558784.000000\n",
            "Train Epoch: 47 [7200/7471 (96%)]\tLoss: 1511889.625000\n",
            "Train Epoch: 47 [7360/7471 (99%)]\tLoss: 1562543.375000\n",
            "Epoch 47 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98889.9452\n",
            "\n",
            "Train Epoch: 48 [160/7471 (2%)]\tLoss: 1628644.625000\n",
            "Train Epoch: 48 [320/7471 (4%)]\tLoss: 1608522.625000\n",
            "Train Epoch: 48 [480/7471 (6%)]\tLoss: 1547956.000000\n",
            "Train Epoch: 48 [640/7471 (9%)]\tLoss: 1610513.125000\n",
            "Train Epoch: 48 [800/7471 (11%)]\tLoss: 1587166.125000\n",
            "Train Epoch: 48 [960/7471 (13%)]\tLoss: 1538055.625000\n",
            "Train Epoch: 48 [1120/7471 (15%)]\tLoss: 1622694.250000\n",
            "Train Epoch: 48 [1280/7471 (17%)]\tLoss: 1612359.125000\n",
            "Train Epoch: 48 [1440/7471 (19%)]\tLoss: 1570942.125000\n",
            "Train Epoch: 48 [1600/7471 (21%)]\tLoss: 1609106.375000\n",
            "Train Epoch: 48 [1760/7471 (24%)]\tLoss: 1598989.500000\n",
            "Train Epoch: 48 [1920/7471 (26%)]\tLoss: 1570139.125000\n",
            "Train Epoch: 48 [2080/7471 (28%)]\tLoss: 1561135.375000\n",
            "Train Epoch: 48 [2240/7471 (30%)]\tLoss: 1577000.375000\n",
            "Train Epoch: 48 [2400/7471 (32%)]\tLoss: 1573323.750000\n",
            "Train Epoch: 48 [2560/7471 (34%)]\tLoss: 1619975.125000\n",
            "Train Epoch: 48 [2720/7471 (36%)]\tLoss: 1593913.000000\n",
            "Train Epoch: 48 [2880/7471 (39%)]\tLoss: 1557236.750000\n",
            "Train Epoch: 48 [3040/7471 (41%)]\tLoss: 1555309.375000\n",
            "Train Epoch: 48 [3200/7471 (43%)]\tLoss: 1598712.125000\n",
            "Train Epoch: 48 [3360/7471 (45%)]\tLoss: 1581639.875000\n",
            "Train Epoch: 48 [3520/7471 (47%)]\tLoss: 1571445.250000\n",
            "Train Epoch: 48 [3680/7471 (49%)]\tLoss: 1565004.000000\n",
            "Train Epoch: 48 [3840/7471 (51%)]\tLoss: 1601053.250000\n",
            "Train Epoch: 48 [4000/7471 (54%)]\tLoss: 1641309.875000\n",
            "Train Epoch: 48 [4160/7471 (56%)]\tLoss: 1584083.875000\n",
            "Train Epoch: 48 [4320/7471 (58%)]\tLoss: 1554798.875000\n",
            "Train Epoch: 48 [4480/7471 (60%)]\tLoss: 1575482.875000\n",
            "Train Epoch: 48 [4640/7471 (62%)]\tLoss: 1612756.375000\n",
            "Train Epoch: 48 [4800/7471 (64%)]\tLoss: 1572782.250000\n",
            "Train Epoch: 48 [4960/7471 (66%)]\tLoss: 1555854.125000\n",
            "Train Epoch: 48 [5120/7471 (69%)]\tLoss: 1592386.000000\n",
            "Train Epoch: 48 [5280/7471 (71%)]\tLoss: 1576855.000000\n",
            "Train Epoch: 48 [5440/7471 (73%)]\tLoss: 1558962.250000\n",
            "Train Epoch: 48 [5600/7471 (75%)]\tLoss: 1609177.375000\n",
            "Train Epoch: 48 [5760/7471 (77%)]\tLoss: 1601519.750000\n",
            "Train Epoch: 48 [5920/7471 (79%)]\tLoss: 1549710.375000\n",
            "Train Epoch: 48 [6080/7471 (81%)]\tLoss: 1589382.750000\n",
            "Train Epoch: 48 [6240/7471 (84%)]\tLoss: 1597648.000000\n",
            "Train Epoch: 48 [6400/7471 (86%)]\tLoss: 1569824.000000\n",
            "Train Epoch: 48 [6560/7471 (88%)]\tLoss: 1559397.375000\n",
            "Train Epoch: 48 [6720/7471 (90%)]\tLoss: 1599203.500000\n",
            "Train Epoch: 48 [6880/7471 (92%)]\tLoss: 1631887.375000\n",
            "Train Epoch: 48 [7040/7471 (94%)]\tLoss: 1555965.375000\n",
            "Train Epoch: 48 [7200/7471 (96%)]\tLoss: 1639110.500000\n",
            "Train Epoch: 48 [7360/7471 (99%)]\tLoss: 1591172.750000\n",
            "Epoch 48 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 99106.1395\n",
            "\n",
            "Train Epoch: 49 [160/7471 (2%)]\tLoss: 1555179.000000\n",
            "Train Epoch: 49 [320/7471 (4%)]\tLoss: 1608024.125000\n",
            "Train Epoch: 49 [480/7471 (6%)]\tLoss: 1644089.375000\n",
            "Train Epoch: 49 [640/7471 (9%)]\tLoss: 1598810.000000\n",
            "Train Epoch: 49 [800/7471 (11%)]\tLoss: 1627471.625000\n",
            "Train Epoch: 49 [960/7471 (13%)]\tLoss: 1622053.500000\n",
            "Train Epoch: 49 [1120/7471 (15%)]\tLoss: 1568197.125000\n",
            "Train Epoch: 49 [1280/7471 (17%)]\tLoss: 1515400.375000\n",
            "Train Epoch: 49 [1440/7471 (19%)]\tLoss: 1600317.125000\n",
            "Train Epoch: 49 [1600/7471 (21%)]\tLoss: 1625315.500000\n",
            "Train Epoch: 49 [1760/7471 (24%)]\tLoss: 1611097.375000\n",
            "Train Epoch: 49 [1920/7471 (26%)]\tLoss: 1605055.125000\n",
            "Train Epoch: 49 [2080/7471 (28%)]\tLoss: 1485101.000000\n",
            "Train Epoch: 49 [2240/7471 (30%)]\tLoss: 1578825.875000\n",
            "Train Epoch: 49 [2400/7471 (32%)]\tLoss: 1620203.250000\n",
            "Train Epoch: 49 [2560/7471 (34%)]\tLoss: 1607283.000000\n",
            "Train Epoch: 49 [2720/7471 (36%)]\tLoss: 1583672.375000\n",
            "Train Epoch: 49 [2880/7471 (39%)]\tLoss: 1596731.875000\n",
            "Train Epoch: 49 [3040/7471 (41%)]\tLoss: 1593184.125000\n",
            "Train Epoch: 49 [3200/7471 (43%)]\tLoss: 1564504.375000\n",
            "Train Epoch: 49 [3360/7471 (45%)]\tLoss: 1625015.500000\n",
            "Train Epoch: 49 [3520/7471 (47%)]\tLoss: 1601564.750000\n",
            "Train Epoch: 49 [3680/7471 (49%)]\tLoss: 1506765.750000\n",
            "Train Epoch: 49 [3840/7471 (51%)]\tLoss: 1631887.000000\n",
            "Train Epoch: 49 [4000/7471 (54%)]\tLoss: 1607942.000000\n",
            "Train Epoch: 49 [4160/7471 (56%)]\tLoss: 1605026.125000\n",
            "Train Epoch: 49 [4320/7471 (58%)]\tLoss: 1621498.250000\n",
            "Train Epoch: 49 [4480/7471 (60%)]\tLoss: 1599889.375000\n",
            "Train Epoch: 49 [4640/7471 (62%)]\tLoss: 1613995.875000\n",
            "Train Epoch: 49 [4800/7471 (64%)]\tLoss: 1545820.875000\n",
            "Train Epoch: 49 [4960/7471 (66%)]\tLoss: 1544253.750000\n",
            "Train Epoch: 49 [5120/7471 (69%)]\tLoss: 1589514.625000\n",
            "Train Epoch: 49 [5280/7471 (71%)]\tLoss: 1521946.000000\n",
            "Train Epoch: 49 [5440/7471 (73%)]\tLoss: 1554908.500000\n",
            "Train Epoch: 49 [5600/7471 (75%)]\tLoss: 1584115.250000\n",
            "Train Epoch: 49 [5760/7471 (77%)]\tLoss: 1561549.125000\n",
            "Train Epoch: 49 [5920/7471 (79%)]\tLoss: 1553825.125000\n",
            "Train Epoch: 49 [6080/7471 (81%)]\tLoss: 1594818.750000\n",
            "Train Epoch: 49 [6240/7471 (84%)]\tLoss: 1388290.250000\n",
            "Train Epoch: 49 [6400/7471 (86%)]\tLoss: 1631772.625000\n",
            "Train Epoch: 49 [6560/7471 (88%)]\tLoss: 1520496.500000\n",
            "Train Epoch: 49 [6720/7471 (90%)]\tLoss: 1614994.500000\n",
            "Train Epoch: 49 [6880/7471 (92%)]\tLoss: 1617952.750000\n",
            "Train Epoch: 49 [7040/7471 (94%)]\tLoss: 1546907.375000\n",
            "Train Epoch: 49 [7200/7471 (96%)]\tLoss: 1607328.625000\n",
            "Train Epoch: 49 [7360/7471 (99%)]\tLoss: 1620234.125000\n",
            "Epoch 49 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98603.7253\n",
            "\n",
            "Train Epoch: 50 [160/7471 (2%)]\tLoss: 1620111.875000\n",
            "Train Epoch: 50 [320/7471 (4%)]\tLoss: 1640216.000000\n",
            "Train Epoch: 50 [480/7471 (6%)]\tLoss: 1575056.625000\n",
            "Train Epoch: 50 [640/7471 (9%)]\tLoss: 1547548.875000\n",
            "Train Epoch: 50 [800/7471 (11%)]\tLoss: 1579250.875000\n",
            "Train Epoch: 50 [960/7471 (13%)]\tLoss: 1611081.000000\n",
            "Train Epoch: 50 [1120/7471 (15%)]\tLoss: 1533414.375000\n",
            "Train Epoch: 50 [1280/7471 (17%)]\tLoss: 1569378.000000\n",
            "Train Epoch: 50 [1440/7471 (19%)]\tLoss: 1598009.375000\n",
            "Train Epoch: 50 [1600/7471 (21%)]\tLoss: 1545798.625000\n",
            "Train Epoch: 50 [1760/7471 (24%)]\tLoss: 1631457.000000\n",
            "Train Epoch: 50 [1920/7471 (26%)]\tLoss: 1553384.625000\n",
            "Train Epoch: 50 [2080/7471 (28%)]\tLoss: 1543299.625000\n",
            "Train Epoch: 50 [2240/7471 (30%)]\tLoss: 1585402.250000\n",
            "Train Epoch: 50 [2400/7471 (32%)]\tLoss: 1514298.875000\n",
            "Train Epoch: 50 [2560/7471 (34%)]\tLoss: 1620287.750000\n",
            "Train Epoch: 50 [2720/7471 (36%)]\tLoss: 1616872.125000\n",
            "Train Epoch: 50 [2880/7471 (39%)]\tLoss: 1561115.875000\n",
            "Train Epoch: 50 [3040/7471 (41%)]\tLoss: 1557945.500000\n",
            "Train Epoch: 50 [3200/7471 (43%)]\tLoss: 1620918.250000\n",
            "Train Epoch: 50 [3360/7471 (45%)]\tLoss: 1586703.000000\n",
            "Train Epoch: 50 [3520/7471 (47%)]\tLoss: 1585004.125000\n",
            "Train Epoch: 50 [3680/7471 (49%)]\tLoss: 1589503.875000\n",
            "Train Epoch: 50 [3840/7471 (51%)]\tLoss: 1553622.500000\n",
            "Train Epoch: 50 [4000/7471 (54%)]\tLoss: 1569042.125000\n",
            "Train Epoch: 50 [4160/7471 (56%)]\tLoss: 1608705.000000\n",
            "Train Epoch: 50 [4320/7471 (58%)]\tLoss: 1602956.750000\n",
            "Train Epoch: 50 [4480/7471 (60%)]\tLoss: 1525430.875000\n",
            "Train Epoch: 50 [4640/7471 (62%)]\tLoss: 1560402.250000\n",
            "Train Epoch: 50 [4800/7471 (64%)]\tLoss: 1538984.750000\n",
            "Train Epoch: 50 [4960/7471 (66%)]\tLoss: 1537379.750000\n",
            "Train Epoch: 50 [5120/7471 (69%)]\tLoss: 1558039.125000\n",
            "Train Epoch: 50 [5280/7471 (71%)]\tLoss: 1531314.500000\n",
            "Train Epoch: 50 [5440/7471 (73%)]\tLoss: 1611209.250000\n",
            "Train Epoch: 50 [5600/7471 (75%)]\tLoss: 1621486.625000\n",
            "Train Epoch: 50 [5760/7471 (77%)]\tLoss: 1537647.500000\n",
            "Train Epoch: 50 [5920/7471 (79%)]\tLoss: 1611742.375000\n",
            "Train Epoch: 50 [6080/7471 (81%)]\tLoss: 1529586.500000\n",
            "Train Epoch: 50 [6240/7471 (84%)]\tLoss: 1546954.875000\n",
            "Train Epoch: 50 [6400/7471 (86%)]\tLoss: 1579989.625000\n",
            "Train Epoch: 50 [6560/7471 (88%)]\tLoss: 1606646.500000\n",
            "Train Epoch: 50 [6720/7471 (90%)]\tLoss: 1572132.625000\n",
            "Train Epoch: 50 [6880/7471 (92%)]\tLoss: 1563099.250000\n",
            "Train Epoch: 50 [7040/7471 (94%)]\tLoss: 1567068.375000\n",
            "Train Epoch: 50 [7200/7471 (96%)]\tLoss: 1588597.125000\n",
            "Train Epoch: 50 [7360/7471 (99%)]\tLoss: 1639485.750000\n",
            "Epoch 50 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98592.2494\n",
            "\n",
            "Train Epoch: 51 [160/7471 (2%)]\tLoss: 1545507.125000\n",
            "Train Epoch: 51 [320/7471 (4%)]\tLoss: 1598190.000000\n",
            "Train Epoch: 51 [480/7471 (6%)]\tLoss: 1593003.250000\n",
            "Train Epoch: 51 [640/7471 (9%)]\tLoss: 1591991.125000\n",
            "Train Epoch: 51 [800/7471 (11%)]\tLoss: 1544646.750000\n",
            "Train Epoch: 51 [960/7471 (13%)]\tLoss: 1533177.375000\n",
            "Train Epoch: 51 [1120/7471 (15%)]\tLoss: 1605477.500000\n",
            "Train Epoch: 51 [1280/7471 (17%)]\tLoss: 1621611.125000\n",
            "Train Epoch: 51 [1440/7471 (19%)]\tLoss: 1574692.250000\n",
            "Train Epoch: 51 [1600/7471 (21%)]\tLoss: 1622129.250000\n",
            "Train Epoch: 51 [1760/7471 (24%)]\tLoss: 1602614.375000\n",
            "Train Epoch: 51 [1920/7471 (26%)]\tLoss: 1628435.000000\n",
            "Train Epoch: 51 [2080/7471 (28%)]\tLoss: 1617743.750000\n",
            "Train Epoch: 51 [2240/7471 (30%)]\tLoss: 1611372.125000\n",
            "Train Epoch: 51 [2400/7471 (32%)]\tLoss: 1628986.750000\n",
            "Train Epoch: 51 [2560/7471 (34%)]\tLoss: 1488620.125000\n",
            "Train Epoch: 51 [2720/7471 (36%)]\tLoss: 1562236.250000\n",
            "Train Epoch: 51 [2880/7471 (39%)]\tLoss: 1637709.375000\n",
            "Train Epoch: 51 [3040/7471 (41%)]\tLoss: 1563164.250000\n",
            "Train Epoch: 51 [3200/7471 (43%)]\tLoss: 1569640.375000\n",
            "Train Epoch: 51 [3360/7471 (45%)]\tLoss: 1578378.375000\n",
            "Train Epoch: 51 [3520/7471 (47%)]\tLoss: 1596096.875000\n",
            "Train Epoch: 51 [3680/7471 (49%)]\tLoss: 1602500.500000\n",
            "Train Epoch: 51 [3840/7471 (51%)]\tLoss: 1579296.125000\n",
            "Train Epoch: 51 [4000/7471 (54%)]\tLoss: 1627398.750000\n",
            "Train Epoch: 51 [4160/7471 (56%)]\tLoss: 1573194.625000\n",
            "Train Epoch: 51 [4320/7471 (58%)]\tLoss: 1603911.375000\n",
            "Train Epoch: 51 [4480/7471 (60%)]\tLoss: 1612024.375000\n",
            "Train Epoch: 51 [4640/7471 (62%)]\tLoss: 1549814.000000\n",
            "Train Epoch: 51 [4800/7471 (64%)]\tLoss: 1509927.000000\n",
            "Train Epoch: 51 [4960/7471 (66%)]\tLoss: 1646668.125000\n",
            "Train Epoch: 51 [5120/7471 (69%)]\tLoss: 1562836.125000\n",
            "Train Epoch: 51 [5280/7471 (71%)]\tLoss: 1597987.000000\n",
            "Train Epoch: 51 [5440/7471 (73%)]\tLoss: 1609987.625000\n",
            "Train Epoch: 51 [5600/7471 (75%)]\tLoss: 1535576.250000\n",
            "Train Epoch: 51 [5760/7471 (77%)]\tLoss: 1600053.500000\n",
            "Train Epoch: 51 [5920/7471 (79%)]\tLoss: 1544519.125000\n",
            "Train Epoch: 51 [6080/7471 (81%)]\tLoss: 1569017.375000\n",
            "Train Epoch: 51 [6240/7471 (84%)]\tLoss: 1575848.625000\n",
            "Train Epoch: 51 [6400/7471 (86%)]\tLoss: 1584058.125000\n",
            "Train Epoch: 51 [6560/7471 (88%)]\tLoss: 1591562.875000\n",
            "Train Epoch: 51 [6720/7471 (90%)]\tLoss: 1528447.000000\n",
            "Train Epoch: 51 [6880/7471 (92%)]\tLoss: 1611568.625000\n",
            "Train Epoch: 51 [7040/7471 (94%)]\tLoss: 1533924.125000\n",
            "Train Epoch: 51 [7200/7471 (96%)]\tLoss: 1571466.500000\n",
            "Train Epoch: 51 [7360/7471 (99%)]\tLoss: 1566466.750000\n",
            "Epoch 51 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98606.9897\n",
            "\n",
            "Train Epoch: 52 [160/7471 (2%)]\tLoss: 1601572.750000\n",
            "Train Epoch: 52 [320/7471 (4%)]\tLoss: 1614755.000000\n",
            "Train Epoch: 52 [480/7471 (6%)]\tLoss: 1608372.500000\n",
            "Train Epoch: 52 [640/7471 (9%)]\tLoss: 1553574.500000\n",
            "Train Epoch: 52 [800/7471 (11%)]\tLoss: 1637379.625000\n",
            "Train Epoch: 52 [960/7471 (13%)]\tLoss: 1571000.250000\n",
            "Train Epoch: 52 [1120/7471 (15%)]\tLoss: 1551815.625000\n",
            "Train Epoch: 52 [1280/7471 (17%)]\tLoss: 1492663.125000\n",
            "Train Epoch: 52 [1440/7471 (19%)]\tLoss: 1595307.250000\n",
            "Train Epoch: 52 [1600/7471 (21%)]\tLoss: 1603084.500000\n",
            "Train Epoch: 52 [1760/7471 (24%)]\tLoss: 1561270.625000\n",
            "Train Epoch: 52 [1920/7471 (26%)]\tLoss: 1558607.500000\n",
            "Train Epoch: 52 [2080/7471 (28%)]\tLoss: 1571844.375000\n",
            "Train Epoch: 52 [2240/7471 (30%)]\tLoss: 1583203.500000\n",
            "Train Epoch: 52 [2400/7471 (32%)]\tLoss: 1602480.500000\n",
            "Train Epoch: 52 [2560/7471 (34%)]\tLoss: 1542979.750000\n",
            "Train Epoch: 52 [2720/7471 (36%)]\tLoss: 1546553.000000\n",
            "Train Epoch: 52 [2880/7471 (39%)]\tLoss: 1593529.250000\n",
            "Train Epoch: 52 [3040/7471 (41%)]\tLoss: 1460760.625000\n",
            "Train Epoch: 52 [3200/7471 (43%)]\tLoss: 1590982.875000\n",
            "Train Epoch: 52 [3360/7471 (45%)]\tLoss: 1617678.875000\n",
            "Train Epoch: 52 [3520/7471 (47%)]\tLoss: 1589595.625000\n",
            "Train Epoch: 52 [3680/7471 (49%)]\tLoss: 1588098.250000\n",
            "Train Epoch: 52 [3840/7471 (51%)]\tLoss: 1628166.625000\n",
            "Train Epoch: 52 [4000/7471 (54%)]\tLoss: 1623358.375000\n",
            "Train Epoch: 52 [4160/7471 (56%)]\tLoss: 1575955.375000\n",
            "Train Epoch: 52 [4320/7471 (58%)]\tLoss: 1601357.500000\n",
            "Train Epoch: 52 [4480/7471 (60%)]\tLoss: 1514750.750000\n",
            "Train Epoch: 52 [4640/7471 (62%)]\tLoss: 1514072.000000\n",
            "Train Epoch: 52 [4800/7471 (64%)]\tLoss: 1615916.500000\n",
            "Train Epoch: 52 [4960/7471 (66%)]\tLoss: 1549871.375000\n",
            "Train Epoch: 52 [5120/7471 (69%)]\tLoss: 1606356.750000\n",
            "Train Epoch: 52 [5280/7471 (71%)]\tLoss: 1607861.500000\n",
            "Train Epoch: 52 [5440/7471 (73%)]\tLoss: 1568715.750000\n",
            "Train Epoch: 52 [5600/7471 (75%)]\tLoss: 1597772.500000\n",
            "Train Epoch: 52 [5760/7471 (77%)]\tLoss: 1619700.875000\n",
            "Train Epoch: 52 [5920/7471 (79%)]\tLoss: 1653830.875000\n",
            "Train Epoch: 52 [6080/7471 (81%)]\tLoss: 1605100.875000\n",
            "Train Epoch: 52 [6240/7471 (84%)]\tLoss: 1614223.875000\n",
            "Train Epoch: 52 [6400/7471 (86%)]\tLoss: 1593505.250000\n",
            "Train Epoch: 52 [6560/7471 (88%)]\tLoss: 1640365.375000\n",
            "Train Epoch: 52 [6720/7471 (90%)]\tLoss: 1506536.875000\n",
            "Train Epoch: 52 [6880/7471 (92%)]\tLoss: 1607338.500000\n",
            "Train Epoch: 52 [7040/7471 (94%)]\tLoss: 1611552.500000\n",
            "Train Epoch: 52 [7200/7471 (96%)]\tLoss: 1637168.500000\n",
            "Train Epoch: 52 [7360/7471 (99%)]\tLoss: 1539757.875000\n",
            "Epoch 52 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98673.9283\n",
            "\n",
            "Train Epoch: 53 [160/7471 (2%)]\tLoss: 1555717.125000\n",
            "Train Epoch: 53 [320/7471 (4%)]\tLoss: 1551914.000000\n",
            "Train Epoch: 53 [480/7471 (6%)]\tLoss: 1595029.750000\n",
            "Train Epoch: 53 [640/7471 (9%)]\tLoss: 1581352.500000\n",
            "Train Epoch: 53 [800/7471 (11%)]\tLoss: 1541470.500000\n",
            "Train Epoch: 53 [960/7471 (13%)]\tLoss: 1543414.500000\n",
            "Train Epoch: 53 [1120/7471 (15%)]\tLoss: 1639724.500000\n",
            "Train Epoch: 53 [1280/7471 (17%)]\tLoss: 1619119.875000\n",
            "Train Epoch: 53 [1440/7471 (19%)]\tLoss: 1584877.375000\n",
            "Train Epoch: 53 [1600/7471 (21%)]\tLoss: 1584422.000000\n",
            "Train Epoch: 53 [1760/7471 (24%)]\tLoss: 1495611.625000\n",
            "Train Epoch: 53 [1920/7471 (26%)]\tLoss: 1522331.000000\n",
            "Train Epoch: 53 [2080/7471 (28%)]\tLoss: 1586252.875000\n",
            "Train Epoch: 53 [2240/7471 (30%)]\tLoss: 1619389.375000\n",
            "Train Epoch: 53 [2400/7471 (32%)]\tLoss: 1586039.750000\n",
            "Train Epoch: 53 [2560/7471 (34%)]\tLoss: 1538611.250000\n",
            "Train Epoch: 53 [2720/7471 (36%)]\tLoss: 1597077.875000\n",
            "Train Epoch: 53 [2880/7471 (39%)]\tLoss: 1611255.625000\n",
            "Train Epoch: 53 [3040/7471 (41%)]\tLoss: 1577120.500000\n",
            "Train Epoch: 53 [3200/7471 (43%)]\tLoss: 1602650.250000\n",
            "Train Epoch: 53 [3360/7471 (45%)]\tLoss: 1603524.000000\n",
            "Train Epoch: 53 [3520/7471 (47%)]\tLoss: 1570258.000000\n",
            "Train Epoch: 53 [3680/7471 (49%)]\tLoss: 1467973.875000\n",
            "Train Epoch: 53 [3840/7471 (51%)]\tLoss: 1507083.125000\n",
            "Train Epoch: 53 [4000/7471 (54%)]\tLoss: 1625100.250000\n",
            "Train Epoch: 53 [4160/7471 (56%)]\tLoss: 1623047.125000\n",
            "Train Epoch: 53 [4320/7471 (58%)]\tLoss: 1535148.750000\n",
            "Train Epoch: 53 [4480/7471 (60%)]\tLoss: 1621311.625000\n",
            "Train Epoch: 53 [4640/7471 (62%)]\tLoss: 1612079.250000\n",
            "Train Epoch: 53 [4800/7471 (64%)]\tLoss: 1567489.500000\n",
            "Train Epoch: 53 [4960/7471 (66%)]\tLoss: 1611600.375000\n",
            "Train Epoch: 53 [5120/7471 (69%)]\tLoss: 1623826.625000\n",
            "Train Epoch: 53 [5280/7471 (71%)]\tLoss: 1600143.375000\n",
            "Train Epoch: 53 [5440/7471 (73%)]\tLoss: 1568674.000000\n",
            "Train Epoch: 53 [5600/7471 (75%)]\tLoss: 1549429.250000\n",
            "Train Epoch: 53 [5760/7471 (77%)]\tLoss: 1638874.250000\n",
            "Train Epoch: 53 [5920/7471 (79%)]\tLoss: 1577698.875000\n",
            "Train Epoch: 53 [6080/7471 (81%)]\tLoss: 1589683.000000\n",
            "Train Epoch: 53 [6240/7471 (84%)]\tLoss: 1557739.500000\n",
            "Train Epoch: 53 [6400/7471 (86%)]\tLoss: 1531233.875000\n",
            "Train Epoch: 53 [6560/7471 (88%)]\tLoss: 1611268.875000\n",
            "Train Epoch: 53 [6720/7471 (90%)]\tLoss: 1602802.750000\n",
            "Train Epoch: 53 [6880/7471 (92%)]\tLoss: 1551344.750000\n",
            "Train Epoch: 53 [7040/7471 (94%)]\tLoss: 1535859.375000\n",
            "Train Epoch: 53 [7200/7471 (96%)]\tLoss: 1613938.375000\n",
            "Train Epoch: 53 [7360/7471 (99%)]\tLoss: 1599985.625000\n",
            "Epoch 53 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98901.1277\n",
            "\n",
            "Train Epoch: 54 [160/7471 (2%)]\tLoss: 1559581.625000\n",
            "Train Epoch: 54 [320/7471 (4%)]\tLoss: 1587398.750000\n",
            "Train Epoch: 54 [480/7471 (6%)]\tLoss: 1532066.625000\n",
            "Train Epoch: 54 [640/7471 (9%)]\tLoss: 1566675.500000\n",
            "Train Epoch: 54 [800/7471 (11%)]\tLoss: 1588218.500000\n",
            "Train Epoch: 54 [960/7471 (13%)]\tLoss: 1613783.500000\n",
            "Train Epoch: 54 [1120/7471 (15%)]\tLoss: 1528939.875000\n",
            "Train Epoch: 54 [1280/7471 (17%)]\tLoss: 1609115.125000\n",
            "Train Epoch: 54 [1440/7471 (19%)]\tLoss: 1558668.625000\n",
            "Train Epoch: 54 [1600/7471 (21%)]\tLoss: 1612624.250000\n",
            "Train Epoch: 54 [1760/7471 (24%)]\tLoss: 1594462.125000\n",
            "Train Epoch: 54 [1920/7471 (26%)]\tLoss: 1643339.750000\n",
            "Train Epoch: 54 [2080/7471 (28%)]\tLoss: 1573850.375000\n",
            "Train Epoch: 54 [2240/7471 (30%)]\tLoss: 1584530.375000\n",
            "Train Epoch: 54 [2400/7471 (32%)]\tLoss: 1525132.500000\n",
            "Train Epoch: 54 [2560/7471 (34%)]\tLoss: 1589924.625000\n",
            "Train Epoch: 54 [2720/7471 (36%)]\tLoss: 1560872.000000\n",
            "Train Epoch: 54 [2880/7471 (39%)]\tLoss: 1569026.250000\n",
            "Train Epoch: 54 [3040/7471 (41%)]\tLoss: 1574885.750000\n",
            "Train Epoch: 54 [3200/7471 (43%)]\tLoss: 1501230.000000\n",
            "Train Epoch: 54 [3360/7471 (45%)]\tLoss: 1615478.625000\n",
            "Train Epoch: 54 [3520/7471 (47%)]\tLoss: 1526249.125000\n",
            "Train Epoch: 54 [3680/7471 (49%)]\tLoss: 1607618.875000\n",
            "Train Epoch: 54 [3840/7471 (51%)]\tLoss: 1601525.125000\n",
            "Train Epoch: 54 [4000/7471 (54%)]\tLoss: 1619305.250000\n",
            "Train Epoch: 54 [4160/7471 (56%)]\tLoss: 1541421.375000\n",
            "Train Epoch: 54 [4320/7471 (58%)]\tLoss: 1582620.875000\n",
            "Train Epoch: 54 [4480/7471 (60%)]\tLoss: 1545361.750000\n",
            "Train Epoch: 54 [4640/7471 (62%)]\tLoss: 1600800.000000\n",
            "Train Epoch: 54 [4800/7471 (64%)]\tLoss: 1628873.250000\n",
            "Train Epoch: 54 [4960/7471 (66%)]\tLoss: 1555583.375000\n",
            "Train Epoch: 54 [5120/7471 (69%)]\tLoss: 1593052.625000\n",
            "Train Epoch: 54 [5280/7471 (71%)]\tLoss: 1583850.125000\n",
            "Train Epoch: 54 [5440/7471 (73%)]\tLoss: 1554910.125000\n",
            "Train Epoch: 54 [5600/7471 (75%)]\tLoss: 1602736.750000\n",
            "Train Epoch: 54 [5760/7471 (77%)]\tLoss: 1578763.250000\n",
            "Train Epoch: 54 [5920/7471 (79%)]\tLoss: 1558581.625000\n",
            "Train Epoch: 54 [6080/7471 (81%)]\tLoss: 1568688.000000\n",
            "Train Epoch: 54 [6240/7471 (84%)]\tLoss: 1583554.125000\n",
            "Train Epoch: 54 [6400/7471 (86%)]\tLoss: 1609308.375000\n",
            "Train Epoch: 54 [6560/7471 (88%)]\tLoss: 1594125.250000\n",
            "Train Epoch: 54 [6720/7471 (90%)]\tLoss: 1578373.625000\n",
            "Train Epoch: 54 [6880/7471 (92%)]\tLoss: 1570674.375000\n",
            "Train Epoch: 54 [7040/7471 (94%)]\tLoss: 1585669.500000\n",
            "Train Epoch: 54 [7200/7471 (96%)]\tLoss: 1536459.000000\n",
            "Train Epoch: 54 [7360/7471 (99%)]\tLoss: 1589563.625000\n",
            "Epoch 54 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98609.7878\n",
            "\n",
            "Train Epoch: 55 [160/7471 (2%)]\tLoss: 1560911.000000\n",
            "Train Epoch: 55 [320/7471 (4%)]\tLoss: 1551171.625000\n",
            "Train Epoch: 55 [480/7471 (6%)]\tLoss: 1598628.625000\n",
            "Train Epoch: 55 [640/7471 (9%)]\tLoss: 1629318.500000\n",
            "Train Epoch: 55 [800/7471 (11%)]\tLoss: 1585198.250000\n",
            "Train Epoch: 55 [960/7471 (13%)]\tLoss: 1538247.750000\n",
            "Train Epoch: 55 [1120/7471 (15%)]\tLoss: 1595349.750000\n",
            "Train Epoch: 55 [1280/7471 (17%)]\tLoss: 1547313.750000\n",
            "Train Epoch: 55 [1440/7471 (19%)]\tLoss: 1597579.250000\n",
            "Train Epoch: 55 [1600/7471 (21%)]\tLoss: 1522829.125000\n",
            "Train Epoch: 55 [1760/7471 (24%)]\tLoss: 1570212.875000\n",
            "Train Epoch: 55 [1920/7471 (26%)]\tLoss: 1601122.875000\n",
            "Train Epoch: 55 [2080/7471 (28%)]\tLoss: 1617820.500000\n",
            "Train Epoch: 55 [2240/7471 (30%)]\tLoss: 1571184.625000\n",
            "Train Epoch: 55 [2400/7471 (32%)]\tLoss: 1614786.375000\n",
            "Train Epoch: 55 [2560/7471 (34%)]\tLoss: 1444993.000000\n",
            "Train Epoch: 55 [2720/7471 (36%)]\tLoss: 1562986.875000\n",
            "Train Epoch: 55 [2880/7471 (39%)]\tLoss: 1508375.125000\n",
            "Train Epoch: 55 [3040/7471 (41%)]\tLoss: 1639723.000000\n",
            "Train Epoch: 55 [3200/7471 (43%)]\tLoss: 1627053.625000\n",
            "Train Epoch: 55 [3360/7471 (45%)]\tLoss: 1620217.250000\n",
            "Train Epoch: 55 [3520/7471 (47%)]\tLoss: 1573061.000000\n",
            "Train Epoch: 55 [3680/7471 (49%)]\tLoss: 1582025.750000\n",
            "Train Epoch: 55 [3840/7471 (51%)]\tLoss: 1597578.000000\n",
            "Train Epoch: 55 [4000/7471 (54%)]\tLoss: 1608164.000000\n",
            "Train Epoch: 55 [4160/7471 (56%)]\tLoss: 1585720.000000\n",
            "Train Epoch: 55 [4320/7471 (58%)]\tLoss: 1591149.625000\n",
            "Train Epoch: 55 [4480/7471 (60%)]\tLoss: 1574526.750000\n",
            "Train Epoch: 55 [4640/7471 (62%)]\tLoss: 1594393.000000\n",
            "Train Epoch: 55 [4800/7471 (64%)]\tLoss: 1574724.625000\n",
            "Train Epoch: 55 [4960/7471 (66%)]\tLoss: 1567489.500000\n",
            "Train Epoch: 55 [5120/7471 (69%)]\tLoss: 1586632.750000\n",
            "Train Epoch: 55 [5280/7471 (71%)]\tLoss: 1596419.125000\n",
            "Train Epoch: 55 [5440/7471 (73%)]\tLoss: 1556864.250000\n",
            "Train Epoch: 55 [5600/7471 (75%)]\tLoss: 1521355.125000\n",
            "Train Epoch: 55 [5760/7471 (77%)]\tLoss: 1539538.250000\n",
            "Train Epoch: 55 [5920/7471 (79%)]\tLoss: 1627608.375000\n",
            "Train Epoch: 55 [6080/7471 (81%)]\tLoss: 1604466.000000\n",
            "Train Epoch: 55 [6240/7471 (84%)]\tLoss: 1598955.625000\n",
            "Train Epoch: 55 [6400/7471 (86%)]\tLoss: 1573514.875000\n",
            "Train Epoch: 55 [6560/7471 (88%)]\tLoss: 1610992.625000\n",
            "Train Epoch: 55 [6720/7471 (90%)]\tLoss: 1563661.750000\n",
            "Train Epoch: 55 [6880/7471 (92%)]\tLoss: 1560164.250000\n",
            "Train Epoch: 55 [7040/7471 (94%)]\tLoss: 1615914.375000\n",
            "Train Epoch: 55 [7200/7471 (96%)]\tLoss: 1626710.125000\n",
            "Train Epoch: 55 [7360/7471 (99%)]\tLoss: 1626796.750000\n",
            "Epoch 55 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98621.3919\n",
            "\n",
            "Train Epoch: 56 [160/7471 (2%)]\tLoss: 1597075.125000\n",
            "Train Epoch: 56 [320/7471 (4%)]\tLoss: 1657889.125000\n",
            "Train Epoch: 56 [480/7471 (6%)]\tLoss: 1559448.875000\n",
            "Train Epoch: 56 [640/7471 (9%)]\tLoss: 1568495.125000\n",
            "Train Epoch: 56 [800/7471 (11%)]\tLoss: 1606716.000000\n",
            "Train Epoch: 56 [960/7471 (13%)]\tLoss: 1592957.125000\n",
            "Train Epoch: 56 [1120/7471 (15%)]\tLoss: 1519969.250000\n",
            "Train Epoch: 56 [1280/7471 (17%)]\tLoss: 1525032.375000\n",
            "Train Epoch: 56 [1440/7471 (19%)]\tLoss: 1550016.750000\n",
            "Train Epoch: 56 [1600/7471 (21%)]\tLoss: 1610154.125000\n",
            "Train Epoch: 56 [1760/7471 (24%)]\tLoss: 1570467.750000\n",
            "Train Epoch: 56 [1920/7471 (26%)]\tLoss: 1590605.375000\n",
            "Train Epoch: 56 [2080/7471 (28%)]\tLoss: 1476956.125000\n",
            "Train Epoch: 56 [2240/7471 (30%)]\tLoss: 1597669.375000\n",
            "Train Epoch: 56 [2400/7471 (32%)]\tLoss: 1649476.250000\n",
            "Train Epoch: 56 [2560/7471 (34%)]\tLoss: 1567092.750000\n",
            "Train Epoch: 56 [2720/7471 (36%)]\tLoss: 1544365.875000\n",
            "Train Epoch: 56 [2880/7471 (39%)]\tLoss: 1557029.500000\n",
            "Train Epoch: 56 [3040/7471 (41%)]\tLoss: 1614865.875000\n",
            "Train Epoch: 56 [3200/7471 (43%)]\tLoss: 1587127.750000\n",
            "Train Epoch: 56 [3360/7471 (45%)]\tLoss: 1546827.375000\n",
            "Train Epoch: 56 [3520/7471 (47%)]\tLoss: 1593554.375000\n",
            "Train Epoch: 56 [3680/7471 (49%)]\tLoss: 1643182.000000\n",
            "Train Epoch: 56 [3840/7471 (51%)]\tLoss: 1579069.250000\n",
            "Train Epoch: 56 [4000/7471 (54%)]\tLoss: 1602214.875000\n",
            "Train Epoch: 56 [4160/7471 (56%)]\tLoss: 1626907.000000\n",
            "Train Epoch: 56 [4320/7471 (58%)]\tLoss: 1588841.625000\n",
            "Train Epoch: 56 [4480/7471 (60%)]\tLoss: 1620462.625000\n",
            "Train Epoch: 56 [4640/7471 (62%)]\tLoss: 1504633.750000\n",
            "Train Epoch: 56 [4800/7471 (64%)]\tLoss: 1648927.875000\n",
            "Train Epoch: 56 [4960/7471 (66%)]\tLoss: 1599379.125000\n",
            "Train Epoch: 56 [5120/7471 (69%)]\tLoss: 1504390.500000\n",
            "Train Epoch: 56 [5280/7471 (71%)]\tLoss: 1626926.125000\n",
            "Train Epoch: 56 [5440/7471 (73%)]\tLoss: 1525977.250000\n",
            "Train Epoch: 56 [5600/7471 (75%)]\tLoss: 1560920.125000\n",
            "Train Epoch: 56 [5760/7471 (77%)]\tLoss: 1559933.000000\n",
            "Train Epoch: 56 [5920/7471 (79%)]\tLoss: 1571362.375000\n",
            "Train Epoch: 56 [6080/7471 (81%)]\tLoss: 1582832.125000\n",
            "Train Epoch: 56 [6240/7471 (84%)]\tLoss: 1544591.000000\n",
            "Train Epoch: 56 [6400/7471 (86%)]\tLoss: 1520650.875000\n",
            "Train Epoch: 56 [6560/7471 (88%)]\tLoss: 1599550.250000\n",
            "Train Epoch: 56 [6720/7471 (90%)]\tLoss: 1635131.250000\n",
            "Train Epoch: 56 [6880/7471 (92%)]\tLoss: 1526688.125000\n",
            "Train Epoch: 56 [7040/7471 (94%)]\tLoss: 1578709.250000\n",
            "Train Epoch: 56 [7200/7471 (96%)]\tLoss: 1533039.250000\n",
            "Train Epoch: 56 [7360/7471 (99%)]\tLoss: 1618232.500000\n",
            "Epoch 56 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98598.1963\n",
            "\n",
            "Train Epoch: 57 [160/7471 (2%)]\tLoss: 1572212.875000\n",
            "Train Epoch: 57 [320/7471 (4%)]\tLoss: 1555545.500000\n",
            "Train Epoch: 57 [480/7471 (6%)]\tLoss: 1581884.250000\n",
            "Train Epoch: 57 [640/7471 (9%)]\tLoss: 1626617.125000\n",
            "Train Epoch: 57 [800/7471 (11%)]\tLoss: 1584222.625000\n",
            "Train Epoch: 57 [960/7471 (13%)]\tLoss: 1537974.250000\n",
            "Train Epoch: 57 [1120/7471 (15%)]\tLoss: 1494748.000000\n",
            "Train Epoch: 57 [1280/7471 (17%)]\tLoss: 1528745.375000\n",
            "Train Epoch: 57 [1440/7471 (19%)]\tLoss: 1596751.250000\n",
            "Train Epoch: 57 [1600/7471 (21%)]\tLoss: 1640793.375000\n",
            "Train Epoch: 57 [1760/7471 (24%)]\tLoss: 1556347.375000\n",
            "Train Epoch: 57 [1920/7471 (26%)]\tLoss: 1602971.000000\n",
            "Train Epoch: 57 [2080/7471 (28%)]\tLoss: 1609091.125000\n",
            "Train Epoch: 57 [2240/7471 (30%)]\tLoss: 1554674.250000\n",
            "Train Epoch: 57 [2400/7471 (32%)]\tLoss: 1572425.125000\n",
            "Train Epoch: 57 [2560/7471 (34%)]\tLoss: 1552727.250000\n",
            "Train Epoch: 57 [2720/7471 (36%)]\tLoss: 1595777.625000\n",
            "Train Epoch: 57 [2880/7471 (39%)]\tLoss: 1608742.750000\n",
            "Train Epoch: 57 [3040/7471 (41%)]\tLoss: 1562902.375000\n",
            "Train Epoch: 57 [3200/7471 (43%)]\tLoss: 1603074.125000\n",
            "Train Epoch: 57 [3360/7471 (45%)]\tLoss: 1617112.625000\n",
            "Train Epoch: 57 [3520/7471 (47%)]\tLoss: 1620204.500000\n",
            "Train Epoch: 57 [3680/7471 (49%)]\tLoss: 1591914.125000\n",
            "Train Epoch: 57 [3840/7471 (51%)]\tLoss: 1558233.500000\n",
            "Train Epoch: 57 [4000/7471 (54%)]\tLoss: 1563282.875000\n",
            "Train Epoch: 57 [4160/7471 (56%)]\tLoss: 1611685.750000\n",
            "Train Epoch: 57 [4320/7471 (58%)]\tLoss: 1553912.000000\n",
            "Train Epoch: 57 [4480/7471 (60%)]\tLoss: 1515292.125000\n",
            "Train Epoch: 57 [4640/7471 (62%)]\tLoss: 1623388.875000\n",
            "Train Epoch: 57 [4800/7471 (64%)]\tLoss: 1566662.000000\n",
            "Train Epoch: 57 [4960/7471 (66%)]\tLoss: 1524004.000000\n",
            "Train Epoch: 57 [5120/7471 (69%)]\tLoss: 1607987.250000\n",
            "Train Epoch: 57 [5280/7471 (71%)]\tLoss: 1524865.250000\n",
            "Train Epoch: 57 [5440/7471 (73%)]\tLoss: 1626453.875000\n",
            "Train Epoch: 57 [5600/7471 (75%)]\tLoss: 1640496.375000\n",
            "Train Epoch: 57 [5760/7471 (77%)]\tLoss: 1620647.750000\n",
            "Train Epoch: 57 [5920/7471 (79%)]\tLoss: 1520981.375000\n",
            "Train Epoch: 57 [6080/7471 (81%)]\tLoss: 1554905.000000\n",
            "Train Epoch: 57 [6240/7471 (84%)]\tLoss: 1634283.500000\n",
            "Train Epoch: 57 [6400/7471 (86%)]\tLoss: 1564017.250000\n",
            "Train Epoch: 57 [6560/7471 (88%)]\tLoss: 1586993.375000\n",
            "Train Epoch: 57 [6720/7471 (90%)]\tLoss: 1619238.875000\n",
            "Train Epoch: 57 [6880/7471 (92%)]\tLoss: 1580614.375000\n",
            "Train Epoch: 57 [7040/7471 (94%)]\tLoss: 1539775.125000\n",
            "Train Epoch: 57 [7200/7471 (96%)]\tLoss: 1590242.125000\n",
            "Train Epoch: 57 [7360/7471 (99%)]\tLoss: 1541983.125000\n",
            "Epoch 57 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98584.1948\n",
            "\n",
            "Train Epoch: 58 [160/7471 (2%)]\tLoss: 1549913.000000\n",
            "Train Epoch: 58 [320/7471 (4%)]\tLoss: 1545476.875000\n",
            "Train Epoch: 58 [480/7471 (6%)]\tLoss: 1593402.875000\n",
            "Train Epoch: 58 [640/7471 (9%)]\tLoss: 1609287.500000\n",
            "Train Epoch: 58 [800/7471 (11%)]\tLoss: 1600863.500000\n",
            "Train Epoch: 58 [960/7471 (13%)]\tLoss: 1550538.250000\n",
            "Train Epoch: 58 [1120/7471 (15%)]\tLoss: 1566993.250000\n",
            "Train Epoch: 58 [1280/7471 (17%)]\tLoss: 1593992.250000\n",
            "Train Epoch: 58 [1440/7471 (19%)]\tLoss: 1570781.875000\n",
            "Train Epoch: 58 [1600/7471 (21%)]\tLoss: 1624733.000000\n",
            "Train Epoch: 58 [1760/7471 (24%)]\tLoss: 1552496.250000\n",
            "Train Epoch: 58 [1920/7471 (26%)]\tLoss: 1569610.625000\n",
            "Train Epoch: 58 [2080/7471 (28%)]\tLoss: 1546566.500000\n",
            "Train Epoch: 58 [2240/7471 (30%)]\tLoss: 1608491.375000\n",
            "Train Epoch: 58 [2400/7471 (32%)]\tLoss: 1564913.375000\n",
            "Train Epoch: 58 [2560/7471 (34%)]\tLoss: 1630935.250000\n",
            "Train Epoch: 58 [2720/7471 (36%)]\tLoss: 1559623.250000\n",
            "Train Epoch: 58 [2880/7471 (39%)]\tLoss: 1622428.375000\n",
            "Train Epoch: 58 [3040/7471 (41%)]\tLoss: 1601566.125000\n",
            "Train Epoch: 58 [3200/7471 (43%)]\tLoss: 1635933.625000\n",
            "Train Epoch: 58 [3360/7471 (45%)]\tLoss: 1582119.125000\n",
            "Train Epoch: 58 [3520/7471 (47%)]\tLoss: 1586135.125000\n",
            "Train Epoch: 58 [3680/7471 (49%)]\tLoss: 1600863.750000\n",
            "Train Epoch: 58 [3840/7471 (51%)]\tLoss: 1578074.750000\n",
            "Train Epoch: 58 [4000/7471 (54%)]\tLoss: 1524630.250000\n",
            "Train Epoch: 58 [4160/7471 (56%)]\tLoss: 1571538.250000\n",
            "Train Epoch: 58 [4320/7471 (58%)]\tLoss: 1566635.000000\n",
            "Train Epoch: 58 [4480/7471 (60%)]\tLoss: 1592891.500000\n",
            "Train Epoch: 58 [4640/7471 (62%)]\tLoss: 1609792.250000\n",
            "Train Epoch: 58 [4800/7471 (64%)]\tLoss: 1591127.750000\n",
            "Train Epoch: 58 [4960/7471 (66%)]\tLoss: 1546163.500000\n",
            "Train Epoch: 58 [5120/7471 (69%)]\tLoss: 1576816.500000\n",
            "Train Epoch: 58 [5280/7471 (71%)]\tLoss: 1588179.875000\n",
            "Train Epoch: 58 [5440/7471 (73%)]\tLoss: 1579636.500000\n",
            "Train Epoch: 58 [5600/7471 (75%)]\tLoss: 1568389.250000\n",
            "Train Epoch: 58 [5760/7471 (77%)]\tLoss: 1629198.875000\n",
            "Train Epoch: 58 [5920/7471 (79%)]\tLoss: 1563530.000000\n",
            "Train Epoch: 58 [6080/7471 (81%)]\tLoss: 1600997.750000\n",
            "Train Epoch: 58 [6240/7471 (84%)]\tLoss: 1608527.125000\n",
            "Train Epoch: 58 [6400/7471 (86%)]\tLoss: 1566345.000000\n",
            "Train Epoch: 58 [6560/7471 (88%)]\tLoss: 1616374.000000\n",
            "Train Epoch: 58 [6720/7471 (90%)]\tLoss: 1583143.375000\n",
            "Train Epoch: 58 [6880/7471 (92%)]\tLoss: 1553582.250000\n",
            "Train Epoch: 58 [7040/7471 (94%)]\tLoss: 1559102.500000\n",
            "Train Epoch: 58 [7200/7471 (96%)]\tLoss: 1528204.875000\n",
            "Train Epoch: 58 [7360/7471 (99%)]\tLoss: 1579766.750000\n",
            "Epoch 58 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98671.6995\n",
            "\n",
            "Train Epoch: 59 [160/7471 (2%)]\tLoss: 1522576.500000\n",
            "Train Epoch: 59 [320/7471 (4%)]\tLoss: 1595441.500000\n",
            "Train Epoch: 59 [480/7471 (6%)]\tLoss: 1599500.500000\n",
            "Train Epoch: 59 [640/7471 (9%)]\tLoss: 1623048.625000\n",
            "Train Epoch: 59 [800/7471 (11%)]\tLoss: 1537039.625000\n",
            "Train Epoch: 59 [960/7471 (13%)]\tLoss: 1612376.750000\n",
            "Train Epoch: 59 [1120/7471 (15%)]\tLoss: 1515690.250000\n",
            "Train Epoch: 59 [1280/7471 (17%)]\tLoss: 1574136.500000\n",
            "Train Epoch: 59 [1440/7471 (19%)]\tLoss: 1562379.125000\n",
            "Train Epoch: 59 [1600/7471 (21%)]\tLoss: 1562160.000000\n",
            "Train Epoch: 59 [1760/7471 (24%)]\tLoss: 1583966.125000\n",
            "Train Epoch: 59 [1920/7471 (26%)]\tLoss: 1600788.000000\n",
            "Train Epoch: 59 [2080/7471 (28%)]\tLoss: 1501805.375000\n",
            "Train Epoch: 59 [2240/7471 (30%)]\tLoss: 1564758.875000\n",
            "Train Epoch: 59 [2400/7471 (32%)]\tLoss: 1581284.500000\n",
            "Train Epoch: 59 [2560/7471 (34%)]\tLoss: 1542471.750000\n",
            "Train Epoch: 59 [2720/7471 (36%)]\tLoss: 1534741.500000\n",
            "Train Epoch: 59 [2880/7471 (39%)]\tLoss: 1549051.125000\n",
            "Train Epoch: 59 [3040/7471 (41%)]\tLoss: 1560492.125000\n",
            "Train Epoch: 59 [3200/7471 (43%)]\tLoss: 1586109.750000\n",
            "Train Epoch: 59 [3360/7471 (45%)]\tLoss: 1612132.125000\n",
            "Train Epoch: 59 [3520/7471 (47%)]\tLoss: 1511895.875000\n",
            "Train Epoch: 59 [3680/7471 (49%)]\tLoss: 1595003.625000\n",
            "Train Epoch: 59 [3840/7471 (51%)]\tLoss: 1609806.875000\n",
            "Train Epoch: 59 [4000/7471 (54%)]\tLoss: 1527434.875000\n",
            "Train Epoch: 59 [4160/7471 (56%)]\tLoss: 1545098.875000\n",
            "Train Epoch: 59 [4320/7471 (58%)]\tLoss: 1580372.750000\n",
            "Train Epoch: 59 [4480/7471 (60%)]\tLoss: 1542532.500000\n",
            "Train Epoch: 59 [4640/7471 (62%)]\tLoss: 1592521.500000\n",
            "Train Epoch: 59 [4800/7471 (64%)]\tLoss: 1552819.125000\n",
            "Train Epoch: 59 [4960/7471 (66%)]\tLoss: 1563264.375000\n",
            "Train Epoch: 59 [5120/7471 (69%)]\tLoss: 1582020.625000\n",
            "Train Epoch: 59 [5280/7471 (71%)]\tLoss: 1575154.250000\n",
            "Train Epoch: 59 [5440/7471 (73%)]\tLoss: 1566490.375000\n",
            "Train Epoch: 59 [5600/7471 (75%)]\tLoss: 1586694.250000\n",
            "Train Epoch: 59 [5760/7471 (77%)]\tLoss: 1606059.375000\n",
            "Train Epoch: 59 [5920/7471 (79%)]\tLoss: 1625530.250000\n",
            "Train Epoch: 59 [6080/7471 (81%)]\tLoss: 1615997.625000\n",
            "Train Epoch: 59 [6240/7471 (84%)]\tLoss: 1542348.250000\n",
            "Train Epoch: 59 [6400/7471 (86%)]\tLoss: 1629745.000000\n",
            "Train Epoch: 59 [6560/7471 (88%)]\tLoss: 1584383.750000\n",
            "Train Epoch: 59 [6720/7471 (90%)]\tLoss: 1578286.375000\n",
            "Train Epoch: 59 [6880/7471 (92%)]\tLoss: 1538938.750000\n",
            "Train Epoch: 59 [7040/7471 (94%)]\tLoss: 1564959.875000\n",
            "Train Epoch: 59 [7200/7471 (96%)]\tLoss: 1594112.375000\n",
            "Train Epoch: 59 [7360/7471 (99%)]\tLoss: 1547347.500000\n",
            "Epoch 59 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98566.4475\n",
            "\n",
            "Train Epoch: 60 [160/7471 (2%)]\tLoss: 1616310.125000\n",
            "Train Epoch: 60 [320/7471 (4%)]\tLoss: 1599511.750000\n",
            "Train Epoch: 60 [480/7471 (6%)]\tLoss: 1622157.125000\n",
            "Train Epoch: 60 [640/7471 (9%)]\tLoss: 1613219.000000\n",
            "Train Epoch: 60 [800/7471 (11%)]\tLoss: 1566761.875000\n",
            "Train Epoch: 60 [960/7471 (13%)]\tLoss: 1530043.375000\n",
            "Train Epoch: 60 [1120/7471 (15%)]\tLoss: 1588026.375000\n",
            "Train Epoch: 60 [1280/7471 (17%)]\tLoss: 1601484.875000\n",
            "Train Epoch: 60 [1440/7471 (19%)]\tLoss: 1529981.500000\n",
            "Train Epoch: 60 [1600/7471 (21%)]\tLoss: 1553933.250000\n",
            "Train Epoch: 60 [1760/7471 (24%)]\tLoss: 1534061.375000\n",
            "Train Epoch: 60 [1920/7471 (26%)]\tLoss: 1550703.250000\n",
            "Train Epoch: 60 [2080/7471 (28%)]\tLoss: 1562380.250000\n",
            "Train Epoch: 60 [2240/7471 (30%)]\tLoss: 1626125.250000\n",
            "Train Epoch: 60 [2400/7471 (32%)]\tLoss: 1597388.625000\n",
            "Train Epoch: 60 [2560/7471 (34%)]\tLoss: 1498892.750000\n",
            "Train Epoch: 60 [2720/7471 (36%)]\tLoss: 1526304.500000\n",
            "Train Epoch: 60 [2880/7471 (39%)]\tLoss: 1598174.875000\n",
            "Train Epoch: 60 [3040/7471 (41%)]\tLoss: 1561219.875000\n",
            "Train Epoch: 60 [3200/7471 (43%)]\tLoss: 1594421.875000\n",
            "Train Epoch: 60 [3360/7471 (45%)]\tLoss: 1587053.500000\n",
            "Train Epoch: 60 [3520/7471 (47%)]\tLoss: 1532564.750000\n",
            "Train Epoch: 60 [3680/7471 (49%)]\tLoss: 1591417.250000\n",
            "Train Epoch: 60 [3840/7471 (51%)]\tLoss: 1625815.125000\n",
            "Train Epoch: 60 [4000/7471 (54%)]\tLoss: 1508454.500000\n",
            "Train Epoch: 60 [4160/7471 (56%)]\tLoss: 1552652.125000\n",
            "Train Epoch: 60 [4320/7471 (58%)]\tLoss: 1603968.750000\n",
            "Train Epoch: 60 [4480/7471 (60%)]\tLoss: 1571734.625000\n",
            "Train Epoch: 60 [4640/7471 (62%)]\tLoss: 1542038.375000\n",
            "Train Epoch: 60 [4800/7471 (64%)]\tLoss: 1539668.875000\n",
            "Train Epoch: 60 [4960/7471 (66%)]\tLoss: 1600006.875000\n",
            "Train Epoch: 60 [5120/7471 (69%)]\tLoss: 1575169.625000\n",
            "Train Epoch: 60 [5280/7471 (71%)]\tLoss: 1619464.375000\n",
            "Train Epoch: 60 [5440/7471 (73%)]\tLoss: 1522759.250000\n",
            "Train Epoch: 60 [5600/7471 (75%)]\tLoss: 1573575.875000\n",
            "Train Epoch: 60 [5760/7471 (77%)]\tLoss: 1622343.250000\n",
            "Train Epoch: 60 [5920/7471 (79%)]\tLoss: 1611878.000000\n",
            "Train Epoch: 60 [6080/7471 (81%)]\tLoss: 1531475.625000\n",
            "Train Epoch: 60 [6240/7471 (84%)]\tLoss: 1552444.625000\n",
            "Train Epoch: 60 [6400/7471 (86%)]\tLoss: 1543035.500000\n",
            "Train Epoch: 60 [6560/7471 (88%)]\tLoss: 1591894.500000\n",
            "Train Epoch: 60 [6720/7471 (90%)]\tLoss: 1578150.875000\n",
            "Train Epoch: 60 [6880/7471 (92%)]\tLoss: 1564408.875000\n",
            "Train Epoch: 60 [7040/7471 (94%)]\tLoss: 1544952.625000\n",
            "Train Epoch: 60 [7200/7471 (96%)]\tLoss: 1546758.000000\n",
            "Train Epoch: 60 [7360/7471 (99%)]\tLoss: 1610163.500000\n",
            "Epoch 60 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98590.6832\n",
            "\n",
            "Train Epoch: 61 [160/7471 (2%)]\tLoss: 1626120.750000\n",
            "Train Epoch: 61 [320/7471 (4%)]\tLoss: 1595785.375000\n",
            "Train Epoch: 61 [480/7471 (6%)]\tLoss: 1512356.500000\n",
            "Train Epoch: 61 [640/7471 (9%)]\tLoss: 1629493.250000\n",
            "Train Epoch: 61 [800/7471 (11%)]\tLoss: 1566918.125000\n",
            "Train Epoch: 61 [960/7471 (13%)]\tLoss: 1577209.375000\n",
            "Train Epoch: 61 [1120/7471 (15%)]\tLoss: 1538974.500000\n",
            "Train Epoch: 61 [1280/7471 (17%)]\tLoss: 1558791.500000\n",
            "Train Epoch: 61 [1440/7471 (19%)]\tLoss: 1628241.750000\n",
            "Train Epoch: 61 [1600/7471 (21%)]\tLoss: 1618684.250000\n",
            "Train Epoch: 61 [1760/7471 (24%)]\tLoss: 1569906.750000\n",
            "Train Epoch: 61 [1920/7471 (26%)]\tLoss: 1566386.875000\n",
            "Train Epoch: 61 [2080/7471 (28%)]\tLoss: 1581768.500000\n",
            "Train Epoch: 61 [2240/7471 (30%)]\tLoss: 1594148.000000\n",
            "Train Epoch: 61 [2400/7471 (32%)]\tLoss: 1568149.875000\n",
            "Train Epoch: 61 [2560/7471 (34%)]\tLoss: 1597597.125000\n",
            "Train Epoch: 61 [2720/7471 (36%)]\tLoss: 1589714.875000\n",
            "Train Epoch: 61 [2880/7471 (39%)]\tLoss: 1543605.625000\n",
            "Train Epoch: 61 [3040/7471 (41%)]\tLoss: 1616698.500000\n",
            "Train Epoch: 61 [3200/7471 (43%)]\tLoss: 1617438.250000\n",
            "Train Epoch: 61 [3360/7471 (45%)]\tLoss: 1624937.000000\n",
            "Train Epoch: 61 [3520/7471 (47%)]\tLoss: 1571450.375000\n",
            "Train Epoch: 61 [3680/7471 (49%)]\tLoss: 1543937.375000\n",
            "Train Epoch: 61 [3840/7471 (51%)]\tLoss: 1566256.750000\n",
            "Train Epoch: 61 [4000/7471 (54%)]\tLoss: 1600753.125000\n",
            "Train Epoch: 61 [4160/7471 (56%)]\tLoss: 1578124.250000\n",
            "Train Epoch: 61 [4320/7471 (58%)]\tLoss: 1615225.375000\n",
            "Train Epoch: 61 [4480/7471 (60%)]\tLoss: 1609885.375000\n",
            "Train Epoch: 61 [4640/7471 (62%)]\tLoss: 1611924.250000\n",
            "Train Epoch: 61 [4800/7471 (64%)]\tLoss: 1579668.750000\n",
            "Train Epoch: 61 [4960/7471 (66%)]\tLoss: 1602186.625000\n",
            "Train Epoch: 61 [5120/7471 (69%)]\tLoss: 1589091.125000\n",
            "Train Epoch: 61 [5280/7471 (71%)]\tLoss: 1633823.250000\n",
            "Train Epoch: 61 [5440/7471 (73%)]\tLoss: 1520223.500000\n",
            "Train Epoch: 61 [5600/7471 (75%)]\tLoss: 1604707.625000\n",
            "Train Epoch: 61 [5760/7471 (77%)]\tLoss: 1618964.875000\n",
            "Train Epoch: 61 [5920/7471 (79%)]\tLoss: 1608344.750000\n",
            "Train Epoch: 61 [6080/7471 (81%)]\tLoss: 1564239.125000\n",
            "Train Epoch: 61 [6240/7471 (84%)]\tLoss: 1529174.500000\n",
            "Train Epoch: 61 [6400/7471 (86%)]\tLoss: 1608838.125000\n",
            "Train Epoch: 61 [6560/7471 (88%)]\tLoss: 1539675.875000\n",
            "Train Epoch: 61 [6720/7471 (90%)]\tLoss: 1617205.000000\n",
            "Train Epoch: 61 [6880/7471 (92%)]\tLoss: 1627341.000000\n",
            "Train Epoch: 61 [7040/7471 (94%)]\tLoss: 1601694.750000\n",
            "Train Epoch: 61 [7200/7471 (96%)]\tLoss: 1500294.250000\n",
            "Train Epoch: 61 [7360/7471 (99%)]\tLoss: 1604112.125000\n",
            "Epoch 61 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98534.8911\n",
            "\n",
            "Train Epoch: 62 [160/7471 (2%)]\tLoss: 1594415.000000\n",
            "Train Epoch: 62 [320/7471 (4%)]\tLoss: 1528453.250000\n",
            "Train Epoch: 62 [480/7471 (6%)]\tLoss: 1515119.375000\n",
            "Train Epoch: 62 [640/7471 (9%)]\tLoss: 1650063.375000\n",
            "Train Epoch: 62 [800/7471 (11%)]\tLoss: 1534624.625000\n",
            "Train Epoch: 62 [960/7471 (13%)]\tLoss: 1605384.375000\n",
            "Train Epoch: 62 [1120/7471 (15%)]\tLoss: 1499732.500000\n",
            "Train Epoch: 62 [1280/7471 (17%)]\tLoss: 1590701.750000\n",
            "Train Epoch: 62 [1440/7471 (19%)]\tLoss: 1568343.375000\n",
            "Train Epoch: 62 [1600/7471 (21%)]\tLoss: 1581985.625000\n",
            "Train Epoch: 62 [1760/7471 (24%)]\tLoss: 1511473.375000\n",
            "Train Epoch: 62 [1920/7471 (26%)]\tLoss: 1582584.875000\n",
            "Train Epoch: 62 [2080/7471 (28%)]\tLoss: 1528797.000000\n",
            "Train Epoch: 62 [2240/7471 (30%)]\tLoss: 1580281.625000\n",
            "Train Epoch: 62 [2400/7471 (32%)]\tLoss: 1592196.125000\n",
            "Train Epoch: 62 [2560/7471 (34%)]\tLoss: 1597916.000000\n",
            "Train Epoch: 62 [2720/7471 (36%)]\tLoss: 1589399.875000\n",
            "Train Epoch: 62 [2880/7471 (39%)]\tLoss: 1526424.000000\n",
            "Train Epoch: 62 [3040/7471 (41%)]\tLoss: 1604601.750000\n",
            "Train Epoch: 62 [3200/7471 (43%)]\tLoss: 1534370.375000\n",
            "Train Epoch: 62 [3360/7471 (45%)]\tLoss: 1607981.125000\n",
            "Train Epoch: 62 [3520/7471 (47%)]\tLoss: 1557078.125000\n",
            "Train Epoch: 62 [3680/7471 (49%)]\tLoss: 1608506.375000\n",
            "Train Epoch: 62 [3840/7471 (51%)]\tLoss: 1575430.500000\n",
            "Train Epoch: 62 [4000/7471 (54%)]\tLoss: 1626652.000000\n",
            "Train Epoch: 62 [4160/7471 (56%)]\tLoss: 1615517.000000\n",
            "Train Epoch: 62 [4320/7471 (58%)]\tLoss: 1574646.125000\n",
            "Train Epoch: 62 [4480/7471 (60%)]\tLoss: 1595710.750000\n",
            "Train Epoch: 62 [4640/7471 (62%)]\tLoss: 1573339.000000\n",
            "Train Epoch: 62 [4800/7471 (64%)]\tLoss: 1564299.250000\n",
            "Train Epoch: 62 [4960/7471 (66%)]\tLoss: 1618410.125000\n",
            "Train Epoch: 62 [5120/7471 (69%)]\tLoss: 1632161.625000\n",
            "Train Epoch: 62 [5280/7471 (71%)]\tLoss: 1578105.125000\n",
            "Train Epoch: 62 [5440/7471 (73%)]\tLoss: 1503605.500000\n",
            "Train Epoch: 62 [5600/7471 (75%)]\tLoss: 1521217.250000\n",
            "Train Epoch: 62 [5760/7471 (77%)]\tLoss: 1589960.375000\n",
            "Train Epoch: 62 [5920/7471 (79%)]\tLoss: 1555083.875000\n",
            "Train Epoch: 62 [6080/7471 (81%)]\tLoss: 1575207.375000\n",
            "Train Epoch: 62 [6240/7471 (84%)]\tLoss: 1598837.750000\n",
            "Train Epoch: 62 [6400/7471 (86%)]\tLoss: 1543928.250000\n",
            "Train Epoch: 62 [6560/7471 (88%)]\tLoss: 1602317.500000\n",
            "Train Epoch: 62 [6720/7471 (90%)]\tLoss: 1566478.750000\n",
            "Train Epoch: 62 [6880/7471 (92%)]\tLoss: 1624488.250000\n",
            "Train Epoch: 62 [7040/7471 (94%)]\tLoss: 1552079.625000\n",
            "Train Epoch: 62 [7200/7471 (96%)]\tLoss: 1608069.500000\n",
            "Train Epoch: 62 [7360/7471 (99%)]\tLoss: 1575081.625000\n",
            "Epoch 62 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98521.2109\n",
            "\n",
            "Train Epoch: 63 [160/7471 (2%)]\tLoss: 1586357.500000\n",
            "Train Epoch: 63 [320/7471 (4%)]\tLoss: 1549684.750000\n",
            "Train Epoch: 63 [480/7471 (6%)]\tLoss: 1621389.250000\n",
            "Train Epoch: 63 [640/7471 (9%)]\tLoss: 1578824.875000\n",
            "Train Epoch: 63 [800/7471 (11%)]\tLoss: 1626627.750000\n",
            "Train Epoch: 63 [960/7471 (13%)]\tLoss: 1579660.250000\n",
            "Train Epoch: 63 [1120/7471 (15%)]\tLoss: 1566030.375000\n",
            "Train Epoch: 63 [1280/7471 (17%)]\tLoss: 1625935.000000\n",
            "Train Epoch: 63 [1440/7471 (19%)]\tLoss: 1615292.625000\n",
            "Train Epoch: 63 [1600/7471 (21%)]\tLoss: 1560971.750000\n",
            "Train Epoch: 63 [1760/7471 (24%)]\tLoss: 1522508.875000\n",
            "Train Epoch: 63 [1920/7471 (26%)]\tLoss: 1580523.125000\n",
            "Train Epoch: 63 [2080/7471 (28%)]\tLoss: 1588127.000000\n",
            "Train Epoch: 63 [2240/7471 (30%)]\tLoss: 1584694.125000\n",
            "Train Epoch: 63 [2400/7471 (32%)]\tLoss: 1582879.750000\n",
            "Train Epoch: 63 [2560/7471 (34%)]\tLoss: 1584647.750000\n",
            "Train Epoch: 63 [2720/7471 (36%)]\tLoss: 1559974.375000\n",
            "Train Epoch: 63 [2880/7471 (39%)]\tLoss: 1570271.000000\n",
            "Train Epoch: 63 [3040/7471 (41%)]\tLoss: 1495432.500000\n",
            "Train Epoch: 63 [3200/7471 (43%)]\tLoss: 1569905.375000\n",
            "Train Epoch: 63 [3360/7471 (45%)]\tLoss: 1632400.125000\n",
            "Train Epoch: 63 [3520/7471 (47%)]\tLoss: 1627480.625000\n",
            "Train Epoch: 63 [3680/7471 (49%)]\tLoss: 1569944.125000\n",
            "Train Epoch: 63 [3840/7471 (51%)]\tLoss: 1570277.500000\n",
            "Train Epoch: 63 [4000/7471 (54%)]\tLoss: 1556156.125000\n",
            "Train Epoch: 63 [4160/7471 (56%)]\tLoss: 1552488.875000\n",
            "Train Epoch: 63 [4320/7471 (58%)]\tLoss: 1532401.750000\n",
            "Train Epoch: 63 [4480/7471 (60%)]\tLoss: 1592886.875000\n",
            "Train Epoch: 63 [4640/7471 (62%)]\tLoss: 1602952.625000\n",
            "Train Epoch: 63 [4800/7471 (64%)]\tLoss: 1570187.875000\n",
            "Train Epoch: 63 [4960/7471 (66%)]\tLoss: 1632122.125000\n",
            "Train Epoch: 63 [5120/7471 (69%)]\tLoss: 1604307.375000\n",
            "Train Epoch: 63 [5280/7471 (71%)]\tLoss: 1615001.375000\n",
            "Train Epoch: 63 [5440/7471 (73%)]\tLoss: 1597046.250000\n",
            "Train Epoch: 63 [5600/7471 (75%)]\tLoss: 1602162.125000\n",
            "Train Epoch: 63 [5760/7471 (77%)]\tLoss: 1595481.875000\n",
            "Train Epoch: 63 [5920/7471 (79%)]\tLoss: 1637029.375000\n",
            "Train Epoch: 63 [6080/7471 (81%)]\tLoss: 1582139.250000\n",
            "Train Epoch: 63 [6240/7471 (84%)]\tLoss: 1621515.500000\n",
            "Train Epoch: 63 [6400/7471 (86%)]\tLoss: 1538853.625000\n",
            "Train Epoch: 63 [6560/7471 (88%)]\tLoss: 1579242.000000\n",
            "Train Epoch: 63 [6720/7471 (90%)]\tLoss: 1530285.000000\n",
            "Train Epoch: 63 [6880/7471 (92%)]\tLoss: 1567759.375000\n",
            "Train Epoch: 63 [7040/7471 (94%)]\tLoss: 1552351.250000\n",
            "Train Epoch: 63 [7200/7471 (96%)]\tLoss: 1594416.250000\n",
            "Train Epoch: 63 [7360/7471 (99%)]\tLoss: 1595598.250000\n",
            "Epoch 63 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98574.8725\n",
            "\n",
            "Train Epoch: 64 [160/7471 (2%)]\tLoss: 1613497.375000\n",
            "Train Epoch: 64 [320/7471 (4%)]\tLoss: 1583599.375000\n",
            "Train Epoch: 64 [480/7471 (6%)]\tLoss: 1588875.125000\n",
            "Train Epoch: 64 [640/7471 (9%)]\tLoss: 1568137.625000\n",
            "Train Epoch: 64 [800/7471 (11%)]\tLoss: 1516114.500000\n",
            "Train Epoch: 64 [960/7471 (13%)]\tLoss: 1567470.375000\n",
            "Train Epoch: 64 [1120/7471 (15%)]\tLoss: 1598300.375000\n",
            "Train Epoch: 64 [1280/7471 (17%)]\tLoss: 1617963.250000\n",
            "Train Epoch: 64 [1440/7471 (19%)]\tLoss: 1625808.250000\n",
            "Train Epoch: 64 [1600/7471 (21%)]\tLoss: 1591862.500000\n",
            "Train Epoch: 64 [1760/7471 (24%)]\tLoss: 1557563.750000\n",
            "Train Epoch: 64 [1920/7471 (26%)]\tLoss: 1596936.125000\n",
            "Train Epoch: 64 [2080/7471 (28%)]\tLoss: 1583402.500000\n",
            "Train Epoch: 64 [2240/7471 (30%)]\tLoss: 1604676.375000\n",
            "Train Epoch: 64 [2400/7471 (32%)]\tLoss: 1566019.000000\n",
            "Train Epoch: 64 [2560/7471 (34%)]\tLoss: 1579500.250000\n",
            "Train Epoch: 64 [2720/7471 (36%)]\tLoss: 1587231.125000\n",
            "Train Epoch: 64 [2880/7471 (39%)]\tLoss: 1588505.250000\n",
            "Train Epoch: 64 [3040/7471 (41%)]\tLoss: 1560711.000000\n",
            "Train Epoch: 64 [3200/7471 (43%)]\tLoss: 1557356.250000\n",
            "Train Epoch: 64 [3360/7471 (45%)]\tLoss: 1620185.250000\n",
            "Train Epoch: 64 [3520/7471 (47%)]\tLoss: 1608733.125000\n",
            "Train Epoch: 64 [3680/7471 (49%)]\tLoss: 1493484.375000\n",
            "Train Epoch: 64 [3840/7471 (51%)]\tLoss: 1560059.250000\n",
            "Train Epoch: 64 [4000/7471 (54%)]\tLoss: 1564269.125000\n",
            "Train Epoch: 64 [4160/7471 (56%)]\tLoss: 1544529.750000\n",
            "Train Epoch: 64 [4320/7471 (58%)]\tLoss: 1615078.375000\n",
            "Train Epoch: 64 [4480/7471 (60%)]\tLoss: 1581826.250000\n",
            "Train Epoch: 64 [4640/7471 (62%)]\tLoss: 1585245.375000\n",
            "Train Epoch: 64 [4800/7471 (64%)]\tLoss: 1587598.000000\n",
            "Train Epoch: 64 [4960/7471 (66%)]\tLoss: 1608852.875000\n",
            "Train Epoch: 64 [5120/7471 (69%)]\tLoss: 1577378.375000\n",
            "Train Epoch: 64 [5280/7471 (71%)]\tLoss: 1622759.875000\n",
            "Train Epoch: 64 [5440/7471 (73%)]\tLoss: 1561058.000000\n",
            "Train Epoch: 64 [5600/7471 (75%)]\tLoss: 1518686.000000\n",
            "Train Epoch: 64 [5760/7471 (77%)]\tLoss: 1539594.000000\n",
            "Train Epoch: 64 [5920/7471 (79%)]\tLoss: 1622682.500000\n",
            "Train Epoch: 64 [6080/7471 (81%)]\tLoss: 1624458.875000\n",
            "Train Epoch: 64 [6240/7471 (84%)]\tLoss: 1512251.500000\n",
            "Train Epoch: 64 [6400/7471 (86%)]\tLoss: 1570385.375000\n",
            "Train Epoch: 64 [6560/7471 (88%)]\tLoss: 1571234.625000\n",
            "Train Epoch: 64 [6720/7471 (90%)]\tLoss: 1616430.250000\n",
            "Train Epoch: 64 [6880/7471 (92%)]\tLoss: 1610618.500000\n",
            "Train Epoch: 64 [7040/7471 (94%)]\tLoss: 1583030.250000\n",
            "Train Epoch: 64 [7200/7471 (96%)]\tLoss: 1541065.250000\n",
            "Train Epoch: 64 [7360/7471 (99%)]\tLoss: 1590209.125000\n",
            "Epoch 64 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98532.6321\n",
            "\n",
            "Train Epoch: 65 [160/7471 (2%)]\tLoss: 1520193.750000\n",
            "Train Epoch: 65 [320/7471 (4%)]\tLoss: 1617007.250000\n",
            "Train Epoch: 65 [480/7471 (6%)]\tLoss: 1590205.000000\n",
            "Train Epoch: 65 [640/7471 (9%)]\tLoss: 1591745.750000\n",
            "Train Epoch: 65 [800/7471 (11%)]\tLoss: 1582867.625000\n",
            "Train Epoch: 65 [960/7471 (13%)]\tLoss: 1606095.625000\n",
            "Train Epoch: 65 [1120/7471 (15%)]\tLoss: 1551547.875000\n",
            "Train Epoch: 65 [1280/7471 (17%)]\tLoss: 1562165.375000\n",
            "Train Epoch: 65 [1440/7471 (19%)]\tLoss: 1615086.000000\n",
            "Train Epoch: 65 [1600/7471 (21%)]\tLoss: 1597669.625000\n",
            "Train Epoch: 65 [1760/7471 (24%)]\tLoss: 1575215.000000\n",
            "Train Epoch: 65 [1920/7471 (26%)]\tLoss: 1624208.125000\n",
            "Train Epoch: 65 [2080/7471 (28%)]\tLoss: 1509318.875000\n",
            "Train Epoch: 65 [2240/7471 (30%)]\tLoss: 1506027.375000\n",
            "Train Epoch: 65 [2400/7471 (32%)]\tLoss: 1618193.625000\n",
            "Train Epoch: 65 [2560/7471 (34%)]\tLoss: 1611752.375000\n",
            "Train Epoch: 65 [2720/7471 (36%)]\tLoss: 1598119.875000\n",
            "Train Epoch: 65 [2880/7471 (39%)]\tLoss: 1493191.250000\n",
            "Train Epoch: 65 [3040/7471 (41%)]\tLoss: 1547006.125000\n",
            "Train Epoch: 65 [3200/7471 (43%)]\tLoss: 1574465.750000\n",
            "Train Epoch: 65 [3360/7471 (45%)]\tLoss: 1626991.500000\n",
            "Train Epoch: 65 [3520/7471 (47%)]\tLoss: 1606502.000000\n",
            "Train Epoch: 65 [3680/7471 (49%)]\tLoss: 1595504.375000\n",
            "Train Epoch: 65 [3840/7471 (51%)]\tLoss: 1603261.750000\n",
            "Train Epoch: 65 [4000/7471 (54%)]\tLoss: 1510692.000000\n",
            "Train Epoch: 65 [4160/7471 (56%)]\tLoss: 1583486.875000\n",
            "Train Epoch: 65 [4320/7471 (58%)]\tLoss: 1618794.250000\n",
            "Train Epoch: 65 [4480/7471 (60%)]\tLoss: 1538513.625000\n",
            "Train Epoch: 65 [4640/7471 (62%)]\tLoss: 1539436.000000\n",
            "Train Epoch: 65 [4800/7471 (64%)]\tLoss: 1564907.625000\n",
            "Train Epoch: 65 [4960/7471 (66%)]\tLoss: 1592330.875000\n",
            "Train Epoch: 65 [5120/7471 (69%)]\tLoss: 1567379.750000\n",
            "Train Epoch: 65 [5280/7471 (71%)]\tLoss: 1471354.000000\n",
            "Train Epoch: 65 [5440/7471 (73%)]\tLoss: 1542351.375000\n",
            "Train Epoch: 65 [5600/7471 (75%)]\tLoss: 1552682.500000\n",
            "Train Epoch: 65 [5760/7471 (77%)]\tLoss: 1610305.000000\n",
            "Train Epoch: 65 [5920/7471 (79%)]\tLoss: 1600940.250000\n",
            "Train Epoch: 65 [6080/7471 (81%)]\tLoss: 1604177.875000\n",
            "Train Epoch: 65 [6240/7471 (84%)]\tLoss: 1647075.500000\n",
            "Train Epoch: 65 [6400/7471 (86%)]\tLoss: 1634561.125000\n",
            "Train Epoch: 65 [6560/7471 (88%)]\tLoss: 1538909.125000\n",
            "Train Epoch: 65 [6720/7471 (90%)]\tLoss: 1601800.875000\n",
            "Train Epoch: 65 [6880/7471 (92%)]\tLoss: 1600202.625000\n",
            "Train Epoch: 65 [7040/7471 (94%)]\tLoss: 1610417.250000\n",
            "Train Epoch: 65 [7200/7471 (96%)]\tLoss: 1537169.500000\n",
            "Train Epoch: 65 [7360/7471 (99%)]\tLoss: 1629184.875000\n",
            "Epoch 65 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98532.1190\n",
            "\n",
            "Train Epoch: 66 [160/7471 (2%)]\tLoss: 1547710.000000\n",
            "Train Epoch: 66 [320/7471 (4%)]\tLoss: 1589372.375000\n",
            "Train Epoch: 66 [480/7471 (6%)]\tLoss: 1573938.875000\n",
            "Train Epoch: 66 [640/7471 (9%)]\tLoss: 1572166.500000\n",
            "Train Epoch: 66 [800/7471 (11%)]\tLoss: 1566636.000000\n",
            "Train Epoch: 66 [960/7471 (13%)]\tLoss: 1555889.000000\n",
            "Train Epoch: 66 [1120/7471 (15%)]\tLoss: 1566964.750000\n",
            "Train Epoch: 66 [1280/7471 (17%)]\tLoss: 1641898.875000\n",
            "Train Epoch: 66 [1440/7471 (19%)]\tLoss: 1576158.625000\n",
            "Train Epoch: 66 [1600/7471 (21%)]\tLoss: 1589236.500000\n",
            "Train Epoch: 66 [1760/7471 (24%)]\tLoss: 1557789.500000\n",
            "Train Epoch: 66 [1920/7471 (26%)]\tLoss: 1613963.250000\n",
            "Train Epoch: 66 [2080/7471 (28%)]\tLoss: 1582955.875000\n",
            "Train Epoch: 66 [2240/7471 (30%)]\tLoss: 1595179.750000\n",
            "Train Epoch: 66 [2400/7471 (32%)]\tLoss: 1586198.875000\n",
            "Train Epoch: 66 [2560/7471 (34%)]\tLoss: 1526503.375000\n",
            "Train Epoch: 66 [2720/7471 (36%)]\tLoss: 1583947.500000\n",
            "Train Epoch: 66 [2880/7471 (39%)]\tLoss: 1585599.500000\n",
            "Train Epoch: 66 [3040/7471 (41%)]\tLoss: 1570502.250000\n",
            "Train Epoch: 66 [3200/7471 (43%)]\tLoss: 1579343.500000\n",
            "Train Epoch: 66 [3360/7471 (45%)]\tLoss: 1622542.500000\n",
            "Train Epoch: 66 [3520/7471 (47%)]\tLoss: 1548073.125000\n",
            "Train Epoch: 66 [3680/7471 (49%)]\tLoss: 1638262.625000\n",
            "Train Epoch: 66 [3840/7471 (51%)]\tLoss: 1599753.000000\n",
            "Train Epoch: 66 [4000/7471 (54%)]\tLoss: 1558612.625000\n",
            "Train Epoch: 66 [4160/7471 (56%)]\tLoss: 1583866.500000\n",
            "Train Epoch: 66 [4320/7471 (58%)]\tLoss: 1530977.625000\n",
            "Train Epoch: 66 [4480/7471 (60%)]\tLoss: 1539600.500000\n",
            "Train Epoch: 66 [4640/7471 (62%)]\tLoss: 1532538.250000\n",
            "Train Epoch: 66 [4800/7471 (64%)]\tLoss: 1537391.875000\n",
            "Train Epoch: 66 [4960/7471 (66%)]\tLoss: 1592712.375000\n",
            "Train Epoch: 66 [5120/7471 (69%)]\tLoss: 1630930.375000\n",
            "Train Epoch: 66 [5280/7471 (71%)]\tLoss: 1622720.625000\n",
            "Train Epoch: 66 [5440/7471 (73%)]\tLoss: 1554485.375000\n",
            "Train Epoch: 66 [5600/7471 (75%)]\tLoss: 1546092.375000\n",
            "Train Epoch: 66 [5760/7471 (77%)]\tLoss: 1545988.875000\n",
            "Train Epoch: 66 [5920/7471 (79%)]\tLoss: 1556826.250000\n",
            "Train Epoch: 66 [6080/7471 (81%)]\tLoss: 1606325.500000\n",
            "Train Epoch: 66 [6240/7471 (84%)]\tLoss: 1645263.625000\n",
            "Train Epoch: 66 [6400/7471 (86%)]\tLoss: 1509169.750000\n",
            "Train Epoch: 66 [6560/7471 (88%)]\tLoss: 1605368.875000\n",
            "Train Epoch: 66 [6720/7471 (90%)]\tLoss: 1603617.250000\n",
            "Train Epoch: 66 [6880/7471 (92%)]\tLoss: 1623859.500000\n",
            "Train Epoch: 66 [7040/7471 (94%)]\tLoss: 1585406.125000\n",
            "Train Epoch: 66 [7200/7471 (96%)]\tLoss: 1522449.375000\n",
            "Train Epoch: 66 [7360/7471 (99%)]\tLoss: 1612648.875000\n",
            "Epoch 66 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98513.7038\n",
            "\n",
            "Train Epoch: 67 [160/7471 (2%)]\tLoss: 1544207.125000\n",
            "Train Epoch: 67 [320/7471 (4%)]\tLoss: 1627045.125000\n",
            "Train Epoch: 67 [480/7471 (6%)]\tLoss: 1547202.000000\n",
            "Train Epoch: 67 [640/7471 (9%)]\tLoss: 1580680.500000\n",
            "Train Epoch: 67 [800/7471 (11%)]\tLoss: 1613385.625000\n",
            "Train Epoch: 67 [960/7471 (13%)]\tLoss: 1495078.250000\n",
            "Train Epoch: 67 [1120/7471 (15%)]\tLoss: 1556826.125000\n",
            "Train Epoch: 67 [1280/7471 (17%)]\tLoss: 1612847.875000\n",
            "Train Epoch: 67 [1440/7471 (19%)]\tLoss: 1533439.750000\n",
            "Train Epoch: 67 [1600/7471 (21%)]\tLoss: 1653761.750000\n",
            "Train Epoch: 67 [1760/7471 (24%)]\tLoss: 1550309.625000\n",
            "Train Epoch: 67 [1920/7471 (26%)]\tLoss: 1570523.625000\n",
            "Train Epoch: 67 [2080/7471 (28%)]\tLoss: 1566290.750000\n",
            "Train Epoch: 67 [2240/7471 (30%)]\tLoss: 1574813.500000\n",
            "Train Epoch: 67 [2400/7471 (32%)]\tLoss: 1509841.875000\n",
            "Train Epoch: 67 [2560/7471 (34%)]\tLoss: 1568609.875000\n",
            "Train Epoch: 67 [2720/7471 (36%)]\tLoss: 1586413.000000\n",
            "Train Epoch: 67 [2880/7471 (39%)]\tLoss: 1590198.750000\n",
            "Train Epoch: 67 [3040/7471 (41%)]\tLoss: 1532111.250000\n",
            "Train Epoch: 67 [3200/7471 (43%)]\tLoss: 1574761.750000\n",
            "Train Epoch: 67 [3360/7471 (45%)]\tLoss: 1622578.125000\n",
            "Train Epoch: 67 [3520/7471 (47%)]\tLoss: 1596637.500000\n",
            "Train Epoch: 67 [3680/7471 (49%)]\tLoss: 1617943.500000\n",
            "Train Epoch: 67 [3840/7471 (51%)]\tLoss: 1615956.125000\n",
            "Train Epoch: 67 [4000/7471 (54%)]\tLoss: 1567422.375000\n",
            "Train Epoch: 67 [4160/7471 (56%)]\tLoss: 1607013.500000\n",
            "Train Epoch: 67 [4320/7471 (58%)]\tLoss: 1542187.375000\n",
            "Train Epoch: 67 [4480/7471 (60%)]\tLoss: 1620050.750000\n",
            "Train Epoch: 67 [4640/7471 (62%)]\tLoss: 1498992.500000\n",
            "Train Epoch: 67 [4800/7471 (64%)]\tLoss: 1551521.500000\n",
            "Train Epoch: 67 [4960/7471 (66%)]\tLoss: 1506346.875000\n",
            "Train Epoch: 67 [5120/7471 (69%)]\tLoss: 1605809.375000\n",
            "Train Epoch: 67 [5280/7471 (71%)]\tLoss: 1624512.375000\n",
            "Train Epoch: 67 [5440/7471 (73%)]\tLoss: 1529372.250000\n",
            "Train Epoch: 67 [5600/7471 (75%)]\tLoss: 1603167.125000\n",
            "Train Epoch: 67 [5760/7471 (77%)]\tLoss: 1617831.000000\n",
            "Train Epoch: 67 [5920/7471 (79%)]\tLoss: 1573380.750000\n",
            "Train Epoch: 67 [6080/7471 (81%)]\tLoss: 1557391.500000\n",
            "Train Epoch: 67 [6240/7471 (84%)]\tLoss: 1571176.250000\n",
            "Train Epoch: 67 [6400/7471 (86%)]\tLoss: 1538272.625000\n",
            "Train Epoch: 67 [6560/7471 (88%)]\tLoss: 1579974.000000\n",
            "Train Epoch: 67 [6720/7471 (90%)]\tLoss: 1572672.625000\n",
            "Train Epoch: 67 [6880/7471 (92%)]\tLoss: 1576264.250000\n",
            "Train Epoch: 67 [7040/7471 (94%)]\tLoss: 1513821.875000\n",
            "Train Epoch: 67 [7200/7471 (96%)]\tLoss: 1592886.000000\n",
            "Train Epoch: 67 [7360/7471 (99%)]\tLoss: 1619928.000000\n",
            "Epoch 67 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98585.6745\n",
            "\n",
            "Train Epoch: 68 [160/7471 (2%)]\tLoss: 1517024.250000\n",
            "Train Epoch: 68 [320/7471 (4%)]\tLoss: 1586429.250000\n",
            "Train Epoch: 68 [480/7471 (6%)]\tLoss: 1561531.625000\n",
            "Train Epoch: 68 [640/7471 (9%)]\tLoss: 1596098.375000\n",
            "Train Epoch: 68 [800/7471 (11%)]\tLoss: 1540614.000000\n",
            "Train Epoch: 68 [960/7471 (13%)]\tLoss: 1556590.125000\n",
            "Train Epoch: 68 [1120/7471 (15%)]\tLoss: 1526057.250000\n",
            "Train Epoch: 68 [1280/7471 (17%)]\tLoss: 1603173.750000\n",
            "Train Epoch: 68 [1440/7471 (19%)]\tLoss: 1600586.750000\n",
            "Train Epoch: 68 [1600/7471 (21%)]\tLoss: 1597122.750000\n",
            "Train Epoch: 68 [1760/7471 (24%)]\tLoss: 1495667.750000\n",
            "Train Epoch: 68 [1920/7471 (26%)]\tLoss: 1593118.000000\n",
            "Train Epoch: 68 [2080/7471 (28%)]\tLoss: 1517703.875000\n",
            "Train Epoch: 68 [2240/7471 (30%)]\tLoss: 1554085.000000\n",
            "Train Epoch: 68 [2400/7471 (32%)]\tLoss: 1583210.625000\n",
            "Train Epoch: 68 [2560/7471 (34%)]\tLoss: 1542621.250000\n",
            "Train Epoch: 68 [2720/7471 (36%)]\tLoss: 1566199.750000\n",
            "Train Epoch: 68 [2880/7471 (39%)]\tLoss: 1521425.125000\n",
            "Train Epoch: 68 [3040/7471 (41%)]\tLoss: 1610111.250000\n",
            "Train Epoch: 68 [3200/7471 (43%)]\tLoss: 1559262.750000\n",
            "Train Epoch: 68 [3360/7471 (45%)]\tLoss: 1611253.375000\n",
            "Train Epoch: 68 [3520/7471 (47%)]\tLoss: 1555148.375000\n",
            "Train Epoch: 68 [3680/7471 (49%)]\tLoss: 1615423.250000\n",
            "Train Epoch: 68 [3840/7471 (51%)]\tLoss: 1576643.000000\n",
            "Train Epoch: 68 [4000/7471 (54%)]\tLoss: 1536250.375000\n",
            "Train Epoch: 68 [4160/7471 (56%)]\tLoss: 1594509.625000\n",
            "Train Epoch: 68 [4320/7471 (58%)]\tLoss: 1538457.125000\n",
            "Train Epoch: 68 [4480/7471 (60%)]\tLoss: 1558231.625000\n",
            "Train Epoch: 68 [4640/7471 (62%)]\tLoss: 1621311.125000\n",
            "Train Epoch: 68 [4800/7471 (64%)]\tLoss: 1546304.750000\n",
            "Train Epoch: 68 [4960/7471 (66%)]\tLoss: 1634198.500000\n",
            "Train Epoch: 68 [5120/7471 (69%)]\tLoss: 1594655.375000\n",
            "Train Epoch: 68 [5280/7471 (71%)]\tLoss: 1577973.750000\n",
            "Train Epoch: 68 [5440/7471 (73%)]\tLoss: 1610684.875000\n",
            "Train Epoch: 68 [5600/7471 (75%)]\tLoss: 1617628.875000\n",
            "Train Epoch: 68 [5760/7471 (77%)]\tLoss: 1550393.250000\n",
            "Train Epoch: 68 [5920/7471 (79%)]\tLoss: 1540146.125000\n",
            "Train Epoch: 68 [6080/7471 (81%)]\tLoss: 1597158.500000\n",
            "Train Epoch: 68 [6240/7471 (84%)]\tLoss: 1599516.875000\n",
            "Train Epoch: 68 [6400/7471 (86%)]\tLoss: 1585939.875000\n",
            "Train Epoch: 68 [6560/7471 (88%)]\tLoss: 1562108.625000\n",
            "Train Epoch: 68 [6720/7471 (90%)]\tLoss: 1588041.250000\n",
            "Train Epoch: 68 [6880/7471 (92%)]\tLoss: 1599906.875000\n",
            "Train Epoch: 68 [7040/7471 (94%)]\tLoss: 1626754.750000\n",
            "Train Epoch: 68 [7200/7471 (96%)]\tLoss: 1607671.500000\n",
            "Train Epoch: 68 [7360/7471 (99%)]\tLoss: 1523298.000000\n",
            "Epoch 68 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98593.5321\n",
            "\n",
            "Train Epoch: 69 [160/7471 (2%)]\tLoss: 1568612.875000\n",
            "Train Epoch: 69 [320/7471 (4%)]\tLoss: 1560236.125000\n",
            "Train Epoch: 69 [480/7471 (6%)]\tLoss: 1607405.375000\n",
            "Train Epoch: 69 [640/7471 (9%)]\tLoss: 1594780.500000\n",
            "Train Epoch: 69 [800/7471 (11%)]\tLoss: 1575582.375000\n",
            "Train Epoch: 69 [960/7471 (13%)]\tLoss: 1572638.375000\n",
            "Train Epoch: 69 [1120/7471 (15%)]\tLoss: 1564486.375000\n",
            "Train Epoch: 69 [1280/7471 (17%)]\tLoss: 1607511.500000\n",
            "Train Epoch: 69 [1440/7471 (19%)]\tLoss: 1605836.375000\n",
            "Train Epoch: 69 [1600/7471 (21%)]\tLoss: 1556159.375000\n",
            "Train Epoch: 69 [1760/7471 (24%)]\tLoss: 1509847.750000\n",
            "Train Epoch: 69 [1920/7471 (26%)]\tLoss: 1500546.625000\n",
            "Train Epoch: 69 [2080/7471 (28%)]\tLoss: 1560809.125000\n",
            "Train Epoch: 69 [2240/7471 (30%)]\tLoss: 1556483.000000\n",
            "Train Epoch: 69 [2400/7471 (32%)]\tLoss: 1591322.625000\n",
            "Train Epoch: 69 [2560/7471 (34%)]\tLoss: 1585673.250000\n",
            "Train Epoch: 69 [2720/7471 (36%)]\tLoss: 1531209.000000\n",
            "Train Epoch: 69 [2880/7471 (39%)]\tLoss: 1619352.250000\n",
            "Train Epoch: 69 [3040/7471 (41%)]\tLoss: 1598102.125000\n",
            "Train Epoch: 69 [3200/7471 (43%)]\tLoss: 1627748.625000\n",
            "Train Epoch: 69 [3360/7471 (45%)]\tLoss: 1613596.500000\n",
            "Train Epoch: 69 [3520/7471 (47%)]\tLoss: 1603738.750000\n",
            "Train Epoch: 69 [3680/7471 (49%)]\tLoss: 1629453.750000\n",
            "Train Epoch: 69 [3840/7471 (51%)]\tLoss: 1628557.125000\n",
            "Train Epoch: 69 [4000/7471 (54%)]\tLoss: 1574972.625000\n",
            "Train Epoch: 69 [4160/7471 (56%)]\tLoss: 1544134.000000\n",
            "Train Epoch: 69 [4320/7471 (58%)]\tLoss: 1594677.125000\n",
            "Train Epoch: 69 [4480/7471 (60%)]\tLoss: 1625565.000000\n",
            "Train Epoch: 69 [4640/7471 (62%)]\tLoss: 1534249.250000\n",
            "Train Epoch: 69 [4800/7471 (64%)]\tLoss: 1484202.500000\n",
            "Train Epoch: 69 [4960/7471 (66%)]\tLoss: 1612048.250000\n",
            "Train Epoch: 69 [5120/7471 (69%)]\tLoss: 1629820.500000\n",
            "Train Epoch: 69 [5280/7471 (71%)]\tLoss: 1560301.625000\n",
            "Train Epoch: 69 [5440/7471 (73%)]\tLoss: 1537326.625000\n",
            "Train Epoch: 69 [5600/7471 (75%)]\tLoss: 1545804.125000\n",
            "Train Epoch: 69 [5760/7471 (77%)]\tLoss: 1595558.625000\n",
            "Train Epoch: 69 [5920/7471 (79%)]\tLoss: 1569201.500000\n",
            "Train Epoch: 69 [6080/7471 (81%)]\tLoss: 1535587.375000\n",
            "Train Epoch: 69 [6240/7471 (84%)]\tLoss: 1607934.250000\n",
            "Train Epoch: 69 [6400/7471 (86%)]\tLoss: 1545861.500000\n",
            "Train Epoch: 69 [6560/7471 (88%)]\tLoss: 1600782.125000\n",
            "Train Epoch: 69 [6720/7471 (90%)]\tLoss: 1589371.875000\n",
            "Train Epoch: 69 [6880/7471 (92%)]\tLoss: 1562103.875000\n",
            "Train Epoch: 69 [7040/7471 (94%)]\tLoss: 1607187.125000\n",
            "Train Epoch: 69 [7200/7471 (96%)]\tLoss: 1544706.500000\n",
            "Train Epoch: 69 [7360/7471 (99%)]\tLoss: 1573849.875000\n",
            "Epoch 69 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98553.6405\n",
            "\n",
            "Train Epoch: 70 [160/7471 (2%)]\tLoss: 1587213.125000\n",
            "Train Epoch: 70 [320/7471 (4%)]\tLoss: 1571982.375000\n",
            "Train Epoch: 70 [480/7471 (6%)]\tLoss: 1592889.125000\n",
            "Train Epoch: 70 [640/7471 (9%)]\tLoss: 1629779.250000\n",
            "Train Epoch: 70 [800/7471 (11%)]\tLoss: 1611395.125000\n",
            "Train Epoch: 70 [960/7471 (13%)]\tLoss: 1577501.625000\n",
            "Train Epoch: 70 [1120/7471 (15%)]\tLoss: 1546368.500000\n",
            "Train Epoch: 70 [1280/7471 (17%)]\tLoss: 1612689.500000\n",
            "Train Epoch: 70 [1440/7471 (19%)]\tLoss: 1544777.000000\n",
            "Train Epoch: 70 [1600/7471 (21%)]\tLoss: 1547279.250000\n",
            "Train Epoch: 70 [1760/7471 (24%)]\tLoss: 1623437.625000\n",
            "Train Epoch: 70 [1920/7471 (26%)]\tLoss: 1536172.125000\n",
            "Train Epoch: 70 [2080/7471 (28%)]\tLoss: 1586823.500000\n",
            "Train Epoch: 70 [2240/7471 (30%)]\tLoss: 1542635.875000\n",
            "Train Epoch: 70 [2400/7471 (32%)]\tLoss: 1610006.125000\n",
            "Train Epoch: 70 [2560/7471 (34%)]\tLoss: 1585775.625000\n",
            "Train Epoch: 70 [2720/7471 (36%)]\tLoss: 1593253.875000\n",
            "Train Epoch: 70 [2880/7471 (39%)]\tLoss: 1612371.125000\n",
            "Train Epoch: 70 [3040/7471 (41%)]\tLoss: 1548234.750000\n",
            "Train Epoch: 70 [3200/7471 (43%)]\tLoss: 1547650.375000\n",
            "Train Epoch: 70 [3360/7471 (45%)]\tLoss: 1644845.750000\n",
            "Train Epoch: 70 [3520/7471 (47%)]\tLoss: 1572392.750000\n",
            "Train Epoch: 70 [3680/7471 (49%)]\tLoss: 1606200.250000\n",
            "Train Epoch: 70 [3840/7471 (51%)]\tLoss: 1560342.500000\n",
            "Train Epoch: 70 [4000/7471 (54%)]\tLoss: 1617912.500000\n",
            "Train Epoch: 70 [4160/7471 (56%)]\tLoss: 1575742.000000\n",
            "Train Epoch: 70 [4320/7471 (58%)]\tLoss: 1586802.250000\n",
            "Train Epoch: 70 [4480/7471 (60%)]\tLoss: 1579385.250000\n",
            "Train Epoch: 70 [4640/7471 (62%)]\tLoss: 1492386.000000\n",
            "Train Epoch: 70 [4800/7471 (64%)]\tLoss: 1562373.000000\n",
            "Train Epoch: 70 [4960/7471 (66%)]\tLoss: 1536081.125000\n",
            "Train Epoch: 70 [5120/7471 (69%)]\tLoss: 1599380.250000\n",
            "Train Epoch: 70 [5280/7471 (71%)]\tLoss: 1612036.625000\n",
            "Train Epoch: 70 [5440/7471 (73%)]\tLoss: 1577070.000000\n",
            "Train Epoch: 70 [5600/7471 (75%)]\tLoss: 1546230.625000\n",
            "Train Epoch: 70 [5760/7471 (77%)]\tLoss: 1557090.750000\n",
            "Train Epoch: 70 [5920/7471 (79%)]\tLoss: 1530592.250000\n",
            "Train Epoch: 70 [6080/7471 (81%)]\tLoss: 1582414.125000\n",
            "Train Epoch: 70 [6240/7471 (84%)]\tLoss: 1577390.125000\n",
            "Train Epoch: 70 [6400/7471 (86%)]\tLoss: 1617354.250000\n",
            "Train Epoch: 70 [6560/7471 (88%)]\tLoss: 1579825.500000\n",
            "Train Epoch: 70 [6720/7471 (90%)]\tLoss: 1607866.625000\n",
            "Train Epoch: 70 [6880/7471 (92%)]\tLoss: 1554806.250000\n",
            "Train Epoch: 70 [7040/7471 (94%)]\tLoss: 1592966.375000\n",
            "Train Epoch: 70 [7200/7471 (96%)]\tLoss: 1611084.250000\n",
            "Train Epoch: 70 [7360/7471 (99%)]\tLoss: 1556811.125000\n",
            "Epoch 70 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98488.4595\n",
            "\n",
            "Train Epoch: 71 [160/7471 (2%)]\tLoss: 1592646.125000\n",
            "Train Epoch: 71 [320/7471 (4%)]\tLoss: 1588022.375000\n",
            "Train Epoch: 71 [480/7471 (6%)]\tLoss: 1608342.375000\n",
            "Train Epoch: 71 [640/7471 (9%)]\tLoss: 1550004.500000\n",
            "Train Epoch: 71 [800/7471 (11%)]\tLoss: 1583011.000000\n",
            "Train Epoch: 71 [960/7471 (13%)]\tLoss: 1614771.750000\n",
            "Train Epoch: 71 [1120/7471 (15%)]\tLoss: 1624190.875000\n",
            "Train Epoch: 71 [1280/7471 (17%)]\tLoss: 1596235.250000\n",
            "Train Epoch: 71 [1440/7471 (19%)]\tLoss: 1590329.375000\n",
            "Train Epoch: 71 [1600/7471 (21%)]\tLoss: 1581016.500000\n",
            "Train Epoch: 71 [1760/7471 (24%)]\tLoss: 1589321.375000\n",
            "Train Epoch: 71 [1920/7471 (26%)]\tLoss: 1625150.875000\n",
            "Train Epoch: 71 [2080/7471 (28%)]\tLoss: 1587509.625000\n",
            "Train Epoch: 71 [2240/7471 (30%)]\tLoss: 1545725.000000\n",
            "Train Epoch: 71 [2400/7471 (32%)]\tLoss: 1588646.750000\n",
            "Train Epoch: 71 [2560/7471 (34%)]\tLoss: 1556990.250000\n",
            "Train Epoch: 71 [2720/7471 (36%)]\tLoss: 1624873.375000\n",
            "Train Epoch: 71 [2880/7471 (39%)]\tLoss: 1579992.500000\n",
            "Train Epoch: 71 [3040/7471 (41%)]\tLoss: 1559854.625000\n",
            "Train Epoch: 71 [3200/7471 (43%)]\tLoss: 1609634.875000\n",
            "Train Epoch: 71 [3360/7471 (45%)]\tLoss: 1498806.875000\n",
            "Train Epoch: 71 [3520/7471 (47%)]\tLoss: 1641013.125000\n",
            "Train Epoch: 71 [3680/7471 (49%)]\tLoss: 1494185.875000\n",
            "Train Epoch: 71 [3840/7471 (51%)]\tLoss: 1568229.375000\n",
            "Train Epoch: 71 [4000/7471 (54%)]\tLoss: 1526252.375000\n",
            "Train Epoch: 71 [4160/7471 (56%)]\tLoss: 1569650.000000\n",
            "Train Epoch: 71 [4320/7471 (58%)]\tLoss: 1560847.750000\n",
            "Train Epoch: 71 [4480/7471 (60%)]\tLoss: 1547043.875000\n",
            "Train Epoch: 71 [4640/7471 (62%)]\tLoss: 1593931.500000\n",
            "Train Epoch: 71 [4800/7471 (64%)]\tLoss: 1585344.750000\n",
            "Train Epoch: 71 [4960/7471 (66%)]\tLoss: 1571191.375000\n",
            "Train Epoch: 71 [5120/7471 (69%)]\tLoss: 1568386.875000\n",
            "Train Epoch: 71 [5280/7471 (71%)]\tLoss: 1550300.875000\n",
            "Train Epoch: 71 [5440/7471 (73%)]\tLoss: 1559691.750000\n",
            "Train Epoch: 71 [5600/7471 (75%)]\tLoss: 1542002.500000\n",
            "Train Epoch: 71 [5760/7471 (77%)]\tLoss: 1559136.000000\n",
            "Train Epoch: 71 [5920/7471 (79%)]\tLoss: 1610969.625000\n",
            "Train Epoch: 71 [6080/7471 (81%)]\tLoss: 1596700.500000\n",
            "Train Epoch: 71 [6240/7471 (84%)]\tLoss: 1588207.625000\n",
            "Train Epoch: 71 [6400/7471 (86%)]\tLoss: 1556318.875000\n",
            "Train Epoch: 71 [6560/7471 (88%)]\tLoss: 1563113.750000\n",
            "Train Epoch: 71 [6720/7471 (90%)]\tLoss: 1601742.500000\n",
            "Train Epoch: 71 [6880/7471 (92%)]\tLoss: 1613868.000000\n",
            "Train Epoch: 71 [7040/7471 (94%)]\tLoss: 1580105.125000\n",
            "Train Epoch: 71 [7200/7471 (96%)]\tLoss: 1543274.875000\n",
            "Train Epoch: 71 [7360/7471 (99%)]\tLoss: 1587711.375000\n",
            "Epoch 71 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98497.9273\n",
            "\n",
            "Train Epoch: 72 [160/7471 (2%)]\tLoss: 1615221.125000\n",
            "Train Epoch: 72 [320/7471 (4%)]\tLoss: 1556410.125000\n",
            "Train Epoch: 72 [480/7471 (6%)]\tLoss: 1568325.375000\n",
            "Train Epoch: 72 [640/7471 (9%)]\tLoss: 1565840.000000\n",
            "Train Epoch: 72 [800/7471 (11%)]\tLoss: 1618634.750000\n",
            "Train Epoch: 72 [960/7471 (13%)]\tLoss: 1554349.750000\n",
            "Train Epoch: 72 [1120/7471 (15%)]\tLoss: 1631630.875000\n",
            "Train Epoch: 72 [1280/7471 (17%)]\tLoss: 1568440.500000\n",
            "Train Epoch: 72 [1440/7471 (19%)]\tLoss: 1545515.875000\n",
            "Train Epoch: 72 [1600/7471 (21%)]\tLoss: 1484479.125000\n",
            "Train Epoch: 72 [1760/7471 (24%)]\tLoss: 1555950.625000\n",
            "Train Epoch: 72 [1920/7471 (26%)]\tLoss: 1628082.000000\n",
            "Train Epoch: 72 [2080/7471 (28%)]\tLoss: 1557620.625000\n",
            "Train Epoch: 72 [2240/7471 (30%)]\tLoss: 1515346.000000\n",
            "Train Epoch: 72 [2400/7471 (32%)]\tLoss: 1542033.000000\n",
            "Train Epoch: 72 [2560/7471 (34%)]\tLoss: 1529635.000000\n",
            "Train Epoch: 72 [2720/7471 (36%)]\tLoss: 1561431.500000\n",
            "Train Epoch: 72 [2880/7471 (39%)]\tLoss: 1494097.000000\n",
            "Train Epoch: 72 [3040/7471 (41%)]\tLoss: 1588675.750000\n",
            "Train Epoch: 72 [3200/7471 (43%)]\tLoss: 1499401.750000\n",
            "Train Epoch: 72 [3360/7471 (45%)]\tLoss: 1570175.375000\n",
            "Train Epoch: 72 [3520/7471 (47%)]\tLoss: 1533629.875000\n",
            "Train Epoch: 72 [3680/7471 (49%)]\tLoss: 1610478.375000\n",
            "Train Epoch: 72 [3840/7471 (51%)]\tLoss: 1567785.375000\n",
            "Train Epoch: 72 [4000/7471 (54%)]\tLoss: 1558448.250000\n",
            "Train Epoch: 72 [4160/7471 (56%)]\tLoss: 1505112.875000\n",
            "Train Epoch: 72 [4320/7471 (58%)]\tLoss: 1617402.375000\n",
            "Train Epoch: 72 [4480/7471 (60%)]\tLoss: 1566586.875000\n",
            "Train Epoch: 72 [4640/7471 (62%)]\tLoss: 1571993.625000\n",
            "Train Epoch: 72 [4800/7471 (64%)]\tLoss: 1582165.750000\n",
            "Train Epoch: 72 [4960/7471 (66%)]\tLoss: 1596908.500000\n",
            "Train Epoch: 72 [5120/7471 (69%)]\tLoss: 1548479.500000\n",
            "Train Epoch: 72 [5280/7471 (71%)]\tLoss: 1547965.375000\n",
            "Train Epoch: 72 [5440/7471 (73%)]\tLoss: 1530389.125000\n",
            "Train Epoch: 72 [5600/7471 (75%)]\tLoss: 1584295.375000\n",
            "Train Epoch: 72 [5760/7471 (77%)]\tLoss: 1552734.500000\n",
            "Train Epoch: 72 [5920/7471 (79%)]\tLoss: 1597945.000000\n",
            "Train Epoch: 72 [6080/7471 (81%)]\tLoss: 1608739.000000\n",
            "Train Epoch: 72 [6240/7471 (84%)]\tLoss: 1548331.625000\n",
            "Train Epoch: 72 [6400/7471 (86%)]\tLoss: 1527353.875000\n",
            "Train Epoch: 72 [6560/7471 (88%)]\tLoss: 1639134.125000\n",
            "Train Epoch: 72 [6720/7471 (90%)]\tLoss: 1548698.500000\n",
            "Train Epoch: 72 [6880/7471 (92%)]\tLoss: 1577973.125000\n",
            "Train Epoch: 72 [7040/7471 (94%)]\tLoss: 1600128.500000\n",
            "Train Epoch: 72 [7200/7471 (96%)]\tLoss: 1610837.000000\n",
            "Train Epoch: 72 [7360/7471 (99%)]\tLoss: 1494770.375000\n",
            "Epoch 72 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98511.3831\n",
            "\n",
            "Train Epoch: 73 [160/7471 (2%)]\tLoss: 1538096.750000\n",
            "Train Epoch: 73 [320/7471 (4%)]\tLoss: 1501041.125000\n",
            "Train Epoch: 73 [480/7471 (6%)]\tLoss: 1565545.375000\n",
            "Train Epoch: 73 [640/7471 (9%)]\tLoss: 1537300.750000\n",
            "Train Epoch: 73 [800/7471 (11%)]\tLoss: 1561558.000000\n",
            "Train Epoch: 73 [960/7471 (13%)]\tLoss: 1541237.000000\n",
            "Train Epoch: 73 [1120/7471 (15%)]\tLoss: 1527250.000000\n",
            "Train Epoch: 73 [1280/7471 (17%)]\tLoss: 1532577.750000\n",
            "Train Epoch: 73 [1440/7471 (19%)]\tLoss: 1479346.000000\n",
            "Train Epoch: 73 [1600/7471 (21%)]\tLoss: 1607326.750000\n",
            "Train Epoch: 73 [1760/7471 (24%)]\tLoss: 1518652.875000\n",
            "Train Epoch: 73 [1920/7471 (26%)]\tLoss: 1626327.500000\n",
            "Train Epoch: 73 [2080/7471 (28%)]\tLoss: 1531127.000000\n",
            "Train Epoch: 73 [2240/7471 (30%)]\tLoss: 1599618.875000\n",
            "Train Epoch: 73 [2400/7471 (32%)]\tLoss: 1557538.250000\n",
            "Train Epoch: 73 [2560/7471 (34%)]\tLoss: 1606235.625000\n",
            "Train Epoch: 73 [2720/7471 (36%)]\tLoss: 1445092.750000\n",
            "Train Epoch: 73 [2880/7471 (39%)]\tLoss: 1604247.250000\n",
            "Train Epoch: 73 [3040/7471 (41%)]\tLoss: 1614210.000000\n",
            "Train Epoch: 73 [3200/7471 (43%)]\tLoss: 1602448.625000\n",
            "Train Epoch: 73 [3360/7471 (45%)]\tLoss: 1551032.500000\n",
            "Train Epoch: 73 [3520/7471 (47%)]\tLoss: 1584389.125000\n",
            "Train Epoch: 73 [3680/7471 (49%)]\tLoss: 1599425.375000\n",
            "Train Epoch: 73 [3840/7471 (51%)]\tLoss: 1540348.500000\n",
            "Train Epoch: 73 [4000/7471 (54%)]\tLoss: 1503586.750000\n",
            "Train Epoch: 73 [4160/7471 (56%)]\tLoss: 1566590.375000\n",
            "Train Epoch: 73 [4320/7471 (58%)]\tLoss: 1619568.125000\n",
            "Train Epoch: 73 [4480/7471 (60%)]\tLoss: 1602599.000000\n",
            "Train Epoch: 73 [4640/7471 (62%)]\tLoss: 1561937.375000\n",
            "Train Epoch: 73 [4800/7471 (64%)]\tLoss: 1587909.250000\n",
            "Train Epoch: 73 [4960/7471 (66%)]\tLoss: 1509232.375000\n",
            "Train Epoch: 73 [5120/7471 (69%)]\tLoss: 1597098.375000\n",
            "Train Epoch: 73 [5280/7471 (71%)]\tLoss: 1626868.625000\n",
            "Train Epoch: 73 [5440/7471 (73%)]\tLoss: 1579243.000000\n",
            "Train Epoch: 73 [5600/7471 (75%)]\tLoss: 1606973.125000\n",
            "Train Epoch: 73 [5760/7471 (77%)]\tLoss: 1629024.375000\n",
            "Train Epoch: 73 [5920/7471 (79%)]\tLoss: 1623509.000000\n",
            "Train Epoch: 73 [6080/7471 (81%)]\tLoss: 1604848.000000\n",
            "Train Epoch: 73 [6240/7471 (84%)]\tLoss: 1583302.000000\n",
            "Train Epoch: 73 [6400/7471 (86%)]\tLoss: 1545320.750000\n",
            "Train Epoch: 73 [6560/7471 (88%)]\tLoss: 1584724.750000\n",
            "Train Epoch: 73 [6720/7471 (90%)]\tLoss: 1567591.750000\n",
            "Train Epoch: 73 [6880/7471 (92%)]\tLoss: 1544342.625000\n",
            "Train Epoch: 73 [7040/7471 (94%)]\tLoss: 1587386.750000\n",
            "Train Epoch: 73 [7200/7471 (96%)]\tLoss: 1597458.875000\n",
            "Train Epoch: 73 [7360/7471 (99%)]\tLoss: 1514675.375000\n",
            "Epoch 73 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98550.0923\n",
            "\n",
            "Train Epoch: 74 [160/7471 (2%)]\tLoss: 1616927.875000\n",
            "Train Epoch: 74 [320/7471 (4%)]\tLoss: 1584419.375000\n",
            "Train Epoch: 74 [480/7471 (6%)]\tLoss: 1587973.125000\n",
            "Train Epoch: 74 [640/7471 (9%)]\tLoss: 1537636.375000\n",
            "Train Epoch: 74 [800/7471 (11%)]\tLoss: 1586388.375000\n",
            "Train Epoch: 74 [960/7471 (13%)]\tLoss: 1566614.000000\n",
            "Train Epoch: 74 [1120/7471 (15%)]\tLoss: 1576090.375000\n",
            "Train Epoch: 74 [1280/7471 (17%)]\tLoss: 1617365.750000\n",
            "Train Epoch: 74 [1440/7471 (19%)]\tLoss: 1515035.625000\n",
            "Train Epoch: 74 [1600/7471 (21%)]\tLoss: 1509535.875000\n",
            "Train Epoch: 74 [1760/7471 (24%)]\tLoss: 1507078.375000\n",
            "Train Epoch: 74 [1920/7471 (26%)]\tLoss: 1603720.250000\n",
            "Train Epoch: 74 [2080/7471 (28%)]\tLoss: 1499028.000000\n",
            "Train Epoch: 74 [2240/7471 (30%)]\tLoss: 1614207.875000\n",
            "Train Epoch: 74 [2400/7471 (32%)]\tLoss: 1512971.375000\n",
            "Train Epoch: 74 [2560/7471 (34%)]\tLoss: 1570925.500000\n",
            "Train Epoch: 74 [2720/7471 (36%)]\tLoss: 1567491.875000\n",
            "Train Epoch: 74 [2880/7471 (39%)]\tLoss: 1574919.500000\n",
            "Train Epoch: 74 [3040/7471 (41%)]\tLoss: 1608094.250000\n",
            "Train Epoch: 74 [3200/7471 (43%)]\tLoss: 1610733.500000\n",
            "Train Epoch: 74 [3360/7471 (45%)]\tLoss: 1566705.000000\n",
            "Train Epoch: 74 [3520/7471 (47%)]\tLoss: 1464720.500000\n",
            "Train Epoch: 74 [3680/7471 (49%)]\tLoss: 1534691.000000\n",
            "Train Epoch: 74 [3840/7471 (51%)]\tLoss: 1598671.875000\n",
            "Train Epoch: 74 [4000/7471 (54%)]\tLoss: 1599923.000000\n",
            "Train Epoch: 74 [4160/7471 (56%)]\tLoss: 1603666.750000\n",
            "Train Epoch: 74 [4320/7471 (58%)]\tLoss: 1523582.750000\n",
            "Train Epoch: 74 [4480/7471 (60%)]\tLoss: 1533118.250000\n",
            "Train Epoch: 74 [4640/7471 (62%)]\tLoss: 1615185.500000\n",
            "Train Epoch: 74 [4800/7471 (64%)]\tLoss: 1586096.125000\n",
            "Train Epoch: 74 [4960/7471 (66%)]\tLoss: 1611904.875000\n",
            "Train Epoch: 74 [5120/7471 (69%)]\tLoss: 1551448.250000\n",
            "Train Epoch: 74 [5280/7471 (71%)]\tLoss: 1624222.000000\n",
            "Train Epoch: 74 [5440/7471 (73%)]\tLoss: 1567477.625000\n",
            "Train Epoch: 74 [5600/7471 (75%)]\tLoss: 1488205.750000\n",
            "Train Epoch: 74 [5760/7471 (77%)]\tLoss: 1627813.250000\n",
            "Train Epoch: 74 [5920/7471 (79%)]\tLoss: 1585261.250000\n",
            "Train Epoch: 74 [6080/7471 (81%)]\tLoss: 1591836.500000\n",
            "Train Epoch: 74 [6240/7471 (84%)]\tLoss: 1607989.375000\n",
            "Train Epoch: 74 [6400/7471 (86%)]\tLoss: 1497846.875000\n",
            "Train Epoch: 74 [6560/7471 (88%)]\tLoss: 1575699.000000\n",
            "Train Epoch: 74 [6720/7471 (90%)]\tLoss: 1578797.500000\n",
            "Train Epoch: 74 [6880/7471 (92%)]\tLoss: 1627025.875000\n",
            "Train Epoch: 74 [7040/7471 (94%)]\tLoss: 1601494.625000\n",
            "Train Epoch: 74 [7200/7471 (96%)]\tLoss: 1551324.500000\n",
            "Train Epoch: 74 [7360/7471 (99%)]\tLoss: 1616236.000000\n",
            "Epoch 74 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98462.9215\n",
            "\n",
            "Train Epoch: 75 [160/7471 (2%)]\tLoss: 1576053.625000\n",
            "Train Epoch: 75 [320/7471 (4%)]\tLoss: 1567635.125000\n",
            "Train Epoch: 75 [480/7471 (6%)]\tLoss: 1570378.250000\n",
            "Train Epoch: 75 [640/7471 (9%)]\tLoss: 1617431.000000\n",
            "Train Epoch: 75 [800/7471 (11%)]\tLoss: 1581302.125000\n",
            "Train Epoch: 75 [960/7471 (13%)]\tLoss: 1603487.500000\n",
            "Train Epoch: 75 [1120/7471 (15%)]\tLoss: 1609792.375000\n",
            "Train Epoch: 75 [1280/7471 (17%)]\tLoss: 1562960.750000\n",
            "Train Epoch: 75 [1440/7471 (19%)]\tLoss: 1498117.500000\n",
            "Train Epoch: 75 [1600/7471 (21%)]\tLoss: 1509005.625000\n",
            "Train Epoch: 75 [1760/7471 (24%)]\tLoss: 1622654.375000\n",
            "Train Epoch: 75 [1920/7471 (26%)]\tLoss: 1539534.500000\n",
            "Train Epoch: 75 [2080/7471 (28%)]\tLoss: 1533045.125000\n",
            "Train Epoch: 75 [2240/7471 (30%)]\tLoss: 1581028.500000\n",
            "Train Epoch: 75 [2400/7471 (32%)]\tLoss: 1632489.750000\n",
            "Train Epoch: 75 [2560/7471 (34%)]\tLoss: 1565987.375000\n",
            "Train Epoch: 75 [2720/7471 (36%)]\tLoss: 1570346.000000\n",
            "Train Epoch: 75 [2880/7471 (39%)]\tLoss: 1542521.000000\n",
            "Train Epoch: 75 [3040/7471 (41%)]\tLoss: 1609477.625000\n",
            "Train Epoch: 75 [3200/7471 (43%)]\tLoss: 1582618.875000\n",
            "Train Epoch: 75 [3360/7471 (45%)]\tLoss: 1640143.500000\n",
            "Train Epoch: 75 [3520/7471 (47%)]\tLoss: 1552897.625000\n",
            "Train Epoch: 75 [3680/7471 (49%)]\tLoss: 1598907.875000\n",
            "Train Epoch: 75 [3840/7471 (51%)]\tLoss: 1628730.875000\n",
            "Train Epoch: 75 [4000/7471 (54%)]\tLoss: 1548550.625000\n",
            "Train Epoch: 75 [4160/7471 (56%)]\tLoss: 1595901.250000\n",
            "Train Epoch: 75 [4320/7471 (58%)]\tLoss: 1566130.625000\n",
            "Train Epoch: 75 [4480/7471 (60%)]\tLoss: 1577923.125000\n",
            "Train Epoch: 75 [4640/7471 (62%)]\tLoss: 1609705.500000\n",
            "Train Epoch: 75 [4800/7471 (64%)]\tLoss: 1612482.875000\n",
            "Train Epoch: 75 [4960/7471 (66%)]\tLoss: 1587183.875000\n",
            "Train Epoch: 75 [5120/7471 (69%)]\tLoss: 1534464.250000\n",
            "Train Epoch: 75 [5280/7471 (71%)]\tLoss: 1568148.125000\n",
            "Train Epoch: 75 [5440/7471 (73%)]\tLoss: 1505655.375000\n",
            "Train Epoch: 75 [5600/7471 (75%)]\tLoss: 1600038.750000\n",
            "Train Epoch: 75 [5760/7471 (77%)]\tLoss: 1611290.125000\n",
            "Train Epoch: 75 [5920/7471 (79%)]\tLoss: 1589015.500000\n",
            "Train Epoch: 75 [6080/7471 (81%)]\tLoss: 1610555.125000\n",
            "Train Epoch: 75 [6240/7471 (84%)]\tLoss: 1615030.625000\n",
            "Train Epoch: 75 [6400/7471 (86%)]\tLoss: 1555033.500000\n",
            "Train Epoch: 75 [6560/7471 (88%)]\tLoss: 1556508.875000\n",
            "Train Epoch: 75 [6720/7471 (90%)]\tLoss: 1637429.875000\n",
            "Train Epoch: 75 [6880/7471 (92%)]\tLoss: 1521980.625000\n",
            "Train Epoch: 75 [7040/7471 (94%)]\tLoss: 1559904.750000\n",
            "Train Epoch: 75 [7200/7471 (96%)]\tLoss: 1564559.000000\n",
            "Train Epoch: 75 [7360/7471 (99%)]\tLoss: 1561194.375000\n",
            "Epoch 75 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98471.5464\n",
            "\n",
            "Train Epoch: 76 [160/7471 (2%)]\tLoss: 1557706.250000\n",
            "Train Epoch: 76 [320/7471 (4%)]\tLoss: 1611413.875000\n",
            "Train Epoch: 76 [480/7471 (6%)]\tLoss: 1617819.000000\n",
            "Train Epoch: 76 [640/7471 (9%)]\tLoss: 1596911.125000\n",
            "Train Epoch: 76 [800/7471 (11%)]\tLoss: 1601965.250000\n",
            "Train Epoch: 76 [960/7471 (13%)]\tLoss: 1628800.875000\n",
            "Train Epoch: 76 [1120/7471 (15%)]\tLoss: 1629856.375000\n",
            "Train Epoch: 76 [1280/7471 (17%)]\tLoss: 1572419.500000\n",
            "Train Epoch: 76 [1440/7471 (19%)]\tLoss: 1546637.500000\n",
            "Train Epoch: 76 [1600/7471 (21%)]\tLoss: 1593876.375000\n",
            "Train Epoch: 76 [1760/7471 (24%)]\tLoss: 1590647.375000\n",
            "Train Epoch: 76 [1920/7471 (26%)]\tLoss: 1575727.375000\n",
            "Train Epoch: 76 [2080/7471 (28%)]\tLoss: 1556300.375000\n",
            "Train Epoch: 76 [2240/7471 (30%)]\tLoss: 1539090.125000\n",
            "Train Epoch: 76 [2400/7471 (32%)]\tLoss: 1570570.375000\n",
            "Train Epoch: 76 [2560/7471 (34%)]\tLoss: 1561054.500000\n",
            "Train Epoch: 76 [2720/7471 (36%)]\tLoss: 1548865.375000\n",
            "Train Epoch: 76 [2880/7471 (39%)]\tLoss: 1588357.000000\n",
            "Train Epoch: 76 [3040/7471 (41%)]\tLoss: 1587873.750000\n",
            "Train Epoch: 76 [3200/7471 (43%)]\tLoss: 1601214.375000\n",
            "Train Epoch: 76 [3360/7471 (45%)]\tLoss: 1599785.250000\n",
            "Train Epoch: 76 [3520/7471 (47%)]\tLoss: 1586047.000000\n",
            "Train Epoch: 76 [3680/7471 (49%)]\tLoss: 1622086.875000\n",
            "Train Epoch: 76 [3840/7471 (51%)]\tLoss: 1554056.375000\n",
            "Train Epoch: 76 [4000/7471 (54%)]\tLoss: 1616798.750000\n",
            "Train Epoch: 76 [4160/7471 (56%)]\tLoss: 1590372.000000\n",
            "Train Epoch: 76 [4320/7471 (58%)]\tLoss: 1556515.375000\n",
            "Train Epoch: 76 [4480/7471 (60%)]\tLoss: 1519115.125000\n",
            "Train Epoch: 76 [4640/7471 (62%)]\tLoss: 1616728.625000\n",
            "Train Epoch: 76 [4800/7471 (64%)]\tLoss: 1562257.625000\n",
            "Train Epoch: 76 [4960/7471 (66%)]\tLoss: 1525420.250000\n",
            "Train Epoch: 76 [5120/7471 (69%)]\tLoss: 1527426.625000\n",
            "Train Epoch: 76 [5280/7471 (71%)]\tLoss: 1503523.875000\n",
            "Train Epoch: 76 [5440/7471 (73%)]\tLoss: 1563998.875000\n",
            "Train Epoch: 76 [5600/7471 (75%)]\tLoss: 1551152.750000\n",
            "Train Epoch: 76 [5760/7471 (77%)]\tLoss: 1600882.875000\n",
            "Train Epoch: 76 [5920/7471 (79%)]\tLoss: 1623056.375000\n",
            "Train Epoch: 76 [6080/7471 (81%)]\tLoss: 1576111.125000\n",
            "Train Epoch: 76 [6240/7471 (84%)]\tLoss: 1572070.625000\n",
            "Train Epoch: 76 [6400/7471 (86%)]\tLoss: 1605803.625000\n",
            "Train Epoch: 76 [6560/7471 (88%)]\tLoss: 1527801.125000\n",
            "Train Epoch: 76 [6720/7471 (90%)]\tLoss: 1529703.000000\n",
            "Train Epoch: 76 [6880/7471 (92%)]\tLoss: 1607289.875000\n",
            "Train Epoch: 76 [7040/7471 (94%)]\tLoss: 1627141.375000\n",
            "Train Epoch: 76 [7200/7471 (96%)]\tLoss: 1549499.625000\n",
            "Train Epoch: 76 [7360/7471 (99%)]\tLoss: 1610946.000000\n",
            "Epoch 76 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98485.1026\n",
            "\n",
            "Train Epoch: 77 [160/7471 (2%)]\tLoss: 1616493.125000\n",
            "Train Epoch: 77 [320/7471 (4%)]\tLoss: 1531444.625000\n",
            "Train Epoch: 77 [480/7471 (6%)]\tLoss: 1491344.250000\n",
            "Train Epoch: 77 [640/7471 (9%)]\tLoss: 1543605.500000\n",
            "Train Epoch: 77 [800/7471 (11%)]\tLoss: 1623383.500000\n",
            "Train Epoch: 77 [960/7471 (13%)]\tLoss: 1604734.125000\n",
            "Train Epoch: 77 [1120/7471 (15%)]\tLoss: 1477951.875000\n",
            "Train Epoch: 77 [1280/7471 (17%)]\tLoss: 1547071.250000\n",
            "Train Epoch: 77 [1440/7471 (19%)]\tLoss: 1591276.250000\n",
            "Train Epoch: 77 [1600/7471 (21%)]\tLoss: 1534355.250000\n",
            "Train Epoch: 77 [1760/7471 (24%)]\tLoss: 1575552.250000\n",
            "Train Epoch: 77 [1920/7471 (26%)]\tLoss: 1499208.125000\n",
            "Train Epoch: 77 [2080/7471 (28%)]\tLoss: 1610800.375000\n",
            "Train Epoch: 77 [2240/7471 (30%)]\tLoss: 1539927.000000\n",
            "Train Epoch: 77 [2400/7471 (32%)]\tLoss: 1578316.125000\n",
            "Train Epoch: 77 [2560/7471 (34%)]\tLoss: 1579287.125000\n",
            "Train Epoch: 77 [2720/7471 (36%)]\tLoss: 1536202.500000\n",
            "Train Epoch: 77 [2880/7471 (39%)]\tLoss: 1563675.750000\n",
            "Train Epoch: 77 [3040/7471 (41%)]\tLoss: 1565413.875000\n",
            "Train Epoch: 77 [3200/7471 (43%)]\tLoss: 1577777.375000\n",
            "Train Epoch: 77 [3360/7471 (45%)]\tLoss: 1611335.875000\n",
            "Train Epoch: 77 [3520/7471 (47%)]\tLoss: 1610559.625000\n",
            "Train Epoch: 77 [3680/7471 (49%)]\tLoss: 1586114.875000\n",
            "Train Epoch: 77 [3840/7471 (51%)]\tLoss: 1539485.125000\n",
            "Train Epoch: 77 [4000/7471 (54%)]\tLoss: 1574020.875000\n",
            "Train Epoch: 77 [4160/7471 (56%)]\tLoss: 1608858.250000\n",
            "Train Epoch: 77 [4320/7471 (58%)]\tLoss: 1594117.125000\n",
            "Train Epoch: 77 [4480/7471 (60%)]\tLoss: 1588778.375000\n",
            "Train Epoch: 77 [4640/7471 (62%)]\tLoss: 1557717.125000\n",
            "Train Epoch: 77 [4800/7471 (64%)]\tLoss: 1612415.875000\n",
            "Train Epoch: 77 [4960/7471 (66%)]\tLoss: 1516790.500000\n",
            "Train Epoch: 77 [5120/7471 (69%)]\tLoss: 1613671.625000\n",
            "Train Epoch: 77 [5280/7471 (71%)]\tLoss: 1583099.125000\n",
            "Train Epoch: 77 [5440/7471 (73%)]\tLoss: 1592548.500000\n",
            "Train Epoch: 77 [5600/7471 (75%)]\tLoss: 1575346.625000\n",
            "Train Epoch: 77 [5760/7471 (77%)]\tLoss: 1628780.625000\n",
            "Train Epoch: 77 [5920/7471 (79%)]\tLoss: 1622577.000000\n",
            "Train Epoch: 77 [6080/7471 (81%)]\tLoss: 1613501.750000\n",
            "Train Epoch: 77 [6240/7471 (84%)]\tLoss: 1557100.750000\n",
            "Train Epoch: 77 [6400/7471 (86%)]\tLoss: 1594000.125000\n",
            "Train Epoch: 77 [6560/7471 (88%)]\tLoss: 1619838.250000\n",
            "Train Epoch: 77 [6720/7471 (90%)]\tLoss: 1607382.250000\n",
            "Train Epoch: 77 [6880/7471 (92%)]\tLoss: 1528027.625000\n",
            "Train Epoch: 77 [7040/7471 (94%)]\tLoss: 1569533.750000\n",
            "Train Epoch: 77 [7200/7471 (96%)]\tLoss: 1544813.875000\n",
            "Train Epoch: 77 [7360/7471 (99%)]\tLoss: 1570149.125000\n",
            "Epoch 77 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98475.7658\n",
            "\n",
            "Train Epoch: 78 [160/7471 (2%)]\tLoss: 1553348.875000\n",
            "Train Epoch: 78 [320/7471 (4%)]\tLoss: 1582764.500000\n",
            "Train Epoch: 78 [480/7471 (6%)]\tLoss: 1631015.750000\n",
            "Train Epoch: 78 [640/7471 (9%)]\tLoss: 1546909.125000\n",
            "Train Epoch: 78 [800/7471 (11%)]\tLoss: 1633212.250000\n",
            "Train Epoch: 78 [960/7471 (13%)]\tLoss: 1597358.500000\n",
            "Train Epoch: 78 [1120/7471 (15%)]\tLoss: 1514821.625000\n",
            "Train Epoch: 78 [1280/7471 (17%)]\tLoss: 1530423.500000\n",
            "Train Epoch: 78 [1440/7471 (19%)]\tLoss: 1517529.750000\n",
            "Train Epoch: 78 [1600/7471 (21%)]\tLoss: 1590077.250000\n",
            "Train Epoch: 78 [1760/7471 (24%)]\tLoss: 1595486.250000\n",
            "Train Epoch: 78 [1920/7471 (26%)]\tLoss: 1593269.875000\n",
            "Train Epoch: 78 [2080/7471 (28%)]\tLoss: 1543205.375000\n",
            "Train Epoch: 78 [2240/7471 (30%)]\tLoss: 1611863.250000\n",
            "Train Epoch: 78 [2400/7471 (32%)]\tLoss: 1612799.250000\n",
            "Train Epoch: 78 [2560/7471 (34%)]\tLoss: 1530137.875000\n",
            "Train Epoch: 78 [2720/7471 (36%)]\tLoss: 1553341.500000\n",
            "Train Epoch: 78 [2880/7471 (39%)]\tLoss: 1561585.375000\n",
            "Train Epoch: 78 [3040/7471 (41%)]\tLoss: 1579177.875000\n",
            "Train Epoch: 78 [3200/7471 (43%)]\tLoss: 1545685.625000\n",
            "Train Epoch: 78 [3360/7471 (45%)]\tLoss: 1571989.750000\n",
            "Train Epoch: 78 [3520/7471 (47%)]\tLoss: 1600993.625000\n",
            "Train Epoch: 78 [3680/7471 (49%)]\tLoss: 1636234.000000\n",
            "Train Epoch: 78 [3840/7471 (51%)]\tLoss: 1604608.875000\n",
            "Train Epoch: 78 [4000/7471 (54%)]\tLoss: 1545391.000000\n",
            "Train Epoch: 78 [4160/7471 (56%)]\tLoss: 1567288.500000\n",
            "Train Epoch: 78 [4320/7471 (58%)]\tLoss: 1530688.375000\n",
            "Train Epoch: 78 [4480/7471 (60%)]\tLoss: 1594742.625000\n",
            "Train Epoch: 78 [4640/7471 (62%)]\tLoss: 1585699.375000\n",
            "Train Epoch: 78 [4800/7471 (64%)]\tLoss: 1589350.250000\n",
            "Train Epoch: 78 [4960/7471 (66%)]\tLoss: 1565256.375000\n",
            "Train Epoch: 78 [5120/7471 (69%)]\tLoss: 1583525.375000\n",
            "Train Epoch: 78 [5280/7471 (71%)]\tLoss: 1514799.250000\n",
            "Train Epoch: 78 [5440/7471 (73%)]\tLoss: 1591569.500000\n",
            "Train Epoch: 78 [5600/7471 (75%)]\tLoss: 1596273.625000\n",
            "Train Epoch: 78 [5760/7471 (77%)]\tLoss: 1512898.375000\n",
            "Train Epoch: 78 [5920/7471 (79%)]\tLoss: 1521757.000000\n",
            "Train Epoch: 78 [6080/7471 (81%)]\tLoss: 1565531.375000\n",
            "Train Epoch: 78 [6240/7471 (84%)]\tLoss: 1524885.500000\n",
            "Train Epoch: 78 [6400/7471 (86%)]\tLoss: 1584368.000000\n",
            "Train Epoch: 78 [6560/7471 (88%)]\tLoss: 1595796.750000\n",
            "Train Epoch: 78 [6720/7471 (90%)]\tLoss: 1552813.500000\n",
            "Train Epoch: 78 [6880/7471 (92%)]\tLoss: 1530221.375000\n",
            "Train Epoch: 78 [7040/7471 (94%)]\tLoss: 1586029.875000\n",
            "Train Epoch: 78 [7200/7471 (96%)]\tLoss: 1622835.625000\n",
            "Train Epoch: 78 [7360/7471 (99%)]\tLoss: 1555076.500000\n",
            "Epoch 78 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98506.3151\n",
            "\n",
            "Train Epoch: 79 [160/7471 (2%)]\tLoss: 1612423.750000\n",
            "Train Epoch: 79 [320/7471 (4%)]\tLoss: 1584480.750000\n",
            "Train Epoch: 79 [480/7471 (6%)]\tLoss: 1577067.375000\n",
            "Train Epoch: 79 [640/7471 (9%)]\tLoss: 1573894.750000\n",
            "Train Epoch: 79 [800/7471 (11%)]\tLoss: 1529640.000000\n",
            "Train Epoch: 79 [960/7471 (13%)]\tLoss: 1583491.750000\n",
            "Train Epoch: 79 [1120/7471 (15%)]\tLoss: 1609954.375000\n",
            "Train Epoch: 79 [1280/7471 (17%)]\tLoss: 1520107.750000\n",
            "Train Epoch: 79 [1440/7471 (19%)]\tLoss: 1573206.125000\n",
            "Train Epoch: 79 [1600/7471 (21%)]\tLoss: 1572334.500000\n",
            "Train Epoch: 79 [1760/7471 (24%)]\tLoss: 1567634.375000\n",
            "Train Epoch: 79 [1920/7471 (26%)]\tLoss: 1606181.625000\n",
            "Train Epoch: 79 [2080/7471 (28%)]\tLoss: 1561636.500000\n",
            "Train Epoch: 79 [2240/7471 (30%)]\tLoss: 1596780.750000\n",
            "Train Epoch: 79 [2400/7471 (32%)]\tLoss: 1634803.625000\n",
            "Train Epoch: 79 [2560/7471 (34%)]\tLoss: 1627043.250000\n",
            "Train Epoch: 79 [2720/7471 (36%)]\tLoss: 1568064.375000\n",
            "Train Epoch: 79 [2880/7471 (39%)]\tLoss: 1533839.750000\n",
            "Train Epoch: 79 [3040/7471 (41%)]\tLoss: 1596802.875000\n",
            "Train Epoch: 79 [3200/7471 (43%)]\tLoss: 1584244.500000\n",
            "Train Epoch: 79 [3360/7471 (45%)]\tLoss: 1587462.500000\n",
            "Train Epoch: 79 [3520/7471 (47%)]\tLoss: 1613699.000000\n",
            "Train Epoch: 79 [3680/7471 (49%)]\tLoss: 1527730.375000\n",
            "Train Epoch: 79 [3840/7471 (51%)]\tLoss: 1610748.500000\n",
            "Train Epoch: 79 [4000/7471 (54%)]\tLoss: 1545823.125000\n",
            "Train Epoch: 79 [4160/7471 (56%)]\tLoss: 1564022.000000\n",
            "Train Epoch: 79 [4320/7471 (58%)]\tLoss: 1547582.750000\n",
            "Train Epoch: 79 [4480/7471 (60%)]\tLoss: 1503474.625000\n",
            "Train Epoch: 79 [4640/7471 (62%)]\tLoss: 1520722.000000\n",
            "Train Epoch: 79 [4800/7471 (64%)]\tLoss: 1539781.000000\n",
            "Train Epoch: 79 [4960/7471 (66%)]\tLoss: 1584487.250000\n",
            "Train Epoch: 79 [5120/7471 (69%)]\tLoss: 1489424.750000\n",
            "Train Epoch: 79 [5280/7471 (71%)]\tLoss: 1584143.000000\n",
            "Train Epoch: 79 [5440/7471 (73%)]\tLoss: 1598693.625000\n",
            "Train Epoch: 79 [5600/7471 (75%)]\tLoss: 1596102.375000\n",
            "Train Epoch: 79 [5760/7471 (77%)]\tLoss: 1617715.875000\n",
            "Train Epoch: 79 [5920/7471 (79%)]\tLoss: 1519298.375000\n",
            "Train Epoch: 79 [6080/7471 (81%)]\tLoss: 1577686.875000\n",
            "Train Epoch: 79 [6240/7471 (84%)]\tLoss: 1555738.875000\n",
            "Train Epoch: 79 [6400/7471 (86%)]\tLoss: 1590445.375000\n",
            "Train Epoch: 79 [6560/7471 (88%)]\tLoss: 1566718.000000\n",
            "Train Epoch: 79 [6720/7471 (90%)]\tLoss: 1555021.125000\n",
            "Train Epoch: 79 [6880/7471 (92%)]\tLoss: 1583214.875000\n",
            "Train Epoch: 79 [7040/7471 (94%)]\tLoss: 1583644.875000\n",
            "Train Epoch: 79 [7200/7471 (96%)]\tLoss: 1608733.375000\n",
            "Train Epoch: 79 [7360/7471 (99%)]\tLoss: 1585396.125000\n",
            "Epoch 79 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98482.8543\n",
            "\n",
            "Train Epoch: 80 [160/7471 (2%)]\tLoss: 1583927.750000\n",
            "Train Epoch: 80 [320/7471 (4%)]\tLoss: 1615514.500000\n",
            "Train Epoch: 80 [480/7471 (6%)]\tLoss: 1616017.125000\n",
            "Train Epoch: 80 [640/7471 (9%)]\tLoss: 1600304.875000\n",
            "Train Epoch: 80 [800/7471 (11%)]\tLoss: 1520297.750000\n",
            "Train Epoch: 80 [960/7471 (13%)]\tLoss: 1547245.000000\n",
            "Train Epoch: 80 [1120/7471 (15%)]\tLoss: 1565920.125000\n",
            "Train Epoch: 80 [1280/7471 (17%)]\tLoss: 1502564.000000\n",
            "Train Epoch: 80 [1440/7471 (19%)]\tLoss: 1515926.000000\n",
            "Train Epoch: 80 [1600/7471 (21%)]\tLoss: 1573698.000000\n",
            "Train Epoch: 80 [1760/7471 (24%)]\tLoss: 1587537.125000\n",
            "Train Epoch: 80 [1920/7471 (26%)]\tLoss: 1601540.875000\n",
            "Train Epoch: 80 [2080/7471 (28%)]\tLoss: 1596580.125000\n",
            "Train Epoch: 80 [2240/7471 (30%)]\tLoss: 1612447.625000\n",
            "Train Epoch: 80 [2400/7471 (32%)]\tLoss: 1572698.250000\n",
            "Train Epoch: 80 [2560/7471 (34%)]\tLoss: 1470451.500000\n",
            "Train Epoch: 80 [2720/7471 (36%)]\tLoss: 1558505.875000\n",
            "Train Epoch: 80 [2880/7471 (39%)]\tLoss: 1629816.250000\n",
            "Train Epoch: 80 [3040/7471 (41%)]\tLoss: 1580590.375000\n",
            "Train Epoch: 80 [3200/7471 (43%)]\tLoss: 1520088.000000\n",
            "Train Epoch: 80 [3360/7471 (45%)]\tLoss: 1555471.500000\n",
            "Train Epoch: 80 [3520/7471 (47%)]\tLoss: 1595465.375000\n",
            "Train Epoch: 80 [3680/7471 (49%)]\tLoss: 1615760.500000\n",
            "Train Epoch: 80 [3840/7471 (51%)]\tLoss: 1633082.625000\n",
            "Train Epoch: 80 [4000/7471 (54%)]\tLoss: 1587166.625000\n",
            "Train Epoch: 80 [4160/7471 (56%)]\tLoss: 1503607.375000\n",
            "Train Epoch: 80 [4320/7471 (58%)]\tLoss: 1502401.750000\n",
            "Train Epoch: 80 [4480/7471 (60%)]\tLoss: 1565214.625000\n",
            "Train Epoch: 80 [4640/7471 (62%)]\tLoss: 1601330.000000\n",
            "Train Epoch: 80 [4800/7471 (64%)]\tLoss: 1613935.250000\n",
            "Train Epoch: 80 [4960/7471 (66%)]\tLoss: 1496106.875000\n",
            "Train Epoch: 80 [5120/7471 (69%)]\tLoss: 1574309.750000\n",
            "Train Epoch: 80 [5280/7471 (71%)]\tLoss: 1591109.500000\n",
            "Train Epoch: 80 [5440/7471 (73%)]\tLoss: 1566015.750000\n",
            "Train Epoch: 80 [5600/7471 (75%)]\tLoss: 1582940.500000\n",
            "Train Epoch: 80 [5760/7471 (77%)]\tLoss: 1601049.625000\n",
            "Train Epoch: 80 [5920/7471 (79%)]\tLoss: 1549037.500000\n",
            "Train Epoch: 80 [6080/7471 (81%)]\tLoss: 1593685.750000\n",
            "Train Epoch: 80 [6240/7471 (84%)]\tLoss: 1616389.125000\n",
            "Train Epoch: 80 [6400/7471 (86%)]\tLoss: 1581626.625000\n",
            "Train Epoch: 80 [6560/7471 (88%)]\tLoss: 1594715.000000\n",
            "Train Epoch: 80 [6720/7471 (90%)]\tLoss: 1607388.375000\n",
            "Train Epoch: 80 [6880/7471 (92%)]\tLoss: 1603074.250000\n",
            "Train Epoch: 80 [7040/7471 (94%)]\tLoss: 1626044.625000\n",
            "Train Epoch: 80 [7200/7471 (96%)]\tLoss: 1571719.125000\n",
            "Train Epoch: 80 [7360/7471 (99%)]\tLoss: 1596523.750000\n",
            "Epoch 80 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98524.2495\n",
            "\n",
            "Train Epoch: 81 [160/7471 (2%)]\tLoss: 1577399.250000\n",
            "Train Epoch: 81 [320/7471 (4%)]\tLoss: 1546306.875000\n",
            "Train Epoch: 81 [480/7471 (6%)]\tLoss: 1560568.125000\n",
            "Train Epoch: 81 [640/7471 (9%)]\tLoss: 1550509.000000\n",
            "Train Epoch: 81 [800/7471 (11%)]\tLoss: 1615546.750000\n",
            "Train Epoch: 81 [960/7471 (13%)]\tLoss: 1574862.375000\n",
            "Train Epoch: 81 [1120/7471 (15%)]\tLoss: 1562901.625000\n",
            "Train Epoch: 81 [1280/7471 (17%)]\tLoss: 1610620.500000\n",
            "Train Epoch: 81 [1440/7471 (19%)]\tLoss: 1543880.250000\n",
            "Train Epoch: 81 [1600/7471 (21%)]\tLoss: 1601976.250000\n",
            "Train Epoch: 81 [1760/7471 (24%)]\tLoss: 1556120.000000\n",
            "Train Epoch: 81 [1920/7471 (26%)]\tLoss: 1610946.750000\n",
            "Train Epoch: 81 [2080/7471 (28%)]\tLoss: 1588512.875000\n",
            "Train Epoch: 81 [2240/7471 (30%)]\tLoss: 1614995.500000\n",
            "Train Epoch: 81 [2400/7471 (32%)]\tLoss: 1629048.000000\n",
            "Train Epoch: 81 [2560/7471 (34%)]\tLoss: 1561204.000000\n",
            "Train Epoch: 81 [2720/7471 (36%)]\tLoss: 1580699.625000\n",
            "Train Epoch: 81 [2880/7471 (39%)]\tLoss: 1527643.500000\n",
            "Train Epoch: 81 [3040/7471 (41%)]\tLoss: 1586189.625000\n",
            "Train Epoch: 81 [3200/7471 (43%)]\tLoss: 1607546.375000\n",
            "Train Epoch: 81 [3360/7471 (45%)]\tLoss: 1589011.000000\n",
            "Train Epoch: 81 [3520/7471 (47%)]\tLoss: 1529034.875000\n",
            "Train Epoch: 81 [3680/7471 (49%)]\tLoss: 1561313.750000\n",
            "Train Epoch: 81 [3840/7471 (51%)]\tLoss: 1625114.625000\n",
            "Train Epoch: 81 [4000/7471 (54%)]\tLoss: 1609508.375000\n",
            "Train Epoch: 81 [4160/7471 (56%)]\tLoss: 1554033.000000\n",
            "Train Epoch: 81 [4320/7471 (58%)]\tLoss: 1621369.875000\n",
            "Train Epoch: 81 [4480/7471 (60%)]\tLoss: 1616121.750000\n",
            "Train Epoch: 81 [4640/7471 (62%)]\tLoss: 1591220.625000\n",
            "Train Epoch: 81 [4800/7471 (64%)]\tLoss: 1585743.125000\n",
            "Train Epoch: 81 [4960/7471 (66%)]\tLoss: 1536815.375000\n",
            "Train Epoch: 81 [5120/7471 (69%)]\tLoss: 1542932.625000\n",
            "Train Epoch: 81 [5280/7471 (71%)]\tLoss: 1596147.250000\n",
            "Train Epoch: 81 [5440/7471 (73%)]\tLoss: 1570459.125000\n",
            "Train Epoch: 81 [5600/7471 (75%)]\tLoss: 1556943.875000\n",
            "Train Epoch: 81 [5760/7471 (77%)]\tLoss: 1554301.375000\n",
            "Train Epoch: 81 [5920/7471 (79%)]\tLoss: 1607869.875000\n",
            "Train Epoch: 81 [6080/7471 (81%)]\tLoss: 1631752.125000\n",
            "Train Epoch: 81 [6240/7471 (84%)]\tLoss: 1539074.750000\n",
            "Train Epoch: 81 [6400/7471 (86%)]\tLoss: 1548135.500000\n",
            "Train Epoch: 81 [6560/7471 (88%)]\tLoss: 1548362.750000\n",
            "Train Epoch: 81 [6720/7471 (90%)]\tLoss: 1543696.000000\n",
            "Train Epoch: 81 [6880/7471 (92%)]\tLoss: 1457218.875000\n",
            "Train Epoch: 81 [7040/7471 (94%)]\tLoss: 1638122.625000\n",
            "Train Epoch: 81 [7200/7471 (96%)]\tLoss: 1564676.500000\n",
            "Train Epoch: 81 [7360/7471 (99%)]\tLoss: 1596966.125000\n",
            "Epoch 81 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98480.2659\n",
            "\n",
            "Train Epoch: 82 [160/7471 (2%)]\tLoss: 1584362.250000\n",
            "Train Epoch: 82 [320/7471 (4%)]\tLoss: 1523651.500000\n",
            "Train Epoch: 82 [480/7471 (6%)]\tLoss: 1593926.875000\n",
            "Train Epoch: 82 [640/7471 (9%)]\tLoss: 1581987.750000\n",
            "Train Epoch: 82 [800/7471 (11%)]\tLoss: 1609361.125000\n",
            "Train Epoch: 82 [960/7471 (13%)]\tLoss: 1550637.500000\n",
            "Train Epoch: 82 [1120/7471 (15%)]\tLoss: 1592675.125000\n",
            "Train Epoch: 82 [1280/7471 (17%)]\tLoss: 1536761.000000\n",
            "Train Epoch: 82 [1440/7471 (19%)]\tLoss: 1600872.875000\n",
            "Train Epoch: 82 [1600/7471 (21%)]\tLoss: 1628331.625000\n",
            "Train Epoch: 82 [1760/7471 (24%)]\tLoss: 1578002.000000\n",
            "Train Epoch: 82 [1920/7471 (26%)]\tLoss: 1606298.375000\n",
            "Train Epoch: 82 [2080/7471 (28%)]\tLoss: 1538032.500000\n",
            "Train Epoch: 82 [2240/7471 (30%)]\tLoss: 1600554.000000\n",
            "Train Epoch: 82 [2400/7471 (32%)]\tLoss: 1595684.625000\n",
            "Train Epoch: 82 [2560/7471 (34%)]\tLoss: 1623559.250000\n",
            "Train Epoch: 82 [2720/7471 (36%)]\tLoss: 1560412.375000\n",
            "Train Epoch: 82 [2880/7471 (39%)]\tLoss: 1601478.250000\n",
            "Train Epoch: 82 [3040/7471 (41%)]\tLoss: 1579391.375000\n",
            "Train Epoch: 82 [3200/7471 (43%)]\tLoss: 1499405.375000\n",
            "Train Epoch: 82 [3360/7471 (45%)]\tLoss: 1544220.625000\n",
            "Train Epoch: 82 [3520/7471 (47%)]\tLoss: 1644165.250000\n",
            "Train Epoch: 82 [3680/7471 (49%)]\tLoss: 1592986.250000\n",
            "Train Epoch: 82 [3840/7471 (51%)]\tLoss: 1563309.375000\n",
            "Train Epoch: 82 [4000/7471 (54%)]\tLoss: 1547019.500000\n",
            "Train Epoch: 82 [4160/7471 (56%)]\tLoss: 1553375.625000\n",
            "Train Epoch: 82 [4320/7471 (58%)]\tLoss: 1594421.750000\n",
            "Train Epoch: 82 [4480/7471 (60%)]\tLoss: 1613992.875000\n",
            "Train Epoch: 82 [4640/7471 (62%)]\tLoss: 1585421.375000\n",
            "Train Epoch: 82 [4800/7471 (64%)]\tLoss: 1606914.000000\n",
            "Train Epoch: 82 [4960/7471 (66%)]\tLoss: 1489671.625000\n",
            "Train Epoch: 82 [5120/7471 (69%)]\tLoss: 1582491.750000\n",
            "Train Epoch: 82 [5280/7471 (71%)]\tLoss: 1528520.375000\n",
            "Train Epoch: 82 [5440/7471 (73%)]\tLoss: 1566139.125000\n",
            "Train Epoch: 82 [5600/7471 (75%)]\tLoss: 1512980.500000\n",
            "Train Epoch: 82 [5760/7471 (77%)]\tLoss: 1555964.375000\n",
            "Train Epoch: 82 [5920/7471 (79%)]\tLoss: 1584941.000000\n",
            "Train Epoch: 82 [6080/7471 (81%)]\tLoss: 1594666.000000\n",
            "Train Epoch: 82 [6240/7471 (84%)]\tLoss: 1553238.250000\n",
            "Train Epoch: 82 [6400/7471 (86%)]\tLoss: 1562228.625000\n",
            "Train Epoch: 82 [6560/7471 (88%)]\tLoss: 1557898.500000\n",
            "Train Epoch: 82 [6720/7471 (90%)]\tLoss: 1506736.375000\n",
            "Train Epoch: 82 [6880/7471 (92%)]\tLoss: 1574904.375000\n",
            "Train Epoch: 82 [7040/7471 (94%)]\tLoss: 1581203.875000\n",
            "Train Epoch: 82 [7200/7471 (96%)]\tLoss: 1495748.750000\n",
            "Train Epoch: 82 [7360/7471 (99%)]\tLoss: 1566338.375000\n",
            "Epoch 82 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98549.2924\n",
            "\n",
            "Train Epoch: 83 [160/7471 (2%)]\tLoss: 1623741.125000\n",
            "Train Epoch: 83 [320/7471 (4%)]\tLoss: 1579273.750000\n",
            "Train Epoch: 83 [480/7471 (6%)]\tLoss: 1570093.750000\n",
            "Train Epoch: 83 [640/7471 (9%)]\tLoss: 1569408.250000\n",
            "Train Epoch: 83 [800/7471 (11%)]\tLoss: 1610023.625000\n",
            "Train Epoch: 83 [960/7471 (13%)]\tLoss: 1571353.875000\n",
            "Train Epoch: 83 [1120/7471 (15%)]\tLoss: 1587601.750000\n",
            "Train Epoch: 83 [1280/7471 (17%)]\tLoss: 1552954.625000\n",
            "Train Epoch: 83 [1440/7471 (19%)]\tLoss: 1614767.500000\n",
            "Train Epoch: 83 [1600/7471 (21%)]\tLoss: 1518389.500000\n",
            "Train Epoch: 83 [1760/7471 (24%)]\tLoss: 1515214.875000\n",
            "Train Epoch: 83 [1920/7471 (26%)]\tLoss: 1610385.750000\n",
            "Train Epoch: 83 [2080/7471 (28%)]\tLoss: 1582681.000000\n",
            "Train Epoch: 83 [2240/7471 (30%)]\tLoss: 1568577.500000\n",
            "Train Epoch: 83 [2400/7471 (32%)]\tLoss: 1587180.125000\n",
            "Train Epoch: 83 [2560/7471 (34%)]\tLoss: 1546247.750000\n",
            "Train Epoch: 83 [2720/7471 (36%)]\tLoss: 1545640.125000\n",
            "Train Epoch: 83 [2880/7471 (39%)]\tLoss: 1623004.750000\n",
            "Train Epoch: 83 [3040/7471 (41%)]\tLoss: 1627225.125000\n",
            "Train Epoch: 83 [3200/7471 (43%)]\tLoss: 1606322.625000\n",
            "Train Epoch: 83 [3360/7471 (45%)]\tLoss: 1567418.250000\n",
            "Train Epoch: 83 [3520/7471 (47%)]\tLoss: 1599969.750000\n",
            "Train Epoch: 83 [3680/7471 (49%)]\tLoss: 1555796.875000\n",
            "Train Epoch: 83 [3840/7471 (51%)]\tLoss: 1569963.250000\n",
            "Train Epoch: 83 [4000/7471 (54%)]\tLoss: 1623122.375000\n",
            "Train Epoch: 83 [4160/7471 (56%)]\tLoss: 1592151.750000\n",
            "Train Epoch: 83 [4320/7471 (58%)]\tLoss: 1556882.750000\n",
            "Train Epoch: 83 [4480/7471 (60%)]\tLoss: 1576774.000000\n",
            "Train Epoch: 83 [4640/7471 (62%)]\tLoss: 1520948.750000\n",
            "Train Epoch: 83 [4800/7471 (64%)]\tLoss: 1580715.625000\n",
            "Train Epoch: 83 [4960/7471 (66%)]\tLoss: 1575210.875000\n",
            "Train Epoch: 83 [5120/7471 (69%)]\tLoss: 1533005.875000\n",
            "Train Epoch: 83 [5280/7471 (71%)]\tLoss: 1607015.750000\n",
            "Train Epoch: 83 [5440/7471 (73%)]\tLoss: 1619940.625000\n",
            "Train Epoch: 83 [5600/7471 (75%)]\tLoss: 1609851.500000\n",
            "Train Epoch: 83 [5760/7471 (77%)]\tLoss: 1558092.250000\n",
            "Train Epoch: 83 [5920/7471 (79%)]\tLoss: 1539472.875000\n",
            "Train Epoch: 83 [6080/7471 (81%)]\tLoss: 1611940.250000\n",
            "Train Epoch: 83 [6240/7471 (84%)]\tLoss: 1577003.125000\n",
            "Train Epoch: 83 [6400/7471 (86%)]\tLoss: 1607269.125000\n",
            "Train Epoch: 83 [6560/7471 (88%)]\tLoss: 1528324.250000\n",
            "Train Epoch: 83 [6720/7471 (90%)]\tLoss: 1574347.125000\n",
            "Train Epoch: 83 [6880/7471 (92%)]\tLoss: 1555097.750000\n",
            "Train Epoch: 83 [7040/7471 (94%)]\tLoss: 1590334.375000\n",
            "Train Epoch: 83 [7200/7471 (96%)]\tLoss: 1550014.125000\n",
            "Train Epoch: 83 [7360/7471 (99%)]\tLoss: 1571051.875000\n",
            "Epoch 83 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98505.6253\n",
            "\n",
            "Train Epoch: 84 [160/7471 (2%)]\tLoss: 1547541.000000\n",
            "Train Epoch: 84 [320/7471 (4%)]\tLoss: 1611054.750000\n",
            "Train Epoch: 84 [480/7471 (6%)]\tLoss: 1591324.750000\n",
            "Train Epoch: 84 [640/7471 (9%)]\tLoss: 1584342.000000\n",
            "Train Epoch: 84 [800/7471 (11%)]\tLoss: 1577366.500000\n",
            "Train Epoch: 84 [960/7471 (13%)]\tLoss: 1611276.125000\n",
            "Train Epoch: 84 [1120/7471 (15%)]\tLoss: 1580636.875000\n",
            "Train Epoch: 84 [1280/7471 (17%)]\tLoss: 1583390.500000\n",
            "Train Epoch: 84 [1440/7471 (19%)]\tLoss: 1617385.375000\n",
            "Train Epoch: 84 [1600/7471 (21%)]\tLoss: 1569882.750000\n",
            "Train Epoch: 84 [1760/7471 (24%)]\tLoss: 1588981.250000\n",
            "Train Epoch: 84 [1920/7471 (26%)]\tLoss: 1568565.625000\n",
            "Train Epoch: 84 [2080/7471 (28%)]\tLoss: 1638637.000000\n",
            "Train Epoch: 84 [2240/7471 (30%)]\tLoss: 1574648.750000\n",
            "Train Epoch: 84 [2400/7471 (32%)]\tLoss: 1619118.500000\n",
            "Train Epoch: 84 [2560/7471 (34%)]\tLoss: 1572107.250000\n",
            "Train Epoch: 84 [2720/7471 (36%)]\tLoss: 1496760.875000\n",
            "Train Epoch: 84 [2880/7471 (39%)]\tLoss: 1611861.500000\n",
            "Train Epoch: 84 [3040/7471 (41%)]\tLoss: 1582562.750000\n",
            "Train Epoch: 84 [3200/7471 (43%)]\tLoss: 1546631.875000\n",
            "Train Epoch: 84 [3360/7471 (45%)]\tLoss: 1575118.500000\n",
            "Train Epoch: 84 [3520/7471 (47%)]\tLoss: 1607516.000000\n",
            "Train Epoch: 84 [3680/7471 (49%)]\tLoss: 1612561.500000\n",
            "Train Epoch: 84 [3840/7471 (51%)]\tLoss: 1574698.500000\n",
            "Train Epoch: 84 [4000/7471 (54%)]\tLoss: 1590083.000000\n",
            "Train Epoch: 84 [4160/7471 (56%)]\tLoss: 1501902.125000\n",
            "Train Epoch: 84 [4320/7471 (58%)]\tLoss: 1586143.750000\n",
            "Train Epoch: 84 [4480/7471 (60%)]\tLoss: 1608838.500000\n",
            "Train Epoch: 84 [4640/7471 (62%)]\tLoss: 1577081.000000\n",
            "Train Epoch: 84 [4800/7471 (64%)]\tLoss: 1622873.625000\n",
            "Train Epoch: 84 [4960/7471 (66%)]\tLoss: 1599261.250000\n",
            "Train Epoch: 84 [5120/7471 (69%)]\tLoss: 1622628.500000\n",
            "Train Epoch: 84 [5280/7471 (71%)]\tLoss: 1559650.750000\n",
            "Train Epoch: 84 [5440/7471 (73%)]\tLoss: 1626918.875000\n",
            "Train Epoch: 84 [5600/7471 (75%)]\tLoss: 1581701.250000\n",
            "Train Epoch: 84 [5760/7471 (77%)]\tLoss: 1557813.000000\n",
            "Train Epoch: 84 [5920/7471 (79%)]\tLoss: 1621357.125000\n",
            "Train Epoch: 84 [6080/7471 (81%)]\tLoss: 1592972.000000\n",
            "Train Epoch: 84 [6240/7471 (84%)]\tLoss: 1597546.250000\n",
            "Train Epoch: 84 [6400/7471 (86%)]\tLoss: 1516983.125000\n",
            "Train Epoch: 84 [6560/7471 (88%)]\tLoss: 1472791.125000\n",
            "Train Epoch: 84 [6720/7471 (90%)]\tLoss: 1540932.750000\n",
            "Train Epoch: 84 [6880/7471 (92%)]\tLoss: 1577836.125000\n",
            "Train Epoch: 84 [7040/7471 (94%)]\tLoss: 1537255.500000\n",
            "Train Epoch: 84 [7200/7471 (96%)]\tLoss: 1578442.500000\n",
            "Train Epoch: 84 [7360/7471 (99%)]\tLoss: 1574974.250000\n",
            "Epoch 84 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98450.1005\n",
            "\n",
            "Train Epoch: 85 [160/7471 (2%)]\tLoss: 1518179.750000\n",
            "Train Epoch: 85 [320/7471 (4%)]\tLoss: 1599835.375000\n",
            "Train Epoch: 85 [480/7471 (6%)]\tLoss: 1598115.250000\n",
            "Train Epoch: 85 [640/7471 (9%)]\tLoss: 1626772.750000\n",
            "Train Epoch: 85 [800/7471 (11%)]\tLoss: 1630179.250000\n",
            "Train Epoch: 85 [960/7471 (13%)]\tLoss: 1603124.750000\n",
            "Train Epoch: 85 [1120/7471 (15%)]\tLoss: 1559357.875000\n",
            "Train Epoch: 85 [1280/7471 (17%)]\tLoss: 1550649.375000\n",
            "Train Epoch: 85 [1440/7471 (19%)]\tLoss: 1585918.875000\n",
            "Train Epoch: 85 [1600/7471 (21%)]\tLoss: 1574477.125000\n",
            "Train Epoch: 85 [1760/7471 (24%)]\tLoss: 1604829.375000\n",
            "Train Epoch: 85 [1920/7471 (26%)]\tLoss: 1563927.625000\n",
            "Train Epoch: 85 [2080/7471 (28%)]\tLoss: 1619480.000000\n",
            "Train Epoch: 85 [2240/7471 (30%)]\tLoss: 1529499.375000\n",
            "Train Epoch: 85 [2400/7471 (32%)]\tLoss: 1581851.500000\n",
            "Train Epoch: 85 [2560/7471 (34%)]\tLoss: 1562012.250000\n",
            "Train Epoch: 85 [2720/7471 (36%)]\tLoss: 1621358.875000\n",
            "Train Epoch: 85 [2880/7471 (39%)]\tLoss: 1600879.125000\n",
            "Train Epoch: 85 [3040/7471 (41%)]\tLoss: 1583291.875000\n",
            "Train Epoch: 85 [3200/7471 (43%)]\tLoss: 1598146.500000\n",
            "Train Epoch: 85 [3360/7471 (45%)]\tLoss: 1522839.875000\n",
            "Train Epoch: 85 [3520/7471 (47%)]\tLoss: 1590861.250000\n",
            "Train Epoch: 85 [3680/7471 (49%)]\tLoss: 1569245.125000\n",
            "Train Epoch: 85 [3840/7471 (51%)]\tLoss: 1493879.625000\n",
            "Train Epoch: 85 [4000/7471 (54%)]\tLoss: 1634240.500000\n",
            "Train Epoch: 85 [4160/7471 (56%)]\tLoss: 1537521.125000\n",
            "Train Epoch: 85 [4320/7471 (58%)]\tLoss: 1535391.750000\n",
            "Train Epoch: 85 [4480/7471 (60%)]\tLoss: 1618039.500000\n",
            "Train Epoch: 85 [4640/7471 (62%)]\tLoss: 1574309.250000\n",
            "Train Epoch: 85 [4800/7471 (64%)]\tLoss: 1580296.875000\n",
            "Train Epoch: 85 [4960/7471 (66%)]\tLoss: 1546345.125000\n",
            "Train Epoch: 85 [5120/7471 (69%)]\tLoss: 1468635.000000\n",
            "Train Epoch: 85 [5280/7471 (71%)]\tLoss: 1539149.250000\n",
            "Train Epoch: 85 [5440/7471 (73%)]\tLoss: 1589696.625000\n",
            "Train Epoch: 85 [5600/7471 (75%)]\tLoss: 1538575.250000\n",
            "Train Epoch: 85 [5760/7471 (77%)]\tLoss: 1608118.250000\n",
            "Train Epoch: 85 [5920/7471 (79%)]\tLoss: 1624446.125000\n",
            "Train Epoch: 85 [6080/7471 (81%)]\tLoss: 1602742.250000\n",
            "Train Epoch: 85 [6240/7471 (84%)]\tLoss: 1578147.500000\n",
            "Train Epoch: 85 [6400/7471 (86%)]\tLoss: 1633280.875000\n",
            "Train Epoch: 85 [6560/7471 (88%)]\tLoss: 1570277.500000\n",
            "Train Epoch: 85 [6720/7471 (90%)]\tLoss: 1612424.125000\n",
            "Train Epoch: 85 [6880/7471 (92%)]\tLoss: 1613671.000000\n",
            "Train Epoch: 85 [7040/7471 (94%)]\tLoss: 1560547.250000\n",
            "Train Epoch: 85 [7200/7471 (96%)]\tLoss: 1555033.500000\n",
            "Train Epoch: 85 [7360/7471 (99%)]\tLoss: 1484191.500000\n",
            "Epoch 85 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98507.3227\n",
            "\n",
            "Train Epoch: 86 [160/7471 (2%)]\tLoss: 1597028.875000\n",
            "Train Epoch: 86 [320/7471 (4%)]\tLoss: 1615188.750000\n",
            "Train Epoch: 86 [480/7471 (6%)]\tLoss: 1615842.625000\n",
            "Train Epoch: 86 [640/7471 (9%)]\tLoss: 1553689.875000\n",
            "Train Epoch: 86 [800/7471 (11%)]\tLoss: 1593413.250000\n",
            "Train Epoch: 86 [960/7471 (13%)]\tLoss: 1523021.625000\n",
            "Train Epoch: 86 [1120/7471 (15%)]\tLoss: 1534948.375000\n",
            "Train Epoch: 86 [1280/7471 (17%)]\tLoss: 1539848.125000\n",
            "Train Epoch: 86 [1440/7471 (19%)]\tLoss: 1513952.250000\n",
            "Train Epoch: 86 [1600/7471 (21%)]\tLoss: 1587914.250000\n",
            "Train Epoch: 86 [1760/7471 (24%)]\tLoss: 1512523.625000\n",
            "Train Epoch: 86 [1920/7471 (26%)]\tLoss: 1570445.375000\n",
            "Train Epoch: 86 [2080/7471 (28%)]\tLoss: 1539501.500000\n",
            "Train Epoch: 86 [2240/7471 (30%)]\tLoss: 1618300.500000\n",
            "Train Epoch: 86 [2400/7471 (32%)]\tLoss: 1548213.500000\n",
            "Train Epoch: 86 [2560/7471 (34%)]\tLoss: 1599806.000000\n",
            "Train Epoch: 86 [2720/7471 (36%)]\tLoss: 1587456.750000\n",
            "Train Epoch: 86 [2880/7471 (39%)]\tLoss: 1580894.500000\n",
            "Train Epoch: 86 [3040/7471 (41%)]\tLoss: 1591703.625000\n",
            "Train Epoch: 86 [3200/7471 (43%)]\tLoss: 1530023.375000\n",
            "Train Epoch: 86 [3360/7471 (45%)]\tLoss: 1562955.250000\n",
            "Train Epoch: 86 [3520/7471 (47%)]\tLoss: 1616975.125000\n",
            "Train Epoch: 86 [3680/7471 (49%)]\tLoss: 1598393.875000\n",
            "Train Epoch: 86 [3840/7471 (51%)]\tLoss: 1623487.250000\n",
            "Train Epoch: 86 [4000/7471 (54%)]\tLoss: 1550512.375000\n",
            "Train Epoch: 86 [4160/7471 (56%)]\tLoss: 1467673.625000\n",
            "Train Epoch: 86 [4320/7471 (58%)]\tLoss: 1597624.125000\n",
            "Train Epoch: 86 [4480/7471 (60%)]\tLoss: 1503784.000000\n",
            "Train Epoch: 86 [4640/7471 (62%)]\tLoss: 1622902.625000\n",
            "Train Epoch: 86 [4800/7471 (64%)]\tLoss: 1593547.875000\n",
            "Train Epoch: 86 [4960/7471 (66%)]\tLoss: 1558259.250000\n",
            "Train Epoch: 86 [5120/7471 (69%)]\tLoss: 1591593.500000\n",
            "Train Epoch: 86 [5280/7471 (71%)]\tLoss: 1452949.500000\n",
            "Train Epoch: 86 [5440/7471 (73%)]\tLoss: 1590785.500000\n",
            "Train Epoch: 86 [5600/7471 (75%)]\tLoss: 1538380.375000\n",
            "Train Epoch: 86 [5760/7471 (77%)]\tLoss: 1624021.125000\n",
            "Train Epoch: 86 [5920/7471 (79%)]\tLoss: 1617932.875000\n",
            "Train Epoch: 86 [6080/7471 (81%)]\tLoss: 1634083.125000\n",
            "Train Epoch: 86 [6240/7471 (84%)]\tLoss: 1565198.375000\n",
            "Train Epoch: 86 [6400/7471 (86%)]\tLoss: 1616291.875000\n",
            "Train Epoch: 86 [6560/7471 (88%)]\tLoss: 1613145.875000\n",
            "Train Epoch: 86 [6720/7471 (90%)]\tLoss: 1587139.500000\n",
            "Train Epoch: 86 [6880/7471 (92%)]\tLoss: 1598133.375000\n",
            "Train Epoch: 86 [7040/7471 (94%)]\tLoss: 1575372.000000\n",
            "Train Epoch: 86 [7200/7471 (96%)]\tLoss: 1587852.875000\n",
            "Train Epoch: 86 [7360/7471 (99%)]\tLoss: 1594381.000000\n",
            "Epoch 86 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98466.0906\n",
            "\n",
            "Train Epoch: 87 [160/7471 (2%)]\tLoss: 1546360.625000\n",
            "Train Epoch: 87 [320/7471 (4%)]\tLoss: 1608239.125000\n",
            "Train Epoch: 87 [480/7471 (6%)]\tLoss: 1594885.625000\n",
            "Train Epoch: 87 [640/7471 (9%)]\tLoss: 1567896.125000\n",
            "Train Epoch: 87 [800/7471 (11%)]\tLoss: 1611130.625000\n",
            "Train Epoch: 87 [960/7471 (13%)]\tLoss: 1590829.375000\n",
            "Train Epoch: 87 [1120/7471 (15%)]\tLoss: 1524003.000000\n",
            "Train Epoch: 87 [1280/7471 (17%)]\tLoss: 1613007.625000\n",
            "Train Epoch: 87 [1440/7471 (19%)]\tLoss: 1557791.875000\n",
            "Train Epoch: 87 [1600/7471 (21%)]\tLoss: 1619387.250000\n",
            "Train Epoch: 87 [1760/7471 (24%)]\tLoss: 1560739.250000\n",
            "Train Epoch: 87 [1920/7471 (26%)]\tLoss: 1492247.375000\n",
            "Train Epoch: 87 [2080/7471 (28%)]\tLoss: 1581337.250000\n",
            "Train Epoch: 87 [2240/7471 (30%)]\tLoss: 1602032.750000\n",
            "Train Epoch: 87 [2400/7471 (32%)]\tLoss: 1606378.500000\n",
            "Train Epoch: 87 [2560/7471 (34%)]\tLoss: 1626186.875000\n",
            "Train Epoch: 87 [2720/7471 (36%)]\tLoss: 1574828.750000\n",
            "Train Epoch: 87 [2880/7471 (39%)]\tLoss: 1603496.000000\n",
            "Train Epoch: 87 [3040/7471 (41%)]\tLoss: 1614505.625000\n",
            "Train Epoch: 87 [3200/7471 (43%)]\tLoss: 1598230.250000\n",
            "Train Epoch: 87 [3360/7471 (45%)]\tLoss: 1572187.750000\n",
            "Train Epoch: 87 [3520/7471 (47%)]\tLoss: 1575185.500000\n",
            "Train Epoch: 87 [3680/7471 (49%)]\tLoss: 1556133.125000\n",
            "Train Epoch: 87 [3840/7471 (51%)]\tLoss: 1593483.625000\n",
            "Train Epoch: 87 [4000/7471 (54%)]\tLoss: 1601395.000000\n",
            "Train Epoch: 87 [4160/7471 (56%)]\tLoss: 1588065.125000\n",
            "Train Epoch: 87 [4320/7471 (58%)]\tLoss: 1603421.500000\n",
            "Train Epoch: 87 [4480/7471 (60%)]\tLoss: 1559590.000000\n",
            "Train Epoch: 87 [4640/7471 (62%)]\tLoss: 1628795.875000\n",
            "Train Epoch: 87 [4800/7471 (64%)]\tLoss: 1589360.000000\n",
            "Train Epoch: 87 [4960/7471 (66%)]\tLoss: 1584171.375000\n",
            "Train Epoch: 87 [5120/7471 (69%)]\tLoss: 1524179.750000\n",
            "Train Epoch: 87 [5280/7471 (71%)]\tLoss: 1608439.625000\n",
            "Train Epoch: 87 [5440/7471 (73%)]\tLoss: 1580338.500000\n",
            "Train Epoch: 87 [5600/7471 (75%)]\tLoss: 1575006.875000\n",
            "Train Epoch: 87 [5760/7471 (77%)]\tLoss: 1618752.375000\n",
            "Train Epoch: 87 [5920/7471 (79%)]\tLoss: 1614086.750000\n",
            "Train Epoch: 87 [6080/7471 (81%)]\tLoss: 1618862.625000\n",
            "Train Epoch: 87 [6240/7471 (84%)]\tLoss: 1532108.000000\n",
            "Train Epoch: 87 [6400/7471 (86%)]\tLoss: 1510169.750000\n",
            "Train Epoch: 87 [6560/7471 (88%)]\tLoss: 1554415.125000\n",
            "Train Epoch: 87 [6720/7471 (90%)]\tLoss: 1540541.875000\n",
            "Train Epoch: 87 [6880/7471 (92%)]\tLoss: 1556799.125000\n",
            "Train Epoch: 87 [7040/7471 (94%)]\tLoss: 1606871.250000\n",
            "Train Epoch: 87 [7200/7471 (96%)]\tLoss: 1552578.625000\n",
            "Train Epoch: 87 [7360/7471 (99%)]\tLoss: 1609180.125000\n",
            "Epoch 87 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98476.5147\n",
            "\n",
            "Train Epoch: 88 [160/7471 (2%)]\tLoss: 1599397.750000\n",
            "Train Epoch: 88 [320/7471 (4%)]\tLoss: 1592910.500000\n",
            "Train Epoch: 88 [480/7471 (6%)]\tLoss: 1479291.500000\n",
            "Train Epoch: 88 [640/7471 (9%)]\tLoss: 1628376.625000\n",
            "Train Epoch: 88 [800/7471 (11%)]\tLoss: 1597183.750000\n",
            "Train Epoch: 88 [960/7471 (13%)]\tLoss: 1559517.625000\n",
            "Train Epoch: 88 [1120/7471 (15%)]\tLoss: 1632966.875000\n",
            "Train Epoch: 88 [1280/7471 (17%)]\tLoss: 1607754.250000\n",
            "Train Epoch: 88 [1440/7471 (19%)]\tLoss: 1562722.250000\n",
            "Train Epoch: 88 [1600/7471 (21%)]\tLoss: 1587572.625000\n",
            "Train Epoch: 88 [1760/7471 (24%)]\tLoss: 1622936.000000\n",
            "Train Epoch: 88 [1920/7471 (26%)]\tLoss: 1560106.250000\n",
            "Train Epoch: 88 [2080/7471 (28%)]\tLoss: 1568418.375000\n",
            "Train Epoch: 88 [2240/7471 (30%)]\tLoss: 1498868.125000\n",
            "Train Epoch: 88 [2400/7471 (32%)]\tLoss: 1558863.125000\n",
            "Train Epoch: 88 [2560/7471 (34%)]\tLoss: 1635278.625000\n",
            "Train Epoch: 88 [2720/7471 (36%)]\tLoss: 1539454.000000\n",
            "Train Epoch: 88 [2880/7471 (39%)]\tLoss: 1584402.375000\n",
            "Train Epoch: 88 [3040/7471 (41%)]\tLoss: 1496897.500000\n",
            "Train Epoch: 88 [3200/7471 (43%)]\tLoss: 1570439.875000\n",
            "Train Epoch: 88 [3360/7471 (45%)]\tLoss: 1589065.625000\n",
            "Train Epoch: 88 [3520/7471 (47%)]\tLoss: 1598213.625000\n",
            "Train Epoch: 88 [3680/7471 (49%)]\tLoss: 1517627.625000\n",
            "Train Epoch: 88 [3840/7471 (51%)]\tLoss: 1611328.875000\n",
            "Train Epoch: 88 [4000/7471 (54%)]\tLoss: 1519245.500000\n",
            "Train Epoch: 88 [4160/7471 (56%)]\tLoss: 1597887.750000\n",
            "Train Epoch: 88 [4320/7471 (58%)]\tLoss: 1628403.500000\n",
            "Train Epoch: 88 [4480/7471 (60%)]\tLoss: 1540903.375000\n",
            "Train Epoch: 88 [4640/7471 (62%)]\tLoss: 1482134.500000\n",
            "Train Epoch: 88 [4800/7471 (64%)]\tLoss: 1548918.000000\n",
            "Train Epoch: 88 [4960/7471 (66%)]\tLoss: 1417743.000000\n",
            "Train Epoch: 88 [5120/7471 (69%)]\tLoss: 1572141.500000\n",
            "Train Epoch: 88 [5280/7471 (71%)]\tLoss: 1573680.000000\n",
            "Train Epoch: 88 [5440/7471 (73%)]\tLoss: 1541394.250000\n",
            "Train Epoch: 88 [5600/7471 (75%)]\tLoss: 1615438.875000\n",
            "Train Epoch: 88 [5760/7471 (77%)]\tLoss: 1581971.375000\n",
            "Train Epoch: 88 [5920/7471 (79%)]\tLoss: 1578495.250000\n",
            "Train Epoch: 88 [6080/7471 (81%)]\tLoss: 1549288.625000\n",
            "Train Epoch: 88 [6240/7471 (84%)]\tLoss: 1584275.000000\n",
            "Train Epoch: 88 [6400/7471 (86%)]\tLoss: 1529769.750000\n",
            "Train Epoch: 88 [6560/7471 (88%)]\tLoss: 1598800.750000\n",
            "Train Epoch: 88 [6720/7471 (90%)]\tLoss: 1554227.500000\n",
            "Train Epoch: 88 [6880/7471 (92%)]\tLoss: 1605949.875000\n",
            "Train Epoch: 88 [7040/7471 (94%)]\tLoss: 1532701.250000\n",
            "Train Epoch: 88 [7200/7471 (96%)]\tLoss: 1512796.000000\n",
            "Train Epoch: 88 [7360/7471 (99%)]\tLoss: 1612351.125000\n",
            "Epoch 88 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98433.7928\n",
            "\n",
            "Train Epoch: 89 [160/7471 (2%)]\tLoss: 1543269.125000\n",
            "Train Epoch: 89 [320/7471 (4%)]\tLoss: 1609692.750000\n",
            "Train Epoch: 89 [480/7471 (6%)]\tLoss: 1604511.000000\n",
            "Train Epoch: 89 [640/7471 (9%)]\tLoss: 1554401.250000\n",
            "Train Epoch: 89 [800/7471 (11%)]\tLoss: 1470431.000000\n",
            "Train Epoch: 89 [960/7471 (13%)]\tLoss: 1609683.625000\n",
            "Train Epoch: 89 [1120/7471 (15%)]\tLoss: 1633988.125000\n",
            "Train Epoch: 89 [1280/7471 (17%)]\tLoss: 1527409.500000\n",
            "Train Epoch: 89 [1440/7471 (19%)]\tLoss: 1571382.125000\n",
            "Train Epoch: 89 [1600/7471 (21%)]\tLoss: 1619583.250000\n",
            "Train Epoch: 89 [1760/7471 (24%)]\tLoss: 1584955.875000\n",
            "Train Epoch: 89 [1920/7471 (26%)]\tLoss: 1618465.875000\n",
            "Train Epoch: 89 [2080/7471 (28%)]\tLoss: 1582989.125000\n",
            "Train Epoch: 89 [2240/7471 (30%)]\tLoss: 1562199.250000\n",
            "Train Epoch: 89 [2400/7471 (32%)]\tLoss: 1627650.250000\n",
            "Train Epoch: 89 [2560/7471 (34%)]\tLoss: 1582782.625000\n",
            "Train Epoch: 89 [2720/7471 (36%)]\tLoss: 1612421.625000\n",
            "Train Epoch: 89 [2880/7471 (39%)]\tLoss: 1626647.250000\n",
            "Train Epoch: 89 [3040/7471 (41%)]\tLoss: 1591710.750000\n",
            "Train Epoch: 89 [3200/7471 (43%)]\tLoss: 1545631.625000\n",
            "Train Epoch: 89 [3360/7471 (45%)]\tLoss: 1605580.000000\n",
            "Train Epoch: 89 [3520/7471 (47%)]\tLoss: 1600222.000000\n",
            "Train Epoch: 89 [3680/7471 (49%)]\tLoss: 1587566.375000\n",
            "Train Epoch: 89 [3840/7471 (51%)]\tLoss: 1544179.625000\n",
            "Train Epoch: 89 [4000/7471 (54%)]\tLoss: 1492735.250000\n",
            "Train Epoch: 89 [4160/7471 (56%)]\tLoss: 1593933.875000\n",
            "Train Epoch: 89 [4320/7471 (58%)]\tLoss: 1570397.125000\n",
            "Train Epoch: 89 [4480/7471 (60%)]\tLoss: 1617943.875000\n",
            "Train Epoch: 89 [4640/7471 (62%)]\tLoss: 1503263.750000\n",
            "Train Epoch: 89 [4800/7471 (64%)]\tLoss: 1573418.500000\n",
            "Train Epoch: 89 [4960/7471 (66%)]\tLoss: 1592846.000000\n",
            "Train Epoch: 89 [5120/7471 (69%)]\tLoss: 1563313.500000\n",
            "Train Epoch: 89 [5280/7471 (71%)]\tLoss: 1534630.125000\n",
            "Train Epoch: 89 [5440/7471 (73%)]\tLoss: 1577041.000000\n",
            "Train Epoch: 89 [5600/7471 (75%)]\tLoss: 1578226.000000\n",
            "Train Epoch: 89 [5760/7471 (77%)]\tLoss: 1560525.125000\n",
            "Train Epoch: 89 [5920/7471 (79%)]\tLoss: 1600989.000000\n",
            "Train Epoch: 89 [6080/7471 (81%)]\tLoss: 1516444.875000\n",
            "Train Epoch: 89 [6240/7471 (84%)]\tLoss: 1594647.375000\n",
            "Train Epoch: 89 [6400/7471 (86%)]\tLoss: 1547038.625000\n",
            "Train Epoch: 89 [6560/7471 (88%)]\tLoss: 1599565.000000\n",
            "Train Epoch: 89 [6720/7471 (90%)]\tLoss: 1596027.625000\n",
            "Train Epoch: 89 [6880/7471 (92%)]\tLoss: 1545694.125000\n",
            "Train Epoch: 89 [7040/7471 (94%)]\tLoss: 1578138.625000\n",
            "Train Epoch: 89 [7200/7471 (96%)]\tLoss: 1583309.375000\n",
            "Train Epoch: 89 [7360/7471 (99%)]\tLoss: 1589730.250000\n",
            "Epoch 89 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98442.6701\n",
            "\n",
            "Train Epoch: 90 [160/7471 (2%)]\tLoss: 1582542.125000\n",
            "Train Epoch: 90 [320/7471 (4%)]\tLoss: 1545481.750000\n",
            "Train Epoch: 90 [480/7471 (6%)]\tLoss: 1577856.375000\n",
            "Train Epoch: 90 [640/7471 (9%)]\tLoss: 1564905.250000\n",
            "Train Epoch: 90 [800/7471 (11%)]\tLoss: 1606089.750000\n",
            "Train Epoch: 90 [960/7471 (13%)]\tLoss: 1622695.750000\n",
            "Train Epoch: 90 [1120/7471 (15%)]\tLoss: 1582087.875000\n",
            "Train Epoch: 90 [1280/7471 (17%)]\tLoss: 1524199.125000\n",
            "Train Epoch: 90 [1440/7471 (19%)]\tLoss: 1545055.500000\n",
            "Train Epoch: 90 [1600/7471 (21%)]\tLoss: 1562655.875000\n",
            "Train Epoch: 90 [1760/7471 (24%)]\tLoss: 1610046.625000\n",
            "Train Epoch: 90 [1920/7471 (26%)]\tLoss: 1508068.750000\n",
            "Train Epoch: 90 [2080/7471 (28%)]\tLoss: 1556983.250000\n",
            "Train Epoch: 90 [2240/7471 (30%)]\tLoss: 1632632.125000\n",
            "Train Epoch: 90 [2400/7471 (32%)]\tLoss: 1568108.750000\n",
            "Train Epoch: 90 [2560/7471 (34%)]\tLoss: 1577853.250000\n",
            "Train Epoch: 90 [2720/7471 (36%)]\tLoss: 1561643.125000\n",
            "Train Epoch: 90 [2880/7471 (39%)]\tLoss: 1535627.875000\n",
            "Train Epoch: 90 [3040/7471 (41%)]\tLoss: 1593610.875000\n",
            "Train Epoch: 90 [3200/7471 (43%)]\tLoss: 1579314.375000\n",
            "Train Epoch: 90 [3360/7471 (45%)]\tLoss: 1571885.375000\n",
            "Train Epoch: 90 [3520/7471 (47%)]\tLoss: 1585744.250000\n",
            "Train Epoch: 90 [3680/7471 (49%)]\tLoss: 1572111.250000\n",
            "Train Epoch: 90 [3840/7471 (51%)]\tLoss: 1565837.375000\n",
            "Train Epoch: 90 [4000/7471 (54%)]\tLoss: 1617641.875000\n",
            "Train Epoch: 90 [4160/7471 (56%)]\tLoss: 1474609.000000\n",
            "Train Epoch: 90 [4320/7471 (58%)]\tLoss: 1559529.625000\n",
            "Train Epoch: 90 [4480/7471 (60%)]\tLoss: 1588875.000000\n",
            "Train Epoch: 90 [4640/7471 (62%)]\tLoss: 1552453.875000\n",
            "Train Epoch: 90 [4800/7471 (64%)]\tLoss: 1582668.125000\n",
            "Train Epoch: 90 [4960/7471 (66%)]\tLoss: 1563310.250000\n",
            "Train Epoch: 90 [5120/7471 (69%)]\tLoss: 1593605.500000\n",
            "Train Epoch: 90 [5280/7471 (71%)]\tLoss: 1570361.125000\n",
            "Train Epoch: 90 [5440/7471 (73%)]\tLoss: 1571690.500000\n",
            "Train Epoch: 90 [5600/7471 (75%)]\tLoss: 1615115.250000\n",
            "Train Epoch: 90 [5760/7471 (77%)]\tLoss: 1630172.625000\n",
            "Train Epoch: 90 [5920/7471 (79%)]\tLoss: 1613107.125000\n",
            "Train Epoch: 90 [6080/7471 (81%)]\tLoss: 1569674.000000\n",
            "Train Epoch: 90 [6240/7471 (84%)]\tLoss: 1599150.750000\n",
            "Train Epoch: 90 [6400/7471 (86%)]\tLoss: 1549097.375000\n",
            "Train Epoch: 90 [6560/7471 (88%)]\tLoss: 1505037.875000\n",
            "Train Epoch: 90 [6720/7471 (90%)]\tLoss: 1559921.250000\n",
            "Train Epoch: 90 [6880/7471 (92%)]\tLoss: 1583059.625000\n",
            "Train Epoch: 90 [7040/7471 (94%)]\tLoss: 1563462.375000\n",
            "Train Epoch: 90 [7200/7471 (96%)]\tLoss: 1625937.750000\n",
            "Train Epoch: 90 [7360/7471 (99%)]\tLoss: 1572733.750000\n",
            "Epoch 90 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98463.4076\n",
            "\n",
            "Train Epoch: 91 [160/7471 (2%)]\tLoss: 1607634.125000\n",
            "Train Epoch: 91 [320/7471 (4%)]\tLoss: 1565086.375000\n",
            "Train Epoch: 91 [480/7471 (6%)]\tLoss: 1603336.375000\n",
            "Train Epoch: 91 [640/7471 (9%)]\tLoss: 1484555.125000\n",
            "Train Epoch: 91 [800/7471 (11%)]\tLoss: 1617050.750000\n",
            "Train Epoch: 91 [960/7471 (13%)]\tLoss: 1581393.625000\n",
            "Train Epoch: 91 [1120/7471 (15%)]\tLoss: 1550789.625000\n",
            "Train Epoch: 91 [1280/7471 (17%)]\tLoss: 1556392.625000\n",
            "Train Epoch: 91 [1440/7471 (19%)]\tLoss: 1615352.125000\n",
            "Train Epoch: 91 [1600/7471 (21%)]\tLoss: 1619191.625000\n",
            "Train Epoch: 91 [1760/7471 (24%)]\tLoss: 1551182.000000\n",
            "Train Epoch: 91 [1920/7471 (26%)]\tLoss: 1566169.000000\n",
            "Train Epoch: 91 [2080/7471 (28%)]\tLoss: 1561045.000000\n",
            "Train Epoch: 91 [2240/7471 (30%)]\tLoss: 1545766.000000\n",
            "Train Epoch: 91 [2400/7471 (32%)]\tLoss: 1573765.750000\n",
            "Train Epoch: 91 [2560/7471 (34%)]\tLoss: 1621935.625000\n",
            "Train Epoch: 91 [2720/7471 (36%)]\tLoss: 1491550.250000\n",
            "Train Epoch: 91 [2880/7471 (39%)]\tLoss: 1578940.000000\n",
            "Train Epoch: 91 [3040/7471 (41%)]\tLoss: 1588658.125000\n",
            "Train Epoch: 91 [3200/7471 (43%)]\tLoss: 1563838.750000\n",
            "Train Epoch: 91 [3360/7471 (45%)]\tLoss: 1545138.375000\n",
            "Train Epoch: 91 [3520/7471 (47%)]\tLoss: 1519119.875000\n",
            "Train Epoch: 91 [3680/7471 (49%)]\tLoss: 1517001.250000\n",
            "Train Epoch: 91 [3840/7471 (51%)]\tLoss: 1638128.375000\n",
            "Train Epoch: 91 [4000/7471 (54%)]\tLoss: 1544661.500000\n",
            "Train Epoch: 91 [4160/7471 (56%)]\tLoss: 1627224.750000\n",
            "Train Epoch: 91 [4320/7471 (58%)]\tLoss: 1580023.625000\n",
            "Train Epoch: 91 [4480/7471 (60%)]\tLoss: 1629746.000000\n",
            "Train Epoch: 91 [4640/7471 (62%)]\tLoss: 1554256.250000\n",
            "Train Epoch: 91 [4800/7471 (64%)]\tLoss: 1607939.625000\n",
            "Train Epoch: 91 [4960/7471 (66%)]\tLoss: 1575751.875000\n",
            "Train Epoch: 91 [5120/7471 (69%)]\tLoss: 1546332.250000\n",
            "Train Epoch: 91 [5280/7471 (71%)]\tLoss: 1617320.375000\n",
            "Train Epoch: 91 [5440/7471 (73%)]\tLoss: 1599555.500000\n",
            "Train Epoch: 91 [5600/7471 (75%)]\tLoss: 1526741.875000\n",
            "Train Epoch: 91 [5760/7471 (77%)]\tLoss: 1544829.375000\n",
            "Train Epoch: 91 [5920/7471 (79%)]\tLoss: 1619966.750000\n",
            "Train Epoch: 91 [6080/7471 (81%)]\tLoss: 1507392.250000\n",
            "Train Epoch: 91 [6240/7471 (84%)]\tLoss: 1580333.750000\n",
            "Train Epoch: 91 [6400/7471 (86%)]\tLoss: 1526373.250000\n",
            "Train Epoch: 91 [6560/7471 (88%)]\tLoss: 1587410.375000\n",
            "Train Epoch: 91 [6720/7471 (90%)]\tLoss: 1627550.000000\n",
            "Train Epoch: 91 [6880/7471 (92%)]\tLoss: 1619702.250000\n",
            "Train Epoch: 91 [7040/7471 (94%)]\tLoss: 1552357.250000\n",
            "Train Epoch: 91 [7200/7471 (96%)]\tLoss: 1584636.375000\n",
            "Train Epoch: 91 [7360/7471 (99%)]\tLoss: 1608614.750000\n",
            "Epoch 91 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98473.8756\n",
            "\n",
            "Train Epoch: 92 [160/7471 (2%)]\tLoss: 1564537.125000\n",
            "Train Epoch: 92 [320/7471 (4%)]\tLoss: 1614207.125000\n",
            "Train Epoch: 92 [480/7471 (6%)]\tLoss: 1578372.625000\n",
            "Train Epoch: 92 [640/7471 (9%)]\tLoss: 1611137.625000\n",
            "Train Epoch: 92 [800/7471 (11%)]\tLoss: 1558831.125000\n",
            "Train Epoch: 92 [960/7471 (13%)]\tLoss: 1564077.625000\n",
            "Train Epoch: 92 [1120/7471 (15%)]\tLoss: 1516363.750000\n",
            "Train Epoch: 92 [1280/7471 (17%)]\tLoss: 1619175.250000\n",
            "Train Epoch: 92 [1440/7471 (19%)]\tLoss: 1595575.375000\n",
            "Train Epoch: 92 [1600/7471 (21%)]\tLoss: 1548647.750000\n",
            "Train Epoch: 92 [1760/7471 (24%)]\tLoss: 1603641.375000\n",
            "Train Epoch: 92 [1920/7471 (26%)]\tLoss: 1610762.625000\n",
            "Train Epoch: 92 [2080/7471 (28%)]\tLoss: 1510929.125000\n",
            "Train Epoch: 92 [2240/7471 (30%)]\tLoss: 1582603.000000\n",
            "Train Epoch: 92 [2400/7471 (32%)]\tLoss: 1593866.500000\n",
            "Train Epoch: 92 [2560/7471 (34%)]\tLoss: 1532696.875000\n",
            "Train Epoch: 92 [2720/7471 (36%)]\tLoss: 1607348.750000\n",
            "Train Epoch: 92 [2880/7471 (39%)]\tLoss: 1564938.250000\n",
            "Train Epoch: 92 [3040/7471 (41%)]\tLoss: 1590881.625000\n",
            "Train Epoch: 92 [3200/7471 (43%)]\tLoss: 1609967.125000\n",
            "Train Epoch: 92 [3360/7471 (45%)]\tLoss: 1620000.625000\n",
            "Train Epoch: 92 [3520/7471 (47%)]\tLoss: 1583417.375000\n",
            "Train Epoch: 92 [3680/7471 (49%)]\tLoss: 1547950.375000\n",
            "Train Epoch: 92 [3840/7471 (51%)]\tLoss: 1580545.875000\n",
            "Train Epoch: 92 [4000/7471 (54%)]\tLoss: 1494219.500000\n",
            "Train Epoch: 92 [4160/7471 (56%)]\tLoss: 1589668.625000\n",
            "Train Epoch: 92 [4320/7471 (58%)]\tLoss: 1592585.000000\n",
            "Train Epoch: 92 [4480/7471 (60%)]\tLoss: 1598769.750000\n",
            "Train Epoch: 92 [4640/7471 (62%)]\tLoss: 1619406.250000\n",
            "Train Epoch: 92 [4800/7471 (64%)]\tLoss: 1575601.750000\n",
            "Train Epoch: 92 [4960/7471 (66%)]\tLoss: 1548147.375000\n",
            "Train Epoch: 92 [5120/7471 (69%)]\tLoss: 1628643.125000\n",
            "Train Epoch: 92 [5280/7471 (71%)]\tLoss: 1585569.250000\n",
            "Train Epoch: 92 [5440/7471 (73%)]\tLoss: 1600776.500000\n",
            "Train Epoch: 92 [5600/7471 (75%)]\tLoss: 1616187.750000\n",
            "Train Epoch: 92 [5760/7471 (77%)]\tLoss: 1594676.250000\n",
            "Train Epoch: 92 [5920/7471 (79%)]\tLoss: 1603516.875000\n",
            "Train Epoch: 92 [6080/7471 (81%)]\tLoss: 1595249.375000\n",
            "Train Epoch: 92 [6240/7471 (84%)]\tLoss: 1565824.125000\n",
            "Train Epoch: 92 [6400/7471 (86%)]\tLoss: 1544123.000000\n",
            "Train Epoch: 92 [6560/7471 (88%)]\tLoss: 1614792.625000\n",
            "Train Epoch: 92 [6720/7471 (90%)]\tLoss: 1574481.375000\n",
            "Train Epoch: 92 [6880/7471 (92%)]\tLoss: 1594961.125000\n",
            "Train Epoch: 92 [7040/7471 (94%)]\tLoss: 1572018.375000\n",
            "Train Epoch: 92 [7200/7471 (96%)]\tLoss: 1600984.875000\n",
            "Train Epoch: 92 [7360/7471 (99%)]\tLoss: 1530855.500000\n",
            "Epoch 92 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98465.5270\n",
            "\n",
            "Train Epoch: 93 [160/7471 (2%)]\tLoss: 1635728.250000\n",
            "Train Epoch: 93 [320/7471 (4%)]\tLoss: 1517170.750000\n",
            "Train Epoch: 93 [480/7471 (6%)]\tLoss: 1564072.375000\n",
            "Train Epoch: 93 [640/7471 (9%)]\tLoss: 1578786.750000\n",
            "Train Epoch: 93 [800/7471 (11%)]\tLoss: 1560762.500000\n",
            "Train Epoch: 93 [960/7471 (13%)]\tLoss: 1487543.750000\n",
            "Train Epoch: 93 [1120/7471 (15%)]\tLoss: 1540907.000000\n",
            "Train Epoch: 93 [1280/7471 (17%)]\tLoss: 1599896.250000\n",
            "Train Epoch: 93 [1440/7471 (19%)]\tLoss: 1602244.000000\n",
            "Train Epoch: 93 [1600/7471 (21%)]\tLoss: 1620847.375000\n",
            "Train Epoch: 93 [1760/7471 (24%)]\tLoss: 1537863.375000\n",
            "Train Epoch: 93 [1920/7471 (26%)]\tLoss: 1591346.375000\n",
            "Train Epoch: 93 [2080/7471 (28%)]\tLoss: 1556067.875000\n",
            "Train Epoch: 93 [2240/7471 (30%)]\tLoss: 1597585.375000\n",
            "Train Epoch: 93 [2400/7471 (32%)]\tLoss: 1529660.375000\n",
            "Train Epoch: 93 [2560/7471 (34%)]\tLoss: 1589395.875000\n",
            "Train Epoch: 93 [2720/7471 (36%)]\tLoss: 1631595.750000\n",
            "Train Epoch: 93 [2880/7471 (39%)]\tLoss: 1601191.125000\n",
            "Train Epoch: 93 [3040/7471 (41%)]\tLoss: 1554682.750000\n",
            "Train Epoch: 93 [3200/7471 (43%)]\tLoss: 1575053.375000\n",
            "Train Epoch: 93 [3360/7471 (45%)]\tLoss: 1531543.375000\n",
            "Train Epoch: 93 [3520/7471 (47%)]\tLoss: 1534141.750000\n",
            "Train Epoch: 93 [3680/7471 (49%)]\tLoss: 1641479.125000\n",
            "Train Epoch: 93 [3840/7471 (51%)]\tLoss: 1600119.375000\n",
            "Train Epoch: 93 [4000/7471 (54%)]\tLoss: 1613320.125000\n",
            "Train Epoch: 93 [4160/7471 (56%)]\tLoss: 1543837.875000\n",
            "Train Epoch: 93 [4320/7471 (58%)]\tLoss: 1604082.250000\n",
            "Train Epoch: 93 [4480/7471 (60%)]\tLoss: 1572296.625000\n",
            "Train Epoch: 93 [4640/7471 (62%)]\tLoss: 1594246.250000\n",
            "Train Epoch: 93 [4800/7471 (64%)]\tLoss: 1526482.875000\n",
            "Train Epoch: 93 [4960/7471 (66%)]\tLoss: 1562529.000000\n",
            "Train Epoch: 93 [5120/7471 (69%)]\tLoss: 1567051.250000\n",
            "Train Epoch: 93 [5280/7471 (71%)]\tLoss: 1542222.875000\n",
            "Train Epoch: 93 [5440/7471 (73%)]\tLoss: 1614915.875000\n",
            "Train Epoch: 93 [5600/7471 (75%)]\tLoss: 1573429.000000\n",
            "Train Epoch: 93 [5760/7471 (77%)]\tLoss: 1538793.000000\n",
            "Train Epoch: 93 [5920/7471 (79%)]\tLoss: 1579806.875000\n",
            "Train Epoch: 93 [6080/7471 (81%)]\tLoss: 1621271.375000\n",
            "Train Epoch: 93 [6240/7471 (84%)]\tLoss: 1561193.750000\n",
            "Train Epoch: 93 [6400/7471 (86%)]\tLoss: 1601599.875000\n",
            "Train Epoch: 93 [6560/7471 (88%)]\tLoss: 1627093.625000\n",
            "Train Epoch: 93 [6720/7471 (90%)]\tLoss: 1589359.625000\n",
            "Train Epoch: 93 [6880/7471 (92%)]\tLoss: 1622143.750000\n",
            "Train Epoch: 93 [7040/7471 (94%)]\tLoss: 1575909.750000\n",
            "Train Epoch: 93 [7200/7471 (96%)]\tLoss: 1551324.500000\n",
            "Train Epoch: 93 [7360/7471 (99%)]\tLoss: 1559093.625000\n",
            "Epoch 93 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98444.7675\n",
            "\n",
            "Train Epoch: 94 [160/7471 (2%)]\tLoss: 1528345.750000\n",
            "Train Epoch: 94 [320/7471 (4%)]\tLoss: 1512573.750000\n",
            "Train Epoch: 94 [480/7471 (6%)]\tLoss: 1613015.625000\n",
            "Train Epoch: 94 [640/7471 (9%)]\tLoss: 1559995.250000\n",
            "Train Epoch: 94 [800/7471 (11%)]\tLoss: 1572908.750000\n",
            "Train Epoch: 94 [960/7471 (13%)]\tLoss: 1615278.250000\n",
            "Train Epoch: 94 [1120/7471 (15%)]\tLoss: 1511891.625000\n",
            "Train Epoch: 94 [1280/7471 (17%)]\tLoss: 1569411.875000\n",
            "Train Epoch: 94 [1440/7471 (19%)]\tLoss: 1576463.000000\n",
            "Train Epoch: 94 [1600/7471 (21%)]\tLoss: 1567419.625000\n",
            "Train Epoch: 94 [1760/7471 (24%)]\tLoss: 1569806.375000\n",
            "Train Epoch: 94 [1920/7471 (26%)]\tLoss: 1569929.250000\n",
            "Train Epoch: 94 [2080/7471 (28%)]\tLoss: 1597311.125000\n",
            "Train Epoch: 94 [2240/7471 (30%)]\tLoss: 1576433.125000\n",
            "Train Epoch: 94 [2400/7471 (32%)]\tLoss: 1563867.500000\n",
            "Train Epoch: 94 [2560/7471 (34%)]\tLoss: 1439818.125000\n",
            "Train Epoch: 94 [2720/7471 (36%)]\tLoss: 1562870.625000\n",
            "Train Epoch: 94 [2880/7471 (39%)]\tLoss: 1525697.625000\n",
            "Train Epoch: 94 [3040/7471 (41%)]\tLoss: 1532642.375000\n",
            "Train Epoch: 94 [3200/7471 (43%)]\tLoss: 1560955.000000\n",
            "Train Epoch: 94 [3360/7471 (45%)]\tLoss: 1510391.500000\n",
            "Train Epoch: 94 [3520/7471 (47%)]\tLoss: 1491343.000000\n",
            "Train Epoch: 94 [3680/7471 (49%)]\tLoss: 1530540.250000\n",
            "Train Epoch: 94 [3840/7471 (51%)]\tLoss: 1560963.125000\n",
            "Train Epoch: 94 [4000/7471 (54%)]\tLoss: 1619856.500000\n",
            "Train Epoch: 94 [4160/7471 (56%)]\tLoss: 1571285.375000\n",
            "Train Epoch: 94 [4320/7471 (58%)]\tLoss: 1570078.250000\n",
            "Train Epoch: 94 [4480/7471 (60%)]\tLoss: 1482132.625000\n",
            "Train Epoch: 94 [4640/7471 (62%)]\tLoss: 1572701.750000\n",
            "Train Epoch: 94 [4800/7471 (64%)]\tLoss: 1634081.000000\n",
            "Train Epoch: 94 [4960/7471 (66%)]\tLoss: 1512120.125000\n",
            "Train Epoch: 94 [5120/7471 (69%)]\tLoss: 1626701.000000\n",
            "Train Epoch: 94 [5280/7471 (71%)]\tLoss: 1590867.375000\n",
            "Train Epoch: 94 [5440/7471 (73%)]\tLoss: 1598024.375000\n",
            "Train Epoch: 94 [5600/7471 (75%)]\tLoss: 1549037.500000\n",
            "Train Epoch: 94 [5760/7471 (77%)]\tLoss: 1590105.500000\n",
            "Train Epoch: 94 [5920/7471 (79%)]\tLoss: 1545227.125000\n",
            "Train Epoch: 94 [6080/7471 (81%)]\tLoss: 1583747.375000\n",
            "Train Epoch: 94 [6240/7471 (84%)]\tLoss: 1622312.750000\n",
            "Train Epoch: 94 [6400/7471 (86%)]\tLoss: 1562346.750000\n",
            "Train Epoch: 94 [6560/7471 (88%)]\tLoss: 1629913.375000\n",
            "Train Epoch: 94 [6720/7471 (90%)]\tLoss: 1576526.000000\n",
            "Train Epoch: 94 [6880/7471 (92%)]\tLoss: 1511802.500000\n",
            "Train Epoch: 94 [7040/7471 (94%)]\tLoss: 1592893.375000\n",
            "Train Epoch: 94 [7200/7471 (96%)]\tLoss: 1577985.250000\n",
            "Train Epoch: 94 [7360/7471 (99%)]\tLoss: 1591320.625000\n",
            "Epoch 94 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98434.8281\n",
            "\n",
            "Train Epoch: 95 [160/7471 (2%)]\tLoss: 1597101.000000\n",
            "Train Epoch: 95 [320/7471 (4%)]\tLoss: 1557706.375000\n",
            "Train Epoch: 95 [480/7471 (6%)]\tLoss: 1613305.000000\n",
            "Train Epoch: 95 [640/7471 (9%)]\tLoss: 1607950.500000\n",
            "Train Epoch: 95 [800/7471 (11%)]\tLoss: 1625206.125000\n",
            "Train Epoch: 95 [960/7471 (13%)]\tLoss: 1539572.625000\n",
            "Train Epoch: 95 [1120/7471 (15%)]\tLoss: 1474848.000000\n",
            "Train Epoch: 95 [1280/7471 (17%)]\tLoss: 1582932.625000\n",
            "Train Epoch: 95 [1440/7471 (19%)]\tLoss: 1600367.875000\n",
            "Train Epoch: 95 [1600/7471 (21%)]\tLoss: 1603795.375000\n",
            "Train Epoch: 95 [1760/7471 (24%)]\tLoss: 1590701.375000\n",
            "Train Epoch: 95 [1920/7471 (26%)]\tLoss: 1579713.375000\n",
            "Train Epoch: 95 [2080/7471 (28%)]\tLoss: 1628841.500000\n",
            "Train Epoch: 95 [2240/7471 (30%)]\tLoss: 1541296.625000\n",
            "Train Epoch: 95 [2400/7471 (32%)]\tLoss: 1574358.125000\n",
            "Train Epoch: 95 [2560/7471 (34%)]\tLoss: 1524317.750000\n",
            "Train Epoch: 95 [2720/7471 (36%)]\tLoss: 1560030.750000\n",
            "Train Epoch: 95 [2880/7471 (39%)]\tLoss: 1521047.500000\n",
            "Train Epoch: 95 [3040/7471 (41%)]\tLoss: 1524025.875000\n",
            "Train Epoch: 95 [3200/7471 (43%)]\tLoss: 1550870.750000\n",
            "Train Epoch: 95 [3360/7471 (45%)]\tLoss: 1531009.125000\n",
            "Train Epoch: 95 [3520/7471 (47%)]\tLoss: 1572848.500000\n",
            "Train Epoch: 95 [3680/7471 (49%)]\tLoss: 1552562.375000\n",
            "Train Epoch: 95 [3840/7471 (51%)]\tLoss: 1553109.000000\n",
            "Train Epoch: 95 [4000/7471 (54%)]\tLoss: 1573882.875000\n",
            "Train Epoch: 95 [4160/7471 (56%)]\tLoss: 1613093.375000\n",
            "Train Epoch: 95 [4320/7471 (58%)]\tLoss: 1521323.625000\n",
            "Train Epoch: 95 [4480/7471 (60%)]\tLoss: 1608323.625000\n",
            "Train Epoch: 95 [4640/7471 (62%)]\tLoss: 1617182.500000\n",
            "Train Epoch: 95 [4800/7471 (64%)]\tLoss: 1612065.250000\n",
            "Train Epoch: 95 [4960/7471 (66%)]\tLoss: 1637708.000000\n",
            "Train Epoch: 95 [5120/7471 (69%)]\tLoss: 1563992.875000\n",
            "Train Epoch: 95 [5280/7471 (71%)]\tLoss: 1621675.625000\n",
            "Train Epoch: 95 [5440/7471 (73%)]\tLoss: 1564084.000000\n",
            "Train Epoch: 95 [5600/7471 (75%)]\tLoss: 1582732.250000\n",
            "Train Epoch: 95 [5760/7471 (77%)]\tLoss: 1633128.000000\n",
            "Train Epoch: 95 [5920/7471 (79%)]\tLoss: 1565364.750000\n",
            "Train Epoch: 95 [6080/7471 (81%)]\tLoss: 1603101.750000\n",
            "Train Epoch: 95 [6240/7471 (84%)]\tLoss: 1504314.875000\n",
            "Train Epoch: 95 [6400/7471 (86%)]\tLoss: 1488778.500000\n",
            "Train Epoch: 95 [6560/7471 (88%)]\tLoss: 1602722.500000\n",
            "Train Epoch: 95 [6720/7471 (90%)]\tLoss: 1589268.000000\n",
            "Train Epoch: 95 [6880/7471 (92%)]\tLoss: 1591887.250000\n",
            "Train Epoch: 95 [7040/7471 (94%)]\tLoss: 1585944.375000\n",
            "Train Epoch: 95 [7200/7471 (96%)]\tLoss: 1557664.750000\n",
            "Train Epoch: 95 [7360/7471 (99%)]\tLoss: 1572737.375000\n",
            "Epoch 95 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98435.7628\n",
            "\n",
            "Train Epoch: 96 [160/7471 (2%)]\tLoss: 1529906.375000\n",
            "Train Epoch: 96 [320/7471 (4%)]\tLoss: 1547458.625000\n",
            "Train Epoch: 96 [480/7471 (6%)]\tLoss: 1572138.875000\n",
            "Train Epoch: 96 [640/7471 (9%)]\tLoss: 1572203.625000\n",
            "Train Epoch: 96 [800/7471 (11%)]\tLoss: 1542554.375000\n",
            "Train Epoch: 96 [960/7471 (13%)]\tLoss: 1556196.625000\n",
            "Train Epoch: 96 [1120/7471 (15%)]\tLoss: 1628000.250000\n",
            "Train Epoch: 96 [1280/7471 (17%)]\tLoss: 1556829.250000\n",
            "Train Epoch: 96 [1440/7471 (19%)]\tLoss: 1585594.500000\n",
            "Train Epoch: 96 [1600/7471 (21%)]\tLoss: 1579845.500000\n",
            "Train Epoch: 96 [1760/7471 (24%)]\tLoss: 1493801.750000\n",
            "Train Epoch: 96 [1920/7471 (26%)]\tLoss: 1564001.500000\n",
            "Train Epoch: 96 [2080/7471 (28%)]\tLoss: 1592259.125000\n",
            "Train Epoch: 96 [2240/7471 (30%)]\tLoss: 1542479.000000\n",
            "Train Epoch: 96 [2400/7471 (32%)]\tLoss: 1537819.000000\n",
            "Train Epoch: 96 [2560/7471 (34%)]\tLoss: 1549188.500000\n",
            "Train Epoch: 96 [2720/7471 (36%)]\tLoss: 1520639.250000\n",
            "Train Epoch: 96 [2880/7471 (39%)]\tLoss: 1556920.375000\n",
            "Train Epoch: 96 [3040/7471 (41%)]\tLoss: 1573162.250000\n",
            "Train Epoch: 96 [3200/7471 (43%)]\tLoss: 1564828.125000\n",
            "Train Epoch: 96 [3360/7471 (45%)]\tLoss: 1523182.375000\n",
            "Train Epoch: 96 [3520/7471 (47%)]\tLoss: 1630648.000000\n",
            "Train Epoch: 96 [3680/7471 (49%)]\tLoss: 1588499.250000\n",
            "Train Epoch: 96 [3840/7471 (51%)]\tLoss: 1594118.125000\n",
            "Train Epoch: 96 [4000/7471 (54%)]\tLoss: 1605000.750000\n",
            "Train Epoch: 96 [4160/7471 (56%)]\tLoss: 1554552.000000\n",
            "Train Epoch: 96 [4320/7471 (58%)]\tLoss: 1588620.875000\n",
            "Train Epoch: 96 [4480/7471 (60%)]\tLoss: 1630138.000000\n",
            "Train Epoch: 96 [4640/7471 (62%)]\tLoss: 1511392.375000\n",
            "Train Epoch: 96 [4800/7471 (64%)]\tLoss: 1618069.375000\n",
            "Train Epoch: 96 [4960/7471 (66%)]\tLoss: 1549049.375000\n",
            "Train Epoch: 96 [5120/7471 (69%)]\tLoss: 1631757.250000\n",
            "Train Epoch: 96 [5280/7471 (71%)]\tLoss: 1609245.000000\n",
            "Train Epoch: 96 [5440/7471 (73%)]\tLoss: 1544146.500000\n",
            "Train Epoch: 96 [5600/7471 (75%)]\tLoss: 1629383.625000\n",
            "Train Epoch: 96 [5760/7471 (77%)]\tLoss: 1604044.250000\n",
            "Train Epoch: 96 [5920/7471 (79%)]\tLoss: 1620026.000000\n",
            "Train Epoch: 96 [6080/7471 (81%)]\tLoss: 1608797.875000\n",
            "Train Epoch: 96 [6240/7471 (84%)]\tLoss: 1544305.750000\n",
            "Train Epoch: 96 [6400/7471 (86%)]\tLoss: 1599473.500000\n",
            "Train Epoch: 96 [6560/7471 (88%)]\tLoss: 1539686.125000\n",
            "Train Epoch: 96 [6720/7471 (90%)]\tLoss: 1558159.125000\n",
            "Train Epoch: 96 [6880/7471 (92%)]\tLoss: 1577005.125000\n",
            "Train Epoch: 96 [7040/7471 (94%)]\tLoss: 1585502.625000\n",
            "Train Epoch: 96 [7200/7471 (96%)]\tLoss: 1610673.625000\n",
            "Train Epoch: 96 [7360/7471 (99%)]\tLoss: 1591725.625000\n",
            "Epoch 96 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98468.6730\n",
            "\n",
            "Train Epoch: 97 [160/7471 (2%)]\tLoss: 1626186.375000\n",
            "Train Epoch: 97 [320/7471 (4%)]\tLoss: 1617719.625000\n",
            "Train Epoch: 97 [480/7471 (6%)]\tLoss: 1549722.000000\n",
            "Train Epoch: 97 [640/7471 (9%)]\tLoss: 1617562.750000\n",
            "Train Epoch: 97 [800/7471 (11%)]\tLoss: 1592881.875000\n",
            "Train Epoch: 97 [960/7471 (13%)]\tLoss: 1456895.500000\n",
            "Train Epoch: 97 [1120/7471 (15%)]\tLoss: 1524315.250000\n",
            "Train Epoch: 97 [1280/7471 (17%)]\tLoss: 1578864.125000\n",
            "Train Epoch: 97 [1440/7471 (19%)]\tLoss: 1516618.000000\n",
            "Train Epoch: 97 [1600/7471 (21%)]\tLoss: 1626210.750000\n",
            "Train Epoch: 97 [1760/7471 (24%)]\tLoss: 1625874.375000\n",
            "Train Epoch: 97 [1920/7471 (26%)]\tLoss: 1547586.500000\n",
            "Train Epoch: 97 [2080/7471 (28%)]\tLoss: 1602562.500000\n",
            "Train Epoch: 97 [2240/7471 (30%)]\tLoss: 1562835.000000\n",
            "Train Epoch: 97 [2400/7471 (32%)]\tLoss: 1603000.250000\n",
            "Train Epoch: 97 [2560/7471 (34%)]\tLoss: 1553661.750000\n",
            "Train Epoch: 97 [2720/7471 (36%)]\tLoss: 1596159.875000\n",
            "Train Epoch: 97 [2880/7471 (39%)]\tLoss: 1550200.000000\n",
            "Train Epoch: 97 [3040/7471 (41%)]\tLoss: 1622686.875000\n",
            "Train Epoch: 97 [3200/7471 (43%)]\tLoss: 1563672.750000\n",
            "Train Epoch: 97 [3360/7471 (45%)]\tLoss: 1549867.875000\n",
            "Train Epoch: 97 [3520/7471 (47%)]\tLoss: 1573066.125000\n",
            "Train Epoch: 97 [3680/7471 (49%)]\tLoss: 1577150.625000\n",
            "Train Epoch: 97 [3840/7471 (51%)]\tLoss: 1583861.625000\n",
            "Train Epoch: 97 [4000/7471 (54%)]\tLoss: 1585603.250000\n",
            "Train Epoch: 97 [4160/7471 (56%)]\tLoss: 1551846.125000\n",
            "Train Epoch: 97 [4320/7471 (58%)]\tLoss: 1476634.625000\n",
            "Train Epoch: 97 [4480/7471 (60%)]\tLoss: 1601529.750000\n",
            "Train Epoch: 97 [4640/7471 (62%)]\tLoss: 1490386.375000\n",
            "Train Epoch: 97 [4800/7471 (64%)]\tLoss: 1572249.750000\n",
            "Train Epoch: 97 [4960/7471 (66%)]\tLoss: 1550135.875000\n",
            "Train Epoch: 97 [5120/7471 (69%)]\tLoss: 1514838.000000\n",
            "Train Epoch: 97 [5280/7471 (71%)]\tLoss: 1573461.000000\n",
            "Train Epoch: 97 [5440/7471 (73%)]\tLoss: 1550615.875000\n",
            "Train Epoch: 97 [5600/7471 (75%)]\tLoss: 1584710.000000\n",
            "Train Epoch: 97 [5760/7471 (77%)]\tLoss: 1513307.375000\n",
            "Train Epoch: 97 [5920/7471 (79%)]\tLoss: 1588630.875000\n",
            "Train Epoch: 97 [6080/7471 (81%)]\tLoss: 1576717.625000\n",
            "Train Epoch: 97 [6240/7471 (84%)]\tLoss: 1570419.625000\n",
            "Train Epoch: 97 [6400/7471 (86%)]\tLoss: 1592976.125000\n",
            "Train Epoch: 97 [6560/7471 (88%)]\tLoss: 1544147.875000\n",
            "Train Epoch: 97 [6720/7471 (90%)]\tLoss: 1585486.875000\n",
            "Train Epoch: 97 [6880/7471 (92%)]\tLoss: 1583296.875000\n",
            "Train Epoch: 97 [7040/7471 (94%)]\tLoss: 1461829.125000\n",
            "Train Epoch: 97 [7200/7471 (96%)]\tLoss: 1507475.000000\n",
            "Train Epoch: 97 [7360/7471 (99%)]\tLoss: 1594706.375000\n",
            "Epoch 97 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98525.7165\n",
            "\n",
            "Train Epoch: 98 [160/7471 (2%)]\tLoss: 1574534.250000\n",
            "Train Epoch: 98 [320/7471 (4%)]\tLoss: 1510457.250000\n",
            "Train Epoch: 98 [480/7471 (6%)]\tLoss: 1623423.750000\n",
            "Train Epoch: 98 [640/7471 (9%)]\tLoss: 1587943.875000\n",
            "Train Epoch: 98 [800/7471 (11%)]\tLoss: 1599030.875000\n",
            "Train Epoch: 98 [960/7471 (13%)]\tLoss: 1549109.250000\n",
            "Train Epoch: 98 [1120/7471 (15%)]\tLoss: 1630971.250000\n",
            "Train Epoch: 98 [1280/7471 (17%)]\tLoss: 1528924.625000\n",
            "Train Epoch: 98 [1440/7471 (19%)]\tLoss: 1601676.375000\n",
            "Train Epoch: 98 [1600/7471 (21%)]\tLoss: 1589057.500000\n",
            "Train Epoch: 98 [1760/7471 (24%)]\tLoss: 1566117.625000\n",
            "Train Epoch: 98 [1920/7471 (26%)]\tLoss: 1614176.625000\n",
            "Train Epoch: 98 [2080/7471 (28%)]\tLoss: 1588644.875000\n",
            "Train Epoch: 98 [2240/7471 (30%)]\tLoss: 1551461.750000\n",
            "Train Epoch: 98 [2400/7471 (32%)]\tLoss: 1584499.000000\n",
            "Train Epoch: 98 [2560/7471 (34%)]\tLoss: 1585488.500000\n",
            "Train Epoch: 98 [2720/7471 (36%)]\tLoss: 1609589.625000\n",
            "Train Epoch: 98 [2880/7471 (39%)]\tLoss: 1561305.125000\n",
            "Train Epoch: 98 [3040/7471 (41%)]\tLoss: 1613804.125000\n",
            "Train Epoch: 98 [3200/7471 (43%)]\tLoss: 1561054.000000\n",
            "Train Epoch: 98 [3360/7471 (45%)]\tLoss: 1503094.750000\n",
            "Train Epoch: 98 [3520/7471 (47%)]\tLoss: 1552439.500000\n",
            "Train Epoch: 98 [3680/7471 (49%)]\tLoss: 1594809.250000\n",
            "Train Epoch: 98 [3840/7471 (51%)]\tLoss: 1541831.750000\n",
            "Train Epoch: 98 [4000/7471 (54%)]\tLoss: 1593633.500000\n",
            "Train Epoch: 98 [4160/7471 (56%)]\tLoss: 1553759.875000\n",
            "Train Epoch: 98 [4320/7471 (58%)]\tLoss: 1593042.250000\n",
            "Train Epoch: 98 [4480/7471 (60%)]\tLoss: 1571399.250000\n",
            "Train Epoch: 98 [4640/7471 (62%)]\tLoss: 1628800.500000\n",
            "Train Epoch: 98 [4800/7471 (64%)]\tLoss: 1531672.500000\n",
            "Train Epoch: 98 [4960/7471 (66%)]\tLoss: 1501724.625000\n",
            "Train Epoch: 98 [5120/7471 (69%)]\tLoss: 1558545.125000\n",
            "Train Epoch: 98 [5280/7471 (71%)]\tLoss: 1557868.750000\n",
            "Train Epoch: 98 [5440/7471 (73%)]\tLoss: 1577409.250000\n",
            "Train Epoch: 98 [5600/7471 (75%)]\tLoss: 1561408.875000\n",
            "Train Epoch: 98 [5760/7471 (77%)]\tLoss: 1609849.875000\n",
            "Train Epoch: 98 [5920/7471 (79%)]\tLoss: 1599615.250000\n",
            "Train Epoch: 98 [6080/7471 (81%)]\tLoss: 1573246.500000\n",
            "Train Epoch: 98 [6240/7471 (84%)]\tLoss: 1524756.000000\n",
            "Train Epoch: 98 [6400/7471 (86%)]\tLoss: 1569628.750000\n",
            "Train Epoch: 98 [6560/7471 (88%)]\tLoss: 1586800.000000\n",
            "Train Epoch: 98 [6720/7471 (90%)]\tLoss: 1550272.250000\n",
            "Train Epoch: 98 [6880/7471 (92%)]\tLoss: 1552778.000000\n",
            "Train Epoch: 98 [7040/7471 (94%)]\tLoss: 1561837.000000\n",
            "Train Epoch: 98 [7200/7471 (96%)]\tLoss: 1584244.000000\n",
            "Train Epoch: 98 [7360/7471 (99%)]\tLoss: 1577333.625000\n",
            "Epoch 98 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98406.9307\n",
            "\n",
            "Train Epoch: 99 [160/7471 (2%)]\tLoss: 1586241.250000\n",
            "Train Epoch: 99 [320/7471 (4%)]\tLoss: 1544545.375000\n",
            "Train Epoch: 99 [480/7471 (6%)]\tLoss: 1615604.250000\n",
            "Train Epoch: 99 [640/7471 (9%)]\tLoss: 1619727.750000\n",
            "Train Epoch: 99 [800/7471 (11%)]\tLoss: 1577767.875000\n",
            "Train Epoch: 99 [960/7471 (13%)]\tLoss: 1560543.125000\n",
            "Train Epoch: 99 [1120/7471 (15%)]\tLoss: 1579481.500000\n",
            "Train Epoch: 99 [1280/7471 (17%)]\tLoss: 1612835.500000\n",
            "Train Epoch: 99 [1440/7471 (19%)]\tLoss: 1617744.000000\n",
            "Train Epoch: 99 [1600/7471 (21%)]\tLoss: 1624702.125000\n",
            "Train Epoch: 99 [1760/7471 (24%)]\tLoss: 1582522.125000\n",
            "Train Epoch: 99 [1920/7471 (26%)]\tLoss: 1619865.875000\n",
            "Train Epoch: 99 [2080/7471 (28%)]\tLoss: 1589063.375000\n",
            "Train Epoch: 99 [2240/7471 (30%)]\tLoss: 1505249.750000\n",
            "Train Epoch: 99 [2400/7471 (32%)]\tLoss: 1577477.125000\n",
            "Train Epoch: 99 [2560/7471 (34%)]\tLoss: 1505537.750000\n",
            "Train Epoch: 99 [2720/7471 (36%)]\tLoss: 1549556.000000\n",
            "Train Epoch: 99 [2880/7471 (39%)]\tLoss: 1611346.500000\n",
            "Train Epoch: 99 [3040/7471 (41%)]\tLoss: 1492504.375000\n",
            "Train Epoch: 99 [3200/7471 (43%)]\tLoss: 1624043.750000\n",
            "Train Epoch: 99 [3360/7471 (45%)]\tLoss: 1597789.625000\n",
            "Train Epoch: 99 [3520/7471 (47%)]\tLoss: 1559112.875000\n",
            "Train Epoch: 99 [3680/7471 (49%)]\tLoss: 1597139.625000\n",
            "Train Epoch: 99 [3840/7471 (51%)]\tLoss: 1596796.750000\n",
            "Train Epoch: 99 [4000/7471 (54%)]\tLoss: 1621146.500000\n",
            "Train Epoch: 99 [4160/7471 (56%)]\tLoss: 1573081.125000\n",
            "Train Epoch: 99 [4320/7471 (58%)]\tLoss: 1558127.375000\n",
            "Train Epoch: 99 [4480/7471 (60%)]\tLoss: 1529818.375000\n",
            "Train Epoch: 99 [4640/7471 (62%)]\tLoss: 1623619.125000\n",
            "Train Epoch: 99 [4800/7471 (64%)]\tLoss: 1532652.875000\n",
            "Train Epoch: 99 [4960/7471 (66%)]\tLoss: 1575077.625000\n",
            "Train Epoch: 99 [5120/7471 (69%)]\tLoss: 1541625.750000\n",
            "Train Epoch: 99 [5280/7471 (71%)]\tLoss: 1552932.750000\n",
            "Train Epoch: 99 [5440/7471 (73%)]\tLoss: 1585141.875000\n",
            "Train Epoch: 99 [5600/7471 (75%)]\tLoss: 1613520.375000\n",
            "Train Epoch: 99 [5760/7471 (77%)]\tLoss: 1497589.000000\n",
            "Train Epoch: 99 [5920/7471 (79%)]\tLoss: 1626544.500000\n",
            "Train Epoch: 99 [6080/7471 (81%)]\tLoss: 1622971.500000\n",
            "Train Epoch: 99 [6240/7471 (84%)]\tLoss: 1601050.375000\n",
            "Train Epoch: 99 [6400/7471 (86%)]\tLoss: 1554395.500000\n",
            "Train Epoch: 99 [6560/7471 (88%)]\tLoss: 1588460.375000\n",
            "Train Epoch: 99 [6720/7471 (90%)]\tLoss: 1639108.000000\n",
            "Train Epoch: 99 [6880/7471 (92%)]\tLoss: 1599797.875000\n",
            "Train Epoch: 99 [7040/7471 (94%)]\tLoss: 1553048.875000\n",
            "Train Epoch: 99 [7200/7471 (96%)]\tLoss: 1589609.125000\n",
            "Train Epoch: 99 [7360/7471 (99%)]\tLoss: 1607403.250000\n",
            "Epoch 99 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 99005.6195\n",
            "\n",
            "Train Epoch: 100 [160/7471 (2%)]\tLoss: 1557447.750000\n",
            "Train Epoch: 100 [320/7471 (4%)]\tLoss: 1613803.000000\n",
            "Train Epoch: 100 [480/7471 (6%)]\tLoss: 1589912.250000\n",
            "Train Epoch: 100 [640/7471 (9%)]\tLoss: 1553577.000000\n",
            "Train Epoch: 100 [800/7471 (11%)]\tLoss: 1618574.250000\n",
            "Train Epoch: 100 [960/7471 (13%)]\tLoss: 1591011.875000\n",
            "Train Epoch: 100 [1120/7471 (15%)]\tLoss: 1559544.375000\n",
            "Train Epoch: 100 [1280/7471 (17%)]\tLoss: 1513707.750000\n",
            "Train Epoch: 100 [1440/7471 (19%)]\tLoss: 1632811.875000\n",
            "Train Epoch: 100 [1600/7471 (21%)]\tLoss: 1592571.125000\n",
            "Train Epoch: 100 [1760/7471 (24%)]\tLoss: 1549451.500000\n",
            "Train Epoch: 100 [1920/7471 (26%)]\tLoss: 1607970.375000\n",
            "Train Epoch: 100 [2080/7471 (28%)]\tLoss: 1610298.875000\n",
            "Train Epoch: 100 [2240/7471 (30%)]\tLoss: 1549423.250000\n",
            "Train Epoch: 100 [2400/7471 (32%)]\tLoss: 1545236.875000\n",
            "Train Epoch: 100 [2560/7471 (34%)]\tLoss: 1524233.875000\n",
            "Train Epoch: 100 [2720/7471 (36%)]\tLoss: 1617395.125000\n",
            "Train Epoch: 100 [2880/7471 (39%)]\tLoss: 1538326.125000\n",
            "Train Epoch: 100 [3040/7471 (41%)]\tLoss: 1597264.500000\n",
            "Train Epoch: 100 [3200/7471 (43%)]\tLoss: 1574156.750000\n",
            "Train Epoch: 100 [3360/7471 (45%)]\tLoss: 1591088.125000\n",
            "Train Epoch: 100 [3520/7471 (47%)]\tLoss: 1579153.000000\n",
            "Train Epoch: 100 [3680/7471 (49%)]\tLoss: 1610317.625000\n",
            "Train Epoch: 100 [3840/7471 (51%)]\tLoss: 1555971.625000\n",
            "Train Epoch: 100 [4000/7471 (54%)]\tLoss: 1622688.250000\n",
            "Train Epoch: 100 [4160/7471 (56%)]\tLoss: 1527957.375000\n",
            "Train Epoch: 100 [4320/7471 (58%)]\tLoss: 1598225.500000\n",
            "Train Epoch: 100 [4480/7471 (60%)]\tLoss: 1575167.250000\n",
            "Train Epoch: 100 [4640/7471 (62%)]\tLoss: 1560734.375000\n",
            "Train Epoch: 100 [4800/7471 (64%)]\tLoss: 1615322.375000\n",
            "Train Epoch: 100 [4960/7471 (66%)]\tLoss: 1610757.000000\n",
            "Train Epoch: 100 [5120/7471 (69%)]\tLoss: 1592696.125000\n",
            "Train Epoch: 100 [5280/7471 (71%)]\tLoss: 1551536.875000\n",
            "Train Epoch: 100 [5440/7471 (73%)]\tLoss: 1584720.000000\n",
            "Train Epoch: 100 [5600/7471 (75%)]\tLoss: 1620469.625000\n",
            "Train Epoch: 100 [5760/7471 (77%)]\tLoss: 1526462.375000\n",
            "Train Epoch: 100 [5920/7471 (79%)]\tLoss: 1518774.875000\n",
            "Train Epoch: 100 [6080/7471 (81%)]\tLoss: 1547443.375000\n",
            "Train Epoch: 100 [6240/7471 (84%)]\tLoss: 1631281.750000\n",
            "Train Epoch: 100 [6400/7471 (86%)]\tLoss: 1593176.875000\n",
            "Train Epoch: 100 [6560/7471 (88%)]\tLoss: 1580194.375000\n",
            "Train Epoch: 100 [6720/7471 (90%)]\tLoss: 1565154.500000\n",
            "Train Epoch: 100 [6880/7471 (92%)]\tLoss: 1555931.500000\n",
            "Train Epoch: 100 [7040/7471 (94%)]\tLoss: 1542754.000000\n",
            "Train Epoch: 100 [7200/7471 (96%)]\tLoss: 1603993.875000\n",
            "Train Epoch: 100 [7360/7471 (99%)]\tLoss: 1589589.375000\n",
            "Epoch 100 model saved!\n",
            "\n",
            "Test set (1868 samples): Average loss: 98457.4625\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZauWbfkuy9i",
        "colab_type": "text"
      },
      "source": [
        "* make Exp Module for <code>Hyperparameter opitmization</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsNLKuYziD8b",
        "colab_type": "text"
      },
      "source": [
        "Add-plot<br>\n",
        "<code>train_val plot</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7EcU6z-nVLA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f9d7e345-5f26-401a-d27f-2b29ce5e614e"
      },
      "source": [
        "epoch_train_losses = np.load('./results_ResNet-VAE_Exp01/ResNet_VAE_training_loss.npy')\n",
        "epoch_test_losses = np.load('./results_ResNet-VAE_Exp01/ResNet_VAE_test_loss.npy')\n",
        "\n",
        "print(epoch_train_losses.shape, epoch_test_losses.shape)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 467) (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo9-u_XXtUJv",
        "colab_type": "text"
      },
      "source": [
        "<code>batch_size</code>를 argparser에서는 50으로 정해줬는데 결과가 16이라 의아했는데 원인을 찾았다.<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "Train_Loader에서 batch_size를 16으로 정해줬었어...!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPd3EkJSsCet",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "4cf8c461-5b62-46e8-c6ee-887b91c9b8bc"
      },
      "source": [
        "print(epoch_train_losses.shape, epoch_test_losses.shape)\n",
        "print(epoch_train_losses[0].shape)\n",
        "\n",
        "print(len(train_loader.dataset))\n",
        "print(len(valid_loader.dataset))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 467) (100,)\n",
            "(467,)\n",
            "7471\n",
            "1868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_t2oJR7_zBO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8960cb5d-5195-4495-f2d4-becdbede45ec"
      },
      "source": [
        "epoch_train_loss = np.array(np.sum(epoch_train_losses, axis=1)/len(train_loader))\n",
        "print(type(epoch_train_loss), epoch_train_loss.shape)\n",
        "print(type(epoch_test_losses), epoch_test_losses.shape)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (100,)\n",
            "<class 'numpy.ndarray'> (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk4l5aRhFnTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "02d014ea-1646-4330-bcda-4c53593f0d76"
      },
      "source": [
        "# print(epoch_train_loss)\n",
        "print(epoch_train_loss)\n",
        "print(epoch_test_losses)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1638542.16300857 1620145.3011242  1613838.34635974 1611309.79309422\n",
            " 1609294.58110278 1604131.50910064 1605361.17665953 1603169.00722698\n",
            " 1597750.57842612 1594670.69566381 1596908.4638651  1594820.83110278\n",
            " 1591787.57441113 1589724.83458244 1589253.08003212 1590662.44619914\n",
            " 1587715.87794433 1586307.64052463 1585468.14641328 1586667.18843683\n",
            " 1585635.30995717 1584972.41595289 1584345.36911135 1585177.11482869\n",
            " 1584231.52382227 1584188.90042827 1582912.37044968 1583511.24116702\n",
            " 1584135.60626338 1582163.17773019 1582088.10438972 1581855.36911135\n",
            " 1581567.88436831 1581862.14025696 1581171.07441113 1580937.06343683\n",
            " 1580699.83752677 1579828.38356531 1580349.66675589 1580600.1761242\n",
            " 1580038.31664882 1579667.83244111 1579829.20262313 1579679.86589936\n",
            " 1579711.6988758  1578713.22135974 1578928.89400428 1578330.98420771\n",
            " 1578694.30968951 1578644.82601713 1578315.04255889 1577831.36884368\n",
            " 1577992.97698073 1577751.36643469 1577560.05888651 1577430.78988223\n",
            " 1577293.02248394 1577069.23688437 1577013.47082441 1576958.40845824\n",
            " 1576406.12366167 1576562.86509636 1576422.33029979 1576229.95583512\n",
            " 1575853.71493576 1575683.0872591  1575741.07307281 1575780.82360814\n",
            " 1575882.30861884 1574954.02168094 1575597.125      1575033.44405782\n",
            " 1575318.14373662 1575119.48126338 1574604.7240364  1574612.86991435\n",
            " 1574526.29255889 1574299.13463597 1574445.46279443 1574408.07494647\n",
            " 1574237.87847966 1574076.05246253 1573984.6367773  1574016.43120985\n",
            " 1573851.62285867 1573942.85706638 1573719.92264454 1574223.66514989\n",
            " 1573480.73875803 1573358.25802998 1573214.28479657 1573489.65149893\n",
            " 1573379.83672377 1573342.13222698 1573365.32414347 1573396.05808351\n",
            " 1573183.27328694 1573067.21279443 1573034.38195931 1573201.46707709]\n",
            "[           inf            inf            inf            inf\n",
            " 2.04339794e+15            inf 1.01770823e+05 1.00493531e+05\n",
            "            inf            inf 9.95151711e+04            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            " 9.91129865e+04            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf            inf            inf            inf\n",
            "            inf 4.64997510e+17 9.86961006e+04 9.86702425e+04\n",
            " 9.86419925e+04 9.86430577e+04 9.88899452e+04 9.91061395e+04\n",
            " 9.86037253e+04 9.85922494e+04 9.86069897e+04 9.86739283e+04\n",
            " 9.89011277e+04 9.86097878e+04 9.86213919e+04 9.85981963e+04\n",
            " 9.85841948e+04 9.86716995e+04 9.85664475e+04 9.85906832e+04\n",
            " 9.85348911e+04 9.85212109e+04 9.85748725e+04 9.85326321e+04\n",
            " 9.85321190e+04 9.85137038e+04 9.85856745e+04 9.85935321e+04\n",
            " 9.85536405e+04 9.84884595e+04 9.84979273e+04 9.85113831e+04\n",
            " 9.85500923e+04 9.84629215e+04 9.84715464e+04 9.84851026e+04\n",
            " 9.84757658e+04 9.85063151e+04 9.84828543e+04 9.85242495e+04\n",
            " 9.84802659e+04 9.85492924e+04 9.85056253e+04 9.84501005e+04\n",
            " 9.85073227e+04 9.84660906e+04 9.84765147e+04 9.84337928e+04\n",
            " 9.84426701e+04 9.84634076e+04 9.84738756e+04 9.84655270e+04\n",
            " 9.84447675e+04 9.84348281e+04 9.84357628e+04 9.84686730e+04\n",
            " 9.85257165e+04 9.84069307e+04 9.90056195e+04 9.84574625e+04]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VciLlUVrAy-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "deb86053-fcfd-4e4d-bbf6-9c21704e4412"
      },
      "source": [
        "list_epoch = np.array(range(100))\n",
        "list_epoch"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
              "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6YdKnjxiI2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "1e180af9-142e-4be8-a85f-17523c4a01a7"
      },
      "source": [
        "# Reference: Stand-DL Assignment #1\n",
        "\n",
        "# ======== Loss Fluctuation ======== #\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax1.plot(list_epoch, epoch_train_loss, label='train_loss')\n",
        "ax1.plot(list_epoch, epoch_test_losses, '--', label='test_loss')\n",
        "ax1.set_xlabel('epoch')\n",
        "ax1.set_ylabel('loss')\n",
        "ax1.grid()\n",
        "ax1.legend()\n",
        "ax1.set_title('epoch vs loss')\n",
        "\n",
        "# ======== save plot ======== #\n",
        "plt.savefig('./train_val_plot.png', dpi=300)\n",
        "plt.show()\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3yJz_I9BCAP",
        "colab_type": "text"
      },
      "source": [
        "### Epoch별로 loss graph를 그려볼까??"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkKHFr1sB1lO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Backup Code\n",
        "\n",
        "backup_epoch_train_loss = epoch_train_loss\n",
        "backup_epoch_test_loss = epoch_test_losses"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcUoN8m7CsxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reference: Stand-DL Assignment #1\n",
        "\n",
        "# ======== Loss Fluctuation ======== #\n",
        "def plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses):\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    ax1.plot(list_epoch, epoch_train_losses, label='train_loss')\n",
        "    ax1.plot(list_epoch, epoch_test_losses, '--', label='test_loss')\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.set_ylabel('loss')\n",
        "    ax1.grid()\n",
        "    ax1.legend()\n",
        "    ax1.set_title('epoch vs loss')\n",
        "\n",
        "    # ======== save plot ======== #\n",
        "    plt.savefig('./results_ResNet-VAE_Exp01/loss_plot/{}train_val_plot.png'.format(list_epoch[-1]+1), dpi=300)\n",
        "    plt.show()\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvYGfea6E2v1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "ab43920a-6d1f-42ca-803c-78e68c2d99a1"
      },
      "source": [
        "# 01. 0 Epoch ~ 8 Epoch\n",
        "list_epoch = np.array(range(8))\n",
        "epoch_train_losses = backup_epoch_train_loss[:8]\n",
        "epoch_test_losses = backup_epoch_test_loss[:8]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4kpKVxCEpBs",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 10::</code>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6tXLL2rErou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "cb95ec76-7443-4b31-e795-b63b11796512"
      },
      "source": [
        "# 01. 0 Epoch ~ 10 Epoch\n",
        "list_epoch = np.array(range(10))\n",
        "epoch_train_losses = backup_epoch_train_loss[:10]\n",
        "epoch_test_losses = backup_epoch_test_loss[:10]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WkyuNtrCLJJ",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 20::</code>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXv9m-SxBIfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "57cc2f3a-a0fa-4c5c-b71d-8329b4d8e524"
      },
      "source": [
        "# 01. 0 Epoch ~ 20 Epoch\n",
        "list_epoch = np.array(range(20))\n",
        "epoch_train_losses = backup_epoch_train_loss[:20]\n",
        "epoch_test_losses = backup_epoch_test_loss[:20]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Ukvua8BBkb",
        "colab_type": "text"
      },
      "source": [
        "<code>::Epoch 0 to 20::</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQGuW3TtC_5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "763e76cc-1991-4b01-8f66-4fa110be8d1e"
      },
      "source": [
        "# 02. 0 Epoch ~ 40 Epoch\n",
        "list_epoch = np.array(range(40))\n",
        "epoch_train_losses = backup_epoch_train_loss[:40]\n",
        "epoch_test_losses = backup_epoch_test_loss[:40]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C7EuJKTDNiA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "2922a558-21b2-4325-9730-c93b170f70c4"
      },
      "source": [
        "# 03. 0 Epoch ~ 60 Epoch\n",
        "list_epoch = np.array(range(60))\n",
        "epoch_train_losses = backup_epoch_train_loss[:60]\n",
        "epoch_test_losses = backup_epoch_test_loss[:60]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAFNCAYAAABL8stCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3xU9Z3/8dcnk4RwRy5GEBUURUAEBESLKNRVKVLv90tLS8v2t25Xt16q3Xa72l93292WaqvVakW73qjipRa1UJF42SoICMp1AcUSrIBYkAAJJPnsH2egCSQhkMycb+a8n4/HPGbmfGfO+XzJkHe+Z77nHHN3REREQpEXdwEiIiI1KZhERCQoCiYREQmKgklERIKiYBIRkaAomEREJCgKJpFmZma9zMzNLD+L2xxtZqXZ2p5IJimYREQkKAomEREJioJJcp6Z9TCzp81so5l9YGb/VKPt38xsmpn91sy2mtkCMxtUo72fmZWY2WYzW2Jm59Voa21mPzWzD81si5m9YWata2z6ajP7s5l9Ymb/Uk9tI8zsYzNL1Vh2oZm9m358spnNM7PPzGy9mU1uZJ8bqnucmS1N93edmd2UXt7VzKan3/Opmb1uZvodIVmnD53ktPQv1t8Di4DDgTOBG8zsnBovOx94CugMPA48Z2YFZlaQfu9M4FDgm8BjZtY3/b6fAEOBz6XfewtQXWO9pwF909v8VzPrt3d97j4H2AZ8vsbiq9J1ANwF3OXuHYBjgCcb0ef91f0g8Pfu3h44AXglvfxGoBToBhQD3wF0zjLJuuCCycymmNkGM1vciNeenv4Lt9LMLqmxfIyZLaxxKzezCzJbuQRqONDN3e9w953u/j7wAHBFjdfMd/dp7r4LmAwUAaekb+2AH6Xf+wowHbgyHXhfBa5393XuXuXuf3L3ihrrvd3dd7j7IqJgHETdngCuBDCz9sC49DKAXUAfM+vq7mXu/lYj+lxv3TXW2d/MOrj7X919QY3l3YGj3H2Xu7/uOpmmxCC4YAIeBsY28rV/Bibwt78uAXD32e4+2N0HE/0lup3or0dJnqOAHundU5vNbDPRSKC4xmvW7n7g7tVEo4Ye6dva9LLdPiQaeXUlCrDVDWz74xqPtxOFRV0eBy4ys1bARcACd/8w3TYROA5YbmZvm9n4BnsbaahugIuJwu9DM3vVzE5NL/8vYBUw08zeN7NbG7EtkWYXXDC5+2vApzWXmdkxZvYHM5uf3u99fPq1a9z9XWrvPtnbJcBL7r49c1VLwNYCH7h7pxq39u4+rsZrjtj9ID0S6gl8lL4dsdf3LEcC64BPgHKi3WtN4u5LiYLjC9TejYe7r3T3K4l2yf0YmGZmbfezyobqxt3fdvfz0+t8jvTuQXff6u43uvvRwHnAt8zszKb2T+RABRdM9bgf+Ka7DwVuAn55AO+9gr/tFpHkmQtsNbNvpycrpMzsBDMbXuM1Q83sovRxRzcAFcBbwByikc4t6e+cRgNfBKamRyNTgMnpyRUpMzs1Peo5GI8D1wOnE33fBYCZXWNm3dLb25xe3NAfYjRUt5kVmtnVZtYxvevys93rM7PxZtbHzAzYAlQ1YlsizS74YDKzdkRfLj9lZguBXxHtB2/Me7sDA4EZmatQQubuVcB4YDDwAdFI59dAxxov+x1wOfBX4FrgovR3LDuJfqF/If2+XwJfcvfl6ffdBLwHvE00yv8xB/9/6gngDOAVd/+kxvKxwBIzKyOaCHGFu+/YT5/3V/e1wBoz+wz4BnB1evmxwMtAGfAm8Et3n32Q/RE5aBbid5tm1guY7u4nmFkHYIW71xtGZvZw+vXT9lp+PTDA3SdlsFxpwczs34A+7n5N3LWISCT4EZO7fwZ8YGaXAlikvtlNe7sS7cYTEWlRggsmM3uCaDdCXzMrNbOJRLsaJprZImAJ0XEnmNlwi84PdinwKzNbUmM9vYi+1H41uz0QEZGmCHJXnoiIJFdwIyYREUk2BZOIiAQla9eLaYyuXbt6r169mrSObdu20bbt/o4/bPmS0k9QX3NRUvoJ6mt95s+f/4m7d6urLahg6tWrF/PmzWvSOkpKShg9enTzFBSwpPQT1NdclJR+gvpaHzP7sL427coTEZGgKJhERCQoCiYREQlKUN8xiYiEYNeuXZSWllJeXt7kdXXs2JFly5Y1Q1Xhq6uvRUVF9OzZk4KCgkavR8EkIrKX0tJS2rdvT69evYhOtn7wtm7dSvv27ZupsrDt3Vd3Z9OmTZSWltK7d+9Gr0e78kRE9lJeXk6XLl2aHEpJZ2Z06dLlgEeeCiYRkToolJrHwfw7KphERCQoCiYRkcBs3ryZX/7yQC7UHRk3bhybN2/e/wv3MmHCBKZNm7b/F2aJgklEJDD1BVNlZWWD73vxxRfp1KlTpsrKGgWTiIRh8TPw9NfjriIIt956K6tXr2bw4MEMHz6cUaNGcd5559G/f38ALrjgAoYOHcqAAQO4//7797yvV69efPLJJ6xZs4Z+/frx9a9/nQEDBnD22WezY8eORm171qxZDBkyhIEDB/LVr36VioqKPTX179+fE088kZtuugmAp556ihNOOIFBgwZx+umnN1v/NV1cRMKwYRm89xRcdD8ENPHg9t8vYelHnx30+6uqqkilUrWW9e/Rge9/cUC97/nRj37E4sWLWbhwISUlJZx77rksXrx4z5TrKVOm0LlzZ3bs2MHw4cO5+OKL6dKlS611rFy5kieeeIIHHniAyy67jKeffpprrrmmwVrLy8uZMGECs2bN4rjjjuNLX/oS9957L9deey3PPvssy5cvx8z27C684447mDFjBocffvhB7UKsj0ZMIhKGVCHgUF0VdyXBOfnkk2sdB/Tzn/+cQYMGccopp7B27VpWrly5z3t69+7N4MGDARg6dChr1qzZ73ZWrFhB7969Oe644wD48pe/zGuvvUbHjh0pKipi4sSJPPPMM7Rp0waAkSNHMmHCBB544AGqqprv56YRk4iEIZU+M0DVTkiF86upoZFNYzTHAbY1LyVRUlLCyy+/zJtvvkmbNm0YPXp0nccJtWrVas/jVCrV6F15dcnPz2fu3LnMmjWLadOmcffdd/PKK69w3333MWfOHF544QWGDh1KSUlJsxxMHM5PX0SSLVUY3VftBNrEWkrc2rdvz9atW+ts27JlC4cccght2rRh+fLlvPXWW8223b59+7JmzRpWrVpFnz59eOSRRzjjjDMoKytj+/btjBs3jpEjR3L00UcDsHr1akaMGMGIESN46aWXWLduHU29ph4omEQkFK3aQ7vDtCsP6NKlCyNHjuSEE06gdevWFBcX72kbO3Ys9913H/369aNv376ccsopzbbdoqIiHnroIS699FIqKysZPnw43/jGN/j00085//zzKS8vx92ZPHkyADfffDMrV67E3TnzzDMZOHBgs9ShYBKRMJx0bXQTAB5//PE6l7dq1YqXXnqpzrbd3yN17dqVxYsX71m+exZdfR5++OE9j88880zeeeedWu3du3dn7ty5+7zvmWeeqfW8vlHegdLkBxERCYqCSUTC8Oe34LHLYPOf464kZ1133XUMHjy41u2hhx6Ku6x9aFeeiIRh20ZYOQPKvxd3JTnrnnvuibuERtGISUTCUGtWniSZgklEwrDnOKZd8dYhsVMwiUgYNGKSNAWTiIShsB106QN5BXFXIjFTMIlIGHoMhm/Oh6NOjbuS2B3s9ZgA7rzzTrZv397ga3afhTxUCiYRkcBkOphCp+niIhKGLevgmUlw+o1wzOfjrqa2h87dd9mAC+Dkr8PO7fDYpfu2D74KhlyNbf8Upl1Ru+0rLzS4uZrXYzrrrLM49NBDefLJJ6moqODCCy/k9ttvZ9u2bVx22WWUlpZSVVXF9773PdavX89HH33EmDFj6Nq1K7Nnz95v1yZPnsyUKVMA+NrXvsYNN9xQ57ovv/xybr31Vp5//nny8/M5++yz+clPfrLf9R8MBZOIhKF6F3z4Bmy9Ou5KYlfzekwzZ85k2rRpzJ07F3fnvPPO47XXXmPjxo306NGDF16IQm7Lli107NiRyZMnM3v2bLp27brf7cyfP5+HHnqIOXPm4O6MGDGCM844g/fff3+fdW/atKnOazJlgoJJRMIQ8qy8hkY4hW0abPc2nfc7QmrIzJkzmTlzJkOGDAGgrKyMlStXMmrUKG688Ua+/e1vM378eEaNGnXA637jjTe48MIL91xW46KLLuL1119n7Nix+6y7srJyzzWZxo8fz/jx4w+6T/uj75hEJAy7g6kywGCKkbtz2223sXDhQhYuXMiqVauYOHEixx13HAsWLGDgwIF897vf5Y477mi2bda17t3XZLrkkkuYPn06Y8eObbbt7U3BJCJhqHmhwISreT2mc845hylTplBWVgbAunXr2LBhAx999BFt2rThmmuu4eabb2bBggX7vHd/Ro0axXPPPcf27dvZtm0bzz77LKNGjapz3WVlZWzZsoVx48bxs5/9jEWLFmWm82hXnoiEItUKug+CNl3iriR2Na/H9IUvfIGrrrqKU0+NptG3a9eORx99lFWrVnHzzTeTl5dHQUEB9957LwCTJk1i7Nix9OjRY7+TH0466SQmTJjAySefDESTH4YMGcKMGTP2WffWrVvrvCZTJpi7Z2zlB2rYsGE+b968Jq2jpKSE0aNHN09BAUtKP0F9zUWh93PZsmX069evWdbVHJdWbynq62td/55mNt/dh9W1Hu3KExGRoGhXnoiE48Gzof/5cOp1cVeSE0aMGEFFRUWtZY888kizXQI9UxRMIhKODcugx0lxV5Ez5syZE3cJB0W78kQkHKmCYGblhfT9e0t2MP+OCiYRCUeqVRDBVFRUxKZNmxROTeTubNq0iaKiogN6n3bliUg4UgVBXCiwZ8+elJaWsnHjxiavq7y8/IB/MbdUdfW1qKiInj17HtB6FEwiEo4jRkTXZIpZQUEBvXv3bpZ1lZSU7DmdUK5rrr4qmEQkHBc/EHcFEoCMf8dkZikze8fMpmd6WyIi0vJlY/LD9cCyLGxHRFq6aV+FZ/4+7iokZhkNJjPrCZwL/DqT2xGRHLF1PWxZG3cVErNMj5juBG4BqjO8HRHJBQEdxyTxydhJXM1sPDDO3f/BzEYDN7n7PleWMrNJwCSA4uLioVOnTm3SdsvKymjXrl2T1tESJKWfoL7movr6OfDdH1C486/MH5a5M1dnW1J+pnBgfR0zZky9J3HF3TNyA/4DKAXWAB8D24FHG3rP0KFDvalmz57d5HW0BEnpp7v6movq7ecTV7nfc0pWa8m0pPxM3Q+sr8A8rycLMrYrz91vc/ee7t4LuAJ4xd2vydT2RCQH9BwGR42MuwqJmY5jEpFwnPbPcVcgAchKMLl7CVCSjW2JiEjLppO4ikg4Xr4dflH39+GSHAomEQlHZTmUrY+7ComZgklEwqHjmAQFk4iEJFUIlRWg6yAlmoJJRMKRKgQcqqvirkRipGASkXAUD4ATLwfXWcySTMcxiUg4jj83ukmiacQkIiJBUTCJSDjeeQz+fzF89lHclUiMFEwiEg6z6Fimyoq4K5EYKZhEJBypwui+ale8dUisFEwiEo49waSDbJNMwSQi4VAwCQomEQnJIUfBsInQpnPclUiMdByTiITj0H4wPncuqy4HRyMmEQlLdRVU68wPSaZgEpFwrFsAd3SGlTPjrkRipGASkXBo8oOgYBKRkCiYBAWTiIQkVRDd6wDbRFMwiUg4NGISFEwiEpKiDvC5f4quyySJpeOYRCQcrdrD2T+IuwqJmUZMIhIOd9ixGXZuj7sSiZGCSUTC4dXw46PgzbvjrkRipGASkXDkpcDyNPkh4RRMIhKWVKGCKeEUTCISllShjmNKOAWTiIQlVaARU8JpuriIhOW0b0G3vnFXITFSMIlIWD73j3FXIDHTrjwRCcvW9VC2Me4qJEYKJhEJyyMXwAv/HHcVEiMFk4iEJVUAlZr8kGQKJhEJi45jSjwFk4iERccxJZ6CSUTCohFT4mm6uIiEZfjXFEwJp2ASkbD0Gx93BRIz7coTkbB89hF8sjLuKiRGCiYRCcvL/waPXhx3FRIjBZOIhEWz8hJPwSQiYdGsvMRTMIlIWDRiSryMBZOZFZnZXDNbZGZLzOz2TG1LRHKIrseUeJmcLl4BfN7dy8ysAHjDzF5y97cyuE0RaekGXACH9o+7ColRxoLJ3R0oSz8tSN88U9sTkRxx+NDoJomV0e+YzCxlZguBDcAf3X1OJrcnIjlg68fw5zlQXRV3JRITiwY2Gd6IWSfgWeCb7r54r7ZJwCSA4uLioVOnTm3StsrKymjXrl2T1tESJKWfoL7moob62XPt7+izegqvn/Y4Vflts1xZ80vKzxQOrK9jxoyZ7+7D6mx096zcgH8FbmroNUOHDvWmmj17dpPX0RIkpZ/u6msuarCfc+53/34H97KNWasnk5LyM3U/sL4C87yeLMjkrLxu6ZESZtYaOAtYnqntiUiOSBVE95qZl1iZnJXXHfiNmaWIvst60t2nZ3B7IpIL8hRMSZfJWXnvAkMytX4RyVGpwuheB9kmls78ICJhOfIUuPQ30K447kokJroek4iEpdMR0U0SSyMmEQnL9k9h1azoXhJJwSQiYVm/GB69CDYsjbsSiYmCSUTCsmfyg2blJZWCSUTCsuc4Js3KSyoFk4iERSOmxFMwiUhYUq2iewVTYmm6uIiEpePhcM3TUDww7kokJgomEQlLYVvo83dxVyEx0q48EQlLZQUs/R1sWh13JRITBZOIhGXXdnjyS/C/M+KuRGKiYBKRsGjyQ+IpmEQkLDq7eOIpmEQkLHkpwDRiSjAFk4iExSwaNSmYEkvTxUUkPBOmQ/vucVchMVEwiUh4jjg57gokRtqVJyLhWfIsfPinuKuQmCiYRCQ8M/8VFjwSdxUSEwWTiIQnVaDJDwmmYBKR8GhWXqIpmEQkPKkCHWCbYAomEQlPqhCqKuKuQmLSqGAys+vNrINFHjSzBWZ2dqaLE5GEuuh++OJdcVchMWnsiOmr7v4ZcDZwCHAt8KOMVSUiydblGOh0ZNxVSEwaG0yWvh8HPOLuS2osExFpXiv+AO8+FXcVEpPGBtN8M5tJFEwzzKw9UJ25skQk0d55BP7nzrirkJg09pREE4HBwPvuvt3MOgNfyVxZIpJoOo4p0Ro7YjoVWOHum83sGuC7wJbMlSUiiabjmBKtscF0L7DdzAYBNwKrgf/OWFUikmw6jinRGhtMle7uwPnA3e5+D9A+c2WJSKJpxJRojQ2mrWZ2G9E08RfMLA8oyFxZIpJoY74Lf/9a3FVITBobTJcDFUTHM30M9AT+K2NViUiyte0CHXrEXYXEpFHBlA6jx4COZjYeKHd3fcckIpmx5n/g1f+MuwqJSWNPSXQZMBe4FLgMmGNml2SyMBFJsDWvw+wfQnVV3JVIDBp7HNO/AMPdfQOAmXUDXgamZaowEUmwVPor7KpdkJeKtxbJusZ+x5S3O5TSNh3Ae0VEDkyqVXSvmXmJ1NgR0x/MbAbwRPr55cCLmSlJRBIvVRjd61imRGpUMLn7zWZ2MTAyveh+d382c2WJSKLt2ZWnEVMSNXbEhLs/DTydwVpERCKDroQBF0KrDnFXIjFoMJjMbCvgdTUB7u761IhI8ysoim6SSA0Gk7vrtEMikn0blsGiJ+CUf4D2h8VdjWSZZtaJSHg+/QD+5y7Y+nHclUgMMhZMZnaEmc02s6VmtsTMrs/UtkQkx2hWXqI1evLDQagEbnT3Bekr3s43sz+6+9IMblNEcoFm5SVaxkZM7v4Xd1+QfrwVWAYcnqntiUgO2TNiUjAlUVa+YzKzXsAQYE42ticiLZyCKdEsuv5fBjdg1g54Ffihuz9TR/skYBJAcXHx0KlTpzZpe2VlZbRr165J62gJktJPUF9z0X776dWAg7X88+Ql5WcKB9bXMWPGzHf3YXU2unvGbkQXE5wBfKsxrx86dKg31ezZs5u8jpYgKf10V19zUVL66a6+1geY5/VkQSZn5RnwILDM3SdnajsikoO2bYIXboS1b8ddicQgk98xjSS6FPvnzWxh+jYug9sTkVxRuQPe/jVsXB53JRKDjE0Xd/c3iE5dJCJyYDT5IdF05gcRCY+OY0o0BZOIhEcjpkRTMIlIeFKFkK+ziydVJk9JJCJycFIF8N31cVchMdGISUREgqJgEpEw/f56WNS0M8FIy6RgEpEwLf0drJsfdxUSAwWTiIQpVahZeQmlYBKRMKUKdaHAhFIwiUiYUgUaMSWUgklEwtT2UChoHXcVEgMdxyQiYZo4I+4KJCYaMYmISFAUTCISplk/gJdvj7sKiYF25YlImErfhsqKuKuQGGjEJCJh0nFMiaVgEpEw6TimxFIwiUiYdBxTYuk7JhEJU4ceUL457iokBgomEQnT2P+IuwKJiXbliYhIUBRMIhKmuQ/AY5fFXYXEQMEkImH66xpY80bcVUgMFEwiEiYdx5RYCiYRCVOqEKp3QXV13JVIlimYRCRMqYLovloH2SaNgklEwtShB3QfBNVVcVciWabjmEQkTIOvim6SOBoxiYhIUBRMIhKmFX+AX50On/0l7kokyxRMIhKmis/gL4tg1/a4K5EsUzCJSJh2z8rTsUyJo2ASkTClCqN7BVPiKJhEJEx7gknHMSWNgklEwtSmCxw1Egpax12JZJmOYxKRMB1+EnzlxbirkBhoxCQiIkFRMIlImDb+L/xiKKyaFXclkmUKJhEJk1fDplVQviXuSiTLFEwiEqY9xzFpVl7SKJhEJEw6jimxFEwiEqbdwaTrMSWOgklEwlTQGo49BzocHnclkmU6jklEwlTUAa5+Mu4qJAYaMYmISFAyFkxmNsXMNpjZ4kxtQ0RymDv8pC+8cWfclUiWZXLE9DAwNoPrF5FcZgbbN+k4pgTKWDC5+2vAp5lav4gkQKpQ08UTyNw9cys36wVMd/cTGnjNJGASQHFx8dCpU6c2aZtlZWW0a9euSetoCZLST1Bfc1Fj+znyjatZX3wGq46dlIWqMiMpP1M4sL6OGTNmvrsPq6st9ll57n4/cD/AsGHDfPTo0U1aX0lJCU1dR0uQlH6C+pqLGt3Pt9vQ87Bu9GzB/yZJ+ZlC8/U19mASEalX//PgsIFxVyFZpmASkXCd+9O4K5AYZHK6+BPAm0BfMys1s4mZ2paIiOSOjI2Y3P3KTK1bRBLiwXOg/WFw2W/irkSySGd+EJFwVe2EnWVxVyFZpmASkXDpOKZEUjCJSLhSBbpQYAIpmEQkXPmtNGJKIE0XF5FwHXsO7NwadxWSZQomEQnXiJZ7KiI5eNqVJyJhq66KuwLJMgWTiITruevgzhPjrkKyTMEkIuFK5WvyQwIpmEQkXDqOKZEUTCISrlQhVFfGXYVkmYJJRMKVKtCIKYE0XVxEwnXUSPDquKuQLFMwiUi4jj0rukmiaFeeiISrcifs2AzVGjUliYJJRMI1bwr8+Cgo3xx3JZJFCiYRCVeqILrXGcYTRcEkIuFKFUb3mpmXKAomEQmXgimRFEwiEq49u/IUTEmiYBKRcB3aH0Z/B9p0ibsSySIdxyQi4Tr0+OgmiaIRk4iEq7ICtqyDXeVxVyJZpGASkXCVzoOf9Ye1c+KuRLJIwSQi4dKsvERSMIlIuDQrL5EUTCISLo2YEknBJCLhym8V3euURImiYBKRcLXtCuf8O3QfFHclkkU6jklEwlXUEU69Lu4qJMs0YhKRcFVXwcYVsG1T3JVIFimYRCRcu7bDPSfDosfjrkSySMEkIuHSrLxEUjCJSLjydKHAJFIwiUi48vIgL18jpoRRMIlI2FKFCqaE0XRxEQnbuT+FrsfFXYVkkYJJRMI2+Kq4K5As0648EQnbx+/BptVxVyFZpGASkbD99loo+VHcVUgWKZhEJGya/JA4CiYRCVuqQMcxJYyCSUTClirQiClhFEwiEjbtykucjE4XN7OxwF1ACvi1u+sbTBE5MGO+E539QRIjYz9tM0sB9wBnAaXA22b2vLsvzdQ2RSQHHT067gokyzL5Z8jJwCp3fx/AzKYC5wMZC6bbf7+EPy3dwb0r3szUJg7IkPI55FFda9knqWI+LDi6yevevDmcfmaa+pp7DqSfR+z6gMEV8/gov+eeZZUUsKhoGADH7FxBp+pPa71np7XivVYnAXDszqV0qN5Sq32HtWFpq+iquMfvXEzb6q212rfltWd54QkADKhYRJFvr9X+WV5HVhb2B2BgxQIKvaJ2//I6s7qwLwB9Nv2JuYsW1GrflOrGmoI+AJxU/haG12rfkDqMtQW9Ma/ipIq5+/ybfJw6nHUFR5LvOxlUMX+f9nX5R/Jx/uG0qi7nhJ3v7NP+5/zebMw/jNbV2+i/89192tcU9GFTqhttq7dy/M7Ftdq25nUkv9cpfP+LA/Z5X3PJZDAdDqyt8bwUGLH3i8xsEjAJoLi4mJKSkoPeYGlpBVVVVWzevPmg19Gcbtjx7xRR+wM7PfV3/KLwa01ed0j9zDT1NfccSD+/svO3jK0qqbVsM+25vPUDAIyreJTTqt+u1f4XO5QJRT8H4KKKhzip+r1a7e/bkfy/ov8E4Iry++jnq2q1L8k7jm+1ugOAL5f/gqO8tFb7vLxB/Eur2wCYVP5TDvXaFzJ8LW8EP2z1zwB8e+fddNi5rVb7jNRoJhd+A4Abd/yAfKpqtT+XGsu9hRMo8J3cUn77Pv8mT+RfwJKCK+jon9XZ/mD+lSwvOJ/DqtdzS8W+7XcXfIWV+edwSPWHdbb/V8E/sDr/dHpUreCWnbXb38k7gV/m96SkZOM+7ysrK2vS7/DdzN33/6qDWbHZJcBYd/9a+vm1wAh3/8f63jNs2DCfN29ek7ZbUlLC6NGjm7SOZvOXd8Frj5ho0wU6HdHkVQfVzwxTX3PPAfVz147oKrY15eXDYdGIhk8/gPLaIyJShVAcjWjYtBoqao+IyC+CQ4+PHm/83+iChDUVtoWux0aPNyyHyvLa7a3aQ5djosfrl+w7nb2oI3TuDcDb0x9i+ElDare36Qydjowef7Rw3z637Qode0J1NXy874iGdk8RDVwAAAauSURBVMXQoXu03fVL9m1v3x3aF0NlBWxYtm97x57RNur6t4WotjadoaIMNtUO7Vp938uB/FzNbL67D6urLZMjpnVAzd/APdPLkqP7iXFXINLyFbSGHoPrb08HQL3q+SW6R7f9nCB2d4DVp7jhXVrb2vVuuP6G2vLyGm5PFTTcnt+q4fb9/du2atdwe4Zkcrr428CxZtbbzAqBK4DnM7g9ERHJARkbMbl7pZn9IzCDaLr4FHevY8wpIiLyNxk9OMDdXwRezOQ2REQkt+jMDyIiEhQFk4iIBEXBJCIiQVEwiYhIUBRMIiISFAWTiIgERcEkIiJBydi58g6GmW0EPmziaroCnzRDOaFLSj9Bfc1FSeknqK/1Ocrdu9XVEFQwNQczm1ffiQFzSVL6CeprLkpKP0F9PRjalSciIkFRMImISFByMZjuj7uALElKP0F9zUVJ6Seorwcs575jEhGRli0XR0wiItKC5UwwmdlYM1thZqvM7Na462lOZjbFzDaY2eIayzqb2R/NbGX6/pA4a2wuZnaEmc02s6VmtsTMrk8vz6n+mlmRmc01s0Xpft6eXt7bzOakP8e/TV9kMyeYWcrM3jGz6ennOddXM1tjZu+Z2UIzm5dellOf3d3MrJOZTTOz5Wa2zMxOba6+5kQwmVkKuAf4AtAfuNLM+sdbVbN6GBi717JbgVnufiwwK/08F1QCN7p7f+AU4Lr0zzLX+lsBfN7dBwGDgbFmdgrwY+Bn7t4H+CswMcYam9v1wLIaz3O1r2PcfXCNadO59tnd7S7gD+5+PDCI6GfbPH119xZ/A04FZtR4fhtwW9x1NXMfewGLazxfAXRPP+4OrIi7xgz1+3fAWbncX6ANsAAYQXRwYn56ea3PdUu+AT3Tv6g+D0wHLBf7CqwBuu61LOc+u0BH4APS8xSau685MWICDgfW1nheml6Wy4rd/S/pxx8DxXEWkwlm1gsYAswhB/ub3rW1ENgA/BFYDWx298r0S3Lpc3wncAtQnX7ehdzsqwMzzWy+mU1KL8u5zy7QG9gIPJTePftrM2tLM/U1V4Ip0Tz68ySnpleaWTvgaeAGd/+sZluu9Nfdq9x9MNFo4mTg+JhLyggzGw9scPf5cdeSBae5+0lEXytcZ2an12zMlc8ukA+cBNzr7kOAbey1264pfc2VYFoHHFHjec/0sly23sy6A6TvN8RcT7MxswKiUHrM3Z9JL87Z/rr7ZmA20e6sTmaWn27Klc/xSOA8M1sDTCXanXcXOdhXd1+Xvt8APEv0B0cufnZLgVJ3n5N+Po0oqJqlr7kSTG8Dx6Zn+RQCVwDPx1xTpj0PfDn9+MtE38W0eGZmwIPAMnefXKMpp/prZt3MrFP6cWui79GWEQXUJemXtfh+Arj7be7e0917Ef3ffMXdrybH+mpmbc2s/e7HwNnAYnLsswvg7h8Da82sb3rRmcBSmqmvOXOArZmNI9qPnQKmuPsPYy6p2ZjZE8BoojP3rge+DzwHPAkcSXRG9svc/dO4amwuZnYa8DrwHn/7PuI7RN8z5Ux/zexE4DdEn9c84El3v8PMjiYaVXQG3gGucfeK+CptXmY2GrjJ3cfnWl/T/Xk2/TQfeNzdf2hmXcihz+5uZjYY+DVQCLwPfIX0Z5km9jVngklERHJDruzKExGRHKFgEhGRoCiYREQkKAomEREJioJJRESComASCZyZjd59Rm6RJFAwiYhIUBRMIs3EzK5JX2NpoZn9Kn2S1jIz+1n6mkuzzKxb+rWDzewtM3vXzJ7dfd0aM+tjZi+nr9O0wMyOSa++XY1r3zyWPkOGSE5SMIk0AzPrB1wOjEyfmLUKuBpoC8xz9wHAq0Rn7QD4b+Db7n4i0Vkudi9/DLjHo+s0fQ7YfabmIcANRNcbO5ro/HMiOSl//y8RkUY4ExgKvJ0ezLQmOoFlNfDb9GseBZ4xs45AJ3d/Nb38N8BT6fOsHe7uzwK4ezlAen1z3b00/Xwh0fW53sh8t0SyT8Ek0jwM+I2731Zrodn39nrdwZ4DrOY55KrQ/13JYdqVJ9I8ZgGXmNmhAGbW2cyOIvo/tvsM2lcBb7j7FuCvZjYqvfxa4FV33wqUmtkF6XW0MrM2We2FSAD0V5dIM3D3pWb2XaKrl+YBu4DriC6gdnK6bQPR91AQXRLgvnTw7D4zM0Qh9SszuyO9jkuz2A2RIOjs4iIZZGZl7t4u7jpEWhLtyhMRkaBoxCQiIkHRiElERIKiYBIRkaAomEREJCgKJhERCYqCSUREgqJgEhGRoPwfVFJHxNFHosEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yehSO166D7Ih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "0fa21a8c-38a8-4f20-8158-19ff71d9553e"
      },
      "source": [
        "# 03. 0 Epoch ~ 80 Epoch\n",
        "list_epoch = np.array(range(80))\n",
        "epoch_train_losses = backup_epoch_train_loss[:80]\n",
        "epoch_test_losses = backup_epoch_test_loss[:80]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f152o_NwD-5z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "7f7fede6-6219-43e7-89d8-4ac065187506"
      },
      "source": [
        "# 03. 0 Epoch ~ 100 Epoch\n",
        "list_epoch = np.array(range(100))\n",
        "epoch_train_losses = backup_epoch_train_loss[:]\n",
        "epoch_test_losses = backup_epoch_test_loss[:]\n",
        "\n",
        "plot_loss_function(list_epoch, epoch_train_losses, epoch_test_losses)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cofyXuQJuI3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2d2f3a21-36cf-4d02-8a18-3b1c68a39ed8"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL8ixXdrdLpi",
        "colab_type": "text"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uKB5_XmdMzz",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation\n",
        "* train, val, test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0uHo2uDdUYr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "ff953b65-6912-4613-f006-1a562a58e87c"
      },
      "source": [
        "transforms = transforms.Compose([\n",
        "                                transforms.Resize((224, 224)),                # Change size of Image to (224, 224)\n",
        "                                transforms.Grayscale(num_output_channels=1),  # Makes it 1-dimension channel\n",
        "                                transforms.ToTensor(),                        # Convert a PIL Image or numpy.ndarray to tensor.\n",
        "                                                                              # Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8\n",
        "                                                                              # In the other cases, tensors are returned without scaling.\n",
        "                                # transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "                                \n",
        "                                ])\n",
        "\n",
        "# make custom dataset \n",
        "# use torchvision.datasets.ImageFolder()\n",
        "trainset = torchvision.datasets.ImageFolder(root='../../../../InformationSecurity_Summer/malimg',\n",
        "                                            transform=transforms)  # make custom dataset"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-11eb43b621a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transforms = transforms.Compose([\n\u001b[0m\u001b[1;32m      2\u001b[0m                                 \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0;31m# Change size of Image to (224, 224)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGrayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_output_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Makes it 1-dimension channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0;31m# Convert a PIL Image or numpy.ndarray to tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                               \u001b[0;31m# Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Compose' object has no attribute 'Compose'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5wfy8WBdWch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_dataset = trainset\n",
        "\n",
        "# maek train, val, test: 8:1:1\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = int(0.1 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "print(train_size, val_size, test_size)\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBJn4oeOdaBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make custom data_loader\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                         batch_size=16,\n",
        "                         shuffle=True,\n",
        "                         pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=16,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True)\n",
        " \n",
        "test_loader = DataLoader(test_dataset,\n",
        "                        batch_size=16,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True)  # Instead, we recommend using automatic memory pinning (i.e., setting pin_memory=True)\n",
        "                                          #  which enables fast data transfer to CUDA-enabled GPUs\n",
        "\n",
        "# First, insert all test dataset\n",
        "# z_loader: for latent vector extraction\n",
        "z_loader = DataLoader(full_dataset,\n",
        "                        batch_size=9339,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}