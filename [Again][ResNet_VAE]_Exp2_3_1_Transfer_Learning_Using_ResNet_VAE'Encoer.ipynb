{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Again][ResNet-VAE] Exp2_3_1. Transfer Learning Using ResNet-VAE'Encoer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOHjvZeo8MJ1nbDLsHuaui2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8e3499a393d5425d830de7a892b31b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9a2097ec17a2461e9cf0f99fcd805ac8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_09cb4d26a5be48fd883245d7f0dee536",
              "IPY_MODEL_dd0a77643dc54fbd8b7e009b45e1e10a"
            ]
          }
        },
        "9a2097ec17a2461e9cf0f99fcd805ac8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09cb4d26a5be48fd883245d7f0dee536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_06cc714535224ef7b13ea0df22c55c7a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43a400b6d6a446b7ac9f9d1e4a1bc4bc"
          }
        },
        "dd0a77643dc54fbd8b7e009b45e1e10a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3df87c414d0b4f548a070b28931e09bb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "‚Äã",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [00:14&lt;00:00, 16.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc2657b1a4634e6facc6ed05b143ee13"
          }
        },
        "06cc714535224ef7b13ea0df22c55c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43a400b6d6a446b7ac9f9d1e4a1bc4bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3df87c414d0b4f548a070b28931e09bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc2657b1a4634e6facc6ed05b143ee13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Steve-YJ/Colab_Exercise/blob/master/%5BAgain%5D%5BResNet_VAE%5D_Exp2_3_1_Transfer_Learning_Using_ResNet_VAE'Encoer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EskqGIauJhlT"
      },
      "source": [
        "# README.MD\n",
        "\n",
        "* For Variant Malware Classification Use pre-trained ResNet-VAE's Encoder Network\n",
        "    * Load pre-trained ResNet-VAE's Encoder Network\n",
        "        * Model State dic and Optim State dic\n",
        "    * 1. Pass Malimg Data to Encoder Network --> Exp 01\n",
        "    * 2. Train transfered Encoder for Malware Classification --> Exp 02\n",
        "\n",
        "‚úÖ Check Point\n",
        ">  1. Load ResNet-VAE's 92th Model&Optim Parameters\n",
        ">  2. Hyperparameter Optimization\n",
        ">  3. Save Model's Parameter\n",
        ">  4. Exp Report & Save Results\n",
        "\n",
        "<code>from--- 20.09.28.mon am 00:30 ~ </code>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs44oSmkvaTh"
      },
      "source": [
        "---\n",
        " ‚úã Hyperparameter Optimization(ÌïòÏù¥Ìçº ÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî)Îäî Ï°∞Í∏à ÎØ∏Î§ÑÎëêÏûê.<br>\n",
        "\n",
        "Ïö∞ÏÑ†ÏùÄ Í≥ÑÌöçÎåÄÎ°ú Ïã§ÌóòÏùÑ ÏßÑÌñâÌï¥Î≥¥Ïûê...! -20.09.28.mon-\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-w7cWmrKVOF"
      },
      "source": [
        "# Mount Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AItuRBfmKXDQ",
        "outputId": "b8873063-4def-4664-889a-34a97dda5e55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7tQumIsMaGD"
      },
      "source": [
        "<code>Auto Reload</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTnSkTNRMeAb"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THlFL3wGMk1H",
        "outputId": "fe346972-eb39-4f1b-a9ae-b017b3fa0527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd drive/My\\ Drive/Post_InfoSec_Exps/ResNet-VAE/ResNetVAE-master.zip (Unzipped Files)/ResNetVAE-master\n",
        "# ! ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Post_InfoSec_Exps/ResNet-VAE/ResNetVAE-master.zip (Unzipped Files)/ResNetVAE-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hESv2ea0MxBb"
      },
      "source": [
        "import os \n",
        "\n",
        "def check_mkdir(dir_name):\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.mkdir(dir_name)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_XBFFaLM0hr"
      },
      "source": [
        "check_mkdir('./ResNet-VAE_Exp2_3_1.ResNet-VAE_Encoder_Transfer_Learning')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUnFTAWtNFLM",
        "outputId": "8af8518d-006a-44d4-cd98-33c61f6656ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        }
      },
      "source": [
        "! ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'01.Tutorial-ResNet-VAE.ipynb·Ñã·Ö¥ ·Ñâ·Ö°·Ñá·Ö©·Ü´'\n",
            " 01.Tutorial-ResNet-VAE-Recon.ipynb\n",
            " 02.Tutorial-ResNet-VAE-Tunning.ipynb\n",
            " 03.Tutorial-ResNet-VAE-Tunning.ipynb\n",
            "'03.Tutorial-ResNet-VAE-Tunning.ipynb·Ñã·Ö¥ ·Ñâ·Ö°·Ñá·Ö©·Ü´'\n",
            "'04.Post-01.Tutorial-ResNet-VAE.ipynb·Ñâ·Ö°·Ñá·Ö©·Ü´·Ñã·Ö¥ ·Ñâ·Ö°·Ñá·Ö©·Ü´'\n",
            "'05.Post-01.Tutorial-ResNet-VAE_Train_Again.ipynb·Ñã·Ö¥ ·Ñâ·Ö°·Ñá·Ö©·Ü´'\n",
            " 19train_val_plot.png\n",
            " 39train_val_plot.png\n",
            " 59train_val_plot.png\n",
            " Again_ResNet-VAE_Exp01\n",
            "'[Again][ResNet-VAE] Exp2_3_1. Transfer Learning Using ResNet-VAE'\\''Encoer.ipynb'\n",
            "'[Again] ResNet-VAE_plus_Classifier .ipynb'\n",
            "'[Again] ResNet-VAE_plus_Classifier .ipynb·Ñã·Ö¥ ·Ñâ·Ö°·Ñá·Ö©·Ü´'\n",
            "'[Again] Re-Start_Training_ResNet-VAE .ipynb'\n",
            " fig\n",
            " generated_Malimg.png\n",
            " modules.py\n",
            " plot_latent.ipynb\n",
            "'[Post_Exp]04-2.ResNet-VAE_Train_Again.ipynb'\n",
            "'[Post_Exp]04-2.ResNet-VAE_Train_Again.ipynb·Ñã·Ö¥ ·Ñâ·Ö°·Ñá·Ö©·Ü´'\n",
            "'[Post_Exp]04-3.ResNet-VAE_Train_Again.ipynb'\n",
            "'[Post_Exp]04-3.ResNet-VAE_Train_Again.ipynb·Ñã·Ö¥ ·Ñâ·Ö°·Ñá·Ö©·Ü´'\n",
            "'[Post_Exp]05-2.ResNet-VAE_Reduce_lr.ipynb'\n",
            " __pycache__\n",
            " README.md\n",
            " reconstruction_Malimg.png\n",
            " ResNetVAE_cifar10.py\n",
            " ResNet-VAE_Exp2_3_1.ResNet-VAE_Encoder_Transfer_Learning\n",
            " ResNetVAE_FACE.py\n",
            " ResNetVAE_MNIST.py\n",
            " ResNetVAE_reconstruction.ipynb\n",
            " results_Malimg_Exp4\n",
            " results_Malimg_Exp4_3\n",
            " results_ResNet-VAE_Exp01\n",
            " results_ResNet-VAE_Exp01-Classification_Test\n",
            " save_loss_graph_accuracy.png\n",
            " train_test_loss_plot.png\n",
            " train_val_plot.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtIskK91Nf6Y",
        "outputId": "0e1806c8-edea-4cc0-da46-8ebeb1e4da06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "save_path =  'ResNet-VAE_Exp2_3_1.ResNet-VAE_Encoder_Transfer_Learning'\n",
        "save_path  # Ïã§Ìóò Í≤∞Í≥º Ï†ÄÏû• Í≤ΩÎ°ú ÏÑ§Ï†ï"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ResNet-VAE_Exp2_3_1.ResNet-VAE_Encoder_Transfer_Learning'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inkcYChOKXN-"
      },
      "source": [
        "# #01. Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1_2kwSgJUV-",
        "outputId": "5f528df9-77c8-4559-9b60-f699f1214a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# save single numpy array\n",
        "from tempfile import TemporaryFile\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import torch\n",
        "import torch.utils.data  # torch.utils.data\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# argparse\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "import torchvision \n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# load modules\n",
        "from torchvision import models\n",
        "from modules import *"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-nGWHHxKbuy"
      },
      "source": [
        "# #02. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui3A2unwKg-M"
      },
      "source": [
        "transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=3), # make 3-channel Images\n",
        "    transforms.ToTensor()])  # Composes several transforms together.\n",
        "\n",
        "# make custom dataset\n",
        "trainset = torchvision.datasets.ImageFolder(root='../../../../InformationSecurity_Summer/malimg',\n",
        "                                            transform=transforms)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUW1zlMsKhR5",
        "outputId": "e456dcb9-7978-4ce4-84e3-d697749c6e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "classes = trainset.classes\n",
        "classes"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Adialer.C',\n",
              " 'Agent.FYI',\n",
              " 'Allaple.A',\n",
              " 'Allaple.L',\n",
              " 'Alueron.gen!J',\n",
              " 'Autorun.K',\n",
              " 'C2LOP.P',\n",
              " 'C2LOP.gen!g',\n",
              " 'Dialplatform.B',\n",
              " 'Dontovo.A',\n",
              " 'Fakerean',\n",
              " 'Instantaccess',\n",
              " 'Lolyda.AA1',\n",
              " 'Lolyda.AA2',\n",
              " 'Lolyda.AA3',\n",
              " 'Lolyda.AT',\n",
              " 'Malex.gen!J',\n",
              " 'Obfuscator.AD',\n",
              " 'Rbot!gen',\n",
              " 'Skintrim.N',\n",
              " 'Swizzor.gen!E',\n",
              " 'Swizzor.gen!I',\n",
              " 'VB.AT',\n",
              " 'Wintrim.BX',\n",
              " 'Yuner.A']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsvvauzuOghr"
      },
      "source": [
        "* Make Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU8D_bMTLC8f",
        "outputId": "b7a52579-dc36-4b4e-c0db-7ffb487f8087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "full_dataset = trainset\n",
        "\n",
        "# maek train, val, test: 8:1:1\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = int(0.1 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "print(train_size, val_size, test_size)\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7471 933 935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOyiSg2uPdBc",
        "outputId": "cc7241ce-75b6-4317-ed34-feadc508f76a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "import argparse\n",
        "\n",
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123\n",
        "np.random.seed(seed)  # Reseed a legacy MT19937 BitGenerator\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "# ===== Model ====== #\n",
        "args.CNN_fc_hidden1 = 1024\n",
        "args.CNN_fc_hidden2 = 1024\n",
        "args.CNN_embed_dim = 256  # latent dim extracted by 2D CNN\n",
        "args.res_size = 224       # ResNet Image size\n",
        "\n",
        "# ===== Regularization ===== #\n",
        "args.dropout_p = 0.2           # dropout probability\n",
        "\n",
        "# ===== training parameters ====== #\n",
        "args.epochs = 20\n",
        "args.learning_rate = 1e-3\n",
        "\n",
        "args.batch_size = 50\n",
        "args.log_interval = 10  # interval for displaying training info\n",
        "\n",
        "print(args)\n",
        "\n",
        "\"\"\"\n",
        "# ===== Experiment Variable ====== #\n",
        "# Reference: https://github.com/Steve-YJ/Assignment_Standalone_DL/blob/master/%5BRe_Fact%5D%20CIFAR-10_CNN_Report_Result.ipynb\n",
        "\n",
        "name_var1 = 'lr'\n",
        "name_var2 = 'hiden_layer_num'\n",
        "list_var1 = [1e-3, 1e-4, 1e-5]\n",
        "list_var2 = [1024, 512, 256, 128, 64, 32]\n",
        "\n",
        "\n",
        "for var1 in list_var1:\n",
        "    for var2 in list_var2:\n",
        "        # setattr ??: name_var1('n_layer')Î•º 1, 2, 3ÏúºÎ°ú Î∞îÍøîÏ§ÄÎã§\n",
        "        # setattr = args.name_var1 = var1\n",
        "        setattr(args, name_var1, var1)\n",
        "        setattr(args, name_var2, var2)\n",
        "        print(args)\n",
        "                \n",
        "        setting, result = experiment(, deepcopy(args))\n",
        "        save_exp_result(setting, result)\n",
        "\"\"\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(CNN_embed_dim=256, CNN_fc_hidden1=1024, CNN_fc_hidden2=1024, batch_size=50, dropout_p=0.2, epochs=20, learning_rate=0.001, log_interval=10, res_size=224)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# ===== Experiment Variable ====== #\\n# Reference: https://github.com/Steve-YJ/Assignment_Standalone_DL/blob/master/%5BRe_Fact%5D%20CIFAR-10_CNN_Report_Result.ipynb\\n\\nname_var1 = 'lr'\\nname_var2 = 'hiden_layer_num'\\nlist_var1 = [1e-3, 1e-4, 1e-5]\\nlist_var2 = [1024, 512, 256, 128, 64, 32]\\n\\n\\nfor var1 in list_var1:\\n    for var2 in list_var2:\\n        # setattr ??: name_var1('n_layer')Î•º 1, 2, 3ÏúºÎ°ú Î∞îÍøîÏ§ÄÎã§\\n        # setattr = args.name_var1 = var1\\n        setattr(args, name_var1, var1)\\n        setattr(args, name_var2, var2)\\n        print(args)\\n                \\n        setting, result = experiment(, deepcopy(args))\\n        save_exp_result(setting, result)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFOiLznDDC1-"
      },
      "source": [
        "* z_loader's batch_size: 9339 -> 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjNp_6bXOVgE"
      },
      "source": [
        "# make custom data_loader\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                         batch_size=args.batch_size,\n",
        "                         shuffle=True,\n",
        "                         pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=args.batch_size,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True)\n",
        " \n",
        "test_loader = DataLoader(test_dataset,\n",
        "                        batch_size=args.batch_size,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True)  # Instead, we recommend using automatic memory pinning (i.e., setting pin_memory=True)\n",
        "                                          #  which enables fast data transfer to CUDA-enabled GPUs\n",
        "\n",
        "# First, insert all test dataset\n",
        "# z_loader: for latent vector extraction\n",
        "z_loader = DataLoader(full_dataset,\n",
        "                        batch_size=32,  # 128 -> 64 -> 32\n",
        "                        shuffle=False,  # Shuffle -> False\n",
        "                        pin_memory=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJCH1X2PKhcb"
      },
      "source": [
        "# #03. Model Architecture\n",
        "## Make ResNet-VAE Architecture\n",
        "* Referenced: https://github.com/hsinyilin19/ResNetVAE/blob/master/modules.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQH5lml_ruWV"
      },
      "source": [
        "Ïã§Ìóò Í≤∞Í≥º Ï†ÄÏû• Í≤ΩÎ°ú ÏÑ§Ï†ï <code>save_model_path</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Iz5mg0Pv1q",
        "outputId": "e7fba7bf-7dba-4302-e646-0cb238164c54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "save_model_path = save_path\n",
        "save_model_path"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ResNet-VAE_Exp2_3_1.ResNet-VAE_Encoder_Transfer_Learning'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVvnNa3QPv4O"
      },
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    # MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    MSE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return MSE + KLD"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB52iWLDPv6c"
      },
      "source": [
        "class ResNet_VAE(nn.Module):\n",
        "    def __init__(self, fc_hidden1=1024, fc_hidden2=768, drop_p=0.3, CNN_embed_dim=256):\n",
        "        super(ResNet_VAE, self).__init__()\n",
        "\n",
        "        self.fc_hidden1, self.fc_hidden2, self.CNN_embed_dim = fc_hidden1, fc_hidden2, CNN_embed_dim\n",
        "\n",
        "        # CNN architechtures\n",
        "        self.ch1, self.ch2, self.ch3, self.ch4 = 16, 32, 64, 128\n",
        "        self.k1, self.k2, self.k3, self.k4 = (5, 5), (3, 3), (3, 3), (3, 3)      # 2d kernal size\n",
        "        self.s1, self.s2, self.s3, self.s4 = (2, 2), (2, 2), (2, 2), (2, 2)      # 2d strides\n",
        "        self.pd1, self.pd2, self.pd3, self.pd4 = (0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\n",
        "\n",
        "        # ====== Encoder ====== #\n",
        "        # encoding components\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
        "                                                    # pre-trained ResNet ArchitectureÏóêÏÑú ÎßàÏßÄÎßâ FCÎ†àÏù¥Ïñ¥Î•º Ï†úÏô∏Ìïú Model\n",
        "        # ====== Notice ======= #\n",
        "        # Ïù¥ ÏΩîÎìúÎ•º Ïûò Í≥†ÏπòÎ©¥ MLP LayerÏùò Í∞úÏàòÎ•º Î≥ÄÍ≤ΩÌï¥ Í∞ÄÎ©¥ÏÑú Ïã§ÌóòÌï† Ïàò ÏûàÍ≤†Îã§\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.fc1 = nn.Linear(resnet.fc.in_features, self.fc_hidden1)\n",
        "        self.bn1 = nn.BatchNorm1d(self.fc_hidden1, momentum=0.01)\n",
        "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
        "        self.bn2 = nn.BatchNorm1d(self.fc_hidden2, momentum=0.01)\n",
        "\n",
        "        # Latent vectors mu and sigma\n",
        "        self.fc3_mu = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)      # output = CNN embedding latent variables\n",
        "        self.fc3_logvar = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)  # output = CNN embedding latent variables\n",
        "\n",
        "        # Sampling vector\n",
        "        self.fc4 = nn.Linear(self.CNN_embed_dim, self.fc_hidden2)\n",
        "        self.fc_bn4 = nn.BatchNorm1d(self.fc_hidden2)\n",
        "        self.fc5 = nn.Linear(self.fc_hidden2, 64 * 4 * 4)\n",
        "        self.fc_bn5 = nn.BatchNorm1d(64 * 4 * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # ====== Decoder ====== #\n",
        "        # Decoder\n",
        "        self.convTrans6 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=self.k4, stride=self.s4,\n",
        "                               padding=self.pd4),\n",
        "            nn.BatchNorm2d(32, momentum=0.01),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.convTrans7 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=32, out_channels=8, kernel_size=self.k3, stride=self.s3,\n",
        "                               padding=self.pd3),\n",
        "            nn.BatchNorm2d(8, momentum=0.01),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.convTrans8 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=self.k2, stride=self.s2,\n",
        "                               padding=self.pd2),\n",
        "            nn.BatchNorm2d(3, momentum=0.01),\n",
        "            nn.Sigmoid()    # y = (y1, y2, y3) \\in [0 ,1]^3\n",
        "        )\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.resnet(x)  # ResNet\n",
        "        x = x.view(x.size(0), -1)  # flatten output of conv\n",
        "\n",
        "        # FC layers\n",
        "        x = self.bn1(self.fc1(x))\n",
        "        x = self.relu(x)\n",
        "        x = self.bn2(self.fc2(x))\n",
        "        x = self.relu(x)\n",
        "        # x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "        mu, logvar = self.fc3_mu(x), self.fc3_logvar(x)\n",
        "        return mu, logvar  # check: Encoder -> hid1-BN-hid2-BN ==> x\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = Variable(std.data.new(std.size()).normal_())\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def decode(self, z):\n",
        "        x = self.relu(self.fc_bn4(self.fc4(z)))\n",
        "        x = self.relu(self.fc_bn5(self.fc5(x))).view(-1, 64, 4, 4)\n",
        "        x = self.convTrans6(x)\n",
        "        x = self.convTrans7(x)\n",
        "        x = self.convTrans8(x)\n",
        "        x = F.interpolate(x, size=(224, 224), mode='bilinear')\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_reconst = self.decode(z)\n",
        "\n",
        "        return x_reconst, z, mu, logvar"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H_gY437OknT"
      },
      "source": [
        "üìå Custom DataLoaderÏùò Í≤ΩÏö∞, ÏûêÎèôÌôîÎ•º ÏúÑÌï¥ Train, Val, Test Ìï®Ïàò ÎÇ¥Ïóê ÎßåÎì§Ïñ¥Ï§ÄÎã§"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j8T8ki7LKZY",
        "outputId": "6e4693d0-09e9-4a4a-dce7-893b9ad43bfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# Detect devices\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I9ydVDNQTnS",
        "outputId": "a87baa71-4ced-4a48-a745-e9e96d9c0afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLay2SvtSakU"
      },
      "source": [
        "95 EpochÏù¥ ÏµúÏÑ†Ïù¥ÏßÄÎßå... Îç∞Ïù¥ÌÑ∞Î•º ÏßÄÏõåÎ≤ÑÎ†∏Í∏∞Ïóê „Ö† Ï∞®ÏÑ†Ïù∏ Epoch 90Ïùò Í∞íÏùÑ Î∂àÎü¨Ïò®Îã§...!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxNeuVHLBHjh"
      },
      "source": [
        "ÏïÑÎûòÏùò ÏÖÄÏùÑ Ïã§ÌñâÌïòÎ©¥ ./results_ResNet-VAE_Exp01Ïóê Ï†ÄÏû•ÎêòÏñ¥ÏûàÎäî pre-trained modelÏùò parameterÍ∞íÏùÑ Î∂àÎü¨Ïò¨ Ïàò ÏûàÎã§"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLgjTPeNQGA3",
        "outputId": "e3764770-1a85-4091-c23c-e12311174d55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "8e3499a393d5425d830de7a892b31b4b",
            "9a2097ec17a2461e9cf0f99fcd805ac8",
            "09cb4d26a5be48fd883245d7f0dee536",
            "dd0a77643dc54fbd8b7e009b45e1e10a",
            "06cc714535224ef7b13ea0df22c55c7a",
            "43a400b6d6a446b7ac9f9d1e4a1bc4bc",
            "3df87c414d0b4f548a070b28931e09bb",
            "bc2657b1a4634e6facc6ed05b143ee13"
          ]
        }
      },
      "source": [
        "### If you want to use pre-trained model ####\n",
        "pre_saved_model_path = './results_ResNet-VAE_Exp01'\n",
        "epoch=90\n",
        "\n",
        "# Create model\n",
        "resnet_vae = ResNet_VAE(fc_hidden1=args.CNN_fc_hidden1, fc_hidden2=args.CNN_fc_hidden2, drop_p=args.dropout_p, CNN_embed_dim=args.CNN_embed_dim).to(device)\n",
        "### If you want to use pre-trained model ####\n",
        "resnet_vae.load_state_dict(torch.load(os.path.join(pre_saved_model_path, 'model_epoch{}.pth'.format(epoch))))\n",
        "\n",
        "model_params = list(resnet_vae.parameters())\n",
        "optimizer = torch.optim.Adam(model_params, lr=args.learning_rate)\n",
        "### If you want to use pre-trained model's optimizer ####\n",
        "optimizer.load_state_dict(torch.load(os.path.join(pre_saved_model_path, 'optimizer_epoch{}.pth'.format(epoch))))\n",
        "\n",
        "\n",
        "print('Number of {} parameters'.format(sum(p.numel() for p in resnet_vae.parameters() if p.requires_grad)))\n",
        "print(\"Using\", torch.cuda.device_count(), \"GPU!\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e3499a393d5425d830de7a892b31b4b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of 63158425 parameters\n",
            "Using 1 GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKlygswKapgc",
        "outputId": "5a06627b-414e-41e7-ccb6-054b050ed13a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "resnet_vae"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet_VAE(\n",
              "  (resnet): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (6): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (7): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (6): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (7): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (8): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (9): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (10): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (11): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (12): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (13): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (14): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (15): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (16): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (17): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (18): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (19): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (20): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (21): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (22): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (23): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (24): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (25): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (26): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (27): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (28): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (29): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (30): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (31): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (32): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (33): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (34): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (35): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              "  (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
              "  (fc3_mu): Linear(in_features=1024, out_features=256, bias=True)\n",
              "  (fc3_logvar): Linear(in_features=1024, out_features=256, bias=True)\n",
              "  (fc4): Linear(in_features=256, out_features=1024, bias=True)\n",
              "  (fc_bn4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "  (fc_bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (convTrans6): Sequential(\n",
              "    (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (convTrans7): Sequential(\n",
              "    (0): ConvTranspose2d(32, 8, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (convTrans8): Sequential(\n",
              "    (0): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
              "    (2): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIDuGdu6LC-Y"
      },
      "source": [
        "# #04. Train, Validation, Test\n",
        "After train ResNet-VAE, Use EncoderNet as Feature Extractor<br>\n",
        "<br>\n",
        "* Extract Latent Vector\n",
        "* Improve Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgkcP9eUa8El"
      },
      "source": [
        "* pre-traned ResNet-VAEÏùò EncoderÎ•º <code>Feature Extractor</code>Î°ú ÌôúÏö©ÌïòÏó¨ Î∂ÑÎ•ò Î¨∏Ï†úÎ•º ÌíÄÏñ¥Î≥¥ÎèÑÎ°ù ÌïúÎã§. <code>20.09.28.mon pm4:00</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGmBlAxJbKBm"
      },
      "source": [
        "* Referernce:\n",
        "* Ï∂îÍ∞ÄÏã§Ìóò List\n",
        "    * Load VAE_State_Dict\n",
        "    * Extract Latent vector & make Training set(for DL/ML)\n",
        "    * => For all training data: (9339, 256)  / Latent dim\n",
        "    * => Add Labels: (9339, 101)\n",
        "    * Make Pandas DataFrame\n",
        "    * Make pd.DataFrame()\n",
        "    * Save DataFrame() to csv\n",
        "    * Make Classifier\n",
        "* Classification\n",
        "    * ML: Voting Classifier\n",
        "    * DL: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-hcJ4gFjkZ0"
      },
      "source": [
        "# ResNet-VAE & Ensemble "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sigZH93KbHzs"
      },
      "source": [
        "## Extract Latent Vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHJlFTvRJINl"
      },
      "source": [
        "* Ï†ÅÎãπÌïú ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏßúÏïºÍ≤†Íµ¨ÎÇò...! -20.09.28.mon pm6:50-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGob8kg4bfTX",
        "outputId": "97397830-c5cf-498c-efa2-374a57e6b83b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i, (data, _) in enumerate(z_loader):  # load_whole data(9339)\n",
        "    data = data.to(device)\n",
        "    # recon_batch, mu, logvar, z = resnet_vae(data)\n",
        "    recon_batch, z, mu, logvar = resnet_vae(data)  # batch_sizeÎßåÌÅº Í∞í Ï†ÄÏû•\n",
        "    \n",
        "    # save latent_vector\n",
        "    latent_vector = z.detach().cpu().clone().numpy()  # change tensor type data to cpu().numpy()\n",
        "                                                        # latent_vector_size: (num_of_data, 256)  # num_of_data, num_of_dim\n",
        "    # save label\n",
        "    label = _.detach().cpu().clone().numpy()\n",
        "    label =label.reshape(1, -1)\n",
        "    print(label.shape)\n",
        "\n",
        "    # concat_data\n",
        "    if i == 0:\n",
        "        latent_z = np.concatenate((latent_vector, label.T), axis=1)  # latent_z: (9339, 101): (num_data, latent_z + label) \n",
        "        print(latent_z.shape)  # (32, 257): batch_num, features + label\n",
        "    elif i > 0:\n",
        "        latent_z = np.concatenate((latent_z,  np.concatenate((latent_vector, label.T), axis=1)), axis=0)\n",
        "        print(latent_z.shape)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 32)\n",
            "(32, 257)\n",
            "(1, 32)\n",
            "(64, 257)\n",
            "(1, 32)\n",
            "(96, 257)\n",
            "(1, 32)\n",
            "(128, 257)\n",
            "(1, 32)\n",
            "(160, 257)\n",
            "(1, 32)\n",
            "(192, 257)\n",
            "(1, 32)\n",
            "(224, 257)\n",
            "(1, 32)\n",
            "(256, 257)\n",
            "(1, 32)\n",
            "(288, 257)\n",
            "(1, 32)\n",
            "(320, 257)\n",
            "(1, 32)\n",
            "(352, 257)\n",
            "(1, 32)\n",
            "(384, 257)\n",
            "(1, 32)\n",
            "(416, 257)\n",
            "(1, 32)\n",
            "(448, 257)\n",
            "(1, 32)\n",
            "(480, 257)\n",
            "(1, 32)\n",
            "(512, 257)\n",
            "(1, 32)\n",
            "(544, 257)\n",
            "(1, 32)\n",
            "(576, 257)\n",
            "(1, 32)\n",
            "(608, 257)\n",
            "(1, 32)\n",
            "(640, 257)\n",
            "(1, 32)\n",
            "(672, 257)\n",
            "(1, 32)\n",
            "(704, 257)\n",
            "(1, 32)\n",
            "(736, 257)\n",
            "(1, 32)\n",
            "(768, 257)\n",
            "(1, 32)\n",
            "(800, 257)\n",
            "(1, 32)\n",
            "(832, 257)\n",
            "(1, 32)\n",
            "(864, 257)\n",
            "(1, 32)\n",
            "(896, 257)\n",
            "(1, 32)\n",
            "(928, 257)\n",
            "(1, 32)\n",
            "(960, 257)\n",
            "(1, 32)\n",
            "(992, 257)\n",
            "(1, 32)\n",
            "(1024, 257)\n",
            "(1, 32)\n",
            "(1056, 257)\n",
            "(1, 32)\n",
            "(1088, 257)\n",
            "(1, 32)\n",
            "(1120, 257)\n",
            "(1, 32)\n",
            "(1152, 257)\n",
            "(1, 32)\n",
            "(1184, 257)\n",
            "(1, 32)\n",
            "(1216, 257)\n",
            "(1, 32)\n",
            "(1248, 257)\n",
            "(1, 32)\n",
            "(1280, 257)\n",
            "(1, 32)\n",
            "(1312, 257)\n",
            "(1, 32)\n",
            "(1344, 257)\n",
            "(1, 32)\n",
            "(1376, 257)\n",
            "(1, 32)\n",
            "(1408, 257)\n",
            "(1, 32)\n",
            "(1440, 257)\n",
            "(1, 32)\n",
            "(1472, 257)\n",
            "(1, 32)\n",
            "(1504, 257)\n",
            "(1, 32)\n",
            "(1536, 257)\n",
            "(1, 32)\n",
            "(1568, 257)\n",
            "(1, 32)\n",
            "(1600, 257)\n",
            "(1, 32)\n",
            "(1632, 257)\n",
            "(1, 32)\n",
            "(1664, 257)\n",
            "(1, 32)\n",
            "(1696, 257)\n",
            "(1, 32)\n",
            "(1728, 257)\n",
            "(1, 32)\n",
            "(1760, 257)\n",
            "(1, 32)\n",
            "(1792, 257)\n",
            "(1, 32)\n",
            "(1824, 257)\n",
            "(1, 32)\n",
            "(1856, 257)\n",
            "(1, 32)\n",
            "(1888, 257)\n",
            "(1, 32)\n",
            "(1920, 257)\n",
            "(1, 32)\n",
            "(1952, 257)\n",
            "(1, 32)\n",
            "(1984, 257)\n",
            "(1, 32)\n",
            "(2016, 257)\n",
            "(1, 32)\n",
            "(2048, 257)\n",
            "(1, 32)\n",
            "(2080, 257)\n",
            "(1, 32)\n",
            "(2112, 257)\n",
            "(1, 32)\n",
            "(2144, 257)\n",
            "(1, 32)\n",
            "(2176, 257)\n",
            "(1, 32)\n",
            "(2208, 257)\n",
            "(1, 32)\n",
            "(2240, 257)\n",
            "(1, 32)\n",
            "(2272, 257)\n",
            "(1, 32)\n",
            "(2304, 257)\n",
            "(1, 32)\n",
            "(2336, 257)\n",
            "(1, 32)\n",
            "(2368, 257)\n",
            "(1, 32)\n",
            "(2400, 257)\n",
            "(1, 32)\n",
            "(2432, 257)\n",
            "(1, 32)\n",
            "(2464, 257)\n",
            "(1, 32)\n",
            "(2496, 257)\n",
            "(1, 32)\n",
            "(2528, 257)\n",
            "(1, 32)\n",
            "(2560, 257)\n",
            "(1, 32)\n",
            "(2592, 257)\n",
            "(1, 32)\n",
            "(2624, 257)\n",
            "(1, 32)\n",
            "(2656, 257)\n",
            "(1, 32)\n",
            "(2688, 257)\n",
            "(1, 32)\n",
            "(2720, 257)\n",
            "(1, 32)\n",
            "(2752, 257)\n",
            "(1, 32)\n",
            "(2784, 257)\n",
            "(1, 32)\n",
            "(2816, 257)\n",
            "(1, 32)\n",
            "(2848, 257)\n",
            "(1, 32)\n",
            "(2880, 257)\n",
            "(1, 32)\n",
            "(2912, 257)\n",
            "(1, 32)\n",
            "(2944, 257)\n",
            "(1, 32)\n",
            "(2976, 257)\n",
            "(1, 32)\n",
            "(3008, 257)\n",
            "(1, 32)\n",
            "(3040, 257)\n",
            "(1, 32)\n",
            "(3072, 257)\n",
            "(1, 32)\n",
            "(3104, 257)\n",
            "(1, 32)\n",
            "(3136, 257)\n",
            "(1, 32)\n",
            "(3168, 257)\n",
            "(1, 32)\n",
            "(3200, 257)\n",
            "(1, 32)\n",
            "(3232, 257)\n",
            "(1, 32)\n",
            "(3264, 257)\n",
            "(1, 32)\n",
            "(3296, 257)\n",
            "(1, 32)\n",
            "(3328, 257)\n",
            "(1, 32)\n",
            "(3360, 257)\n",
            "(1, 32)\n",
            "(3392, 257)\n",
            "(1, 32)\n",
            "(3424, 257)\n",
            "(1, 32)\n",
            "(3456, 257)\n",
            "(1, 32)\n",
            "(3488, 257)\n",
            "(1, 32)\n",
            "(3520, 257)\n",
            "(1, 32)\n",
            "(3552, 257)\n",
            "(1, 32)\n",
            "(3584, 257)\n",
            "(1, 32)\n",
            "(3616, 257)\n",
            "(1, 32)\n",
            "(3648, 257)\n",
            "(1, 32)\n",
            "(3680, 257)\n",
            "(1, 32)\n",
            "(3712, 257)\n",
            "(1, 32)\n",
            "(3744, 257)\n",
            "(1, 32)\n",
            "(3776, 257)\n",
            "(1, 32)\n",
            "(3808, 257)\n",
            "(1, 32)\n",
            "(3840, 257)\n",
            "(1, 32)\n",
            "(3872, 257)\n",
            "(1, 32)\n",
            "(3904, 257)\n",
            "(1, 32)\n",
            "(3936, 257)\n",
            "(1, 32)\n",
            "(3968, 257)\n",
            "(1, 32)\n",
            "(4000, 257)\n",
            "(1, 32)\n",
            "(4032, 257)\n",
            "(1, 32)\n",
            "(4064, 257)\n",
            "(1, 32)\n",
            "(4096, 257)\n",
            "(1, 32)\n",
            "(4128, 257)\n",
            "(1, 32)\n",
            "(4160, 257)\n",
            "(1, 32)\n",
            "(4192, 257)\n",
            "(1, 32)\n",
            "(4224, 257)\n",
            "(1, 32)\n",
            "(4256, 257)\n",
            "(1, 32)\n",
            "(4288, 257)\n",
            "(1, 32)\n",
            "(4320, 257)\n",
            "(1, 32)\n",
            "(4352, 257)\n",
            "(1, 32)\n",
            "(4384, 257)\n",
            "(1, 32)\n",
            "(4416, 257)\n",
            "(1, 32)\n",
            "(4448, 257)\n",
            "(1, 32)\n",
            "(4480, 257)\n",
            "(1, 32)\n",
            "(4512, 257)\n",
            "(1, 32)\n",
            "(4544, 257)\n",
            "(1, 32)\n",
            "(4576, 257)\n",
            "(1, 32)\n",
            "(4608, 257)\n",
            "(1, 32)\n",
            "(4640, 257)\n",
            "(1, 32)\n",
            "(4672, 257)\n",
            "(1, 32)\n",
            "(4704, 257)\n",
            "(1, 32)\n",
            "(4736, 257)\n",
            "(1, 32)\n",
            "(4768, 257)\n",
            "(1, 32)\n",
            "(4800, 257)\n",
            "(1, 32)\n",
            "(4832, 257)\n",
            "(1, 32)\n",
            "(4864, 257)\n",
            "(1, 32)\n",
            "(4896, 257)\n",
            "(1, 32)\n",
            "(4928, 257)\n",
            "(1, 32)\n",
            "(4960, 257)\n",
            "(1, 32)\n",
            "(4992, 257)\n",
            "(1, 32)\n",
            "(5024, 257)\n",
            "(1, 32)\n",
            "(5056, 257)\n",
            "(1, 32)\n",
            "(5088, 257)\n",
            "(1, 32)\n",
            "(5120, 257)\n",
            "(1, 32)\n",
            "(5152, 257)\n",
            "(1, 32)\n",
            "(5184, 257)\n",
            "(1, 32)\n",
            "(5216, 257)\n",
            "(1, 32)\n",
            "(5248, 257)\n",
            "(1, 32)\n",
            "(5280, 257)\n",
            "(1, 32)\n",
            "(5312, 257)\n",
            "(1, 32)\n",
            "(5344, 257)\n",
            "(1, 32)\n",
            "(5376, 257)\n",
            "(1, 32)\n",
            "(5408, 257)\n",
            "(1, 32)\n",
            "(5440, 257)\n",
            "(1, 32)\n",
            "(5472, 257)\n",
            "(1, 32)\n",
            "(5504, 257)\n",
            "(1, 32)\n",
            "(5536, 257)\n",
            "(1, 32)\n",
            "(5568, 257)\n",
            "(1, 32)\n",
            "(5600, 257)\n",
            "(1, 32)\n",
            "(5632, 257)\n",
            "(1, 32)\n",
            "(5664, 257)\n",
            "(1, 32)\n",
            "(5696, 257)\n",
            "(1, 32)\n",
            "(5728, 257)\n",
            "(1, 32)\n",
            "(5760, 257)\n",
            "(1, 32)\n",
            "(5792, 257)\n",
            "(1, 32)\n",
            "(5824, 257)\n",
            "(1, 32)\n",
            "(5856, 257)\n",
            "(1, 32)\n",
            "(5888, 257)\n",
            "(1, 32)\n",
            "(5920, 257)\n",
            "(1, 32)\n",
            "(5952, 257)\n",
            "(1, 32)\n",
            "(5984, 257)\n",
            "(1, 32)\n",
            "(6016, 257)\n",
            "(1, 32)\n",
            "(6048, 257)\n",
            "(1, 32)\n",
            "(6080, 257)\n",
            "(1, 32)\n",
            "(6112, 257)\n",
            "(1, 32)\n",
            "(6144, 257)\n",
            "(1, 32)\n",
            "(6176, 257)\n",
            "(1, 32)\n",
            "(6208, 257)\n",
            "(1, 32)\n",
            "(6240, 257)\n",
            "(1, 32)\n",
            "(6272, 257)\n",
            "(1, 32)\n",
            "(6304, 257)\n",
            "(1, 32)\n",
            "(6336, 257)\n",
            "(1, 32)\n",
            "(6368, 257)\n",
            "(1, 32)\n",
            "(6400, 257)\n",
            "(1, 32)\n",
            "(6432, 257)\n",
            "(1, 32)\n",
            "(6464, 257)\n",
            "(1, 32)\n",
            "(6496, 257)\n",
            "(1, 32)\n",
            "(6528, 257)\n",
            "(1, 32)\n",
            "(6560, 257)\n",
            "(1, 32)\n",
            "(6592, 257)\n",
            "(1, 32)\n",
            "(6624, 257)\n",
            "(1, 32)\n",
            "(6656, 257)\n",
            "(1, 32)\n",
            "(6688, 257)\n",
            "(1, 32)\n",
            "(6720, 257)\n",
            "(1, 32)\n",
            "(6752, 257)\n",
            "(1, 32)\n",
            "(6784, 257)\n",
            "(1, 32)\n",
            "(6816, 257)\n",
            "(1, 32)\n",
            "(6848, 257)\n",
            "(1, 32)\n",
            "(6880, 257)\n",
            "(1, 32)\n",
            "(6912, 257)\n",
            "(1, 32)\n",
            "(6944, 257)\n",
            "(1, 32)\n",
            "(6976, 257)\n",
            "(1, 32)\n",
            "(7008, 257)\n",
            "(1, 32)\n",
            "(7040, 257)\n",
            "(1, 32)\n",
            "(7072, 257)\n",
            "(1, 32)\n",
            "(7104, 257)\n",
            "(1, 32)\n",
            "(7136, 257)\n",
            "(1, 32)\n",
            "(7168, 257)\n",
            "(1, 32)\n",
            "(7200, 257)\n",
            "(1, 32)\n",
            "(7232, 257)\n",
            "(1, 32)\n",
            "(7264, 257)\n",
            "(1, 32)\n",
            "(7296, 257)\n",
            "(1, 32)\n",
            "(7328, 257)\n",
            "(1, 32)\n",
            "(7360, 257)\n",
            "(1, 32)\n",
            "(7392, 257)\n",
            "(1, 32)\n",
            "(7424, 257)\n",
            "(1, 32)\n",
            "(7456, 257)\n",
            "(1, 32)\n",
            "(7488, 257)\n",
            "(1, 32)\n",
            "(7520, 257)\n",
            "(1, 32)\n",
            "(7552, 257)\n",
            "(1, 32)\n",
            "(7584, 257)\n",
            "(1, 32)\n",
            "(7616, 257)\n",
            "(1, 32)\n",
            "(7648, 257)\n",
            "(1, 32)\n",
            "(7680, 257)\n",
            "(1, 32)\n",
            "(7712, 257)\n",
            "(1, 32)\n",
            "(7744, 257)\n",
            "(1, 32)\n",
            "(7776, 257)\n",
            "(1, 32)\n",
            "(7808, 257)\n",
            "(1, 32)\n",
            "(7840, 257)\n",
            "(1, 32)\n",
            "(7872, 257)\n",
            "(1, 32)\n",
            "(7904, 257)\n",
            "(1, 32)\n",
            "(7936, 257)\n",
            "(1, 32)\n",
            "(7968, 257)\n",
            "(1, 32)\n",
            "(8000, 257)\n",
            "(1, 32)\n",
            "(8032, 257)\n",
            "(1, 32)\n",
            "(8064, 257)\n",
            "(1, 32)\n",
            "(8096, 257)\n",
            "(1, 32)\n",
            "(8128, 257)\n",
            "(1, 32)\n",
            "(8160, 257)\n",
            "(1, 32)\n",
            "(8192, 257)\n",
            "(1, 32)\n",
            "(8224, 257)\n",
            "(1, 32)\n",
            "(8256, 257)\n",
            "(1, 32)\n",
            "(8288, 257)\n",
            "(1, 32)\n",
            "(8320, 257)\n",
            "(1, 32)\n",
            "(8352, 257)\n",
            "(1, 32)\n",
            "(8384, 257)\n",
            "(1, 32)\n",
            "(8416, 257)\n",
            "(1, 32)\n",
            "(8448, 257)\n",
            "(1, 32)\n",
            "(8480, 257)\n",
            "(1, 32)\n",
            "(8512, 257)\n",
            "(1, 32)\n",
            "(8544, 257)\n",
            "(1, 32)\n",
            "(8576, 257)\n",
            "(1, 32)\n",
            "(8608, 257)\n",
            "(1, 32)\n",
            "(8640, 257)\n",
            "(1, 32)\n",
            "(8672, 257)\n",
            "(1, 32)\n",
            "(8704, 257)\n",
            "(1, 32)\n",
            "(8736, 257)\n",
            "(1, 32)\n",
            "(8768, 257)\n",
            "(1, 32)\n",
            "(8800, 257)\n",
            "(1, 32)\n",
            "(8832, 257)\n",
            "(1, 32)\n",
            "(8864, 257)\n",
            "(1, 32)\n",
            "(8896, 257)\n",
            "(1, 32)\n",
            "(8928, 257)\n",
            "(1, 32)\n",
            "(8960, 257)\n",
            "(1, 32)\n",
            "(8992, 257)\n",
            "(1, 32)\n",
            "(9024, 257)\n",
            "(1, 32)\n",
            "(9056, 257)\n",
            "(1, 32)\n",
            "(9088, 257)\n",
            "(1, 32)\n",
            "(9120, 257)\n",
            "(1, 32)\n",
            "(9152, 257)\n",
            "(1, 32)\n",
            "(9184, 257)\n",
            "(1, 32)\n",
            "(9216, 257)\n",
            "(1, 32)\n",
            "(9248, 257)\n",
            "(1, 32)\n",
            "(9280, 257)\n",
            "(1, 32)\n",
            "(9312, 257)\n",
            "(1, 27)\n",
            "(9339, 257)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJZidFR_I_VT",
        "outputId": "862ae4b6-0459-42ac-ee39-7fa91ee5a4bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "latent_z.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9339, 257)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NJxurmYSuiG"
      },
      "source": [
        "save_model_path = 'ResNet-VAE_Exp2_3_1.ResNet-VAE_Encoder_Transfer_Learning'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnn_sFXrcKnJ",
        "outputId": "5977ea84-2f5a-4d09-8847-fbdb1d12eae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "with open(('./'+save_model_path+'/{}Epoch_z_vector.npy'.format(epoch)), 'wb') as f:\n",
        "        np.save(f, latent_z)  # but latent vector size is (16, 100).... just 16...\n",
        "                                    # 1 Epoch Îã®ÏúÑÎ°ú latent vectorÎ•º Ï†ÄÏû•ÌïúÎã§\n",
        "                                    # Ïù¥ ÎñÑ, latent vectorÏùò sizeÎäî test_datasetÏùò ÌÅ¨Í∏∞Í∞Ä ÎêòÏñ¥Ïïº ÌïúÎã§\n",
        "                                    # Í≤∞Í≥ºÍ∞íÏù¥ Ï¢ãÏùÄ vectorÎäî Ï¢ãÏùÄ featureÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎã§.\n",
        "# plot latent vector Every 10 Epochs\n",
        "print(\"Svae Latent vector!\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Svae Latent vector!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNvPkfeTimDv",
        "outputId": "196e0493-09f4-4347-b54e-38aff8df25db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "latent_z.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9339, 257)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuUr0n1OjhCb"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0M0CZRIjBCV",
        "outputId": "3e0ed6c7-3e3d-418c-f522-77dc34e34ec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "latent_vector = np.load('./'+save_model_path+'/{}Epoch_z_vector.npy'.format(epoch))\n",
        "print(\"Load Latent_Vector!!\")\n",
        "latent_vector.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load Latent_Vector!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9339, 257)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAISASxainYg",
        "outputId": "6c4704de-1a22-42da-98d5-1c4224b208dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data = latent_vector\n",
        "print(data.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9339, 257)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_W9dDJHinbB",
        "outputId": "4c611f26-7bed-4c13-8b79-955736b191eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "# numpy to pandas\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame(data=data)\n",
        "data.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.240568</td>\n",
              "      <td>-0.561261</td>\n",
              "      <td>-1.473960</td>\n",
              "      <td>-0.404962</td>\n",
              "      <td>0.245697</td>\n",
              "      <td>1.038918</td>\n",
              "      <td>1.497949</td>\n",
              "      <td>-0.449939</td>\n",
              "      <td>1.170961</td>\n",
              "      <td>-0.546789</td>\n",
              "      <td>-0.780814</td>\n",
              "      <td>1.361100</td>\n",
              "      <td>-1.116053</td>\n",
              "      <td>1.910452</td>\n",
              "      <td>1.412538</td>\n",
              "      <td>0.478924</td>\n",
              "      <td>0.373539</td>\n",
              "      <td>-2.539976</td>\n",
              "      <td>1.891036</td>\n",
              "      <td>0.528984</td>\n",
              "      <td>-1.579915</td>\n",
              "      <td>1.082758</td>\n",
              "      <td>0.560673</td>\n",
              "      <td>0.480609</td>\n",
              "      <td>0.039558</td>\n",
              "      <td>0.151825</td>\n",
              "      <td>-0.786566</td>\n",
              "      <td>-0.029136</td>\n",
              "      <td>1.107318</td>\n",
              "      <td>0.188695</td>\n",
              "      <td>0.495078</td>\n",
              "      <td>-2.756053</td>\n",
              "      <td>-0.256838</td>\n",
              "      <td>-1.180869</td>\n",
              "      <td>-0.122568</td>\n",
              "      <td>-0.018253</td>\n",
              "      <td>-1.009968</td>\n",
              "      <td>0.068781</td>\n",
              "      <td>0.692828</td>\n",
              "      <td>0.367560</td>\n",
              "      <td>...</td>\n",
              "      <td>0.567895</td>\n",
              "      <td>-1.819931</td>\n",
              "      <td>0.934984</td>\n",
              "      <td>-1.330302</td>\n",
              "      <td>-0.783299</td>\n",
              "      <td>0.377807</td>\n",
              "      <td>0.361624</td>\n",
              "      <td>1.070406</td>\n",
              "      <td>1.513979</td>\n",
              "      <td>1.717499</td>\n",
              "      <td>-1.591013</td>\n",
              "      <td>0.140981</td>\n",
              "      <td>0.141379</td>\n",
              "      <td>1.136888</td>\n",
              "      <td>1.406761</td>\n",
              "      <td>-0.208655</td>\n",
              "      <td>1.209202</td>\n",
              "      <td>2.338400</td>\n",
              "      <td>2.392874</td>\n",
              "      <td>2.012412</td>\n",
              "      <td>-0.083929</td>\n",
              "      <td>-0.493434</td>\n",
              "      <td>1.134609</td>\n",
              "      <td>1.298404</td>\n",
              "      <td>0.937509</td>\n",
              "      <td>-1.001865</td>\n",
              "      <td>1.713861</td>\n",
              "      <td>0.247447</td>\n",
              "      <td>0.213539</td>\n",
              "      <td>-0.402759</td>\n",
              "      <td>-0.426054</td>\n",
              "      <td>0.811708</td>\n",
              "      <td>1.755919</td>\n",
              "      <td>-1.964023</td>\n",
              "      <td>0.227610</td>\n",
              "      <td>-0.768265</td>\n",
              "      <td>-1.295807</td>\n",
              "      <td>-0.405288</td>\n",
              "      <td>0.462285</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.996075</td>\n",
              "      <td>0.650010</td>\n",
              "      <td>-0.192071</td>\n",
              "      <td>-0.454621</td>\n",
              "      <td>-1.276196</td>\n",
              "      <td>-0.767499</td>\n",
              "      <td>-0.760912</td>\n",
              "      <td>0.700617</td>\n",
              "      <td>-0.618626</td>\n",
              "      <td>-0.309001</td>\n",
              "      <td>0.553122</td>\n",
              "      <td>-0.545304</td>\n",
              "      <td>-0.615293</td>\n",
              "      <td>-0.224021</td>\n",
              "      <td>-0.132841</td>\n",
              "      <td>2.032287</td>\n",
              "      <td>-0.279049</td>\n",
              "      <td>-0.337486</td>\n",
              "      <td>-0.510281</td>\n",
              "      <td>0.435519</td>\n",
              "      <td>-0.275984</td>\n",
              "      <td>0.629615</td>\n",
              "      <td>-1.974528</td>\n",
              "      <td>-0.119307</td>\n",
              "      <td>0.051937</td>\n",
              "      <td>1.114540</td>\n",
              "      <td>0.164383</td>\n",
              "      <td>0.793738</td>\n",
              "      <td>0.474289</td>\n",
              "      <td>0.513670</td>\n",
              "      <td>-0.203033</td>\n",
              "      <td>0.933196</td>\n",
              "      <td>0.038842</td>\n",
              "      <td>-0.398890</td>\n",
              "      <td>-1.007496</td>\n",
              "      <td>-2.026744</td>\n",
              "      <td>-0.937173</td>\n",
              "      <td>-0.139504</td>\n",
              "      <td>-2.569224</td>\n",
              "      <td>1.689294</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.106298</td>\n",
              "      <td>-1.845786</td>\n",
              "      <td>1.896894</td>\n",
              "      <td>0.181780</td>\n",
              "      <td>-0.117664</td>\n",
              "      <td>0.027586</td>\n",
              "      <td>1.873160</td>\n",
              "      <td>0.400704</td>\n",
              "      <td>0.385745</td>\n",
              "      <td>1.251343</td>\n",
              "      <td>0.150390</td>\n",
              "      <td>0.823379</td>\n",
              "      <td>1.523866</td>\n",
              "      <td>0.766172</td>\n",
              "      <td>-0.780572</td>\n",
              "      <td>0.003207</td>\n",
              "      <td>-0.167644</td>\n",
              "      <td>-1.065065</td>\n",
              "      <td>1.136096</td>\n",
              "      <td>-1.246378</td>\n",
              "      <td>-0.335723</td>\n",
              "      <td>-0.483589</td>\n",
              "      <td>1.081218</td>\n",
              "      <td>-0.601342</td>\n",
              "      <td>0.948993</td>\n",
              "      <td>-0.955097</td>\n",
              "      <td>0.871614</td>\n",
              "      <td>-0.092773</td>\n",
              "      <td>0.880835</td>\n",
              "      <td>-1.132668</td>\n",
              "      <td>2.680984</td>\n",
              "      <td>2.019951</td>\n",
              "      <td>-1.422524</td>\n",
              "      <td>1.331898</td>\n",
              "      <td>-1.140199</td>\n",
              "      <td>-0.439111</td>\n",
              "      <td>1.876634</td>\n",
              "      <td>1.189979</td>\n",
              "      <td>1.602775</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.171825</td>\n",
              "      <td>0.165917</td>\n",
              "      <td>-0.203169</td>\n",
              "      <td>0.767052</td>\n",
              "      <td>0.591684</td>\n",
              "      <td>-1.322803</td>\n",
              "      <td>-0.564557</td>\n",
              "      <td>-1.029487</td>\n",
              "      <td>1.375214</td>\n",
              "      <td>0.408249</td>\n",
              "      <td>0.619360</td>\n",
              "      <td>1.183983</td>\n",
              "      <td>-0.888452</td>\n",
              "      <td>1.377965</td>\n",
              "      <td>0.774443</td>\n",
              "      <td>-0.445341</td>\n",
              "      <td>-1.066941</td>\n",
              "      <td>-0.125578</td>\n",
              "      <td>-1.220309</td>\n",
              "      <td>0.132049</td>\n",
              "      <td>-0.187190</td>\n",
              "      <td>-1.152096</td>\n",
              "      <td>0.454156</td>\n",
              "      <td>0.557448</td>\n",
              "      <td>-0.402943</td>\n",
              "      <td>-1.225979</td>\n",
              "      <td>0.127531</td>\n",
              "      <td>0.069677</td>\n",
              "      <td>-0.021814</td>\n",
              "      <td>-0.184043</td>\n",
              "      <td>0.218814</td>\n",
              "      <td>0.811766</td>\n",
              "      <td>0.761276</td>\n",
              "      <td>-1.585932</td>\n",
              "      <td>-1.304321</td>\n",
              "      <td>0.297011</td>\n",
              "      <td>0.376011</td>\n",
              "      <td>0.308563</td>\n",
              "      <td>0.393351</td>\n",
              "      <td>-0.975130</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.994814</td>\n",
              "      <td>0.706768</td>\n",
              "      <td>0.388398</td>\n",
              "      <td>0.682229</td>\n",
              "      <td>0.553078</td>\n",
              "      <td>0.006548</td>\n",
              "      <td>0.635607</td>\n",
              "      <td>-0.107498</td>\n",
              "      <td>0.240612</td>\n",
              "      <td>0.177998</td>\n",
              "      <td>0.529921</td>\n",
              "      <td>-1.271759</td>\n",
              "      <td>0.508364</td>\n",
              "      <td>-0.844720</td>\n",
              "      <td>0.723552</td>\n",
              "      <td>1.203746</td>\n",
              "      <td>-0.178693</td>\n",
              "      <td>-1.185009</td>\n",
              "      <td>-1.225258</td>\n",
              "      <td>-0.080642</td>\n",
              "      <td>1.270459</td>\n",
              "      <td>0.549151</td>\n",
              "      <td>-0.083043</td>\n",
              "      <td>0.940601</td>\n",
              "      <td>-0.525292</td>\n",
              "      <td>-0.501228</td>\n",
              "      <td>1.030271</td>\n",
              "      <td>-0.824522</td>\n",
              "      <td>1.334434</td>\n",
              "      <td>-1.053619</td>\n",
              "      <td>-0.267697</td>\n",
              "      <td>-1.669339</td>\n",
              "      <td>-0.214436</td>\n",
              "      <td>0.967534</td>\n",
              "      <td>0.748140</td>\n",
              "      <td>0.714027</td>\n",
              "      <td>-0.638480</td>\n",
              "      <td>-0.124279</td>\n",
              "      <td>0.366332</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.418800</td>\n",
              "      <td>-0.323658</td>\n",
              "      <td>-0.353201</td>\n",
              "      <td>0.025223</td>\n",
              "      <td>0.577787</td>\n",
              "      <td>-0.317023</td>\n",
              "      <td>-0.554338</td>\n",
              "      <td>0.847408</td>\n",
              "      <td>-0.778099</td>\n",
              "      <td>0.850690</td>\n",
              "      <td>1.304112</td>\n",
              "      <td>-0.434819</td>\n",
              "      <td>-0.771734</td>\n",
              "      <td>-0.899485</td>\n",
              "      <td>-1.215115</td>\n",
              "      <td>-0.775774</td>\n",
              "      <td>-0.647416</td>\n",
              "      <td>-0.542243</td>\n",
              "      <td>-0.782168</td>\n",
              "      <td>1.324322</td>\n",
              "      <td>0.349843</td>\n",
              "      <td>-0.937292</td>\n",
              "      <td>0.429282</td>\n",
              "      <td>-1.328041</td>\n",
              "      <td>-1.736635</td>\n",
              "      <td>-1.935650</td>\n",
              "      <td>-0.518941</td>\n",
              "      <td>1.616143</td>\n",
              "      <td>1.371421</td>\n",
              "      <td>-0.277386</td>\n",
              "      <td>-0.162207</td>\n",
              "      <td>-0.983132</td>\n",
              "      <td>-0.485843</td>\n",
              "      <td>-0.145070</td>\n",
              "      <td>0.565974</td>\n",
              "      <td>0.353951</td>\n",
              "      <td>0.936010</td>\n",
              "      <td>0.721208</td>\n",
              "      <td>-0.076706</td>\n",
              "      <td>-1.273901</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.363823</td>\n",
              "      <td>1.495660</td>\n",
              "      <td>0.103310</td>\n",
              "      <td>-1.316497</td>\n",
              "      <td>0.356881</td>\n",
              "      <td>0.567666</td>\n",
              "      <td>-0.200409</td>\n",
              "      <td>-0.110373</td>\n",
              "      <td>-0.781076</td>\n",
              "      <td>0.307884</td>\n",
              "      <td>-0.069431</td>\n",
              "      <td>-0.132790</td>\n",
              "      <td>-1.144245</td>\n",
              "      <td>0.333528</td>\n",
              "      <td>0.523856</td>\n",
              "      <td>0.867204</td>\n",
              "      <td>-0.016039</td>\n",
              "      <td>-1.615564</td>\n",
              "      <td>-1.395351</td>\n",
              "      <td>0.766038</td>\n",
              "      <td>1.031234</td>\n",
              "      <td>1.919342</td>\n",
              "      <td>-0.098541</td>\n",
              "      <td>-0.379554</td>\n",
              "      <td>0.981259</td>\n",
              "      <td>-0.855776</td>\n",
              "      <td>0.859527</td>\n",
              "      <td>0.458385</td>\n",
              "      <td>0.499763</td>\n",
              "      <td>0.061307</td>\n",
              "      <td>-0.604850</td>\n",
              "      <td>-1.219235</td>\n",
              "      <td>-0.852428</td>\n",
              "      <td>0.065624</td>\n",
              "      <td>1.236948</td>\n",
              "      <td>0.441680</td>\n",
              "      <td>-0.749305</td>\n",
              "      <td>-0.381263</td>\n",
              "      <td>-0.214752</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.892403</td>\n",
              "      <td>-1.174784</td>\n",
              "      <td>1.358834</td>\n",
              "      <td>0.672741</td>\n",
              "      <td>-0.919037</td>\n",
              "      <td>0.451660</td>\n",
              "      <td>1.270008</td>\n",
              "      <td>0.863308</td>\n",
              "      <td>0.221963</td>\n",
              "      <td>0.234472</td>\n",
              "      <td>0.736504</td>\n",
              "      <td>-0.086315</td>\n",
              "      <td>-1.201007</td>\n",
              "      <td>0.158627</td>\n",
              "      <td>-1.076775</td>\n",
              "      <td>1.030661</td>\n",
              "      <td>-0.773292</td>\n",
              "      <td>0.787029</td>\n",
              "      <td>-0.436171</td>\n",
              "      <td>-1.774708</td>\n",
              "      <td>1.092351</td>\n",
              "      <td>2.226049</td>\n",
              "      <td>-0.264831</td>\n",
              "      <td>0.176730</td>\n",
              "      <td>-3.456142</td>\n",
              "      <td>-1.355500</td>\n",
              "      <td>0.387206</td>\n",
              "      <td>0.151994</td>\n",
              "      <td>-2.672254</td>\n",
              "      <td>0.742959</td>\n",
              "      <td>-0.655962</td>\n",
              "      <td>-0.843967</td>\n",
              "      <td>-1.155222</td>\n",
              "      <td>0.157424</td>\n",
              "      <td>-0.010366</td>\n",
              "      <td>1.309886</td>\n",
              "      <td>-0.681162</td>\n",
              "      <td>2.305023</td>\n",
              "      <td>-0.637423</td>\n",
              "      <td>0.178157</td>\n",
              "      <td>...</td>\n",
              "      <td>0.439072</td>\n",
              "      <td>-1.782575</td>\n",
              "      <td>-1.465596</td>\n",
              "      <td>2.058444</td>\n",
              "      <td>-0.182285</td>\n",
              "      <td>-0.186349</td>\n",
              "      <td>0.868791</td>\n",
              "      <td>-0.181700</td>\n",
              "      <td>-0.072199</td>\n",
              "      <td>-0.986495</td>\n",
              "      <td>0.999168</td>\n",
              "      <td>-0.285254</td>\n",
              "      <td>0.090523</td>\n",
              "      <td>-1.240313</td>\n",
              "      <td>-1.137443</td>\n",
              "      <td>-0.208913</td>\n",
              "      <td>-1.043211</td>\n",
              "      <td>0.586708</td>\n",
              "      <td>-0.749179</td>\n",
              "      <td>-0.122272</td>\n",
              "      <td>-1.871484</td>\n",
              "      <td>0.067748</td>\n",
              "      <td>0.122957</td>\n",
              "      <td>0.033766</td>\n",
              "      <td>-1.158714</td>\n",
              "      <td>0.466875</td>\n",
              "      <td>-0.003578</td>\n",
              "      <td>-0.511014</td>\n",
              "      <td>0.396934</td>\n",
              "      <td>0.508805</td>\n",
              "      <td>-0.936099</td>\n",
              "      <td>-1.209839</td>\n",
              "      <td>0.131328</td>\n",
              "      <td>0.776226</td>\n",
              "      <td>-0.344520</td>\n",
              "      <td>0.186719</td>\n",
              "      <td>0.894092</td>\n",
              "      <td>-0.298576</td>\n",
              "      <td>-0.288496</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 257 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3    ...       253       254       255  256\n",
              "0  1.240568 -0.561261 -1.473960 -0.404962  ... -1.295807 -0.405288  0.462285  0.0\n",
              "1 -1.996075  0.650010 -0.192071 -0.454621  ...  1.876634  1.189979  1.602775  0.0\n",
              "2  0.171825  0.165917 -0.203169  0.767052  ... -0.638480 -0.124279  0.366332  0.0\n",
              "3  0.418800 -0.323658 -0.353201  0.025223  ... -0.749305 -0.381263 -0.214752  0.0\n",
              "4  0.892403 -1.174784  1.358834  0.672741  ...  0.894092 -0.298576 -0.288496  0.0\n",
              "\n",
              "[5 rows x 257 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hK_pLQ6inet"
      },
      "source": [
        "data.to_csv('./'+save_model_path+ '/ResNet-VAE_Latent_dataset.csv', index=False)\n",
        "df = pd.read_csv('./'+save_model_path+ '/ResNet-VAE_Latent_dataset.csv')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CFtZi0oingA",
        "outputId": "1e21d768-fd98-42eb-c386-11bed3c47c07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.240568</td>\n",
              "      <td>-0.561261</td>\n",
              "      <td>-1.473960</td>\n",
              "      <td>-0.404962</td>\n",
              "      <td>0.245697</td>\n",
              "      <td>1.038918</td>\n",
              "      <td>1.497949</td>\n",
              "      <td>-0.449939</td>\n",
              "      <td>1.170961</td>\n",
              "      <td>-0.546789</td>\n",
              "      <td>-0.780814</td>\n",
              "      <td>1.361100</td>\n",
              "      <td>-1.116053</td>\n",
              "      <td>1.910452</td>\n",
              "      <td>1.412538</td>\n",
              "      <td>0.478924</td>\n",
              "      <td>0.373539</td>\n",
              "      <td>-2.539976</td>\n",
              "      <td>1.891036</td>\n",
              "      <td>0.528984</td>\n",
              "      <td>-1.579915</td>\n",
              "      <td>1.082758</td>\n",
              "      <td>0.560673</td>\n",
              "      <td>0.480609</td>\n",
              "      <td>0.039558</td>\n",
              "      <td>0.151825</td>\n",
              "      <td>-0.786566</td>\n",
              "      <td>-0.029136</td>\n",
              "      <td>1.107318</td>\n",
              "      <td>0.188695</td>\n",
              "      <td>0.495078</td>\n",
              "      <td>-2.756053</td>\n",
              "      <td>-0.256838</td>\n",
              "      <td>-1.180869</td>\n",
              "      <td>-0.122568</td>\n",
              "      <td>-0.018253</td>\n",
              "      <td>-1.009968</td>\n",
              "      <td>0.068781</td>\n",
              "      <td>0.692828</td>\n",
              "      <td>0.367560</td>\n",
              "      <td>...</td>\n",
              "      <td>0.567895</td>\n",
              "      <td>-1.819931</td>\n",
              "      <td>0.934984</td>\n",
              "      <td>-1.330302</td>\n",
              "      <td>-0.783299</td>\n",
              "      <td>0.377807</td>\n",
              "      <td>0.361624</td>\n",
              "      <td>1.070406</td>\n",
              "      <td>1.513979</td>\n",
              "      <td>1.717499</td>\n",
              "      <td>-1.591013</td>\n",
              "      <td>0.140981</td>\n",
              "      <td>0.141379</td>\n",
              "      <td>1.136888</td>\n",
              "      <td>1.406761</td>\n",
              "      <td>-0.208655</td>\n",
              "      <td>1.209202</td>\n",
              "      <td>2.338400</td>\n",
              "      <td>2.392874</td>\n",
              "      <td>2.012412</td>\n",
              "      <td>-0.083929</td>\n",
              "      <td>-0.493434</td>\n",
              "      <td>1.134609</td>\n",
              "      <td>1.298404</td>\n",
              "      <td>0.937509</td>\n",
              "      <td>-1.001865</td>\n",
              "      <td>1.713861</td>\n",
              "      <td>0.247447</td>\n",
              "      <td>0.213539</td>\n",
              "      <td>-0.402759</td>\n",
              "      <td>-0.426054</td>\n",
              "      <td>0.811708</td>\n",
              "      <td>1.755919</td>\n",
              "      <td>-1.964023</td>\n",
              "      <td>0.227610</td>\n",
              "      <td>-0.768265</td>\n",
              "      <td>-1.295807</td>\n",
              "      <td>-0.405288</td>\n",
              "      <td>0.462285</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.996075</td>\n",
              "      <td>0.650010</td>\n",
              "      <td>-0.192071</td>\n",
              "      <td>-0.454621</td>\n",
              "      <td>-1.276196</td>\n",
              "      <td>-0.767499</td>\n",
              "      <td>-0.760912</td>\n",
              "      <td>0.700617</td>\n",
              "      <td>-0.618626</td>\n",
              "      <td>-0.309001</td>\n",
              "      <td>0.553122</td>\n",
              "      <td>-0.545304</td>\n",
              "      <td>-0.615293</td>\n",
              "      <td>-0.224021</td>\n",
              "      <td>-0.132841</td>\n",
              "      <td>2.032287</td>\n",
              "      <td>-0.279049</td>\n",
              "      <td>-0.337486</td>\n",
              "      <td>-0.510281</td>\n",
              "      <td>0.435519</td>\n",
              "      <td>-0.275984</td>\n",
              "      <td>0.629615</td>\n",
              "      <td>-1.974528</td>\n",
              "      <td>-0.119307</td>\n",
              "      <td>0.051937</td>\n",
              "      <td>1.114540</td>\n",
              "      <td>0.164383</td>\n",
              "      <td>0.793738</td>\n",
              "      <td>0.474289</td>\n",
              "      <td>0.513670</td>\n",
              "      <td>-0.203033</td>\n",
              "      <td>0.933196</td>\n",
              "      <td>0.038842</td>\n",
              "      <td>-0.398890</td>\n",
              "      <td>-1.007496</td>\n",
              "      <td>-2.026744</td>\n",
              "      <td>-0.937173</td>\n",
              "      <td>-0.139504</td>\n",
              "      <td>-2.569224</td>\n",
              "      <td>1.689294</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.106298</td>\n",
              "      <td>-1.845786</td>\n",
              "      <td>1.896894</td>\n",
              "      <td>0.181780</td>\n",
              "      <td>-0.117664</td>\n",
              "      <td>0.027586</td>\n",
              "      <td>1.873160</td>\n",
              "      <td>0.400704</td>\n",
              "      <td>0.385745</td>\n",
              "      <td>1.251343</td>\n",
              "      <td>0.150390</td>\n",
              "      <td>0.823379</td>\n",
              "      <td>1.523866</td>\n",
              "      <td>0.766172</td>\n",
              "      <td>-0.780572</td>\n",
              "      <td>0.003207</td>\n",
              "      <td>-0.167644</td>\n",
              "      <td>-1.065065</td>\n",
              "      <td>1.136096</td>\n",
              "      <td>-1.246378</td>\n",
              "      <td>-0.335723</td>\n",
              "      <td>-0.483589</td>\n",
              "      <td>1.081218</td>\n",
              "      <td>-0.601342</td>\n",
              "      <td>0.948993</td>\n",
              "      <td>-0.955097</td>\n",
              "      <td>0.871614</td>\n",
              "      <td>-0.092773</td>\n",
              "      <td>0.880835</td>\n",
              "      <td>-1.132668</td>\n",
              "      <td>2.680984</td>\n",
              "      <td>2.019951</td>\n",
              "      <td>-1.422524</td>\n",
              "      <td>1.331898</td>\n",
              "      <td>-1.140199</td>\n",
              "      <td>-0.439111</td>\n",
              "      <td>1.876634</td>\n",
              "      <td>1.189979</td>\n",
              "      <td>1.602775</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.171825</td>\n",
              "      <td>0.165917</td>\n",
              "      <td>-0.203169</td>\n",
              "      <td>0.767052</td>\n",
              "      <td>0.591684</td>\n",
              "      <td>-1.322803</td>\n",
              "      <td>-0.564557</td>\n",
              "      <td>-1.029487</td>\n",
              "      <td>1.375214</td>\n",
              "      <td>0.408249</td>\n",
              "      <td>0.619360</td>\n",
              "      <td>1.183983</td>\n",
              "      <td>-0.888452</td>\n",
              "      <td>1.377965</td>\n",
              "      <td>0.774443</td>\n",
              "      <td>-0.445341</td>\n",
              "      <td>-1.066941</td>\n",
              "      <td>-0.125578</td>\n",
              "      <td>-1.220309</td>\n",
              "      <td>0.132049</td>\n",
              "      <td>-0.187190</td>\n",
              "      <td>-1.152096</td>\n",
              "      <td>0.454156</td>\n",
              "      <td>0.557448</td>\n",
              "      <td>-0.402943</td>\n",
              "      <td>-1.225979</td>\n",
              "      <td>0.127531</td>\n",
              "      <td>0.069677</td>\n",
              "      <td>-0.021814</td>\n",
              "      <td>-0.184043</td>\n",
              "      <td>0.218814</td>\n",
              "      <td>0.811766</td>\n",
              "      <td>0.761276</td>\n",
              "      <td>-1.585932</td>\n",
              "      <td>-1.304321</td>\n",
              "      <td>0.297011</td>\n",
              "      <td>0.376011</td>\n",
              "      <td>0.308563</td>\n",
              "      <td>0.393351</td>\n",
              "      <td>-0.975130</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.994814</td>\n",
              "      <td>0.706768</td>\n",
              "      <td>0.388398</td>\n",
              "      <td>0.682229</td>\n",
              "      <td>0.553078</td>\n",
              "      <td>0.006548</td>\n",
              "      <td>0.635607</td>\n",
              "      <td>-0.107498</td>\n",
              "      <td>0.240612</td>\n",
              "      <td>0.177998</td>\n",
              "      <td>0.529921</td>\n",
              "      <td>-1.271759</td>\n",
              "      <td>0.508364</td>\n",
              "      <td>-0.844720</td>\n",
              "      <td>0.723552</td>\n",
              "      <td>1.203746</td>\n",
              "      <td>-0.178693</td>\n",
              "      <td>-1.185009</td>\n",
              "      <td>-1.225258</td>\n",
              "      <td>-0.080642</td>\n",
              "      <td>1.270459</td>\n",
              "      <td>0.549151</td>\n",
              "      <td>-0.083043</td>\n",
              "      <td>0.940601</td>\n",
              "      <td>-0.525292</td>\n",
              "      <td>-0.501228</td>\n",
              "      <td>1.030271</td>\n",
              "      <td>-0.824522</td>\n",
              "      <td>1.334434</td>\n",
              "      <td>-1.053619</td>\n",
              "      <td>-0.267697</td>\n",
              "      <td>-1.669339</td>\n",
              "      <td>-0.214436</td>\n",
              "      <td>0.967534</td>\n",
              "      <td>0.748140</td>\n",
              "      <td>0.714027</td>\n",
              "      <td>-0.638480</td>\n",
              "      <td>-0.124279</td>\n",
              "      <td>0.366332</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.418800</td>\n",
              "      <td>-0.323658</td>\n",
              "      <td>-0.353201</td>\n",
              "      <td>0.025223</td>\n",
              "      <td>0.577787</td>\n",
              "      <td>-0.317023</td>\n",
              "      <td>-0.554338</td>\n",
              "      <td>0.847408</td>\n",
              "      <td>-0.778099</td>\n",
              "      <td>0.850690</td>\n",
              "      <td>1.304112</td>\n",
              "      <td>-0.434819</td>\n",
              "      <td>-0.771734</td>\n",
              "      <td>-0.899485</td>\n",
              "      <td>-1.215115</td>\n",
              "      <td>-0.775774</td>\n",
              "      <td>-0.647416</td>\n",
              "      <td>-0.542243</td>\n",
              "      <td>-0.782168</td>\n",
              "      <td>1.324322</td>\n",
              "      <td>0.349843</td>\n",
              "      <td>-0.937292</td>\n",
              "      <td>0.429282</td>\n",
              "      <td>-1.328041</td>\n",
              "      <td>-1.736635</td>\n",
              "      <td>-1.935650</td>\n",
              "      <td>-0.518941</td>\n",
              "      <td>1.616143</td>\n",
              "      <td>1.371421</td>\n",
              "      <td>-0.277386</td>\n",
              "      <td>-0.162207</td>\n",
              "      <td>-0.983132</td>\n",
              "      <td>-0.485843</td>\n",
              "      <td>-0.145070</td>\n",
              "      <td>0.565974</td>\n",
              "      <td>0.353951</td>\n",
              "      <td>0.936010</td>\n",
              "      <td>0.721208</td>\n",
              "      <td>-0.076706</td>\n",
              "      <td>-1.273901</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.363823</td>\n",
              "      <td>1.495660</td>\n",
              "      <td>0.103310</td>\n",
              "      <td>-1.316497</td>\n",
              "      <td>0.356881</td>\n",
              "      <td>0.567666</td>\n",
              "      <td>-0.200409</td>\n",
              "      <td>-0.110373</td>\n",
              "      <td>-0.781076</td>\n",
              "      <td>0.307884</td>\n",
              "      <td>-0.069431</td>\n",
              "      <td>-0.132790</td>\n",
              "      <td>-1.144245</td>\n",
              "      <td>0.333528</td>\n",
              "      <td>0.523856</td>\n",
              "      <td>0.867204</td>\n",
              "      <td>-0.016039</td>\n",
              "      <td>-1.615564</td>\n",
              "      <td>-1.395351</td>\n",
              "      <td>0.766038</td>\n",
              "      <td>1.031234</td>\n",
              "      <td>1.919342</td>\n",
              "      <td>-0.098541</td>\n",
              "      <td>-0.379554</td>\n",
              "      <td>0.981259</td>\n",
              "      <td>-0.855776</td>\n",
              "      <td>0.859527</td>\n",
              "      <td>0.458385</td>\n",
              "      <td>0.499763</td>\n",
              "      <td>0.061307</td>\n",
              "      <td>-0.604850</td>\n",
              "      <td>-1.219235</td>\n",
              "      <td>-0.852428</td>\n",
              "      <td>0.065624</td>\n",
              "      <td>1.236948</td>\n",
              "      <td>0.441680</td>\n",
              "      <td>-0.749305</td>\n",
              "      <td>-0.381263</td>\n",
              "      <td>-0.214752</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.892403</td>\n",
              "      <td>-1.174784</td>\n",
              "      <td>1.358834</td>\n",
              "      <td>0.672741</td>\n",
              "      <td>-0.919037</td>\n",
              "      <td>0.451660</td>\n",
              "      <td>1.270008</td>\n",
              "      <td>0.863308</td>\n",
              "      <td>0.221963</td>\n",
              "      <td>0.234472</td>\n",
              "      <td>0.736504</td>\n",
              "      <td>-0.086315</td>\n",
              "      <td>-1.201007</td>\n",
              "      <td>0.158627</td>\n",
              "      <td>-1.076775</td>\n",
              "      <td>1.030661</td>\n",
              "      <td>-0.773292</td>\n",
              "      <td>0.787029</td>\n",
              "      <td>-0.436171</td>\n",
              "      <td>-1.774708</td>\n",
              "      <td>1.092351</td>\n",
              "      <td>2.226049</td>\n",
              "      <td>-0.264831</td>\n",
              "      <td>0.176730</td>\n",
              "      <td>-3.456142</td>\n",
              "      <td>-1.355500</td>\n",
              "      <td>0.387206</td>\n",
              "      <td>0.151994</td>\n",
              "      <td>-2.672254</td>\n",
              "      <td>0.742959</td>\n",
              "      <td>-0.655962</td>\n",
              "      <td>-0.843967</td>\n",
              "      <td>-1.155222</td>\n",
              "      <td>0.157424</td>\n",
              "      <td>-0.010366</td>\n",
              "      <td>1.309886</td>\n",
              "      <td>-0.681162</td>\n",
              "      <td>2.305023</td>\n",
              "      <td>-0.637423</td>\n",
              "      <td>0.178157</td>\n",
              "      <td>...</td>\n",
              "      <td>0.439072</td>\n",
              "      <td>-1.782575</td>\n",
              "      <td>-1.465596</td>\n",
              "      <td>2.058444</td>\n",
              "      <td>-0.182285</td>\n",
              "      <td>-0.186349</td>\n",
              "      <td>0.868791</td>\n",
              "      <td>-0.181700</td>\n",
              "      <td>-0.072199</td>\n",
              "      <td>-0.986495</td>\n",
              "      <td>0.999168</td>\n",
              "      <td>-0.285254</td>\n",
              "      <td>0.090523</td>\n",
              "      <td>-1.240313</td>\n",
              "      <td>-1.137443</td>\n",
              "      <td>-0.208913</td>\n",
              "      <td>-1.043211</td>\n",
              "      <td>0.586708</td>\n",
              "      <td>-0.749179</td>\n",
              "      <td>-0.122272</td>\n",
              "      <td>-1.871484</td>\n",
              "      <td>0.067748</td>\n",
              "      <td>0.122957</td>\n",
              "      <td>0.033766</td>\n",
              "      <td>-1.158714</td>\n",
              "      <td>0.466875</td>\n",
              "      <td>-0.003578</td>\n",
              "      <td>-0.511014</td>\n",
              "      <td>0.396934</td>\n",
              "      <td>0.508805</td>\n",
              "      <td>-0.936099</td>\n",
              "      <td>-1.209839</td>\n",
              "      <td>0.131328</td>\n",
              "      <td>0.776226</td>\n",
              "      <td>-0.344520</td>\n",
              "      <td>0.186719</td>\n",
              "      <td>0.894092</td>\n",
              "      <td>-0.298576</td>\n",
              "      <td>-0.288496</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 257 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3  ...       253       254       255  256\n",
              "0  1.240568 -0.561261 -1.473960 -0.404962  ... -1.295807 -0.405288  0.462285  0.0\n",
              "1 -1.996075  0.650010 -0.192071 -0.454621  ...  1.876634  1.189979  1.602775  0.0\n",
              "2  0.171825  0.165917 -0.203169  0.767052  ... -0.638480 -0.124279  0.366332  0.0\n",
              "3  0.418800 -0.323658 -0.353201  0.025223  ... -0.749305 -0.381263 -0.214752  0.0\n",
              "4  0.892403 -1.174784  1.358834  0.672741  ...  0.894092 -0.298576 -0.288496  0.0\n",
              "\n",
              "[5 rows x 257 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mewfnzPWk2Kc"
      },
      "source": [
        "## Ensemble Classifier\n",
        "### 01. Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erIHB_z5iniZ",
        "outputId": "a151e735-07c3-44c7-f222-744aa2479747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Test-Voting classifier\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# iris = datasets.load_iris()\n",
        "data = latent_vector  # re-change it to numpy\n",
        "X, y = data[:, :256], data[:, 256]\n",
        "\n",
        "clf1 = LogisticRegression(random_state=1)\n",
        "clf2 = RandomForestClassifier(n_estimators=100, random_state=1)\n",
        "clf3 = GaussianNB()\n",
        "\n",
        "eclf = VotingClassifier(\n",
        "         estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
        "         voting='hard')\n",
        "\n",
        "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
        "     scores = cross_val_score(clf, X, y, scoring='accuracy', cv=10)  # cross validation = 10\n",
        "     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.31 (+/- 0.01) [Logistic Regression]\n",
            "Accuracy: 0.62 (+/- 0.03) [Random Forest]\n",
            "Accuracy: 0.44 (+/- 0.01) [naive Bayes]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.51 (+/- 0.02) [Ensemble]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4hEhLKvinkn",
        "outputId": "03cff518-c614-4d7f-dd06-c2d4d1d12d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"Done!\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-QOR5c6innj",
        "outputId": "0713412a-71ba-4210-824c-eca12e76473e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Test-Voting classifier\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# iris = datasets.load_iris()\n",
        "data = latent_vector  # re-change it to numpy\n",
        "X, y = data[:, :256], data[:, 256]\n",
        "print('='*20,'Shape of X and y', '='*20)\n",
        "print(X.shape, y.shape)\n",
        "print('='*50)\n",
        "\n",
        "\n",
        "clf1 = LogisticRegression(random_state=1)\n",
        "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
        "clf3 = GaussianNB()\n",
        "\n",
        "eclf = VotingClassifier(\n",
        "         estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
        "         voting='hard')\n",
        "\n",
        "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
        "     scores = cross_val_score(clf, X, y, scoring='accuracy', cv=10)  # cross validation = 10\n",
        "     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
        "print(\"Done!!\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================== Shape of X and y ====================\n",
            "(9339, 256) (9339,)\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.31 (+/- 0.01) [Logistic Regression]\n",
            "Accuracy: 0.61 (+/- 0.02) [Random Forest]\n",
            "Accuracy: 0.44 (+/- 0.01) [naive Bayes]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.51 (+/- 0.02) [Ensemble]\n",
            "Done!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uFS5y_4oIVT"
      },
      "source": [
        "### Voting Classifier3\n",
        "* 5 RandomForest\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFT3pQ5Hinpe",
        "outputId": "8ff5b615-8974-4f82-a9d7-a409bdc3ac73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# Test-Voting classifier\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# iris = datasets.load_iris()\n",
        "data = latent_vector  # re-change it to numpy\n",
        "X, y = data[:, :256], data[:, 256]\n",
        "print('='*20,'Shape of X and y', '='*20)\n",
        "print(X.shape, y.shape)\n",
        "print('='*50)\n",
        "\n",
        "\n",
        "clf1 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
        "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
        "clf3 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
        "clf4 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
        "clf5 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
        "\n",
        "eclf = VotingClassifier(\n",
        "         estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
        "         voting='hard')\n",
        "\n",
        "for clf, label in zip([clf1, clf2, clf3, clf4, clf5,  eclf], ['Random Forest', 'Random Forest', 'Random Forest', 'Random Forest', 'Random Forest', 'Ensemble']):\n",
        "     scores = cross_val_score(clf, X, y, scoring='accuracy', cv=10)  # cross validation = 10\n",
        "     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
        "print(\"Done!!\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================== Shape of X and y ====================\n",
            "(9339, 256) (9339,)\n",
            "==================================================\n",
            "Accuracy: 0.61 (+/- 0.02) [Random Forest]\n",
            "Accuracy: 0.61 (+/- 0.02) [Random Forest]\n",
            "Accuracy: 0.61 (+/- 0.02) [Random Forest]\n",
            "Accuracy: 0.61 (+/- 0.02) [Random Forest]\n",
            "Accuracy: 0.61 (+/- 0.02) [Random Forest]\n",
            "Accuracy: 0.61 (+/- 0.02) [Ensemble]\n",
            "Done!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cJivB-FS0v2"
      },
      "source": [
        "üìå ÏµúÍ∑ºÏóê RefactoringÌïú ÏΩîÎìúÎ•º Ï∞∏Í≥†Ìï¥ÏÑú Î≥∏ Ïó∞Íµ¨Ïùò Ïã§Ìóò ÏΩîÎìúÎèÑ Refactoring Ìï¥Î≥¥Ïûê..! <code>-20.09.28.Mon am 01:00</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N7aJlnVplo-"
      },
      "source": [
        "### Voting Classifier4\n",
        "* üë®‚Äçüè´ 1.11.6.4. Using the VotingClassifier with GridSearchCV\n",
        "   * GridSearchÎ•º ÌÜµÌï¥ ÏµúÏ†ÅÏùò ÌååÎùºÎØ∏ÌÑ∞Î•º Ï∞æÏïÑÎ≥¥Ïûê\n",
        "   * sklearn Documentation Ï∞∏Í≥†"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js3KHAWwq3Ir"
      },
      "source": [
        "Voting Classifier can also be used together with GridSearchCV in order to tune the hyperparameters of the individual estimators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP2rrKPeppv6",
        "outputId": "33c3b2ad-ed5d-49c2-a6b5-714966eef083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# iris = datasets.load_iris()\n",
        "data = latent_vector  # re-change it to numpy\n",
        "X, y = data[:, :256], data[:, 256]\n",
        "print('='*20,'Shape of X and y', '='*20)\n",
        "print(X.shape, y.shape)\n",
        "print('='*50)\n",
        "\n",
        "\n",
        "clf1 = LogisticRegression(random_state=1)\n",
        "clf2 = RandomForestClassifier(random_state=7)\n",
        "clf3 = GaussianNB()\n",
        "eclf = VotingClassifier(\n",
        "    estimators = [('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}\n",
        "\n",
        "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=10)\n",
        "grid = grid.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================== Shape of X and y ====================\n",
            "(9339, 256) (9339,)\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMDvIJYLsSbU"
      },
      "source": [
        "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
        "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=10)\n",
        "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKujaslRybr1"
      },
      "source": [
        "ÏïÑÎûòÏùò train codeÎäî ResNet-VAE train CodeÏù¥Îã§<br>\n",
        "üìåüìå => Encoder NetworkÏùò Train CodeÎ°ú Î≥ÄÍ≤ΩÏãúÏºúÎ≥¥Ïûê...!<br>\n",
        "* Ï∞∏Í≥†ÌïòÍ∏∞\n",
        "    * Reference: https://github.com/Steve-YJ/Assignment_Standalone_DL/blob/master/%5BRe_Fact%5D%20CIFAR-10_CNN_Report_Result.ipynb\n",
        "\n",
        "* Encoder NetworkÏóêÏÑú <code>muÏôÄ logvar</code>ÏùÑ Ïûò ÌôúÏö©Ìï¥Î≥¥Ïûê -20.09.28.mon-\n",
        "    * ResNet-152 -> FC1 -> FC2 -> mu or logvar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSTYVavF0Af8"
      },
      "source": [
        "## Original Train Code - for ResNet-VAE Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai8Y_drVQGJS"
      },
      "source": [
        "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
        "    # set model as training mode\n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "    all_y, all_z, all_mu, all_logvar = [], [], [], []\n",
        "    N_count = 0   # counting total trained sample in one epoch\n",
        "    for batch_idx, (X, y) in enumerate(train_loader):\n",
        "        # distribute data to device\n",
        "        X, y = X.to(device), y.to(device).view(-1, )\n",
        "        N_count += X.size(0)  # count batch_size sample\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        X_reconst, z, mu, logvar = model(X)  # VAE\n",
        "        loss = loss_function(X_reconst, X, mu, logvar)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        all_y.extend(y.data.cpu().numpy())\n",
        "        all_z.extend(z.data.cpu().numpy())\n",
        "        all_mu.extend(mu.data.cpu().numpy())\n",
        "        all_logvar.extend(logvar.data.cpu().numpy())\n",
        "\n",
        "        # show information\n",
        "        if (batch_idx + 1) % log_interval == 0:  # if batch_size = 16 => 160\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "    # calculate train_loss\n",
        "    losses /= len(train_loader.dataset)\n",
        "    all_y = np.stack(all_y, axis=0)\n",
        "    all_z = np.stack(all_z, axis=0)\n",
        "    all_mu = np.stack(all_mu, axis=0)\n",
        "    all_logvar = np.stack(all_logvar, axis=0)\n",
        "\n",
        "    # save Pytorch models of best record\n",
        "    torch.save(model.state_dict(), os.path.join(save_model_path, 'model_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
        "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
        "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
        "\n",
        "    return X.data.cpu().numpy(), all_y, all_z, all_mu, all_logvar, losses\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRd5W1pw0F0c"
      },
      "source": [
        "## Train Function for EncoderNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFagIEQ50MrF"
      },
      "source": [
        "* <code>Refactoring</code> Train Function for EncoderNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ECiiksW0I7F"
      },
      "source": [
        "def train(net, trainloader, optimizer, criterion, args):\n",
        "    \n",
        "    \"\"\"\n",
        "    Ïù¥ÎØ∏ ÏúÑÏóêÏÑú Ï†ïÏùòÎ•º Ìï¥Ï§å\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
        "                                              batch_size=args.train_batch_size, \n",
        "                                              shuffle=True, num_workers=2)\n",
        "    \"\"\"\n",
        "\n",
        "    net.train()  # train mode\n",
        "    optimizer.zero_grad()  # optimizer Ï¥àÍ∏∞Ìôî\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # print('input dimension: ', inputs.shape)\n",
        "        # raise RuntimeError\n",
        "        # inputs = inputs.view(-1, 3072)\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net.encoder(inputs)\n",
        "        \n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "    return net, train_loss, train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDVTUgOH127o"
      },
      "source": [
        "def val(net, valloader, optimizer, criterion, args):\n",
        "    \n",
        "    \"\"\"\n",
        "    Ïù¥ÎØ∏ ÏúÑÏóêÏÑú Ï†ïÏùòÎ•º Ìï¥Ï§å\n",
        "    valloader = torch.utils.data.DataLoader(partition['train'], \n",
        "                                              batch_size=args.test_batch_size, \n",
        "                                              shuffle=True, num_workers=2)\n",
        "    \"\"\"\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(valloader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            # print('input dimension: ', inputs.shape)\n",
        "            # raise RuntimeError\n",
        "            # inputs = inputs.view(-1, 3072)\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5FQ0cwc22hj"
      },
      "source": [
        "def test(net, testloader, args):\n",
        "    \n",
        "    \"\"\"\n",
        "    Ïù¥ÎØ∏ ÏúÑÏóêÏÑú Ï†ïÏùòÎ•º Ìï¥Ï§å\n",
        "    testloader = torch.utils.data.DataLoader(partition['train'], \n",
        "                                              batch_size=args.test_batch_size, \n",
        "                                              shuffle=True, num_workers=2)\n",
        "    \"\"\"\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(testloader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            # print('input dimension: ', inputs.shape)\n",
        "            # raise RuntimeError\n",
        "            # inputs = inputs.view(-1, 3072)\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "    return test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2jF7Wpu3ZAb"
      },
      "source": [
        "def experiment(trainloader, valloader, testloader, args):\n",
        "\n",
        "    \"\"\"\n",
        "    net = CNN(model_code = args.model_code,\n",
        "              in_channels = args.in_channels,\n",
        "              out_dim = args.out_dim,\n",
        "              act = args.act,\n",
        "              use_bn = args.use_bn)\n",
        "    net.cuda()\n",
        "    \"\"\"\n",
        "    # Detect devices\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    print(use_cuda)\n",
        "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    \"\"\" \n",
        "    # ====== If you want to use pre-trained model ====== #\n",
        "    \"\"\"\n",
        "    pre_saved_model_path = './results_ResNet-VAE_Exp01'\n",
        "    epoch=90  # pre-trained ResNet-VAE parameter\n",
        "\n",
        "    # Create model\n",
        "    resnet_vae = ResNet_VAE(fc_hidden1=args.CNN_fc_hidden1, fc_hidden2=args.CNN_fc_hidden2, drop_p=args.dropout_p, CNN_embed_dim=args.CNN_embed_dim).to(device)\n",
        "    ### If you want to use pre-trained model ####\n",
        "    resnet_vae.load_state_dict(torch.load(os.path.join(pre_saved_model_path, 'model_epoch{}.pth'.format(epoch))))\n",
        "    \n",
        "    \"\"\"\n",
        "    # resnet_vaeÏóêÏÑú encodernetÏùÑ Ïñ¥ÎñªÍ≤å ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏùÑÍπå? -20.09.28.mon-\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "\n",
        "    model_params = list(resnet_vae.parameters())\n",
        "    optimizer = torch.optim.Adam(model_params, lr=args.learning_rate)\n",
        "    ### If you want to use pre-trained model's optimizer ####\n",
        "    optimizer.load_state_dict(torch.load(os.path.join(pre_saved_model_path, 'optimizer_epoch{}.pth'.format(epoch))))\n",
        "\n",
        "\n",
        "    print('Number of {} parameters'.format(sum(p.numel() for p in resnet_vae.parameters() if p.requires_grad)))\n",
        "    print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "    \n",
        "    # ===== List for epoch-wise data ====== #\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    # ================= Training ==================== #\n",
        "        \n",
        "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
        "        ts = time.time()\n",
        "        net, train_loss, train_acc = train(net, trainloader, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, valloader, criterion, args)\n",
        "        te = time.time()\n",
        "        \n",
        "        # ====== Add Epoch Data ====== #\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        # ============================ #\n",
        "        \n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "        \n",
        "    test_acc = test(net, testloader, args)    \n",
        "    \n",
        "    # ======= Add Result to Dictionary ======= #\n",
        "    # Ï†ÑÏ≤¥ Í≤∞Í≥ºÍ∞íÏùÑ Ï†ÄÏû•ÌïòÎäî Results Dic \n",
        "    # Q. Ïôú ÎïåÎ¨∏Ïù∏ÏßÑ Î™®Î•¥Í≤†ÏßÄÎßå train_accsÏôÄ train_accÍ∞Ä Î∂ÑÎ¶¨ÎêòÏñ¥ÏûàÎÑ§?!\n",
        "        # train_accÏôÄ val_accÏùÄ ÏóÜÏñ¥ÎèÑ ÎêòÎäîÍ±∞ ÏïÑÎÉê?\n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    # result['train_acc'] = train_acc\n",
        "    # result['val_acc'] = val_acc\n",
        "    result['test_acc'] = test_acc\n",
        "    return vars(args), result  # vars(args)Î•º Ìï¥Ï£ºÎ©¥ argsÎ•º dicÎ°ú Ï†ÄÏû•Ìï¥Ï§ÄÎã§\n",
        "    # ===================================== #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQeEjFenLKb0"
      },
      "source": [
        "# #05. Manage Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO_M_QL2LNos"
      },
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result):  # setting: args\n",
        "                                       # result : train_losses, val_losses, train_accs, val_accs,  \n",
        "    exp_name = setting['exp_name']\n",
        "    del setting['epoch']               # settingÏóêÏÑú 'epoch'Í≥º 'test_batch_size'Î•º Ï†úÍ±∞Ìï¥Ï§ÄÎã§Í≥†...\n",
        "    del setting['test_batch_size']     # Ïôú ÎñºÎäîÍ±∞ÏßÄ?\n",
        "\n",
        "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]  # settingÏóê Îî∞Îùº Îã§Î•∏ ÌååÏùºÎ™ÖÏùÑ Í∞ñÎèÑÎ°ù ÎßåÎì§Ïñ¥Ï§ÄÎã§\n",
        "    filename = './'+save_model_path+'/{}-{}.json'.format(exp_name, hash_key)\n",
        "    # print(filename)\n",
        "    result.update(setting)  # Í≤∞Í≥º dictionaryÏóê settingÍ∞íÏùÑ ÎçîÌï¥Ï§ÄÎã§ => Dic\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(result, f)  # JSON Ìè¨Î©ßÏúºÎ°ú dictionary Í∞í Ï†ÄÏû•\n",
        "\n",
        "    \n",
        "def load_exp_result(exp_name):\n",
        "    dir_path = './results'\n",
        "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]  # ./resutsÏóê Ï†ÄÏû•ÎêòÏñ¥ÏûàÎäî fileÎì§ÏùÑ Î¶¨Ïä§Ìä∏ ÌòïÌÉúÎ°ú Ï†ÄÏû•\n",
        "    list_result = []\n",
        "    for filename in filenames:\n",
        "        if exp_name in filename:\n",
        "            # print(exp_name)\n",
        "            with open(join(dir_path, filename), 'r') as infile:\n",
        "                results = json.load(infile)\n",
        "                list_result.append(results)  # DicÏùÑ listÌòïÌÉúÎ°ú Ï†ÄÏû•\n",
        "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSqc43qILONJ"
      },
      "source": [
        "# #06. Experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIuSI_v_4jly",
        "outputId": "204de890-9653-48d2-a167-e87e5119e7c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123  # ÎûúÎç§ ÏãúÎìúÍ∞í Ï¥àÍ∏∞Ìôî\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp1_lr_model_code\"\n",
        "\n",
        "# ====== Model ====== #\n",
        "# args.model_code ='VGG11'\n",
        "# args.in_channels = 3\n",
        "args.out_dim = 10\n",
        "# args.hid_dim = 100\n",
        "args.act = 'relu'\n",
        "args.CNN_fc_hidden1 = 1024\n",
        "args.CNN_fc_hidden2 = 1024\n",
        "args.CNN_embed_dim = 256  # latent dim extracted by 2D CNN\n",
        "args.res_size = 224       # ResNet Image size\n",
        "args.dropout_p = 0.2           # dropout probability\n",
        "\n",
        "\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "# args.l2 = 0.00001\n",
        "# args.use_bn = True\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.epochs = 20\n",
        "args.learning_rate = 1e-3\n",
        "\n",
        "args.batch_size = 50\n",
        "args.log_interval = 10  # interval for displaying training info\n",
        "\n",
        "print(args)\n",
        "\n",
        "\n",
        "\n",
        "# ===== Experiment Variable ====== #\n",
        "# Reference: https://github.com/Steve-YJ/Assignment_Standalone_DL/blob/master/%5BRe_Fact%5D%20CIFAR-10_CNN_Report_Result.ipynb\n",
        "\n",
        "name_var1 = 'lr'\n",
        "name_var2 = 'hiden_layer_num'\n",
        "list_var1 = [1e-3, 1e-4, 1e-5]\n",
        "list_var2 = [1024, 512, 256, 128, 64, 32]\n",
        "\n",
        "\n",
        "for var1 in list_var1:\n",
        "    for var2 in list_var2:\n",
        "        # setattr ??: name_var1('n_layer')Î•º 1, 2, 3ÏúºÎ°ú Î∞îÍøîÏ§ÄÎã§\n",
        "        # setattr = args.name_var1 = var1\n",
        "        setattr(args, name_var1, var1)\n",
        "        setattr(args, name_var2, var2)\n",
        "        print(args)\n",
        "                \n",
        "        setting, result = experiment(train_loader, val_loader, test_loader, args)\n",
        "        save_exp_result(setting, result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(CNN_embed_dim=256, CNN_fc_hidden1=1024, CNN_fc_hidden2=1024, act='relu', batch_size=50, dropout_p=0.2, epochs=20, exp_name='exp1_lr_model_code', learning_rate=0.001, log_interval=10, out_dim=10, res_size=224)\n",
            "Namespace(CNN_embed_dim=256, CNN_fc_hidden1=1024, CNN_fc_hidden2=1024, act='relu', batch_size=50, dropout_p=0.2, epochs=20, exp_name='exp1_lr_model_code', hiden_layer_num=1024, learning_rate=0.001, log_interval=10, lr=0.001, out_dim=10, res_size=224)\n",
            "True\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-bf95261e6ef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0msave_exp_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-5a88565c6513>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(trainloader, valloader, testloader, args)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mresnet_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_saved_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_epoch{}.pth'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# resnet_vaeÏóêÏÑú encodernetÏùÑ Ïñ¥ÎñªÍ≤å ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏùÑÍπå?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmodel_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: encode() missing 1 required positional argument: 'x'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T-bIQmv6pFz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeeIDGghLS3J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivli0f62LS84"
      },
      "source": [
        "# #07. Report Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5yifMWjLVtK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}